File: requirements.txt
# File: requirements.txt
pygame>=2.1.0
numpy>=1.20.0
torch>=1.10.0
torchvision>=0.11.0 # Added for potential NN architectures
cloudpickle>=2.0.0 # For state persistence
numba>=0.55.0 # For grid performance
mlflow>=1.20.0 # Added for experiment tracking and artifact storage
matplotlib>=3.5.0 # Added for advanced plotting
ray>=2.8.0 # Added for parallelization
pydantic>=2.0.0 # Added for data validation and schemas

File: README.md
# File: README.md
# AlphaTriangle Project

## Overview

AlphaTriangle is a project implementing an artificial intelligence agent based on AlphaZero principles to learn and play a custom puzzle game involving placing triangular shapes onto a grid. The agent learns through self-play reinforcement learning, guided by Monte Carlo Tree Search (MCTS) and a deep neural network (PyTorch).

The project includes:
*   A playable version of the triangle puzzle game using Pygame.
*   An implementation of the MCTS algorithm tailored for the game.
*   A deep neural network (policy and value heads) implemented in PyTorch.
*   A reinforcement learning pipeline coordinating **parallel self-play (using Ray)**, data storage, and network training.
*   Visualization tools for interactive play, debugging, and monitoring training progress (**with near real-time plot updates**).
*   Experiment tracking using MLflow.

## Core Technologies

*   **Python 3.10+**
*   **Pygame:** For game visualization and interactive modes.
*   **PyTorch:** For the deep learning model and training.
*   **NumPy:** For numerical operations, especially state representation.
*   **Ray:** For parallelizing self-play data generation and statistics collection across multiple CPU cores/processes.
*   **Numba:** (Optional, used in `features.grid_features`) For performance optimization of specific grid calculations.
*   **Cloudpickle:** For serializing the experience replay buffer and training checkpoints.
*   **MLflow:** For logging parameters, metrics, and artifacts (checkpoints, buffers) during training runs.

## Project Structure

```markdown
.
├── .alphatriangle_data/ # Root directory for ALL persistent data
│   ├── mlruns/          # MLflow tracking data (metrics, params, artifacts link here)
│   └── runs/            # Stores temporary/local artifacts per run
│       └── <run_name>/  # Data specific to a training run
│           ├── checkpoints/ # Saved model weights, optimizer state, etc. (.pkl)
│           ├── buffers/     # Saved experience replay buffers (.pkl)
│           ├── logs/        # Log files (if not solely using MLflow/stdout)
│           └── configs.json # Copy of run configuration
├── src/ # Source code for the project
│   ├── config/ # Configuration files (game rules, NN architecture, training params)
│   ├── data/ # Data management (saving/loading checkpoints, buffers)
│   ├── environment/ # Game environment logic (state, actions, rules - NO NN features)
│   ├── features/ # Feature extraction logic (GameState -> NN input)
│   ├── interaction/ # User input handling for interactive modes
│   ├── mcts/ # Monte Carlo Tree Search implementation
│   ├── nn/ # Neural Network definition and interface
│   ├── rl/ # Reinforcement Learning pipeline (orchestrator, trainer, self-play actors)
│   ├── stats/ # Statistics collection (Ray actor) and plotting
│   ├── structs/ # Core data structures (Triangle, Shape) to avoid circular imports
│   ├── utils/ # Common utilities (types, helpers, geometry)
│   └── visualization/ # Pygame-based visualization components
├── requirements.txt # Python package dependencies
├── run_interactive.py # Script to run the game in interactive (play/debug) modes
├── run_training_headless.py # Script to run training without visualization (logs to MLflow)
├── run_training_visual.py # Script to run training with live visualization (logs to MLflow)
└── README.md # This file
```

## Key Modules (`src`)

*   **`config`:** Centralized configuration classes. `PersistenceConfig` defines the unified `.alphatriangle_data` structure.
*   **`structs`:** Defines core, low-level data structures (`Triangle`, `Shape`) and constants (`SHAPE_COLORS`).
*   **`environment`:** Defines the game rules, `GameState`, action encoding/decoding, and grid/shape *logic*.
*   **`features`:** Contains logic to convert `GameState` objects into numerical features (`StateType`).
*   **`nn`:** Contains the PyTorch `nn.Module` definition (`AlphaTriangleNet`) and a wrapper class (`NeuralNetwork`).
*   **`mcts`:** Implements the Monte Carlo Tree Search algorithm (`Node`, `run_mcts_simulations`).
*   **`rl`:** Orchestrates the reinforcement learning loop (`TrainingOrchestrator`), manages network updates (`Trainer`), handles **parallel self-play data generation using Ray actors (`SelfPlayWorker`)**, and stores experiences (`ExperienceBuffer`). The `TrainingOrchestrator` logs parameters, metrics, and artifacts to MLflow.
*   **`stats`:** Contains the `StatsCollectorActor` (Ray actor) for **asynchronous statistics collection** and the `Plotter` class for rendering plots using Matplotlib.
*   **`visualization`:** Uses Pygame to render the game state, previews, HUD, plots, etc. `GameRenderer` fetches data from `StatsCollectorActor` for **near real-time plot updates**.
*   **`interaction`:** Handles keyboard/mouse input for interactive modes.
*   **`data`:** Manages saving and loading of training artifacts like NN checkpoints and replay buffers (`DataManager`) within the `.alphatriangle_data/runs/<run_name>/` structure.
*   **`utils`:** Provides common helper functions, shared type definitions, and geometry helpers.

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd alphatriangle # Or your project directory name
    ```
2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```
3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *Note: Ensure you have the correct PyTorch version installed for your system (CPU/CUDA). See [pytorch.org](https://pytorch.org/). Ray may have specific system requirements.*
4.  **(Optional) Add data directory to `.gitignore`:**
    Create or edit the `.gitignore` file in your project root and add the line:
    ```
    .alphatriangle_data/
    ```

## Running the Code

Execute the following scripts from the **project root directory**:

*   **Interactive Play Mode:**
    ```bash
    python run_interactive.py --mode play
    ```
*   **Interactive Debug Mode:**
    ```bash
    python run_interactive.py --mode debug
    ```
*   **Headless Training:**
    ```bash
    python run_training_headless.py
    ```
    (Runs the RL training loop without visualization, using Ray for parallel self-play. Logs parameters, metrics, and artifacts to MLflow).
*   **Training with Visualization:**
    ```bash
    python run_training_visual.py
    ```
    (Runs the RL training loop and displays the game state from workers and **near real-time plots**. Uses Ray for parallel self-play and stats collection. Logs parameters, metrics, and artifacts to MLflow).
*   **Monitoring Training (MLflow UI):**
    While training (headless or visual), or after runs have completed, open a separate terminal in the project root and run:
    ```bash
    mlflow ui --backend-store-uri file:./.alphatriangle_data/mlruns
    ```
    Then navigate to `http://localhost:5000` (or the specified port) in your browser. This UI allows you to compare runs, view logged parameters/metrics, and access saved artifacts (like checkpoints).

## Configuration

All major parameters are defined in the classes within the `src/config/` directory. Modify these files to experiment with different settings. The `src/config/validation.py` script performs basic checks on startup.

## Data Storage

All persistent data, including MLflow tracking data and run-specific artifacts, is stored within the `.alphatriangle_data/` directory in the project root.

*   **`.alphatriangle_data/mlruns/`**: Contains MLflow tracking data (managed by MLflow).
    *   `<experiment_id>/<run_id>/`: Contains subdirectories for `artifacts/`, `metrics/`, `params/`, and `meta.yaml`. MLflow logs artifacts by copying them from the temporary run directory (`.alphatriangle_data/runs/<run_name>/...`) into its own artifact store, typically within this structure.
*   **`.alphatriangle_data/runs/`**: Contains temporary and local artifacts generated during training runs.
    *   `<run_name>/`: Directory specific to a training run (identified by `RUN_NAME`).
        *   `checkpoints/`: Stores saved checkpoints (`.pkl` files containing model weights, optimizer state, training progress, and stats collector state).
        *   `buffers/`: Stores saved experience replay buffers (`.pkl` files).
        *   `logs/`: Optional directory for non-MLflow log files.
        *   `configs.json`: A JSON copy of the configuration used for the run.

The `DataManager` saves files to the appropriate subdirectory within `.alphatriangle_data/runs/<run_name>/`, and the `TrainingOrchestrator` (via helpers) logs these files as artifacts to MLflow, which then manages them within the `.alphatriangle_data/mlruns/` structure. Checkpoints and buffers are saved robustly using `cloudpickle`.

## Maintainability

This project includes README files within each major `src` submodule. **Please keep these READMEs updated** when making changes to the code's structure, interfaces, or core logic. Accurate documentation significantly aids understanding and future development. Consider regenerating documentation snippets if major refactoring occurs.

File: .python-version
3.10.13


File: run_training_headless.py
# File: run_training_headless.py
# File: run_training_headless.py
import sys
import os
import logging
import traceback
import torch
import mlflow
import ray

# Ensure the src directory is in the Python path
script_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(script_dir, "src")
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

# Updated imports
from src import config, utils, nn, data
from src.rl import TrainingOrchestrator, ExperienceBuffer, Trainer
from src.mcts import MCTSConfig # Import Pydantic MCTSConfig
from src.stats import StatsCollectorActor

# --- Configuration ---
LOG_LEVEL = logging.INFO
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s", handlers=[logging.StreamHandler(sys.stdout)])
logger = logging.getLogger(__name__)

if __name__ == "__main__":
    orchestrator = None
    mlflow_run_active = False
    ray_initialized = False
    stats_collector_actor = None

    try:
        # --- Initialize Ray ---
        ray.init(logging_level=logging.WARNING)
        ray_initialized = True
        logger.info(f"Ray initialized. Cluster resources: {ray.cluster_resources()}")

        # --- Initialize Configurations ---
        train_config = config.TrainConfig()
        env_config = config.EnvConfig()
        model_config = config.ModelConfig()
        mcts_config = MCTSConfig() # Instantiate Pydantic model
        persist_config = config.PersistenceConfig()

        # --- Configuration Overrides ---
        persist_config.RUN_NAME = train_config.RUN_NAME # Ensure RUN_NAME is consistent
        # Example: train_config.LOAD_CHECKPOINT_PATH = ".alphatriangle_data/runs/<PREVIOUS_RUN_NAME>/checkpoints/latest.pkl"

        # --- Set MLflow Tracking URI ---
        mlflow_tracking_uri = persist_config.MLFLOW_TRACKING_URI
        os.makedirs(os.path.dirname(mlflow_tracking_uri.replace("file:", "")), exist_ok=True)
        mlflow.set_tracking_uri(mlflow_tracking_uri)
        logger.info(f"Set MLflow tracking URI to: {mlflow_tracking_uri}")

        config.print_config_info_and_validate(mcts_config) # Pass Pydantic instance

        # --- Setup ---
        utils.set_random_seeds(train_config.RANDOM_SEED)
        device = utils.get_device(train_config.DEVICE)

        # --- Start MLflow Run ---
        mlflow.start_run(run_name=train_config.RUN_NAME)
        mlflow_run_active = True
        logger.info(f"MLflow Run started (ID: {mlflow.active_run().info.run_id}).")

        # --- Initialize Components ---
        stats_collector_actor = StatsCollectorActor.remote(max_history=1000)
        neural_net = nn.NeuralNetwork(model_config, env_config, train_config, device)
        buffer = ExperienceBuffer(train_config)
        trainer = Trainer(neural_net, train_config, env_config)
        # Pass train_config to DataManager for auto-resume logic
        data_manager = data.DataManager(persist_config, train_config)

        # Initialize Orchestrator (it will handle loading state internally)
        orchestrator = TrainingOrchestrator(
            nn=neural_net,
            buffer=buffer,
            trainer=trainer,
            data_manager=data_manager,
            stats_collector_actor=stats_collector_actor,
            train_config=train_config,
            env_config=env_config,
            mcts_config=mcts_config, # Pass Pydantic instance
            persist_config=persist_config,
            visual_state_queue=None, # No visualization
        )

        # --- Run Training ---
        logger.info("Starting headless training...")
        orchestrator.run_training_loop()

    except Exception as e:
        logger.critical(f"An unhandled error occurred during headless training: {e}")
        traceback.print_exc()
        if mlflow_run_active:
            try:
                mlflow.log_param("training_status", "FAILED")
                mlflow.log_param("error_message", str(e))
            except Exception as mlf_err:
                 logger.error(f"Failed to log error status to MLflow: {mlf_err}")
        sys.exit(1)

    finally:
        final_status = "INTERRUPTED"
        error_msg = ""
        exit_code = 1

        if orchestrator:
            if orchestrator.training_exception:
                final_status = "FAILED"
                error_msg = str(orchestrator.training_exception)
            elif orchestrator.training_complete:
                final_status = "COMPLETED"
                exit_code = 0

            # --- Force Save Final State ---
            logger.info("Attempting to save final training state...")
            orchestrator.save_final_state() # Calls DataManager internally

            # --- Perform Orchestrator Cleanup (kills actors) ---
            orchestrator._final_cleanup()
        elif "e" in locals() and isinstance(e, Exception):
             final_status = "FAILED"
             error_msg = str(e)

        logger.info(f"Final Training Status: {final_status}")

        if mlflow_run_active:
            try:
                mlflow.log_param("training_status", final_status)
                if error_msg:
                    mlflow.log_param("error_message", error_msg)
                mlflow.end_run()
                logger.info("MLflow Run ended.")
            except Exception as mlf_end_err:
                logger.error(f"Error ending MLflow run: {mlf_end_err}")

        if ray_initialized:
            ray.shutdown()
            logger.info("Ray shut down.")

        logger.info("Headless training script finished.")
        sys.exit(exit_code)

File: run_interactive.py
# File: run_interactive.py
# File: run_interactive.py
# Change: Updated imports for app, environment, visualization.
import sys
import os
import argparse
import logging
import traceback

# Ensure the src directory is in the Python path
script_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(script_dir, "src")
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

# Updated imports using the new structure
from src import app, config, utils

# Environment and Visualization might be needed for type hints or specific setup later
# but the Application class handles their internal use.
from src import environment
from src import visualization
from src.mcts import MCTSConfig  # Import Pydantic MCTSConfig


logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger(__name__)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AlphaTriangle Interactive Modes")
    parser.add_argument(
        "--mode",
        type=str,
        default="play",
        choices=["play", "debug"],
        help="Interaction mode ('play' or 'debug')",
    )
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    args = parser.parse_args()

    logger.info(f"Running in {args.mode.capitalize()} mode...")
    utils.set_random_seeds(args.seed)

    # Instantiate MCTSConfig needed for validation function
    mcts_config = MCTSConfig() # Instantiate Pydantic model
    # Pass MCTSConfig instance to validation
    config.print_config_info_and_validate(mcts_config)

    try:
        # Pass mode to the Application constructor
        # Application now uses the refactored environment and visualization internally
        app_instance = app.Application(mode=args.mode)
        app_instance.run()
    except ImportError as e:
        logger.error(f"ImportError: {e}")
        logger.error("Please ensure:")
        logger.error("1. You are running from the project root directory.")
        logger.error("2. All required files exist in 'src'.")
        logger.error(
            "3. Dependencies are installed (`pip install -r requirements.txt`)."
        )
        sys.exit(1)
    except Exception as e:
        logger.critical(f"An unhandled error occurred: {e}")
        traceback.print_exc()
        sys.exit(1)

    logger.info("Exiting.")

File: run_training_visual.py
# File: run_training_visual.py
import sys
import os
import logging
import traceback
import threading
import queue
import time
import pygame
import torch
import mlflow
import ray
from typing import Optional, Dict, Any, List

# Ensure the src directory is in the Python path
script_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(script_dir, "src")
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

# Updated imports
from src import config, utils, nn, data
from src.rl import TrainingOrchestrator, ExperienceBuffer, Trainer
from src.mcts import MCTSConfig # Import Pydantic MCTSConfig
from src import visualization
from src import environment
from src.stats import StatsCollectorActor

# --- Configuration ---
LOG_LEVEL = logging.INFO
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s", handlers=[logging.StreamHandler(sys.stdout)], force=True)
logger = logging.getLogger(__name__)
logger.info(f"Set main process root logger level to {logging.getLevelName(LOG_LEVEL)}")

visual_state_queue: queue.Queue[Optional[Dict[int, Any]]] = queue.Queue(maxsize=5)

def training_thread_func(orchestrator: TrainingOrchestrator):
    """Function to run the training orchestrator in a separate thread."""
    try:
        logger.info("Training thread started.")
        orchestrator.run_training_loop()
        logger.info("Training thread finished.")
    except Exception as e:
        logger.critical(f"Error in training thread: {e}", exc_info=True)
        if orchestrator:
            orchestrator.training_exception = e
    finally:
        try:
            while not visual_state_queue.empty():
                try: visual_state_queue.get_nowait()
                except queue.Empty: break
            visual_state_queue.put(None, timeout=1.0)
        except queue.Full: logger.error("Visual queue still full after clearing attempt during shutdown.")
        except Exception as e_q: logger.error(f"Error putting None signal into visual queue during thread shutdown: {e_q}")


if __name__ == "__main__":
    main_thread_exception = None
    train_thread = None
    orchestrator = None
    mlflow_run_active = False
    ray_initialized = False
    error_logged_in_except_block = False
    stats_collector_actor = None

    try:
        # --- Initialize Ray ---
        ray.init(logging_level=logging.WARNING, log_to_driver=True)
        ray_initialized = True
        logger.info(f"Ray initialized. Cluster resources: {ray.cluster_resources()}")

        # --- Initialize Configurations ---
        train_config = config.TrainConfig()
        env_config = config.EnvConfig()
        model_config = config.ModelConfig()
        mcts_config = MCTSConfig() # Instantiate Pydantic model
        persist_config = config.PersistenceConfig()
        vis_config = config.VisConfig()

        # --- Configuration Overrides ---
        persist_config.RUN_NAME = train_config.RUN_NAME
        # Example: train_config.LOAD_CHECKPOINT_PATH = ".alphatriangle_data/runs/<PREVIOUS_RUN_NAME>/checkpoints/latest.pkl"

        # --- Set MLflow Tracking URI ---
        mlflow_tracking_uri = persist_config.MLFLOW_TRACKING_URI
        os.makedirs(os.path.dirname(mlflow_tracking_uri.replace("file:", "")), exist_ok=True)
        mlflow.set_tracking_uri(mlflow_tracking_uri)
        logger.info(f"Set MLflow tracking URI to: {mlflow_tracking_uri}")

        config.print_config_info_and_validate(mcts_config) # Pass Pydantic instance

        # --- Setup ---
        utils.set_random_seeds(train_config.RANDOM_SEED)
        device = utils.get_device(train_config.DEVICE)

        # --- Start MLflow Run ---
        mlflow.start_run(run_name=train_config.RUN_NAME)
        mlflow_run_active = True
        logger.info(f"MLflow Run started (ID: {mlflow.active_run().info.run_id}).")

        # --- Initialize Components ---
        stats_collector_actor = StatsCollectorActor.remote(max_history=1000)
        neural_net = nn.NeuralNetwork(model_config, env_config, train_config, device)
        buffer = ExperienceBuffer(train_config)
        trainer = Trainer(neural_net, train_config, env_config)
        # Pass train_config to DataManager for auto-resume logic
        data_manager = data.DataManager(persist_config, train_config)

        # Initialize Orchestrator (it will handle loading state internally)
        orchestrator = TrainingOrchestrator(
            nn=neural_net,
            buffer=buffer,
            trainer=trainer,
            data_manager=data_manager,
            stats_collector_actor=stats_collector_actor,
            train_config=train_config,
            env_config=env_config,
            mcts_config=mcts_config, # Pass Pydantic instance
            persist_config=persist_config,
            visual_state_queue=visual_state_queue,
        )

        # --- Start Training Thread ---
        train_thread = threading.Thread(target=training_thread_func, args=(orchestrator,), daemon=True)
        train_thread.start()
        logger.info("Training thread launched.")

        # --- Initialize Visualization ---
        pygame.init()
        pygame.font.init()
        screen = pygame.display.set_mode((vis_config.SCREEN_WIDTH, vis_config.SCREEN_HEIGHT), pygame.RESIZABLE)
        pygame.display.set_caption(f"{config.APP_NAME} - Training Visual Mode ({train_config.RUN_NAME})")
        clock = pygame.time.Clock()
        fonts = visualization.load_fonts()
        # Pass stats_collector_actor handle to GameRenderer
        game_renderer = visualization.GameRenderer(screen, vis_config, env_config, fonts, stats_collector_actor)
        current_worker_states: Dict[int, environment.GameState] = {}
        global_stats_for_hud: Dict[str, Any] = {}
        has_received_states = False # Flag: Have we received *any* dict from the queue?
        has_received_worker_states = False # Flag: Have we received a dict with actual worker states?

        # --- Visualization Loop (Main Thread) ---
        running = True
        while running:
            for event in pygame.event.get():
                if event.type == pygame.QUIT: running = False
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE: running = False
                if event.type == pygame.VIDEORESIZE:
                    try:
                        w, h = max(640, event.w), max(480, event.h)
                        screen = pygame.display.set_mode((w, h), pygame.RESIZABLE)
                        game_renderer.screen = screen
                        game_renderer.layout_rects = None # Force layout recalc
                    except pygame.error as e: logger.error(f"Error resizing window: {e}")

            # --- Process Visual Queue ---
            try:
                new_state_obj = visual_state_queue.get(timeout=0.05)
                if new_state_obj is None:
                    if train_thread and not train_thread.is_alive():
                        running = False
                        logger.info("Received exit signal from training thread.")
                elif isinstance(new_state_obj, dict):
                    has_received_states = True # Received something
                    global_stats_for_hud = new_state_obj.pop(-1, {}) # Extract global stats
                    # Check if any worker states (keys >= 0) are present
                    worker_states_in_dict = {k: v for k, v in new_state_obj.items() if isinstance(k, int) and k >= 0 and isinstance(v, environment.GameState)}
                    if worker_states_in_dict:
                        has_received_worker_states = True # Received actual worker states
                        current_worker_states = worker_states_in_dict
                    # If only global stats were received, current_worker_states remains empty/unchanged
                else:
                    logger.warning(f"Received unexpected item from visual queue: {type(new_state_obj)}")
            except queue.Empty: pass
            except Exception as q_get_err:
                logger.error(f"Error getting from visual queue: {q_get_err}")
                time.sleep(0.1)

            # --- Rendering Logic ---
            screen.fill(visualization.colors.DARK_GRAY)

            if has_received_states:
                # We have received at least global stats, render HUD/Plots/Progress
                try:
                    # Pass current_worker_states (might be empty) and global_stats
                    game_renderer.render(current_worker_states, global_stats_for_hud)
                except Exception as render_err:
                    logger.error(f"Error during rendering: {render_err}", exc_info=True)
                    err_font = fonts.get("help")
                    if err_font:
                        err_surf = err_font.render(f"Render Error: {render_err}", True, visualization.colors.RED)
                        screen.blit(err_surf, (10, screen.get_height() // 2))

                # Show specific message if we only have global stats so far
                if not has_received_worker_states:
                    if fonts.get("help"):
                        wait_font = fonts["help"]
                        wait_surf = wait_font.render("Waiting for worker states...", True, visualization.colors.LIGHT_GRAY)
                        # Position message somewhere sensible, e.g., top-center of worker area
                        layout_rects = game_renderer._calculate_layout()
                        worker_area = layout_rects.get("worker_grid") if layout_rects else screen.get_rect()
                        wait_rect = wait_surf.get_rect(centerx=worker_area.centerx, top=worker_area.top + 20)
                        screen.blit(wait_surf, wait_rect)

            else:
                # Still waiting for the very first dictionary from the queue
                if fonts.get("help"):
                    wait_font = fonts["help"]
                    wait_surf = wait_font.render("Waiting for first game states from workers...", True, visualization.colors.LIGHT_GRAY)
                    wait_rect = wait_surf.get_rect(center=(screen.get_width() // 2, screen.get_height() // 2))
                    screen.blit(wait_surf, wait_rect)

            pygame.display.flip()

            # --- Check Training Thread Status ---
            if train_thread and not train_thread.is_alive() and running:
                logger.warning("Training thread terminated.")
                if orchestrator and orchestrator.training_exception:
                    logger.error(f"Training thread terminated due to exception: {orchestrator.training_exception}")
                    main_thread_exception = orchestrator.training_exception
                running = False

            clock.tick(vis_config.FPS)

    except Exception as e:
        logger.critical(f"An unhandled error occurred in visual training script (main thread): {e}")
        traceback.print_exc()
        main_thread_exception = e
        if mlflow_run_active:
            try:
                mlflow.log_param("training_status", "FAILED")
                mlflow.log_param("error_message", f"MainThread: {str(e)}")
                error_logged_in_except_block = True
            except Exception as mlf_err: logger.error(f"Failed to log main thread error to MLflow: {mlf_err}")

    finally:
        logger.info("Initiating shutdown sequence...")
        if orchestrator:
            orchestrator.request_stop()

        if train_thread and train_thread.is_alive():
            logger.info("Waiting for training thread to join...")
            train_thread.join(timeout=15.0)
            if train_thread.is_alive():
                logger.error("Training thread did not exit gracefully within timeout.")

        # --- Force Save Final State (after thread join/timeout) ---
        if orchestrator:
             logger.info("Attempting to save final training state...")
             orchestrator.save_final_state() # Calls DataManager internally
             # --- Perform Orchestrator Cleanup (kills actors) ---
             orchestrator._final_cleanup()

        final_status = "INTERRUPTED"
        final_error = None
        exit_code = 1

        if main_thread_exception:
            final_status = "FAILED"
            final_error = main_thread_exception
        elif orchestrator and orchestrator.training_exception:
            final_status = "FAILED"
            final_error = orchestrator.training_exception
        elif orchestrator and orchestrator.training_complete:
            final_status = "COMPLETED"
            exit_code = 0

        logger.info(f"Final Training Status: {final_status}")

        if mlflow_run_active:
            try:
                mlflow.log_param("training_status", final_status)
                if final_error and not error_logged_in_except_block:
                    mlflow.log_param("error_message", str(final_error))
                mlflow.end_run()
                logger.info("MLflow Run ended.")
            except Exception as mlf_end_err:
                logger.error(f"Error ending MLflow run: {mlf_end_err}")

        if ray_initialized:
            ray.shutdown()
            logger.info("Ray shut down.")

        pygame.quit()
        logger.info("Visual training script finished.")
        sys.exit(exit_code)

File: src/__init__.py


File: src/app.py
# File: src/app.py
# Change: Updated imports for visualization, environment, interaction.
import pygame
import sys
import logging
from . import config, utils

# Updated imports
from . import visualization
from . import environment
from . import interaction

logger = logging.getLogger(__name__)


class Application:
    """Main application integrating visualization and interaction."""

    def __init__(self, mode: str = "play"):
        self.vis_config = config.VisConfig()
        self.env_config = config.EnvConfig()
        self.mode = mode

        pygame.init()
        pygame.font.init()
        self.screen = self._setup_screen()
        self.clock = pygame.time.Clock()
        # Use load_fonts from the visualization module
        self.fonts = visualization.load_fonts()

        # Game state is now managed by the specific run script (interactive or training)
        # For interactive modes, we still need it here.
        if self.mode in ["play", "debug"]:
            # Use GameState from the environment module
            self.game_state = environment.GameState(self.env_config)
            # Use Visualizer from the visualization module
            self.visualizer = visualization.Visualizer(
                self.screen, self.vis_config, self.env_config, self.fonts
            )
            # Use InputHandler from the interaction module
            self.input_handler = interaction.InputHandler(
                self.game_state, self.visualizer, self.mode, self.env_config
            )
        # Removed 'training_visual' mode handling from App, as it's managed by run_training_visual.py
        # If this App class were reused for visual training display, it would need setup.
        else:
            raise ValueError(f"Unsupported application mode: {self.mode}")

        self.running = True

    def _setup_screen(self) -> pygame.Surface:
        """Initializes the Pygame screen."""
        screen = pygame.display.set_mode(
            (self.vis_config.SCREEN_WIDTH, self.vis_config.SCREEN_HEIGHT),
            pygame.RESIZABLE,
        )
        pygame.display.set_caption(f"{config.APP_NAME} - {self.mode.capitalize()} Mode")
        return screen

    def run(self):
        """Main application loop."""
        logger.info(f"Starting application in {self.mode} mode.")
        while self.running:
            dt = self.clock.tick(self.vis_config.FPS) / 1000.0  # dt not used currently

            # Process input (if handler exists) and update state
            if self.input_handler:
                self.running = self.input_handler.handle_input()
                if not self.running:
                    break
            else:
                # Basic event handling if no specific input handler is setup (shouldn't happen with current modes)
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        self.running = False
                    if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                        self.running = False
                    if event.type == pygame.VIDEORESIZE:
                        # Basic resize handling if visualizer exists (e.g., if app were reused)
                        if self.visualizer:
                            try:
                                w, h = max(320, event.w), max(240, event.h)
                                # Update screen directly on visualizer
                                self.visualizer.screen = pygame.display.set_mode(
                                    (w, h), pygame.RESIZABLE
                                )
                                self.visualizer.layout_rects = None  # Force recalc
                            except pygame.error as e:
                                logger.error(f"Error resizing window: {e}")

                if not self.running:
                    break

            # Render current state (if applicable)
            if self.mode in ["play", "debug"] and self.visualizer and self.game_state:
                # Visualizer handles rendering the game state
                self.visualizer.render(self.game_state, self.mode)
                pygame.display.flip()
            # Removed training_visual rendering logic here

        logger.info("Application loop finished.")
        pygame.quit()


File: src/visualization/__init__.py
# File: src/visualization/__init__.py
# File: src/visualization/__init__.py
"""
Visualization module for rendering the game state using Pygame.
"""

# Core components
from .core.visualizer import Visualizer
from .core.game_renderer import GameRenderer  # Keep GameRenderer, it's adapted
from .core.layout import calculate_layout  # Keep layout, though simplified
from .core.fonts import load_fonts
from .core import colors  # Expose colors directly
from .core.coord_mapper import (
    get_grid_coords_from_screen,
    get_preview_index_from_screen,
)

# Drawing functions (imported directly from specific files)
from .drawing.grid import draw_grid_background, draw_grid_triangles
from .drawing.shapes import draw_shape
from .drawing.previews import (
    render_previews,
    draw_placement_preview,
    draw_floating_preview,
)
from .drawing.hud import render_hud
from .drawing.highlight import draw_debug_highlight

# UI Components
from .ui.progress_bar import ProgressBar

# Import the function directly from utils.geometry now
from src.utils.geometry import is_point_in_polygon

# Configuration
from src.config import VisConfig

# Removed re-exports of Triangle and Shape as they are in src.structs

__all__ = [
    # Core Classes & Functions
    "Visualizer",
    "GameRenderer",  # Keep GameRenderer
    "calculate_layout",
    "load_fonts",
    "colors",
    "get_grid_coords_from_screen",
    "get_preview_index_from_screen",
    # Drawing Functions
    "draw_grid_background",
    "draw_grid_triangles",
    "draw_shape",
    "render_previews",
    "draw_placement_preview",
    "draw_floating_preview",
    "render_hud",
    "draw_debug_highlight",
    "is_point_in_polygon",  # Re-exported from utils
    # UI Components
    "ProgressBar",
    # Config
    "VisConfig",
]


File: src/visualization/README.md
# File: src/visualization/README.md
# Visualization Module (`src.visualization`)

## Purpose and Architecture

This module is responsible for rendering the game state visually using the Pygame library. It provides components for drawing the grid, shapes, previews, HUD elements, and statistics plots. **In training visualization mode, it now renders the states of multiple self-play workers in a grid layout alongside plots and progress bars.**

-   **Core Components (`src.visualization.core`):**
    -   `Visualizer`: Orchestrates the rendering process for interactive modes ("play", "debug"). It manages the layout, calls drawing functions, and handles hover/selection states specific to visualization.
    -   `GameRenderer`: **Adapted renderer** for displaying **multiple** game states and statistics during training visualization (`run_training_visual.py`). It uses `layout.py` to divide the screen. It renders worker game states in one area and statistics plots/progress bars in another. It re-instantiates `src.stats.Plotter`.
    -   `layout`: Calculates the screen positions and sizes for different UI areas (worker grid, stats area, plots).
    -   `fonts`: Loads necessary font files.
    -   `colors`: Defines a centralized palette of RGB color tuples.
    -   `coord_mapper`: Provides functions to map screen coordinates to grid coordinates (`get_grid_coords_from_screen`) and preview indices (`get_preview_index_from_screen`).
-   **Drawing Components (`src.visualization.drawing`):**
    -   Contains specific functions for drawing different elements onto Pygame surfaces:
        -   `grid`: Draws the grid background and occupied/empty triangles.
        -   `shapes`: Draws individual shapes (used by previews).
        -   `previews`: Renders the shape preview area.
        -   `hud`: Renders text information like global training stats and help text at the bottom.
        -   `highlight`: Draws debug highlights.
-   **UI Components (`src.visualization.ui`):**
    -   Contains reusable UI elements like `ProgressBar`.

## Exposed Interfaces

-   **Core Classes & Functions:**
    -   `Visualizer`: Main renderer for interactive modes.
    -   `GameRenderer`: Renderer for combined multi-game/stats training visualization.
    -   `calculate_layout`: Calculates UI layout rectangles.
    -   `load_fonts`: Loads Pygame fonts.
    -   `colors`: Module containing color constants (e.g., `colors.WHITE`).
    -   `get_grid_coords_from_screen`: Maps screen to grid coordinates.
    -   `get_preview_index_from_screen`: Maps screen to preview index.
-   **Drawing Functions (primarily used internally by Visualizer/GameRenderer but exposed):**
    -   `draw_grid_background`, `draw_grid_triangles`
    -   `draw_shape`
    -   `render_previews`
    -   `render_hud`
    -   `draw_debug_highlight`
-   **UI Components:**
    -   `ProgressBar`: Class for rendering progress bars.
-   **Config:**
    -   `VisConfig`: Configuration class (re-exported from `src.config`).

## Dependencies

-   **`src.config`**:
    -   `VisConfig`, `EnvConfig`: Used extensively for layout, sizing, and coordinate mapping.
-   **`src.environment`**:
    -   `GameState`: The primary object whose state is visualized.
    -   `GridData`: Accessed via `GameState` or passed directly to drawing functions.
-   **`src.structs`**:
    -   Uses `Triangle`, `Shape`.
-   **`src.stats`**:
    -   Uses `Plotter` within `GameRenderer`.
-   **`src.utils`**:
    -   Uses `geometry.is_point_in_polygon`, `helpers.format_eta`, `types.StatsCollectorData`.
-   **`pygame`**:
    -   The core library used for all drawing, surface manipulation, event handling (via `interaction`), and font rendering.
-   **`matplotlib`**:
    -   Used by `src.stats.Plotter`.
-   **Standard Libraries:** `typing`, `logging`, `math`, `time`.

---

**Note:** Please keep this README updated when changing rendering logic, adding new visual elements, modifying layout calculations, or altering the interfaces exposed to other modules (like `interaction` or the main application scripts). Accurate documentation is crucial for maintainability.

File: src/visualization/ui/__init__.py
# File: src/visualization/ui/__init__.py
"""
UI Components subpackage for visualization.
Contains reusable UI elements like progress bars, buttons, etc.
"""
from .progress_bar import ProgressBar

__all__ = [
    "ProgressBar",
]


File: src/visualization/ui/progress_bar.py
# File: src/visualization/ui/progress_bar.py
import pygame
import time
import math
from typing import Tuple, Optional, Dict, Any

from ..core import colors
from src.utils import format_eta


class ProgressBar:
    """A reusable progress bar component for visualization."""

    def __init__(
        self,
        entity_title: str,
        total_steps: int,
        start_time: Optional[float] = None,
        initial_steps: int = 0,
    ):
        self.entity_title = entity_title
        # Ensure total_steps is at least 1. If original was None/0, it becomes 1.
        self.total_steps = max(1, total_steps if total_steps is not None else 1)
        self.initial_steps = max(0, initial_steps)  # Store initial steps
        self.current_steps = self.initial_steps  # Initialize current steps
        self.start_time = start_time if start_time is not None else time.time()
        self._last_step_time = self.start_time
        self._step_times = []  # Optional: for smoother ETA later
        self.extra_data: Dict[str, Any] = {}  # For additional info like buffer size

    def add_steps(self, steps_added: int):
        """Adds steps to the progress bar's current count."""
        if steps_added <= 0:
            return
        # Only cap steps if total_steps is > 1 (treat 1 as potentially infinite)
        if self.total_steps > 1:
            self.current_steps = min(self.total_steps, self.current_steps + steps_added)
        else:
            self.current_steps += steps_added

    def set_current_steps(self, steps: int):
        """Directly sets the current step count."""
        # Only cap steps if total_steps is > 1 (treat 1 as potentially infinite)
        if self.total_steps > 1:
            self.current_steps = max(0, min(self.total_steps, steps))
        else:
            self.current_steps = max(0, steps)

    def update_extra_data(self, data: Dict[str, Any]):
        """Updates or adds key-value pairs to display."""
        self.extra_data.update(data)

    def reset_time(self):
        """Resets the start time to now, keeping current steps."""
        self.start_time = time.time()
        self._last_step_time = self.start_time
        self._step_times = []
        # Also reset initial_steps to current_steps when resetting time
        self.initial_steps = self.current_steps

    def reset_all(self, new_total_steps: Optional[int] = None):
        """Resets steps to 0 and start time to now. Optionally updates total steps."""
        self.current_steps = 0
        self.initial_steps = 0  # Reset initial steps as well
        if new_total_steps is not None:
            self.total_steps = max(1, new_total_steps)
        self.start_time = time.time()
        self._last_step_time = self.start_time
        self._step_times = []
        self.extra_data = {}

    def get_progress(self) -> float:
        """Returns progress as a fraction (0.0 to 1.0). Returns 1.0 if total_steps is 1."""
        if self.total_steps <= 1:  # Treat 1 as having no defined end for percentage
            return (
                0.0  # Or perhaps return a different indicator? Let's use 0.0 for now.
            )
        # Calculate progress relative to the initial steps if needed, but usually total_steps is the goal
        # Progress should reflect current_steps towards total_steps
        return min(
            1.0, self.current_steps / self.total_steps
        )  # Ensure progress doesn't exceed 1.0 visually

    def get_elapsed_time(self) -> float:
        """Returns the time elapsed since the start time."""
        return time.time() - self.start_time

    def get_eta_seconds(self) -> Optional[float]:
        """Calculates the estimated time remaining in seconds."""
        # Cannot estimate ETA if total_steps is 1 (likely infinite)
        if self.total_steps <= 1:
            return None

        # Calculate steps processed since the timer started
        steps_processed = self.current_steps - self.initial_steps
        if steps_processed <= 0:
            return None  # Cannot estimate if no progress made since start

        elapsed = self.get_elapsed_time()
        if elapsed < 1.0:  # Avoid unstable ETA at the beginning
            return None

        speed = steps_processed / elapsed  # Speed based on progress since start_time
        if speed < 1e-6:
            return None  # Avoid division by zero if speed is negligible

        remaining_steps = self.total_steps - self.current_steps
        if remaining_steps <= 0:
            return 0.0  # Already done or potentially exceeded

        eta = remaining_steps / speed
        return eta

    def render(
        self,
        surface: pygame.Surface,
        position: Tuple[int, int],
        width: int,
        height: int,
        font: pygame.font.Font,
        bar_color: Tuple[int, int, int] = colors.BLUE,
        bg_color: Tuple[int, int, int] = colors.DARK_GRAY,
        text_color: Tuple[int, int, int] = colors.WHITE,
        border_width: int = 1,
        border_color: Tuple[int, int, int] = colors.GRAY,
    ):
        """Draws the progress bar onto the given surface."""
        x, y = position
        progress = self.get_progress()  # Progress is 0.0 if total_steps is 1
        elapsed_time_str = format_eta(self.get_elapsed_time())
        eta_seconds = self.get_eta_seconds()
        eta_str = format_eta(eta_seconds) if eta_seconds is not None else "N/A"

        # Background
        bg_rect = pygame.Rect(x, y, width, height)
        pygame.draw.rect(surface, bg_color, bg_rect)

        # Progress Fill
        # If total_steps is 1, fill width should represent something else or be 0.
        # Let's make it 0 if total_steps is 1, as progress is undefined.
        fill_width = 0
        if self.total_steps > 1:
            fill_width = int(width * progress)

        if fill_width > 0:
            fill_rect = pygame.Rect(x, y, fill_width, height)
            pygame.draw.rect(surface, bar_color, fill_rect)

        # Border
        if border_width > 0:
            pygame.draw.rect(surface, border_color, bg_rect, border_width)

        # --- Text Content ---
        text_y_offset = 2
        available_text_height = height - 2 * text_y_offset
        line_height = font.get_height()
        num_lines = 0
        if available_text_height >= line_height:
            num_lines += 1  # Title
        if available_text_height >= line_height * 2:
            num_lines += 1  # Progress Text
        if available_text_height >= line_height * 3:
            num_lines += 1  # Timings
        if self.extra_data and available_text_height >= line_height * 4:
            num_lines += 1  # Extra

        total_text_height = num_lines * line_height + max(0, num_lines - 1) * 2
        if total_text_height < available_text_height:
            current_y = (
                y + text_y_offset + (available_text_height - total_text_height) // 2
            )
        else:
            current_y = y + text_y_offset

        center_x = x + width // 2

        # 1. Title (Centered)
        if num_lines >= 1:
            title_surf = font.render(self.entity_title, True, text_color)
            title_rect = title_surf.get_rect(centerx=center_x, top=current_y)
            surface.blit(title_surf, title_rect)
            current_y += line_height + 2

        # 2. Progress Text (Centered) - MODIFIED
        if num_lines >= 2:
            processed_steps = self.current_steps
            expected_steps = self.total_steps

            # --- Conditional Formatting ---
            if expected_steps <= 1:  # Treat total=1 as "no defined maximum"
                progress_text = f"{processed_steps} steps"
            else:
                # Standard format with percentage
                progress_text = f"{processed_steps} / {expected_steps} ({progress:.1%})"
            # --- End Conditional Formatting ---

            progress_surf = font.render(progress_text, True, text_color)
            progress_rect = progress_surf.get_rect(centerx=center_x, top=current_y)
            surface.blit(progress_surf, progress_rect)
            current_y += line_height + 2

        # 3. Timings (Centered)
        if num_lines >= 3:
            # ETA is now "N/A" if expected_steps <= 1 due to get_eta_seconds change
            time_text = f"Elapsed: {elapsed_time_str} | ETA: {eta_str}"
            time_surf = font.render(time_text, True, text_color)
            time_rect = time_surf.get_rect(centerx=center_x, top=current_y)
            surface.blit(time_surf, time_rect)
            current_y += line_height + 2

        # 4. Extra Data (Centered)
        if num_lines >= 4 and self.extra_data:
            extra_texts = [f"{k}: {v}" for k, v in self.extra_data.items()]
            extra_text = " | ".join(extra_texts)
            extra_surf = font.render(extra_text, True, text_color)
            extra_rect = extra_surf.get_rect(centerx=center_x, top=current_y)
            surface.blit(extra_surf, extra_rect)


File: src/visualization/drawing/grid.py
# File: src/visualization/drawing/grid.py
import pygame
from typing import TYPE_CHECKING

# Use relative imports within visualization package
from ..core import colors, coord_mapper

# Import Triangle from the new structs module
from src.structs import Triangle

if TYPE_CHECKING:
    from src.environment import GridData  # GridData remains in environment
    from src.config import EnvConfig


def draw_grid_background(surface: pygame.Surface, bg_color: tuple) -> None:
    """Fills the grid area surface with a background color."""
    surface.fill(bg_color)


def draw_grid_triangles(
    surface: pygame.Surface, grid_data: "GridData", config: "EnvConfig"
) -> None:
    """Draws all triangles (empty, occupied) on the grid surface."""
    if surface.get_width() <= 0 or surface.get_height() <= 0:
        return

    cw, ch, ox, oy = coord_mapper._calculate_render_params(
        surface.get_width(), surface.get_height(), config
    )
    if cw <= 0 or ch <= 0:
        return

    for r in range(grid_data.rows):
        for c in range(grid_data.cols):
            tri: Triangle = grid_data.triangles[r][c]  # Use Triangle from structs
            if tri.is_death:
                continue  # Skip death cells

            color = tri.color if tri.is_occupied else colors.TRIANGLE_EMPTY_COLOR
            pts = tri.get_points(
                ox, oy, cw, ch
            )  # Get points relative to grid surface origin

            pygame.draw.polygon(surface, color, pts)
            pygame.draw.polygon(surface, colors.GRID_LINE_COLOR, pts, 1)  # Border


File: src/visualization/drawing/shapes.py
# File: src/visualization/drawing/shapes.py
import pygame
from typing import TYPE_CHECKING, Tuple, Optional

# Use relative imports within visualization package
from ..core import colors

# Import Triangle and Shape from the new structs module
from src.structs import Triangle, Shape

if TYPE_CHECKING:
    pass  # No other type hints needed here currently


def draw_shape(
    surface: pygame.Surface,
    shape: Shape,  # Use Shape from structs
    topleft: Tuple[int, int],
    cell_size: float,
    is_selected: bool = False,  # Example parameter, adjust as needed
    origin_offset: Tuple[int, int] = (0, 0),  # Offset for relative coords
) -> None:
    """Draws a single shape onto a surface."""
    if not shape or not shape.triangles or cell_size <= 0:
        return

    # Use shape's color, maybe adjust if selected?
    shape_color = shape.color
    border_color = colors.GRAY  # Example border color

    # Calculate effective cell width/height for triangles
    cw = cell_size
    ch = cell_size

    for dr, dc, is_up in shape.triangles:
        # Adjust relative coords by origin offset
        adj_r, adj_c = dr + origin_offset[0], dc + origin_offset[1]

        # Calculate top-left corner for this triangle's bounding box
        # relative to the provided topleft
        tri_x = topleft[0] + adj_c * (cw * 0.75)
        tri_y = topleft[1] + adj_r * ch

        # Create a temporary Triangle object just for getting points
        temp_tri = Triangle(0, 0, is_up)  # Use Triangle from structs
        # Get points relative to (0,0) with cell_size, then offset
        pts = [(px + tri_x, py + tri_y) for px, py in temp_tri.get_points(0, 0, cw, ch)]

        pygame.draw.polygon(surface, shape_color, pts)
        pygame.draw.polygon(surface, border_color, pts, 1)  # Draw border


File: src/visualization/drawing/previews.py
# File: src/visualization/drawing/previews.py
import pygame
import logging
from typing import TYPE_CHECKING, Tuple, Dict

# Use relative imports within visualization package for core elements
from ..core import colors, coord_mapper

# Import the specific function needed directly using relative path
from .shapes import draw_shape

# Import Triangle and Shape from the new structs module
from src.structs import Triangle, Shape

if TYPE_CHECKING:
    from src.environment import GameState  # GameState remains in environment
    from src.config import EnvConfig, VisConfig

logger = logging.getLogger(__name__)

# === Preview Area Rendering ===


def render_previews(
    surface: pygame.Surface,
    game_state: "GameState",
    area_topleft: Tuple[int, int],  # Screen coord of preview area's top-left
    mode: str,
    env_config: "EnvConfig",
    vis_config: "VisConfig",
) -> Dict[int, pygame.Rect]:
    """Renders shape previews in their area. Returns dict {index: screen_rect}."""
    surface.fill(colors.PREVIEW_BG)
    preview_rects_screen: Dict[int, pygame.Rect] = {}
    num_slots = env_config.NUM_SHAPE_SLOTS
    pad = vis_config.PREVIEW_PADDING
    inner_pad = vis_config.PREVIEW_INNER_PADDING
    border = vis_config.PREVIEW_BORDER_WIDTH
    selected_border = vis_config.PREVIEW_SELECTED_BORDER_WIDTH

    if num_slots <= 0:
        return {}

    total_pad_h = (num_slots + 1) * pad
    available_h = surface.get_height() - total_pad_h
    slot_h = available_h / num_slots if num_slots > 0 else 0
    slot_w = surface.get_width() - 2 * pad

    current_y = pad

    for i in range(num_slots):
        slot_rect_local = pygame.Rect(pad, current_y, slot_w, slot_h)
        # Calculate screen coordinates for the returned dictionary
        slot_rect_screen = slot_rect_local.move(area_topleft)
        preview_rects_screen[i] = slot_rect_screen

        shape: Optional[Shape] = game_state.shapes[i]  # Use Shape from structs
        is_selected = mode == "play" and game_state.demo_selected_shape_idx == i

        # Draw border
        border_width = selected_border if is_selected else border
        border_color = (
            colors.PREVIEW_SELECTED_BORDER if is_selected else colors.PREVIEW_BORDER
        )
        pygame.draw.rect(surface, border_color, slot_rect_local, border_width)

        # Draw shape inside the slot if available
        if shape:
            # Calculate drawing area inside border and padding
            draw_area_w = slot_w - 2 * (border_width + inner_pad)
            draw_area_h = slot_h - 2 * (border_width + inner_pad)

            if draw_area_w > 0 and draw_area_h > 0:
                # Determine cell size based on shape bounds and available area
                min_r, min_c, max_r, max_c = shape.bbox()
                shape_rows = max_r - min_r + 1
                # Effective cols calculation needs care for triangles
                shape_cols_eff = (
                    (max_c - min_c + 1) * 0.75 + 0.25 if shape.triangles else 1
                )

                scale_w = (
                    draw_area_w / shape_cols_eff if shape_cols_eff > 0 else draw_area_w
                )
                scale_h = draw_area_h / shape_rows if shape_rows > 0 else draw_area_h
                cell_size = max(1.0, min(scale_w, scale_h))

                # Center the shape within the drawing area
                shape_render_w = shape_cols_eff * cell_size
                shape_render_h = shape_rows * cell_size
                draw_topleft_x = (
                    slot_rect_local.left
                    + border_width
                    + inner_pad
                    + (draw_area_w - shape_render_w) / 2
                )
                draw_topleft_y = (
                    slot_rect_local.top
                    + border_width
                    + inner_pad
                    + (draw_area_h - shape_render_h) / 2
                )

                # Use the directly imported draw_shape function
                draw_shape(
                    surface,
                    shape,
                    (draw_topleft_x, draw_topleft_y),
                    cell_size,
                    is_selected=is_selected,  # Pass selection status if needed by draw_shape
                    origin_offset=(-min_r, -min_c),  # Offset drawing by shape's min r/c
                )

        current_y += slot_h + pad

    return preview_rects_screen


# === Placement/Hover Preview Rendering ===


def draw_placement_preview(
    surface: pygame.Surface,
    shape: "Shape",  # Use Shape from structs
    r: int,
    c: int,
    is_valid: bool,  # Caller determines validity
    config: "EnvConfig",
) -> None:
    """Draws a semi-transparent shape snapped to the grid."""
    if not shape or not shape.triangles:
        return

    # Use coord_mapper to get rendering parameters for the target surface (grid)
    cw, ch, ox, oy = coord_mapper._calculate_render_params(
        surface.get_width(), surface.get_height(), config
    )
    if cw <= 0 or ch <= 0:
        return

    color = list(shape.color) + [150]  # Add alpha channel
    # Use a different color/alpha if invalid? For now, just use shape color + alpha
    # color = colors.PLACEMENT_VALID_COLOR if is_valid else colors.PLACEMENT_INVALID_COLOR

    temp_surface = pygame.Surface(surface.get_size(), pygame.SRCALPHA)
    temp_surface.fill((0, 0, 0, 0))  # Transparent background

    for dr, dc, is_up in shape.triangles:
        tri_r, tri_c = r + dr, c + dc
        # Create temporary triangle for geometry calculation
        temp_tri = Triangle(tri_r, tri_c, is_up)  # Use Triangle from structs
        pts = temp_tri.get_points(ox, oy, cw, ch)
        pygame.draw.polygon(temp_surface, color, pts)

    surface.blit(temp_surface, (0, 0))


def draw_floating_preview(
    surface: pygame.Surface,
    shape: "Shape",  # Use Shape from structs
    screen_pos: Tuple[int, int],  # Mouse position relative to the surface
    config: "EnvConfig",
) -> None:
    """Draws a semi-transparent shape floating at the screen position."""
    if not shape or not shape.triangles:
        return

    # Estimate cell size based on typical grid rendering (might not be perfect)
    # A fixed size might be better here? Or pass VisConfig?
    # Let's use a fixed moderate size for floating preview
    cell_size = 20.0
    color = list(shape.color) + [100]  # More transparent

    temp_surface = pygame.Surface(surface.get_size(), pygame.SRCALPHA)
    temp_surface.fill((0, 0, 0, 0))

    # Calculate relative center of the shape to draw around mouse pos
    min_r, min_c, max_r, max_c = shape.bbox()
    center_r = (min_r + max_r) / 2.0
    center_c = (min_c + max_c) / 2.0

    for dr, dc, is_up in shape.triangles:
        # Calculate position relative to mouse cursor, centered
        # Adjusting for triangle geometry (0.75 factor)
        pt_x = screen_pos[0] + (dc - center_c) * (cell_size * 0.75)
        pt_y = screen_pos[1] + (dr - center_r) * cell_size

        # Create temporary triangle for geometry calculation at this floating position
        # The r/c here are just for orientation, position is handled by pt_x, pt_y
        temp_tri = Triangle(0, 0, is_up)  # Use Triangle from structs
        # Get points relative to (0,0) with cell_size, then offset by pt_x, pt_y
        pts = [
            (px + pt_x, py + pt_y)
            for px, py in temp_tri.get_points(0, 0, cell_size, cell_size)
        ]
        pygame.draw.polygon(temp_surface, color, pts)

    surface.blit(temp_surface, (0, 0))


File: src/visualization/drawing/__init__.py
# File: src/visualization/drawing/__init__.py
# This file is intentionally left empty to avoid complex import cycles
# during package initialization. Modules should import directly from
# specific files like drawing.grid, drawing.shapes, etc.
pass


File: src/visualization/drawing/README.md
# File: src/visualization/drawing/README.md
# Visualization Drawing Submodule (`src.visualization.drawing`)

## Purpose and Architecture

This submodule contains specialized functions responsible for drawing specific visual elements of the game onto Pygame surfaces. These functions are typically called by the core renderers (`Visualizer`, `GameRenderer`) in `src.visualization.core`. Separating drawing logic makes the core renderers cleaner and promotes reusability of drawing code.

-   **`grid.py`:** Functions for drawing the grid background (`draw_grid_background`) and the individual triangles within it, colored based on occupancy or emptiness (`draw_grid_triangles`). Uses `Triangle` from `src.structs`.
-   **`shapes.py`:** Contains `draw_shape`, a function to render a given `Shape` object at a specific location on a surface (used primarily for previews). Uses `Shape` and `Triangle` from `src.structs`.
-   **`previews.py`:** Handles rendering related to shape previews:
    -   `render_previews`: Draws the dedicated preview area, including borders and the shapes within their slots, handling selection highlights. Uses `Shape` from `src.structs`.
    -   `draw_placement_preview`: Draws a semi-transparent version of a shape snapped to the grid, indicating a potential placement location (used in play mode hover). Uses `Shape` and `Triangle` from `src.structs`.
    -   `draw_floating_preview`: Draws a semi-transparent shape directly under the mouse cursor when hovering over the grid but not snapped (used in play mode hover). Uses `Shape` and `Triangle` from `src.structs`.
-   **`hud.py`:** `render_hud` draws Heads-Up Display elements like the game score, help text, and optional training statistics onto the main screen surface.
-   **`highlight.py`:** `draw_debug_highlight` draws a distinct border around a specific triangle, used for visual feedback in debug mode. Uses `Triangle` from `src.structs`.
-   **`utils.py`:** Contains general drawing utility functions (currently empty).

## Exposed Interfaces

-   **Grid Drawing:**
    -   `draw_grid_background(surface: pygame.Surface, bg_color: tuple)`
    -   `draw_grid_triangles(surface: pygame.Surface, grid_data: GridData, config: EnvConfig)`
-   **Shape Drawing:**
    -   `draw_shape(surface: pygame.Surface, shape: Shape, topleft: Tuple[int, int], cell_size: float, is_selected: bool = False)` (Note: Signature might vary slightly based on implementation details)
-   **Preview Drawing:**
    -   `render_previews(surface: pygame.Surface, game_state: GameState, area_topleft: Tuple[int, int], mode: str, env_config: EnvConfig, vis_config: VisConfig) -> Dict[int, pygame.Rect]`
    -   `draw_placement_preview(surface: pygame.Surface, shape: Shape, r: int, c: int, is_valid: bool, config: EnvConfig)`
    -   `draw_floating_preview(surface: pygame.Surface, shape: Shape, screen_pos: Tuple[int, int], config: EnvConfig)`
-   **HUD Drawing:**
    -   `render_hud(surface: pygame.Surface, game_state: GameState, mode: str, fonts: Dict[str, Optional[pygame.font.Font]], display_stats: Optional[Dict[str, Any]] = None)`
-   **Highlight Drawing:**
    -   `draw_debug_highlight(surface: pygame.Surface, r: int, c: int, config: EnvConfig)`
-   **Utility Functions:**
    -   (Currently empty or contains other drawing-specific utils)

## Dependencies

-   **`src.visualization.core`**:
    -   `colors`: Used extensively for drawing colors.
    -   `coord_mapper`: Used internally (e.g., by `draw_placement_preview`) or relies on its calculations passed in.
-   **`src.config`**:
    -   `EnvConfig`, `VisConfig`: Provide dimensions, padding, etc., needed for drawing calculations.
-   **`src.environment`**:
    -   `GameState`, `GridData`: Provide the data to be drawn.
-   **`src.structs`**:
    -   Uses `Triangle`, `Shape`.
-   **`pygame`**:
    -   The core library used for all drawing operations (`pygame.draw.polygon`, `surface.fill`, `surface.blit`, etc.) and font rendering.
-   **Standard Libraries:** `typing`, `logging`, `math`.

---

**Note:** Please keep this README updated when adding new drawing functions, modifying existing ones, or changing their dependencies on configuration or environment data structures. Accurate documentation is crucial for maintainability.

File: src/visualization/drawing/utils.py
# File: src/visualization/drawing/utils.py
# (Content of is_point_in_polygon moved to src/utils/geometry.py)

# This file might be empty now, or contain other drawing-specific utils.
# If it's empty, it can potentially be removed, but let's keep it for now
# in case other drawing utils are added later.

# Example placeholder if empty:
pass


File: src/visualization/drawing/hud.py
# File: src/visualization/drawing/hud.py
import pygame
from typing import TYPE_CHECKING, Dict, Optional, Any

# Use relative imports within visualization package
from ..core import colors

if TYPE_CHECKING:
    from ...environment import GameState  # Correct path


def render_hud(
    surface: pygame.Surface,
    game_state: "GameState",  # Can be a representative state or dummy
    mode: str,
    fonts: Dict[str, Optional[pygame.font.Font]],
    display_stats: Optional[Dict[str, Any]] = None,  # Should contain global stats now
) -> None:
    """
    Renders global information (like step count, worker status) at the bottom.
    Individual game scores are not shown here anymore.
    """
    screen_w, screen_h = surface.get_size()
    help_font = fonts.get("help")
    stats_font = fonts.get("help")  # Use same font for stats

    # --- Bottom HUD: Global Stats and Help Text ---
    bottom_y = screen_h - 10  # Bottom padding

    # Global Stats (bottom-left)
    stats_rect = None  # Initialize stats_rect
    if stats_font and display_stats:
        stats_items = []
        # Get global step from the trainer's progress bar if available
        train_progress = display_stats.get("train_progress")
        global_step = (
            train_progress.current_steps
            if train_progress
            else display_stats.get("global_step", "?")
        )

        episodes = display_stats.get("total_episodes", "?")
        sims = display_stats.get("total_simulations", "?")
        num_workers = display_stats.get("num_workers", "?")
        pending_tasks = display_stats.get("pending_tasks", "?")

        stats_items.append(f"Step: {global_step}")
        stats_items.append(f"Episodes: {episodes}")
        if isinstance(sims, (int, float)):
            sims_str = (
                f"{sims/1e6:.2f}M"
                if sims >= 1e6
                else (f"{sims/1e3:.1f}k" if sims >= 1000 else str(int(sims)))
            )
            stats_items.append(f"Sims: {sims_str}")
        stats_items.append(f"Workers: {pending_tasks}/{num_workers} busy")

        stats_text = " | ".join(stats_items)
        stats_surf = stats_font.render(stats_text, True, colors.CYAN)
        stats_rect = stats_surf.get_rect(bottomleft=(15, bottom_y))
        surface.blit(stats_surf, stats_rect)

    # Help Text (bottom-right)
    if help_font:
        help_text = "[ESC] Quit"
        # Add mode-specific help if needed, but keep it simple
        # if mode == "training_visual":
        #     help_text += " | Training Visual Mode"

        help_surf = help_font.render(help_text, True, colors.LIGHT_GRAY)
        help_rect = help_surf.get_rect(bottomright=(screen_w - 15, bottom_y))
        # Adjust position if overlapping with stats
        if stats_rect and stats_rect.right > help_rect.left - 10:
            help_rect.right = screen_w - 15  # Keep right aligned
            help_rect.bottom = (
                bottom_y - stats_rect.height - 5
            )  # Move up if stats are long

        surface.blit(help_surf, help_rect)


File: src/visualization/drawing/highlight.py
# File: src/visualization/drawing/highlight.py
import pygame
from typing import TYPE_CHECKING

# Use relative imports within visualization package
from ..core import colors, coord_mapper

# Import Triangle from the new structs module
from src.structs import Triangle

if TYPE_CHECKING:
    from src.config import EnvConfig


def draw_debug_highlight(
    surface: pygame.Surface, r: int, c: int, config: "EnvConfig"
) -> None:
    """Highlights a specific triangle border for debugging."""
    if surface.get_width() <= 0 or surface.get_height() <= 0:
        return

    cw, ch, ox, oy = coord_mapper._calculate_render_params(
        surface.get_width(), surface.get_height(), config
    )
    if cw <= 0 or ch <= 0:
        return

    is_up = (r + c) % 2 == 0
    temp_tri = Triangle(
        r, c, is_up
    )  # Temp triangle for geometry (uses Triangle from structs)
    pts = temp_tri.get_points(ox, oy, cw, ch)

    pygame.draw.polygon(surface, colors.DEBUG_TOGGLE_COLOR, pts, 3)  # Thicker border


File: src/visualization/core/game_renderer.py
# File: src/visualization/core/game_renderer.py
import pygame
import logging
import math
import ray # Import Ray
from typing import TYPE_CHECKING, Dict, Optional, Any

# Use relative imports within visualization package
from . import colors, layout, coord_mapper
from ..ui import ProgressBar

# Import drawing functions/modules directly from their files
from ..drawing import grid as grid_drawing
from ..drawing import previews as preview_drawing
from ..drawing import hud as hud_drawing

# Import Plotter and StatsCollectorData
from src.stats import Plotter, StatsCollectorData # Keep Plotter import
# Removed: from src.utils.types import StatsCollectorData

# Import GameState for type hinting AND for creating a default instance
from src.environment import GameState

if TYPE_CHECKING:
    from ...config import VisConfig, EnvConfig
    from src.stats import StatsCollectorActor # Import actor type hint

logger = logging.getLogger(__name__)


class GameRenderer:
    """
    Renders multiple GameStates (from workers) in a top grid area,
    and statistics plots/progress bars in a bottom area.
    Fetches plot data from StatsCollectorActor.
    """

    def __init__(
        self,
        screen: pygame.Surface,
        vis_config: "VisConfig",
        env_config: "EnvConfig",
        fonts: Dict[str, Optional[pygame.font.Font]],
        stats_collector_actor: Optional["StatsCollectorActor"] = None, # Add actor handle
    ):
        self.screen = screen
        self.vis_config = vis_config
        self.env_config = env_config
        self.fonts = fonts
        self.stats_collector_actor = stats_collector_actor # Store handle
        self.layout_rects: Optional[Dict[str, pygame.Rect]] = None
        self.worker_sub_rects: Dict[int, Dict[str, pygame.Rect]] = {}
        self.last_worker_grid_size = (0, 0)
        self.last_num_workers = 0

        # Instantiate Plotter locally - it will be used within render
        # Its internal caching will handle performance.
        self.plotter = Plotter(plot_update_interval=0.2) # Faster update interval for plotter cache check

        # Progress bar layout constants
        self.progress_bar_height_per_bar = 45
        self.num_progress_bars = 2
        self.progress_bar_spacing = 5
        self.progress_bars_total_height = (self.progress_bar_height_per_bar + self.progress_bar_spacing) * self.num_progress_bars

    def _calculate_layout(self):
        """Calculates or retrieves the main layout areas."""
        current_w, current_h = self.screen.get_size()
        needs_recalc = True
        if self.layout_rects:
            if self.layout_rects.get("screen_size") == (current_w, current_h):
                needs_recalc = False

        if needs_recalc:
            required_bottom_margin_for_stats = self.progress_bars_total_height + self.vis_config.PADDING
            self.layout_rects = layout.calculate_layout(current_w, current_h, self.vis_config, bottom_margin=required_bottom_margin_for_stats)
            self.layout_rects["screen_size"] = (current_w, current_h)
            logger.debug(f"Recalculated main layout for screen {current_w}x{current_h}: {self.layout_rects}")
            self.last_worker_grid_size = (0, 0)
            self.worker_sub_rects = {}

        return self.layout_rects

    def _calculate_worker_sub_layout(self, worker_grid_area: pygame.Rect, num_workers: int):
        """Calculates the grid layout within the worker_grid_area."""
        area_w, area_h = worker_grid_area.size
        if (area_w, area_h) == self.last_worker_grid_size and num_workers == self.last_num_workers:
            return

        logger.debug(f"Recalculating worker sub-layout for {num_workers} workers in area {area_w}x{area_h}")
        self.last_worker_grid_size = (area_w, area_h)
        self.last_num_workers = num_workers
        self.worker_sub_rects = {}
        pad = 5

        if area_h <= 10 or area_w <= 10 or num_workers <= 0:
            logger.warning(f"Worker grid area too small ({area_w}x{area_h}) or no workers ({num_workers}). Cannot calculate sub-layout.")
            return

        cols = math.ceil(math.sqrt(num_workers))
        rows = math.ceil(num_workers / cols)
        cell_w = (area_w - (cols - 1) * pad) / cols
        cell_h = (area_h - (rows - 1) * pad) / rows
        min_cell_w, min_cell_h = 80, 60
        if cell_w < min_cell_w or cell_h < min_cell_h:
            logger.warning(f"Worker grid cells too small ({cell_w:.1f}x{cell_h:.1f}). May impact rendering.")

        logger.info(f"Calculated worker sub-layout: {rows}x{cols} for {num_workers} workers. Cell: {cell_w:.1f}x{cell_h:.1f}")

        for worker_id in range(num_workers):
            row = worker_id // cols
            col = worker_id % cols
            worker_area_x = worker_grid_area.left + col * (cell_w + pad)
            worker_area_y = worker_grid_area.top + row * (cell_h + pad)
            worker_area_w = cell_w
            worker_area_h = cell_h
            preview_w = max(10, min(worker_area_w * 0.2, 60))
            grid_w = worker_area_w - preview_w - pad
            grid_w = max(0, grid_w)
            preview_w = max(0, preview_w)
            worker_area_h = max(0, worker_area_h)
            grid_rect = pygame.Rect(worker_area_x, worker_area_y, grid_w, worker_area_h)
            preview_rect = pygame.Rect(grid_rect.right + pad, worker_area_y, preview_w, worker_area_h)
            worker_rect = pygame.Rect(worker_area_x, worker_area_y, worker_area_w, worker_area_h)

            self.worker_sub_rects[worker_id] = {
                "area": worker_rect.clip(worker_grid_area),
                "grid": grid_rect.clip(worker_grid_area),
                "preview": preview_rect.clip(worker_grid_area),
            }
            logger.debug(f"  Worker {worker_id} layout: Area={self.worker_sub_rects[worker_id]['area']}, Grid={self.worker_sub_rects[worker_id]['grid']}, Preview={self.worker_sub_rects[worker_id]['preview']}")

    def render(
        self,
        worker_states: Dict[int, "GameState"],
        global_stats: Optional[Dict[str, Any]] = None,
    ):
        """
        Renders worker states, plots (fetching data from actor), and progress bars.
        """
        layout_rects = self._calculate_layout()
        if not layout_rects:
            logger.error("Main layout calculation failed.")
            return

        worker_grid_area = layout_rects.get("worker_grid")
        stats_area = layout_rects.get("stats_area")
        plots_rect = layout_rects.get("plots")

        # --- Render Worker Grid Area ---
        if worker_grid_area and worker_grid_area.width > 0 and worker_grid_area.height > 0:
            num_workers_to_display = len(worker_states)
            self._calculate_worker_sub_layout(worker_grid_area, num_workers_to_display)
            pygame.draw.rect(self.screen, colors.DARK_GRAY, worker_grid_area)

            for worker_id, game_state in worker_states.items():
                if worker_id not in self.worker_sub_rects:
                    logger.warning(f"Skipping render for worker {worker_id}: No sub-layout calculated.")
                    continue
                sub_layout = self.worker_sub_rects[worker_id]
                area_rect, grid_rect, preview_rect = sub_layout["area"], sub_layout["grid"], sub_layout["preview"]
                pygame.draw.rect(self.screen, colors.GRAY, area_rect, 1)
                logger.debug(f"Rendering Worker {worker_id} in area {area_rect}")

                # Render Grid
                if grid_rect.width > 0 and grid_rect.height > 0:
                    try:
                        grid_surf = self.screen.subsurface(grid_rect)
                        bg_color = colors.GRID_BG_GAME_OVER if game_state.is_over() else colors.GRID_BG_DEFAULT
                        grid_drawing.draw_grid_background(grid_surf, bg_color)
                        grid_drawing.draw_grid_triangles(grid_surf, game_state.grid_data, self.env_config)
                        id_font = self.fonts.get("help")
                        if id_font:
                            id_surf = id_font.render(f"W{worker_id}", True, colors.LIGHT_GRAY)
                            grid_surf.blit(id_surf, (3, 3))
                    except ValueError as e:
                        if "subsurface rectangle is invalid" not in str(e): logger.error(f"Error creating grid subsurface for W{worker_id} ({grid_rect}): {e}")
                        pygame.draw.rect(self.screen, colors.RED, grid_rect, 1)

                # Render Previews
                if preview_rect.width > 0 and preview_rect.height > 0:
                    try:
                        preview_surf = self.screen.subsurface(preview_rect)
                        _ = preview_drawing.render_previews(preview_surf, game_state, preview_rect.topleft, "training_visual", self.env_config, self.vis_config)
                    except ValueError as e:
                        if "subsurface rectangle is invalid" not in str(e): logger.error(f"Error creating preview subsurface for W{worker_id} ({preview_rect}): {e}")
                        pygame.draw.rect(self.screen, colors.RED, preview_rect, 1)
        else:
            logger.warning("Worker grid area not available or too small.")

        # --- Render Stats Area (Plots and Progress Bars) ---
        if stats_area and global_stats:
            pygame.draw.rect(self.screen, colors.DARK_GRAY, stats_area) # Background

            # --- Render Plots ---
            plot_surface = None
            if plots_rect and plots_rect.width > 0 and plots_rect.height > 0:
                # Fetch data from StatsCollectorActor if available
                stats_data_for_plot: Optional[StatsCollectorData] = global_stats.get("stats_data") # Get data passed via queue

                if stats_data_for_plot is not None:
                    # Use the local plotter instance to get the surface
                    plot_surface = self.plotter.get_plot_surface(
                        stats_data_for_plot,
                        plots_rect.width,
                        plots_rect.height,
                    )

                if plot_surface:
                    self.screen.blit(plot_surface, plots_rect.topleft)
                else:
                    # Draw placeholder if no surface generated (no data or error)
                    pygame.draw.rect(self.screen, colors.DARK_GRAY, plots_rect)
                    plot_font = self.fonts.get("help")
                    if plot_font:
                        wait_surf = plot_font.render("Plot Area (Waiting for data...)", True, colors.LIGHT_GRAY)
                        wait_rect = wait_surf.get_rect(center=plots_rect.center)
                        self.screen.blit(wait_surf, wait_rect)
                    pygame.draw.rect(self.screen, colors.GRAY, plots_rect, 1)

            # --- Render Progress Bars below plots ---
            progress_bar_font = self.fonts.get("help")
            if progress_bar_font and plots_rect:
                bar_y = plots_rect.bottom + self.progress_bar_spacing
                bar_width = stats_area.width
                bar_x = stats_area.left
                bar_height = self.progress_bar_height_per_bar

                if bar_y + bar_height <= stats_area.bottom:
                    train_progress = global_stats.get("train_progress")
                    if isinstance(train_progress, ProgressBar):
                        train_progress.render(self.screen, (bar_x, bar_y), bar_width, bar_height, progress_bar_font, bar_color=colors.GREEN)
                        bar_y += bar_height + self.progress_bar_spacing
                    else: logger.debug("Train progress bar data not available.")

                    if bar_y + bar_height <= stats_area.bottom:
                        buffer_progress = global_stats.get("buffer_progress")
                        if isinstance(buffer_progress, ProgressBar):
                            buffer_progress.render(self.screen, (bar_x, bar_y), bar_width, bar_height, progress_bar_font, bar_color=colors.ORANGE)
                        else: logger.debug("Buffer progress bar data not available.")
                    else: logger.warning("Not enough vertical space in stats_area for the second progress bar.")
                else: logger.warning("Not enough vertical space in stats_area for the first progress bar.")

        # --- Render Global HUD (at the very bottom) ---
        representative_gs = worker_states.get(0) or next(iter(worker_states.values()), GameState(self.env_config))
        hud_drawing.render_hud(self.screen, representative_gs, "training_visual", self.fonts, global_stats)


File: src/visualization/core/layout.py
# File: src/visualization/core/layout.py
import pygame
from typing import Tuple, Dict, Optional

# Use VisConfig from central config
from ...config import VisConfig


def calculate_layout(
    screen_width: int,
    screen_height: int,
    vis_config: VisConfig,
    bottom_margin: int = 0,  # Margin needed below plots (for progress bars)
) -> Dict[str, pygame.Rect]:
    """
    Calculates layout rectangles splitting screen between worker grid and stats area.
    Aiming for ~60% height for worker grid, ~40% for stats (plots + progress + HUD).
    """
    sw, sh = screen_width, screen_height
    pad = vis_config.PADDING

    # Calculate total available height excluding top/bottom padding and HUD
    hud_h = vis_config.HUD_HEIGHT
    total_available_h = max(0, sh - hud_h - 2 * pad)

    # Target height for the top area (worker grid) ~60%
    top_area_h = int(total_available_h * 0.60)  # Increased from 0.30
    top_area_w = sw - 2 * pad

    worker_grid_rect = pygame.Rect(pad, pad, top_area_w, top_area_h)

    # Position stats area (plots + progress bars) below worker grid
    stats_area_y = worker_grid_rect.bottom + pad
    stats_area_w = sw - 2 * pad

    # Calculate stats area height: remaining space
    stats_area_h = max(0, sh - stats_area_y - pad - hud_h)
    stats_area_rect = pygame.Rect(pad, stats_area_y, stats_area_w, stats_area_h)

    # Subdivide stats area for plots and progress bars
    # Progress bars take 'bottom_margin' height from the bottom of the stats area
    plot_h = max(0, stats_area_h - bottom_margin - pad)  # Space above progress bars
    plot_rect = pygame.Rect(pad, stats_area_y, stats_area_w, plot_h)

    # Clip rectangles to screen bounds just in case
    screen_rect = pygame.Rect(0, 0, sw, sh)
    worker_grid_rect = worker_grid_rect.clip(screen_rect)
    stats_area_rect = stats_area_rect.clip(screen_rect)
    plot_rect = plot_rect.clip(screen_rect)

    return {
        "worker_grid": worker_grid_rect,
        "stats_area": stats_area_rect,
        "plots": plot_rect,
    }


File: src/visualization/core/__init__.py


File: src/visualization/core/README.md
# File: src/visualization/core/README.md
# Visualization Core Submodule (`src.visualization.core`)

## Purpose and Architecture

This submodule contains the central classes and foundational elements for the visualization system. It orchestrates rendering, manages layout and coordinate systems, and defines core visual properties like colors and fonts.

-   **Render Orchestration:**
    -   `Visualizer`: The main class for rendering in interactive modes ("play", "debug"). It maintains the Pygame screen, calculates layout using `layout.py`, manages cached preview area rectangles, and calls appropriate drawing functions from `src.visualization.drawing` based on the `GameState` and interaction mode. It handles visual feedback like hover previews and selection highlights.
    -   `GameRenderer`: **Adapted renderer** specifically for the non-interactive training visualization mode. It now uses `layout.py` to divide the screen into a worker game grid area and a statistics area. It renders multiple worker `GameState` objects in the top grid and displays statistics plots (using `src.stats.Plotter`) and progress bars in the bottom area. It takes a dictionary mapping worker IDs to `GameState` objects (`Dict[int, GameState]`) and a dictionary of global statistics.
-   **Layout Management:**
    -   `layout.py`: Contains the `calculate_layout` function, which determines the size and position of the main UI areas (worker grid, stats area, plots) based on the screen dimensions and `VisConfig`.
-   **Coordinate System:**
    -   `coord_mapper.py`: Provides essential mapping functions:
        -   `_calculate_render_params`: Internal helper to get scaling and offset for grid rendering.
        -   `get_grid_coords_from_screen`: Converts mouse/screen coordinates into logical grid (row, column) coordinates, handling the triangular grid geometry.
        -   `get_preview_index_from_screen`: Converts mouse/screen coordinates into the index of the shape preview slot being pointed at.
-   **Visual Properties:**
    -   `colors.py`: Defines a centralized palette of named color constants (RGB tuples) used throughout the visualization drawing functions.
    -   `fonts.py`: Contains the `load_fonts` function to load and manage Pygame font objects based on sizes defined in `VisConfig`.

## Exposed Interfaces

-   **Classes:**
    -   `Visualizer`: Renderer for interactive modes.
        -   `__init__(...)`
        -   `render(game_state: GameState, mode: str)`
        -   `ensure_layout() -> Dict[str, pygame.Rect]`
        -   `screen`: Public attribute (Pygame Surface).
        -   `preview_rects`: Public attribute (cached preview area rects).
    -   `GameRenderer`: Renderer for combined multi-game/stats training visualization.
        -   `__init__(...)`
        -   `render(worker_states: Dict[int, GameState], global_stats: Optional[Dict[str, Any]])`
        -   `screen`: Public attribute (Pygame Surface).
-   **Functions:**
    -   `calculate_layout(screen_width: int, screen_height: int, vis_config: VisConfig, bottom_margin: int) -> Dict[str, pygame.Rect]`
    -   `load_fonts() -> Dict[str, Optional[pygame.font.Font]]`
    -   `get_grid_coords_from_screen(screen_pos: Tuple[int, int], grid_area_rect: pygame.Rect, config: EnvConfig) -> Optional[Tuple[int, int]]`
    -   `get_preview_index_from_screen(screen_pos: Tuple[int, int], preview_rects: Dict[int, pygame.Rect]) -> Optional[int]`
-   **Modules:**
    -   `colors`: Provides color constants (e.g., `colors.RED`).

## Dependencies

-   **`src.config`**:
    -   `VisConfig`, `EnvConfig`: Used for layout, fonts, coordinate mapping.
-   **`src.environment`**:
    -   `GameState`, `GridData`: Needed for rendering and coordinate mapping.
-   **`src.stats`**:
    -   `Plotter`: Used by `GameRenderer`.
-   **`src.utils`**:
    -   `types`: `StatsCollectorData`.
-   **`src.visualization.drawing`**:
    -   Drawing functions (`grid`, `previews`, `hud`, `highlight`) are called by `Visualizer` and `GameRenderer`.
-   **`src.visualization.ui`**:
    -   `ProgressBar`: Used by `GameRenderer`.
-   **`pygame`**:
    -   Used for surfaces, rectangles, fonts, display management.
-   **Standard Libraries:** `typing`, `logging`, `math`.

---

**Note:** Please keep this README updated when changing the core rendering logic, layout calculations, coordinate mapping, or the interfaces of `Visualizer` or `GameRenderer`. Accurate documentation is crucial for maintainability.

File: src/visualization/core/coord_mapper.py
# File: src/visualization/core/coord_mapper.py
import math
from typing import Tuple, Optional, List, Dict
import pygame

# Use configs and types from respective modules
from ...config import EnvConfig
from ...environment.grid.triangle import Triangle

# Import the geometry utility from the central utils package
from ...utils.geometry import is_point_in_polygon

# Import drawing utils directly (even if empty, for structure)
from ..drawing import utils as drawing_utils


def _calculate_render_params(
    width: int, height: int, config: EnvConfig
) -> Tuple[float, float, float, float]:
    """Calculates scale (cw, ch) and offset (ox, oy) for rendering the grid."""
    rows, cols = config.ROWS, config.COLS
    cols_eff = cols * 0.75 + 0.25 if cols > 0 else 1
    scale_w = width / cols_eff if cols_eff > 0 else 1
    scale_h = height / rows if rows > 0 else 1
    scale = max(1.0, min(scale_w, scale_h))
    cell_size = scale
    grid_w_px = cols_eff * cell_size
    grid_h_px = rows * cell_size
    offset_x = (width - grid_w_px) / 2
    offset_y = (height - grid_h_px) / 2
    return cell_size, cell_size, offset_x, offset_y


def get_grid_coords_from_screen(
    screen_pos: Tuple[int, int], grid_area_rect: pygame.Rect, config: EnvConfig
) -> Optional[Tuple[int, int]]:
    """Maps screen coordinates (relative to screen) to grid row/column."""
    if not grid_area_rect or not grid_area_rect.collidepoint(screen_pos):
        return None

    local_x = screen_pos[0] - grid_area_rect.left
    local_y = screen_pos[1] - grid_area_rect.top
    cw, ch, ox, oy = _calculate_render_params(
        grid_area_rect.width, grid_area_rect.height, config
    )
    if cw <= 0 or ch <= 0:
        return None

    row = int((local_y - oy) / ch) if ch > 0 else -1
    approx_col_center_index = (local_x - ox - cw / 4) / (cw * 0.75) if cw > 0 else -1
    col = int(round(approx_col_center_index))

    for r_check in [row, row - 1, row + 1]:
        if not (0 <= r_check < config.ROWS):
            continue
        for c_check in [col, col - 1, col + 1]:
            if not (0 <= c_check < config.COLS):
                continue
            is_up = (r_check + c_check) % 2 == 0
            temp_tri = Triangle(r_check, c_check, is_up)
            pts = temp_tri.get_points(ox, oy, cw, ch)
            if is_point_in_polygon((local_x, local_y), pts):
                return r_check, c_check

    if 0 <= row < config.ROWS and 0 <= col < config.COLS:
        return row, col
    return None


def get_preview_index_from_screen(
    screen_pos: Tuple[int, int], preview_rects: Dict[int, pygame.Rect]
) -> Optional[int]:
    """Maps screen coordinates to a shape preview index."""
    if not preview_rects:
        return None
    for idx, rect in preview_rects.items():
        if rect and rect.collidepoint(screen_pos):
            return idx
    return None


File: src/visualization/core/fonts.py
# File: src/visualization/core/fonts.py
import pygame
import logging
from typing import Dict, Optional

logger = logging.getLogger(__name__)

DEFAULT_FONT_NAME = None  # Use Pygame default
# Fallback font if default fails (common system font)
FALLBACK_FONT_NAME = "arial,freesans"


def load_single_font(name: Optional[str], size: int) -> Optional[pygame.font.Font]:
    """Loads a single font, handling potential errors."""
    try:
        font = pygame.font.SysFont(name, size)
        # logger.info(f"Loaded font: {name or 'Default'} size {size}")
        return font
    except Exception as e:
        logger.error(f"Error loading font '{name}' size {size}: {e}")
        # Try fallback if primary failed
        if name != FALLBACK_FONT_NAME:
            logger.warning(f"Attempting fallback font: {FALLBACK_FONT_NAME}")
            try:
                font = pygame.font.SysFont(FALLBACK_FONT_NAME, size)
                logger.info(f"Loaded fallback font: {FALLBACK_FONT_NAME} size {size}")
                return font
            except Exception as e_fallback:
                logger.error(f"Fallback font failed: {e_fallback}")
                return None
        return None


def load_fonts(
    font_sizes: Optional[Dict[str, int]] = None,
) -> Dict[str, Optional[pygame.font.Font]]:
    """Loads standard game fonts."""
    if font_sizes is None:
        # Default sizes if none provided
        font_sizes = {
            "ui": 24,
            "score": 30,
            "help": 18,
            "title": 48,  # Example addition
        }

    fonts: Dict[str, Optional[pygame.font.Font]] = {}
    required_fonts = ["score", "help"]  # Ensure these exist

    logger.info("Loading fonts...")
    for name, size in font_sizes.items():
        fonts[name] = load_single_font(DEFAULT_FONT_NAME, size)

    # Check if essential fonts loaded
    for name in required_fonts:
        if fonts.get(name) is None:
            logger.critical(
                f"Essential font '{name}' failed to load. Text rendering will be affected."
            )
            # Depending on severity, could raise an error here

    return fonts


File: src/visualization/core/visualizer.py
# File: src/visualization/core/visualizer.py
import pygame
import logging
from typing import TYPE_CHECKING, Dict, Optional

# Use relative imports within the visualization package
from . import colors, layout, coord_mapper

# Import drawing functions/modules directly from their files
from ..drawing import grid as grid_drawing
from ..drawing import previews as preview_drawing
from ..drawing import hud as hud_drawing
from ..drawing import highlight as highlight_drawing

# Type hinting imports from other top-level packages
if TYPE_CHECKING:
    from src.config import VisConfig, EnvConfig
    from src.environment.core.game_state import GameState

logger = logging.getLogger(__name__)


class Visualizer:
    """Orchestrates rendering of the game state for interactive modes."""

    def __init__(
        self,
        screen: pygame.Surface,
        vis_config: "VisConfig",
        env_config: "EnvConfig",
        fonts: Dict[str, Optional[pygame.font.Font]],
    ):
        self.screen = screen
        self.vis_config = vis_config
        self.env_config = env_config
        self.fonts = fonts
        self.layout_rects: Optional[Dict[str, pygame.Rect]] = None
        # Cache for preview area rects (mapping index to screen Rect)
        self.preview_rects: Dict[int, pygame.Rect] = {}
        self.ensure_layout()

    def ensure_layout(self) -> Dict[str, pygame.Rect]:
        """Returns cached layout or calculates it if needed."""
        current_w, current_h = self.screen.get_size()
        layout_w, layout_h = 0, 0
        if self.layout_rects:
            # Estimate expected size based on current layout
            grid_r = self.layout_rects.get("grid")
            preview_r = self.layout_rects.get("preview")
            hud_h = self.vis_config.HUD_HEIGHT
            pad = self.vis_config.PADDING
            if grid_r and preview_r:
                layout_w = grid_r.width + preview_r.width + 3 * pad
                layout_h = max(grid_r.height, preview_r.height) + 2 * pad + hud_h
            elif grid_r:
                layout_w = grid_r.width + 2 * pad
                layout_h = grid_r.height + 2 * pad + hud_h

        # Recalculate if layout is missing or screen size mismatch significantly
        if self.layout_rects is None or layout_w != current_w or layout_h != current_h:
            self.layout_rects = layout.calculate_layout(
                current_w, current_h, self.vis_config
            )
            logger.info(f"Recalculated layout: {self.layout_rects}")
            self.preview_rects = {}  # Clear preview cache on layout change
        return self.layout_rects

    def render(self, game_state: "GameState", mode: str):
        """Renders the entire game visualization for interactive modes."""
        self.screen.fill(colors.GRID_BG_DEFAULT)
        layout_rects = self.ensure_layout()
        grid_rect = layout_rects.get("grid")
        preview_rect = layout_rects.get("preview")

        # --- Render Grid Area ---
        if grid_rect and grid_rect.width > 0 and grid_rect.height > 0:
            try:
                grid_surf = self.screen.subsurface(grid_rect)
                self._render_grid_area(grid_surf, game_state, mode, grid_rect)
            except ValueError as e:
                logger.error(f"Error creating grid subsurface ({grid_rect}): {e}")
                pygame.draw.rect(self.screen, colors.RED, grid_rect, 1)

        # --- Render Preview Area ---
        if preview_rect and preview_rect.width > 0 and preview_rect.height > 0:
            try:
                preview_surf = self.screen.subsurface(preview_rect)
                self._render_preview_area(preview_surf, game_state, mode, preview_rect)
            except ValueError as e:
                logger.error(f"Error creating preview subsurface ({preview_rect}): {e}")
                pygame.draw.rect(self.screen, colors.RED, preview_rect, 1)

        # --- Render HUD (Pass None for display_stats in interactive mode) ---
        hud_drawing.render_hud(
            self.screen, game_state, mode, self.fonts, display_stats=None
        )

    def _render_grid_area(
        self,
        grid_surf: pygame.Surface,
        game_state: "GameState",
        mode: str,
        grid_rect: pygame.Rect,
    ):
        """Renders the main game grid and overlays onto the provided grid_surf."""
        bg_color = (
            colors.GRID_BG_GAME_OVER if game_state.is_over() else colors.GRID_BG_DEFAULT
        )
        grid_drawing.draw_grid_background(grid_surf, bg_color)
        grid_drawing.draw_grid_triangles(
            grid_surf, game_state.grid_data, self.env_config
        )

        if mode == "play" and game_state.demo_selected_shape_idx != -1:
            self._draw_play_previews(grid_surf, game_state, grid_rect)

        if mode == "debug" and game_state.debug_highlight_pos:
            r, c = game_state.debug_highlight_pos
            highlight_drawing.draw_debug_highlight(grid_surf, r, c, self.env_config)

    def _draw_play_previews(
        self, grid_surf: pygame.Surface, game_state: "GameState", grid_rect: pygame.Rect
    ):
        """Draws placement or floating previews in play mode onto grid_surf."""
        shape_idx = game_state.demo_selected_shape_idx
        if not (0 <= shape_idx < len(game_state.shapes)):
            return
        shape = game_state.shapes[shape_idx]
        if not shape:
            return

        snapped_pos = game_state.demo_snapped_position
        if snapped_pos:
            preview_drawing.draw_placement_preview(
                grid_surf,
                shape,
                snapped_pos[0],
                snapped_pos[1],
                is_valid=True,
                config=self.env_config,
            )
        else:
            mouse_screen_pos = pygame.mouse.get_pos()
            if grid_rect.collidepoint(mouse_screen_pos):
                mouse_local_x = mouse_screen_pos[0] - grid_rect.left
                mouse_local_y = mouse_screen_pos[1] - grid_rect.top
                if grid_surf.get_rect().collidepoint(mouse_local_x, mouse_local_y):
                    preview_drawing.draw_floating_preview(
                        grid_surf,
                        shape,
                        (mouse_local_x, mouse_local_y),
                        self.env_config,
                    )

    def _render_preview_area(
        self,
        preview_surf: pygame.Surface,
        game_state: "GameState",
        mode: str,
        preview_rect: pygame.Rect,
    ):
        """Renders the shape preview slots onto preview_surf and caches rects."""
        self.preview_rects = preview_drawing.render_previews(
            preview_surf,
            game_state,
            preview_rect.topleft,
            mode,
            self.env_config,
            self.vis_config,
        )


File: src/visualization/core/colors.py
# File: src/visualization/core/colors.py
"""Centralized color definitions (RGB tuples 0-255)."""

WHITE: tuple[int, int, int] = (255, 255, 255)
BLACK: tuple[int, int, int] = (0, 0, 0)
LIGHT_GRAY: tuple[int, int, int] = (180, 180, 180)
GRAY: tuple[int, int, int] = (100, 100, 100)
DARK_GRAY: tuple[int, int, int] = (40, 40, 40)
RED: tuple[int, int, int] = (220, 40, 40)
DARK_RED: tuple[int, int, int] = (100, 10, 10)
BLUE: tuple[int, int, int] = (60, 60, 220)
YELLOW: tuple[int, int, int] = (230, 230, 40)
GREEN: tuple[int, int, int] = (40, 200, 40)
DARK_GREEN: tuple[int, int, int] = (10, 80, 10)
ORANGE: tuple[int, int, int] = (240, 150, 20)
PURPLE: tuple[int, int, int] = (140, 40, 140)
CYAN: tuple[int, int, int] = (40, 200, 200)
LIGHTG: tuple[int, int, int] = (144, 238, 144)  # Added Light Green

# Google Colors (example)
GOOGLE_COLORS: list[tuple[int, int, int]] = [
    (15, 157, 88),  # Green
    (244, 180, 0),  # Yellow
    (66, 133, 244),  # Blue
    (219, 68, 55),  # Red
]

# Game Specific Visuals
GRID_BG_DEFAULT: tuple[int, int, int] = (20, 20, 30)
GRID_BG_GAME_OVER: tuple[int, int, int] = DARK_RED
GRID_LINE_COLOR: tuple[int, int, int] = GRAY
TRIANGLE_EMPTY_COLOR: tuple[int, int, int] = (60, 60, 70)
PREVIEW_BG: tuple[int, int, int] = (30, 30, 40)
PREVIEW_BORDER: tuple[int, int, int] = GRAY
PREVIEW_SELECTED_BORDER: tuple[int, int, int] = BLUE
PLACEMENT_VALID_COLOR: tuple[int, int, int, int] = (*GREEN, 150)  # RGBA
PLACEMENT_INVALID_COLOR: tuple[int, int, int, int] = (*RED, 100)  # RGBA
DEBUG_TOGGLE_COLOR: tuple[int, int, int] = YELLOW


File: src/nn/__init__.py
# File: src/nn/__init__.py
"""
Neural Network module for the AlphaTriangle agent.
Contains the model definition and a wrapper for inference and training interface.
"""
from .model import AlphaTriangleNet
from .network import NeuralNetwork

__all__ = [
    "AlphaTriangleNet",  # The PyTorch nn.Module class
    "NeuralNetwork",  # Wrapper class providing evaluate() and train() methods
]


File: src/nn/model.py
# File: src/nn/model.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Tuple, Optional
from src.config import ModelConfig, EnvConfig


# Helper function for convolutional layers
def conv_block(
    in_channels, out_channels, kernel_size, stride, padding, use_batch_norm, activation
):
    layers = [
        nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size,
            stride,
            padding,
            bias=not use_batch_norm,
        )
    ]
    if use_batch_norm:
        layers.append(nn.BatchNorm2d(out_channels))
    layers.append(activation())
    return nn.Sequential(*layers)


# Helper for Residual Blocks
class ResidualBlock(nn.Module):
    def __init__(self, channels, use_batch_norm, activation):
        super().__init__()
        self.conv1 = conv_block(channels, channels, 3, 1, 1, use_batch_norm, activation)
        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1, bias=not use_batch_norm)
        self.bn2 = nn.BatchNorm2d(channels) if use_batch_norm else nn.Identity()
        self.activation = activation()

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.conv2(out)
        out = self.bn2(out)
        out += residual  # Skip connection
        out = self.activation(out)
        return out


class AlphaTriangleNet(nn.Module):
    """Neural Network architecture for AlphaTriangle."""

    def __init__(self, model_config: ModelConfig, env_config: EnvConfig):
        super().__init__()
        self.model_config = model_config
        self.env_config = env_config
        self.action_dim = env_config.ACTION_DIM

        activation = getattr(nn, model_config.ACTIVATION_FUNCTION)

        # --- Convolutional Body (Grid Processing) ---
        conv_layers = []
        in_channels = model_config.GRID_INPUT_CHANNELS
        for i, out_channels in enumerate(model_config.CONV_FILTERS):
            conv_layers.append(
                conv_block(
                    in_channels,
                    out_channels,
                    model_config.CONV_KERNEL_SIZES[i],
                    model_config.CONV_STRIDES[i],
                    model_config.CONV_PADDING[i],
                    model_config.USE_BATCH_NORM,
                    activation,
                )
            )
            in_channels = out_channels
        self.conv_body = nn.Sequential(*conv_layers)

        # --- Residual Blocks (Optional) ---
        res_layers = []
        if model_config.NUM_RESIDUAL_BLOCKS > 0:
            res_channels = model_config.RESIDUAL_BLOCK_FILTERS
            if in_channels != res_channels:
                res_layers.append(
                    conv_block(
                        in_channels,
                        res_channels,
                        1,
                        1,
                        0,
                        model_config.USE_BATCH_NORM,
                        activation,
                    )
                )
                in_channels = res_channels
            for _ in range(model_config.NUM_RESIDUAL_BLOCKS):
                res_layers.append(
                    ResidualBlock(in_channels, model_config.USE_BATCH_NORM, activation)
                )
        self.res_body = nn.Sequential(*res_layers)

        # Calculate flattened size after conv/res blocks
        dummy_input_grid = torch.zeros(
            1, model_config.GRID_INPUT_CHANNELS, env_config.ROWS, env_config.COLS
        )
        with torch.no_grad():
            conv_output = self.conv_body(dummy_input_grid)
            res_output = self.res_body(conv_output)
            self.flattened_conv_size = res_output.numel()

        # --- Shared Fully Connected Layers ---
        combined_input_size = (
            self.flattened_conv_size + model_config.OTHER_NN_INPUT_FEATURES_DIM
        )
        shared_fc_layers = []
        in_features = combined_input_size
        for hidden_dim in model_config.FC_DIMS_SHARED:
            shared_fc_layers.append(nn.Linear(in_features, hidden_dim))
            if model_config.USE_BATCH_NORM:
                shared_fc_layers.append(nn.BatchNorm1d(hidden_dim))
            shared_fc_layers.append(activation())
            in_features = hidden_dim
        self.shared_fc = nn.Sequential(*shared_fc_layers)

        # --- Policy Head ---
        policy_head_layers = []
        policy_in_features = in_features
        for hidden_dim in model_config.POLICY_HEAD_DIMS:
            policy_head_layers.append(nn.Linear(policy_in_features, hidden_dim))
            if model_config.USE_BATCH_NORM:
                policy_head_layers.append(nn.BatchNorm1d(hidden_dim))
            policy_head_layers.append(activation())
            policy_in_features = hidden_dim
        policy_head_layers.append(nn.Linear(policy_in_features, self.action_dim))
        self.policy_head = nn.Sequential(*policy_head_layers)

        # --- Value Head ---
        value_head_layers = []
        value_in_features = in_features
        for hidden_dim in model_config.VALUE_HEAD_DIMS[:-1]:
            value_head_layers.append(nn.Linear(value_in_features, hidden_dim))
            if model_config.USE_BATCH_NORM:
                value_head_layers.append(nn.BatchNorm1d(hidden_dim))
            value_head_layers.append(activation())
            value_in_features = hidden_dim
        # Final linear layer
        value_head_layers.append(
            nn.Linear(value_in_features, model_config.VALUE_HEAD_DIMS[-1])
        )
        # Add Tanh activation to bound output between -1 and 1
        value_head_layers.append(nn.Tanh())
        self.value_head = nn.Sequential(*value_head_layers)

    def forward(
        self, grid_state: torch.Tensor, other_features: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass through the network.
        Returns: (policy_logits, value)
        """
        conv_out = self.conv_body(grid_state)
        res_out = self.res_body(conv_out)
        flattened_conv = res_out.view(res_out.size(0), -1)
        combined_features = torch.cat([flattened_conv, other_features], dim=1)
        shared_out = self.shared_fc(combined_features)
        policy_logits = self.policy_head(shared_out)
        value = self.value_head(shared_out)
        return policy_logits, value


File: src/nn/README.md
# File: src/nn/README.md
# Neural Network Module (`src.nn`)

## Purpose and Architecture

This module defines and manages the neural network used by the AlphaTriangle agent. It follows the AlphaZero paradigm, featuring a shared body and separate heads for policy and value prediction.

-   **Model Definition (`model.py`):**
    -   The `AlphaTriangleNet` class (inheriting from `torch.nn.Module`) defines the network architecture.
    -   It typically includes convolutional layers for processing the grid state, potentially residual blocks, and fully connected layers.
    -   It splits into two heads:
        -   **Policy Head:** Outputs logits representing the probability distribution over all possible actions.
        -   **Value Head:** Outputs a single scalar value estimating the expected outcome from the current state.
    -   The architecture is configurable via `ModelConfig`.
-   **Network Interface (`network.py`):**
    -   The `NeuralNetwork` class acts as a wrapper around the `AlphaTriangleNet` PyTorch model.
    -   It provides a clean interface for the rest of the system (MCTS, Trainer) to interact with the network, abstracting away PyTorch specifics.
    -   It **internally uses `src.features.extract_state_features`** to convert input `GameState` objects into tensors before feeding them to the underlying `AlphaTriangleNet` model.
    -   Key methods:
        -   `evaluate(state: GameState)`: Takes a `GameState`, extracts features, performs a forward pass, and returns the policy probabilities (as a dictionary) and the scalar value estimate. Conforms to the `ActionPolicyValueEvaluator` protocol required by MCTS.
        -   `evaluate_batch(states: List[GameState])`: Extracts features from a batch of `GameState` objects and performs batched evaluation for efficiency.
        -   `save_checkpoint(...)`: Saves the model's state dictionary and configuration.
        -   `load_checkpoint(...)`: Loads the model's state dictionary, performs basic config compatibility checks, and returns the optimizer state (if saved).
    -   It handles device placement (`torch.device`).

## Exposed Interfaces

-   **Classes:**
    -   `AlphaTriangleNet(model_config: ModelConfig, env_config: EnvConfig)`: The PyTorch `nn.Module` defining the architecture.
    -   `NeuralNetwork(model_config: ModelConfig, env_config: EnvConfig, train_config: TrainConfig, device: torch.device)`: The wrapper class providing the primary interface.
        -   `evaluate(state: GameState) -> PolicyValueOutput`
        -   `evaluate_batch(states: List[GameState]) -> List[PolicyValueOutput]`
        -   `save_checkpoint(path: str, optimizer_state: Optional[dict] = None, step: Optional[int] = None)`
        -   `load_checkpoint(path: str) -> Tuple[Optional[dict], Optional[int]]`
        -   `model`: Public attribute to access the underlying `AlphaTriangleNet` instance.
        -   `device`: Public attribute indicating the `torch.device`.
        -   `model_config`: Public attribute.

## Dependencies

-   **`src.config`**:
    -   `ModelConfig`: Defines the network architecture parameters (including expected feature dimensions).
    -   `EnvConfig`: Provides environment dimensions (grid size, action space size) needed by the model.
    -   `TrainConfig`: Used by `NeuralNetwork` init.
-   **`src.environment`**:
    -   `GameState`: Input type for `evaluate` and `evaluate_batch`.
-   **`src.features`**:
    -   `extract_state_features`: Used internally by `NeuralNetwork` to process `GameState` inputs.
-   **`src.utils.types`**:
    -   `ActionType`, `PolicyValueOutput`, `StateType`: Used in method signatures and return types.
-   **`torch`**:
    -   The core deep learning framework (`torch`, `torch.nn`, `torch.nn.functional`).
-   **`numpy`**:
    -   Used for converting state components to tensors.
-   **Standard Libraries:** `typing`, `os`, `logging`.

---

**Note:** Please keep this README updated when changing the neural network architecture (`AlphaTriangleNet`), the `NeuralNetwork` interface methods, or its interaction with configuration or other modules (especially `src.features`). Accurate documentation is crucial for maintainability.

File: src/nn/network.py
# File: src/nn/network.py
import torch
import torch.nn.functional as F
import numpy as np
import os
import logging
from typing import List, Tuple, Optional, Mapping, Dict, Any

# Import necessary components
from src.config import ModelConfig, EnvConfig, TrainConfig
from src.environment import GameState
from src.utils.types import ActionType, PolicyValueOutput, StateType
from .model import AlphaTriangleNet
from src.features import extract_state_features

logger = logging.getLogger(__name__)


class NeuralNetwork:
    """Wrapper for the PyTorch model providing evaluation and state management."""

    def __init__(
        self,
        model_config: ModelConfig,
        env_config: EnvConfig,
        train_config: TrainConfig,
        device: torch.device,
    ):
        self.model_config = model_config
        self.env_config = env_config
        self.train_config = train_config
        self.device = device
        self.model = AlphaTriangleNet(model_config, env_config).to(device)
        self.action_dim = env_config.ACTION_DIM

    def _state_to_tensors(self, state: GameState) -> Tuple[torch.Tensor, torch.Tensor]:
        """Extracts features from GameState and converts them to tensors."""
        state_dict: StateType = extract_state_features(state, self.model_config)
        grid_tensor = torch.from_numpy(state_dict["grid"]).unsqueeze(0).to(self.device)
        other_features_tensor = (
            torch.from_numpy(state_dict["other_features"]).unsqueeze(0).to(self.device)
        )
        return grid_tensor, other_features_tensor

    def _batch_states_to_tensors(
        self, states: List[GameState]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Extracts features from a batch of GameStates and converts to batched tensors."""
        if not states:
            grid_shape = (0, self.model_config.GRID_INPUT_CHANNELS, self.env_config.ROWS, self.env_config.COLS)
            other_shape = (0, self.model_config.OTHER_NN_INPUT_FEATURES_DIM)
            return torch.empty(grid_shape, device=self.device), torch.empty(other_shape, device=self.device)

        batch_grid = []
        batch_other = []
        for state in states:
            state_dict: StateType = extract_state_features(state, self.model_config)
            batch_grid.append(state_dict["grid"])
            batch_other.append(state_dict["other_features"])

        grid_tensor = torch.from_numpy(np.stack(batch_grid)).to(self.device)
        other_features_tensor = torch.from_numpy(np.stack(batch_other)).to(self.device)
        return grid_tensor, other_features_tensor

    @torch.inference_mode()
    def evaluate(self, state: GameState) -> PolicyValueOutput:
        """Evaluates a single state by extracting features and running the model."""
        self.model.eval()
        grid_tensor, other_features_tensor = self._state_to_tensors(state)
        policy_logits, value = self.model(grid_tensor, other_features_tensor)
        policy_probs = F.softmax(policy_logits, dim=1).squeeze(0).cpu().numpy()
        value_scalar = value.squeeze(0).item()
        action_policy: Mapping[ActionType, float] = dict(enumerate(policy_probs))
        return action_policy, value_scalar

    @torch.inference_mode()
    def evaluate_batch(self, states: List[GameState]) -> List[PolicyValueOutput]:
        """Evaluates a batch of states by extracting features and running the model."""
        if not states: return []
        self.model.eval()
        grid_tensor, other_features_tensor = self._batch_states_to_tensors(states)
        policy_logits, value = self.model(grid_tensor, other_features_tensor)
        policy_probs = F.softmax(policy_logits, dim=1).cpu().numpy()
        values = value.squeeze(1).cpu().numpy()
        results: List[PolicyValueOutput] = [
            (dict(enumerate(policy_probs[i])), values[i]) for i in range(len(states))
        ]
        return results

    def get_weights(self) -> Dict[str, torch.Tensor]:
        """Returns the model's state dictionary, moved to CPU."""
        return {k: v.cpu() for k, v in self.model.state_dict().items()}

    def set_weights(self, weights: Dict[str, torch.Tensor]):
        """Loads the model's state dictionary from the provided weights."""
        try:
            # Ensure weights are loaded to the correct device for this NN instance
            weights_on_device = {k: v.to(self.device) for k, v in weights.items()}
            self.model.load_state_dict(weights_on_device)
            self.model.eval() # Ensure model is in eval mode after loading weights
            logger.debug("NN weights set successfully.")
        except Exception as e:
            logger.error(f"Error setting weights on NN instance: {e}", exc_info=True)
            raise


File: src/config/model_config.py
# File: src/config/model_config.py
from typing import List, Tuple, Union, Literal
from pydantic import BaseModel, Field, field_validator, model_validator

class ModelConfig(BaseModel):
    """Configuration for the Neural Network model (Pydantic model)."""

    GRID_INPUT_CHANNELS: int = Field(2, gt=0)

    # --- Architecture Parameters ---
    # Increased complexity for a more serious run
    CONV_FILTERS: List[int] = Field(default=[64, 128, 128])
    CONV_KERNEL_SIZES: List[Union[int, Tuple[int, int]]] = Field(default=[3, 3, 3])
    CONV_STRIDES: List[Union[int, Tuple[int, int]]] = Field(default=[1, 1, 1])
    CONV_PADDING: List[Union[int, Tuple[int, int], str]] = Field(default=[1, 1, 1])

    NUM_RESIDUAL_BLOCKS: int = Field(3, ge=0) # Increased residual blocks
    RESIDUAL_BLOCK_FILTERS: int = Field(128, gt=0) # Match last conv filter

    FC_DIMS_SHARED: List[int] = Field(default=[256]) # Increased shared layer size
    POLICY_HEAD_DIMS: List[int] = Field(default=[128]) # Increased policy head size
    VALUE_HEAD_DIMS: List[int] = Field(default=[128, 1]) # Increased value head size

    # --- Other Hyperparameters ---
    ACTIVATION_FUNCTION: Literal["ReLU", "GELU", "SiLU", "Tanh", "Sigmoid"] = Field("ReLU")
    USE_BATCH_NORM: bool = Field(True)

    # --- Input Feature Dimension ---
    # This depends on src/features/extractor.py and should match its output.
    # Default calculation: 3 slots * 7 shape feats + 3 avail feats + 6 explicit feats = 30
    OTHER_NN_INPUT_FEATURES_DIM: int = Field(30, gt=0)

    @model_validator(mode='after')
    def check_conv_layers_consistency(self) -> 'ModelConfig':
        n_filters = len(self.CONV_FILTERS)
        if not (len(self.CONV_KERNEL_SIZES) == n_filters and
                len(self.CONV_STRIDES) == n_filters and
                len(self.CONV_PADDING) == n_filters):
            raise ValueError("Lengths of CONV_FILTERS, CONV_KERNEL_SIZES, CONV_STRIDES, and CONV_PADDING must match.")
        return self

    @field_validator('VALUE_HEAD_DIMS')
    @classmethod
    def check_value_head_last_dim(cls, v: List[int]) -> List[int]:
        if not v:
            raise ValueError("VALUE_HEAD_DIMS cannot be empty.")
        if v[-1] != 1:
            raise ValueError(f"The last dimension of VALUE_HEAD_DIMS must be 1 (got {v[-1]}).")
        return v

    @model_validator(mode='after')
    def check_residual_filter_match(self) -> 'ModelConfig':
        if self.NUM_RESIDUAL_BLOCKS > 0 and self.CONV_FILTERS:
            if self.RESIDUAL_BLOCK_FILTERS != self.CONV_FILTERS[-1]:
                # This is a common pattern but not strictly required, make it a warning?
                # For now, let's keep it flexible, but one could add validation here.
                print(f"Warning: RESIDUAL_BLOCK_FILTERS ({self.RESIDUAL_BLOCK_FILTERS}) does not match last CONV_FILTER ({self.CONV_FILTERS[-1]}). Ensure this is intended.")
        return self

File: src/config/env_config.py
# File: src/config/env_config.py
from typing import List, Tuple
from pydantic import BaseModel, Field, computed_field, field_validator

class EnvConfig(BaseModel):
    """Configuration for the game environment (Pydantic model)."""
    ROWS: int = Field(8, gt=0)
    COLS: int = Field(15, gt=0)
    NUM_SHAPE_SLOTS: int = Field(3, gt=0)
    MIN_LINE_LENGTH: int = Field(3, gt=0)

    # Example hourglass shape definition for the grid
    COLS_PER_ROW: List[int] = Field(default=[9, 11, 13, 15, 15, 13, 11, 9])

    @field_validator('COLS_PER_ROW')
    @classmethod
    def check_cols_per_row(cls, v: List[int], info) -> List[int]:
        rows = info.data.get('ROWS')
        cols = info.data.get('COLS')
        if rows is None or cols is None:
            # This can happen during initial validation before all fields are set
            # Pydantic usually handles this, but we add a check.
            # Alternatively, make ROWS/COLS positional args in __init__ if needed earlier.
            return v # Skip validation if ROWS/COLS not available yet

        if len(v) != rows:
            raise ValueError(f"COLS_PER_ROW length ({len(v)}) must equal ROWS ({rows})")
        if max(v, default=0) > cols:
            raise ValueError(f"Max COLS_PER_ROW ({max(v, default=0)}) cannot exceed COLS ({cols})")
        return v

    @computed_field # type: ignore[misc]
    @property
    def ACTION_DIM(self) -> int:
        """Total number of possible actions (shape_slot * row * col)."""
        return self.NUM_SHAPE_SLOTS * self.ROWS * self.COLS

File: src/config/__init__.py
# File: src/config/__init__.py
from .app_config import APP_NAME
from .env_config import EnvConfig
from .model_config import ModelConfig
from .persistence_config import PersistenceConfig
from .train_config import TrainConfig
from .vis_config import VisConfig
from .mcts_config import MCTSConfig # Re-add MCTSConfig
from .validation import print_config_info_and_validate

__all__ = [
    "APP_NAME",
    "EnvConfig",
    "ModelConfig",
    "PersistenceConfig",
    "TrainConfig",
    "VisConfig",
    "MCTSConfig", # Re-add MCTSConfig
    "print_config_info_and_validate",
]

File: src/config/persistence_config.py
# File: src/config/persistence_config.py
from typing import Optional
import os
from pydantic import BaseModel, Field, computed_field

class PersistenceConfig(BaseModel):
    """Configuration for saving/loading artifacts (Pydantic model)."""

    ROOT_DATA_DIR: str = Field(".alphatriangle_data")
    RUNS_DIR_NAME: str = Field("runs")
    MLFLOW_DIR_NAME: str = Field("mlruns")

    CHECKPOINT_SAVE_DIR_NAME: str = Field("checkpoints")
    BUFFER_SAVE_DIR_NAME: str = Field("buffers")
    GAME_STATE_SAVE_DIR_NAME: str = Field("game_states")
    LOG_DIR_NAME: str = Field("logs")

    LATEST_CHECKPOINT_FILENAME: str = Field("latest.pkl")
    BEST_CHECKPOINT_FILENAME: str = Field("best.pkl")
    BUFFER_FILENAME: str = Field("buffer.pkl")
    CONFIG_FILENAME: str = Field("configs.json")

    # RUN_NAME is typically set dynamically by TrainConfig, but provide a default
    RUN_NAME: str = Field("default_run")

    SAVE_GAME_STATES: bool = Field(False)
    GAME_STATE_SAVE_FREQ_EPISODES: int = Field(5, ge=1)

    SAVE_BUFFER: bool = Field(True)
    BUFFER_SAVE_FREQ_STEPS: int = Field(10, ge=1)

    @computed_field # type: ignore[misc]
    @property
    def MLFLOW_TRACKING_URI(self) -> str:
        """Constructs the file URI for MLflow tracking."""
        # Ensure the path is absolute for MLflow
        abs_path = os.path.abspath(os.path.join(self.ROOT_DATA_DIR, self.MLFLOW_DIR_NAME))
        # Replace backslashes with forward slashes for file URI compatibility on Windows
        uri_path = abs_path.replace(os.sep, '/')
        # Ensure the URI starts with file:///
        if not uri_path.startswith('/'):
            uri_path = '/' + uri_path
        return f"file://{uri_path}"

    def get_run_base_dir(self, run_name: Optional[str] = None) -> str:
        """Gets the base directory for a specific run."""
        name = run_name if run_name else self.RUN_NAME
        return os.path.join(self.ROOT_DATA_DIR, self.RUNS_DIR_NAME, name)

File: src/config/train_config.py
# File: src/config/train_config.py
import time
from typing import Optional, Literal
from pydantic import BaseModel, Field, field_validator, model_validator

class TrainConfig(BaseModel):
    """Configuration for the training process (Pydantic model)."""

    RUN_NAME: str = Field(default_factory=lambda: f"train_run_{time.strftime('%Y%m%d_%H%M%S')}")
    LOAD_CHECKPOINT_PATH: Optional[str] = Field(None)
    LOAD_BUFFER_PATH: Optional[str] = Field(None)
    AUTO_RESUME_LATEST: bool = Field(True)
    DEVICE: Literal["auto", "cuda", "cpu", "mps"] = Field("auto")
    RANDOM_SEED: int = Field(42)

    # --- Training Loop ---
    # Increased steps for overnight run
    MAX_TRAINING_STEPS: Optional[int] = Field(default=200_000, ge=1)

    # --- Workers & Batching ---
    NUM_SELF_PLAY_WORKERS: int = Field(12, ge=1) # Increased workers
    WORKER_DEVICE: Literal["auto", "cuda", "cpu", "mps"] = Field("cpu") # Keep workers on CPU unless GPUs available
    BATCH_SIZE: int = Field(128, ge=1) # Increased batch size
    BUFFER_CAPACITY: int = Field(80_000, ge=1) # Increased buffer capacity
    MIN_BUFFER_SIZE_TO_TRAIN: int = Field(15_000, ge=1) # Increased min buffer size
    WORKER_UPDATE_FREQ_STEPS: int = Field(10, ge=1) # Slightly increased update frequency

    # --- Optimizer ---
    OPTIMIZER_TYPE: Literal["Adam", "AdamW", "SGD"] = Field("AdamW")
    LEARNING_RATE: float = Field(1e-4, gt=0) # Keep LR, scheduler will handle decay
    WEIGHT_DECAY: float = Field(1e-5, ge=0)
    GRADIENT_CLIP_VALUE: Optional[float] = Field(default=1.0) # Allow None or positive

    # --- LR Scheduler ---
    LR_SCHEDULER_TYPE: Optional[Literal["StepLR", "CosineAnnealingLR"]] = Field(default="CosineAnnealingLR")
    LR_SCHEDULER_T_MAX: Optional[int] = Field(default=None) # Will be set based on MAX_TRAINING_STEPS
    LR_SCHEDULER_ETA_MIN: float = Field(1e-6, ge=0)

    # --- Loss Weights ---
    POLICY_LOSS_WEIGHT: float = Field(1.0, ge=0)
    VALUE_LOSS_WEIGHT: float = Field(1.0, ge=0)
    ENTROPY_BONUS_WEIGHT: float = Field(0.01, ge=0)

    # --- Checkpointing ---
    CHECKPOINT_SAVE_FREQ_STEPS: int = Field(1000, ge=1) # Increased save frequency

    @model_validator(mode='after')
    def check_buffer_sizes(self) -> 'TrainConfig':
        if self.MIN_BUFFER_SIZE_TO_TRAIN > self.BUFFER_CAPACITY:
            raise ValueError("MIN_BUFFER_SIZE_TO_TRAIN cannot be greater than BUFFER_CAPACITY.")
        if self.BATCH_SIZE > self.BUFFER_CAPACITY:
            raise ValueError("BATCH_SIZE cannot be greater than BUFFER_CAPACITY.")
        # Ensure batch size is reasonable compared to min buffer size
        if self.BATCH_SIZE > self.MIN_BUFFER_SIZE_TO_TRAIN:
             print(f"Warning: BATCH_SIZE ({self.BATCH_SIZE}) is larger than MIN_BUFFER_SIZE_TO_TRAIN ({self.MIN_BUFFER_SIZE_TO_TRAIN}). This might lead to slow startup.")
        return self

    @model_validator(mode='after')
    def set_scheduler_t_max(self) -> 'TrainConfig':
        if self.LR_SCHEDULER_TYPE == "CosineAnnealingLR" and self.LR_SCHEDULER_T_MAX is None:
            if self.MAX_TRAINING_STEPS is not None:
                self.LR_SCHEDULER_T_MAX = self.MAX_TRAINING_STEPS
            else:
                # Fallback if MAX_TRAINING_STEPS is somehow None despite default
                self.LR_SCHEDULER_T_MAX = 100_000
                print(f"Warning: MAX_TRAINING_STEPS is None, setting LR_SCHEDULER_T_MAX to default {self.LR_SCHEDULER_T_MAX}")
        # Ensure T_max is positive if set
        if self.LR_SCHEDULER_T_MAX is not None and self.LR_SCHEDULER_T_MAX <= 0:
            raise ValueError("LR_SCHEDULER_T_MAX must be positive if set.")
        return self

    @field_validator('GRADIENT_CLIP_VALUE')
    @classmethod
    def check_gradient_clip(cls, v: Optional[float]) -> Optional[float]:
        if v is not None and v <= 0:
            raise ValueError("GRADIENT_CLIP_VALUE must be positive if set.")
        return v

File: src/config/README.md
# File: src/config/README.md
# Configuration Module (`src.config`)

## Purpose and Architecture

This module centralizes all configuration parameters for the AlphaTriangle project. It uses separate **Pydantic models** for different aspects of the application (environment, model, training, visualization, persistence) to promote modularity, clarity, and automatic validation.

-   **Modularity:** Separating configurations makes it easier to manage parameters for different components.
-   **Type Safety & Validation:** Using Pydantic models (`BaseModel`) provides strong type hinting, automatic parsing, and validation of configuration values based on defined types and constraints (e.g., `Field(gt=0)`).
-   **Validation Script:** The `validation.py` script instantiates all configuration models, triggering Pydantic's validation, and prints a summary.
-   **Dynamic Defaults:** Some configurations, like `RUN_NAME` in `TrainConfig`, use `default_factory` for dynamic defaults (e.g., timestamp).
-   **Computed Fields:** Properties like `ACTION_DIM` in `EnvConfig` or `MLFLOW_TRACKING_URI` in `PersistenceConfig` are defined using `@computed_field` for clarity.

## Exposed Interfaces

-   **Pydantic Models:**
    -   `EnvConfig`: Environment parameters (grid size, shapes).
    -   `ModelConfig`: Neural network architecture parameters.
    -   `TrainConfig`: Training loop hyperparameters (batch size, learning rate, workers, etc.).
    -   `VisConfig`: Visualization parameters (screen size, FPS, layout).
    -   `PersistenceConfig`: Data saving/loading parameters (directories, filenames).
    -   `MCTSConfig`: MCTS parameters (simulations, exploration constants, temperature). - *Note: Defined in `src.mcts.core.config` but often used alongside other configs.*
-   **Constants:**
    -   `APP_NAME`: The name of the application.
-   **Functions:**
    -   `print_config_info_and_validate(mcts_config_instance: MCTSConfig)`: Validates and prints a summary of all configurations by instantiating the Pydantic models.

## Dependencies

This module primarily defines configurations and relies heavily on **Pydantic**.

-   **`pydantic`**: The core library used for defining models and validation.
-   **`src.mcts.core.config`**: The `validation.py` script imports `MCTSConfig` from this location.
-   **Standard Libraries:** `typing`, `time`, `os`.

---

**Note:** Please keep this README updated when adding, removing, or significantly modifying configuration parameters or the structure of the Pydantic models. Accurate documentation is crucial for maintainability.

File: src/config/mcts_config.py
# File: src/config/mcts_config.py
from pydantic import BaseModel, Field, field_validator

class MCTSConfig(BaseModel):
    """Configuration for Monte Carlo Tree Search (Pydantic model)."""

    num_simulations: int = Field(300, ge=1) # Increased simulations
    puct_coefficient: float = Field(1.0, gt=0)
    temperature_initial: float = Field(1.0, ge=0)
    temperature_final: float = Field(0.01, ge=0)
    temperature_anneal_steps: int = Field(10_000, ge=0) # Keep relatively short annealing
    dirichlet_alpha: float = Field(0.3, gt=0)
    dirichlet_epsilon: float = Field(0.25, ge=0, le=1.0)
    max_search_depth: int = Field(100, ge=1)

    @field_validator('temperature_final')
    @classmethod
    def check_temp_final_le_initial(cls, v: float, info) -> float:
        initial_temp = info.data.get('temperature_initial')
        if initial_temp is not None and v > initial_temp:
            raise ValueError("temperature_final cannot be greater than temperature_initial")
        return v

File: src/config/vis_config.py
# File: src/config/vis_config.py
from pydantic import BaseModel, Field

class VisConfig(BaseModel):
    """Configuration for visualization (Pydantic model)."""

    FPS: int = Field(30, gt=0)
    SCREEN_WIDTH: int = Field(1000, gt=0)
    SCREEN_HEIGHT: int = Field(800, gt=0)

    # Layout
    GRID_AREA_RATIO: float = Field(0.7, gt=0, le=1.0) # Portion of width for main grid
    PREVIEW_AREA_WIDTH: int = Field(150, gt=0)
    PADDING: int = Field(10, ge=0)
    HUD_HEIGHT: int = Field(40, ge=0)

    # Fonts (sizes)
    FONT_UI_SIZE: int = Field(24, gt=0)
    FONT_SCORE_SIZE: int = Field(30, gt=0)
    FONT_HELP_SIZE: int = Field(18, gt=0)

    # Preview Area
    PREVIEW_PADDING: int = Field(5, ge=0)
    PREVIEW_BORDER_WIDTH: int = Field(1, ge=0)
    PREVIEW_SELECTED_BORDER_WIDTH: int = Field(3, ge=0)
    PREVIEW_INNER_PADDING: int = Field(2, ge=0)

File: src/config/app_config.py
# File: src/config/app_config.py
APP_NAME: str = "AlphaTriangle"


File: src/config/validation.py
# File: src/config/validation.py
import logging
from pydantic import ValidationError

# Import config classes directly
from .env_config import EnvConfig
from .model_config import ModelConfig
from .train_config import TrainConfig
from .vis_config import VisConfig
from .persistence_config import PersistenceConfig
from .app_config import APP_NAME

# Import MCTSConfig from its location
from .mcts_config import MCTSConfig # Updated import

logger = logging.getLogger(__name__)

def print_config_info_and_validate(mcts_config_instance: MCTSConfig): # Accept instance
    """Prints configuration summary and performs validation using Pydantic."""
    print("-" * 40)
    print("Configuration Validation & Summary")
    print("-" * 40)
    all_valid = True
    configs_validated = {}

    config_classes = {
        "Environment": EnvConfig,
        "Model": ModelConfig,
        "Training": TrainConfig,
        "Visualization": VisConfig,
        "Persistence": PersistenceConfig,
        "MCTS": MCTSConfig, # Add MCTSConfig here
    }

    # Validate and store instances
    for name, ConfigClass in config_classes.items():
        try:
            # If an instance is passed (like mcts_config_instance), use it, otherwise instantiate
            if name == "MCTS" and mcts_config_instance is not None:
                instance = mcts_config_instance
                # Re-validate if needed, though Pydantic validates on init
                # MCTSConfig.model_validate(instance.model_dump()) # Optional re-validation
                print(f"[{name}] - Instance provided OK")
            else:
                instance = ConfigClass() # Instantiate with defaults or loaded values
                print(f"[{name}] - Validated OK")
            configs_validated[name] = instance
        except ValidationError as e:
            logger.error(f"Validation failed for {name} Config:")
            logger.error(e)
            all_valid = False
            configs_validated[name] = None # Mark as invalid
        except Exception as e:
            logger.error(f"Unexpected error instantiating/validating {name} Config: {e}")
            all_valid = False
            configs_validated[name] = None

    print("-" * 40)
    print("Configuration Values:")
    print("-" * 40)

    # Print validated config values
    for name, instance in configs_validated.items():
        print(f"--- {name} Config ---")
        if instance:
            # Use model_dump for clean output
            dump_data = instance.model_dump()
            for field_name, value in dump_data.items():
                # Exclude potentially long lists/dicts from summary?
                if isinstance(value, list) and len(value) > 5:
                    print(f"  {field_name}: [List with {len(value)} items]")
                elif isinstance(value, dict) and len(value) > 5:
                    print(f"  {field_name}: {{Dict with {len(value)} keys}}")
                else:
                    print(f"  {field_name}: {value}")
        else:
            print("  <Validation Failed>")
        print("-" * 20)


    print("-" * 40)
    if not all_valid:
        logger.critical("Configuration validation failed. Please check errors above.")
        raise ValueError("Invalid configuration settings.")
    else:
        logger.info("All configurations validated successfully.")
    print("-" * 40)

File: src/features/__init__.py
# File: src/features/__init__.py
"""
Feature extraction module.
Converts raw GameState objects into numerical representations suitable for NN input.
"""
from .extractor import extract_state_features, GameStateFeatures
from . import grid_features

__all__ = [
    "extract_state_features",
    "GameStateFeatures",
    "grid_features",
]


File: src/features/extractor.py
# File: src/features/extractor.py
import numpy as np
from typing import TYPE_CHECKING

# Import necessary components from environment and utils
from src.environment import GameState  # Import GameState directly
from src.utils.types import StateType  # Use core StateType (StateDict)
from src.config import EnvConfig, ModelConfig  # Need configs for dimensions

# Import grid_features from the same features package
from . import grid_features

# Import Triangle from the new structs module
from src.structs import Triangle

if TYPE_CHECKING:
    pass


class GameStateFeatures:
    """Extracts features from GameState for NN input."""

    def __init__(self, game_state: "GameState", model_config: ModelConfig):
        self.gs = game_state
        self.env_config = game_state.env_config  # Get env_config from GameState
        self.model_config = model_config  # Need model_config for feature dimensions

    def _get_grid_state(self) -> np.ndarray:
        """Returns grid occupancy and death state as channels."""
        grid_occupied = self.gs.grid_data.get_occupied_state()
        grid_death = self.gs.grid_data.get_death_state()
        # Ensure the number of channels matches ModelConfig
        expected_channels = self.model_config.GRID_INPUT_CHANNELS
        grid_state = np.stack([grid_occupied, grid_death], axis=0).astype(np.float32)
        if grid_state.shape[0] != expected_channels:
            # This basic extractor only provides 2 channels.
            # If ModelConfig expects more, this needs adjustment or ModelConfig change.
            # For now, we'll raise an error if mismatched.
            raise ValueError(
                f"Mismatch between extracted grid channels ({grid_state.shape[0]}) and ModelConfig.GRID_INPUT_CHANNELS ({expected_channels})"
            )
        return grid_state

    def _get_shape_features(self) -> np.ndarray:
        """Extracts features for each shape slot."""
        num_slots = self.env_config.NUM_SHAPE_SLOTS
        # Calculate features per shape based on ModelConfig's expectation
        # This assumes OTHER_NN_INPUT_FEATURES_DIM is correctly set in ModelConfig
        # based on the concatenation logic in get_combined_other_features.
        # We need a way to know how many features *this function* produces per shape.
        # Let's define it locally for clarity, ensuring it matches the calculation
        # used for OTHER_NN_INPUT_FEATURES_DIM in ModelConfig.
        FEATURES_PER_SHAPE_HERE = 7  # Number of features calculated below
        shape_feature_matrix = np.zeros(
            (num_slots, FEATURES_PER_SHAPE_HERE), dtype=np.float32
        )

        for i, shape in enumerate(
            self.gs.shapes
        ):  # Uses Shape from structs (via GameState)
            if shape and shape.triangles:
                n_tris = len(shape.triangles)
                ups = sum(1 for _, _, is_up in shape.triangles if is_up)
                downs = n_tris - ups
                min_r, min_c, max_r, max_c = shape.bbox()
                height = max_r - min_r + 1
                width_eff = (max_c - min_c + 1) * 0.75 + 0.25 if n_tris > 0 else 0

                # Populate features
                shape_feature_matrix[i, 0] = np.clip(n_tris / 5.0, 0, 1)
                shape_feature_matrix[i, 1] = ups / n_tris if n_tris > 0 else 0
                shape_feature_matrix[i, 2] = downs / n_tris if n_tris > 0 else 0
                shape_feature_matrix[i, 3] = np.clip(
                    height / self.env_config.ROWS, 0, 1
                )
                shape_feature_matrix[i, 4] = np.clip(
                    width_eff / self.env_config.COLS, 0, 1
                )
                shape_feature_matrix[i, 5] = np.clip(
                    ((min_r + max_r) / 2.0) / self.env_config.ROWS, 0, 1
                )
                shape_feature_matrix[i, 6] = np.clip(
                    ((min_c + max_c) / 2.0) / self.env_config.COLS, 0, 1
                )

        return shape_feature_matrix.flatten()

    def _get_shape_availability(self) -> np.ndarray:
        """Returns a binary vector indicating which shape slots are filled."""
        return np.array([1.0 if s else 0.0 for s in self.gs.shapes], dtype=np.float32)

    def _get_explicit_features(self) -> np.ndarray:
        """Extracts scalar features like score, heights, holes, etc."""
        # Define the number of explicit features calculated here.
        # This must match the calculation for OTHER_NN_INPUT_FEATURES_DIM in ModelConfig.
        EXPLICIT_FEATURES_DIM_HERE = 6
        features = np.zeros(EXPLICIT_FEATURES_DIM_HERE, dtype=np.float32)
        occupied = self.gs.grid_data.get_occupied_state()  # Use method from GridData
        death = self.gs.grid_data.get_death_state()  # Use method from GridData
        rows, cols = self.env_config.ROWS, self.env_config.COLS

        heights = grid_features.get_column_heights(occupied, death, rows, cols)
        holes = grid_features.count_holes(occupied, death, heights, rows, cols)
        bump = grid_features.get_bumpiness(heights)
        total_playable_cells = np.sum(~death)

        # Populate features
        features[0] = np.clip(self.gs.game_score / 100.0, -5.0, 5.0)
        features[1] = np.mean(heights) / rows if rows > 0 else 0
        features[2] = np.max(heights) / rows if rows > 0 else 0
        features[3] = holes / total_playable_cells if total_playable_cells > 0 else 0
        features[4] = (bump / (cols - 1)) / rows if cols > 1 and rows > 0 else 0
        features[5] = np.clip(self.gs.pieces_placed_this_episode / 100.0, 0, 1)

        return np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)

    def get_combined_other_features(self) -> np.ndarray:
        """Combines all non-grid features into a single flat vector."""
        shape_feats = self._get_shape_features()
        avail_feats = self._get_shape_availability()
        explicit_feats = self._get_explicit_features()
        combined = np.concatenate([shape_feats, avail_feats, explicit_feats])

        # Validate shape against ModelConfig expectation
        expected_dim = self.model_config.OTHER_NN_INPUT_FEATURES_DIM
        if combined.shape[0] != expected_dim:
            raise ValueError(
                f"Combined other_features dimension mismatch! Extracted {combined.shape[0]}, but ModelConfig expects {expected_dim}"
            )

        return combined.astype(np.float32)


def extract_state_features(
    game_state: "GameState", model_config: ModelConfig
) -> StateType:
    """
    Extracts and returns the state dictionary {grid, other_features} for NN input.
    Requires ModelConfig to ensure dimensions match the network's expectations.
    """
    extractor = GameStateFeatures(game_state, model_config)
    state_dict: StateType = {
        "grid": extractor._get_grid_state(),
        "other_features": extractor.get_combined_other_features(),
    }
    return state_dict


File: src/features/README.md
# File: src/features/README.md
# Feature Extraction Module (`src.features`)

## Purpose and Architecture

This module is solely responsible for converting raw `GameState` objects from the `src.environment` module into numerical representations (features) suitable for input into the neural network (`src.nn`). It acts as a bridge between the game's internal state and the requirements of the machine learning model.

-   **Decoupling:** This module completely decouples feature engineering from the core game environment logic. The `environment` module focuses only on game rules and state transitions, while this module handles the transformation for the NN.
-   **Feature Engineering:**
    -   `extractor.py`: Contains the `GameStateFeatures` class and the main `extract_state_features` function. This orchestrates the extraction process, calling helper functions to generate different feature types. It uses `Triangle` and `Shape` from `src.structs`.
    -   `grid_features.py`: Contains low-level, potentially performance-optimized (e.g., using Numba) functions for calculating specific scalar metrics derived from the grid state (like column heights, holes, bumpiness).
-   **Output Format:** The `extract_state_features` function returns a `StateType` (a `TypedDict` defined in `src.utils.types` containing `grid` and `other_features` numpy arrays), which is the standard input format expected by the `NeuralNetwork` interface.
-   **Configuration Dependency:** The extractor requires `ModelConfig` to ensure the dimensions of the extracted features match the expectations of the neural network architecture.

## Exposed Interfaces

-   **Functions:**
    -   `extract_state_features(game_state: GameState, model_config: ModelConfig) -> StateType`: The main function to perform feature extraction.
    -   Low-level grid feature functions from `grid_features` (e.g., `get_column_heights`, `count_holes`, `get_bumpiness`).
-   **Classes:**
    -   `GameStateFeatures`: Class containing the feature extraction logic (primarily used internally by `extract_state_features`).

## Dependencies

-   **`src.environment`**:
    -   `GameState`: The input object for feature extraction.
    -   `GridData`: Accessed via `GameState` to get grid information.
-   **`src.config`**:
    -   `EnvConfig`: Accessed via `GameState` for environment dimensions.
    -   `ModelConfig`: Required by `extract_state_features` to ensure output dimensions match the NN input layer.
-   **`src.structs`**:
    -   Uses `Triangle`, `Shape`.
-   **`src.utils.types`**:
    -   `StateType`: The return type dictionary format.
-   **`numpy`**:
    -   Used extensively for creating and manipulating the numerical feature arrays.
-   **`numba`**:
    -   Used in `grid_features` for performance optimization.
-   **Standard Libraries:** `typing`.

---

**Note:** Please keep this README updated when changing the feature extraction logic, the set of extracted features, or the output format (`StateType`). Accurate documentation is crucial for maintainability.

File: src/features/grid_features.py
# File: src/features/grid_features.py
# (Content moved from src/environment/grid/grid_features.py)
# Note: Ensure Numba is installed (`pip install numba`)
import numpy as np
import numba
from numba import njit, prange

# --- Numba Optimized Grid Feature Calculations ---
# These functions operate on numpy arrays for speed.


@njit(cache=True)
def get_column_heights(
    occupied: np.ndarray, death: np.ndarray, rows: int, cols: int
) -> np.ndarray:
    """Calculates the height of each column (highest occupied non-death cell)."""
    heights = np.zeros(cols, dtype=np.int32)
    for c in prange(cols):
        max_r = -1
        for r in range(rows):
            # Check if the cell is occupied AND not a death cell
            if occupied[r, c] and not death[r, c]:
                max_r = r
        # Height is row index + 1 (or 0 if column is empty/all death)
        heights[c] = max_r + 1
    return heights


@njit(cache=True)
def count_holes(
    occupied: np.ndarray, death: np.ndarray, heights: np.ndarray, rows: int, cols: int
) -> int:
    """Counts the number of empty, non-death cells below the column height."""
    holes = 0
    for c in prange(cols):
        col_height = heights[c]
        # Iterate from row 0 up to (but not including) the column height
        for r in range(col_height):
            # A hole is an empty cell that is not a death cell, below the highest block
            if not occupied[r, c] and not death[r, c]:
                holes += 1
    return holes


@njit(cache=True)
def get_bumpiness(heights: np.ndarray) -> float:
    """Calculates the total absolute difference between adjacent column heights."""
    bumpiness = 0.0
    for i in range(len(heights) - 1):
        bumpiness += abs(heights[i] - heights[i + 1])
    return bumpiness


# Add other potential Numba-optimized grid features here if needed
# e.g., completed lines (might be complex with triangles), wells, etc.


File: src/utils/__init__.py
# File: src/utils/__init__.py
from .helpers import get_device, set_random_seeds, format_eta
from .types import (
    StateType,
    ActionType,
    Experience,
    ExperienceBatch,
    PolicyValueOutput,
    StatsCollectorData,
    # Removed SelfPlayResult import/export
)
from .geometry import is_point_in_polygon  # Export the new geometry function

__all__ = [
    # helpers
    "get_device",
    "set_random_seeds",
    "format_eta",
    # types
    "StateType",
    "ActionType",
    "Experience",
    "ExperienceBatch",
    "PolicyValueOutput",
    "StatsCollectorData",
    # Removed SelfPlayResult export
    # geometry
    "is_point_in_polygon",
]

File: src/utils/types.py
# File: src/utils/types.py
from typing import Dict, Any, List, Tuple, Mapping, TypedDict, Optional, Deque
import numpy as np
# Removed Pydantic imports as they are no longer needed here for SelfPlayResult
# from pydantic import BaseModel, Field, ConfigDict

# Import GameState for Experience type hint - use TYPE_CHECKING to avoid circular import at runtime
from typing import TYPE_CHECKING
from collections import deque  # Import deque for StatsCollectorData

if TYPE_CHECKING:
    # Import GameState ONLY for type checking
    from src.environment import GameState


# Basic type for representing the dictionary structure of features extracted from a state
# This is the *output* of the feature extractor and the *input* to the NN.
# Kept as TypedDict for performance in NN/feature extraction path.
class StateType(TypedDict):
    grid: np.ndarray  # (C, H, W) float32
    other_features: np.ndarray  # (OtherFeatDim,) float32


# Action representation (integer index)
ActionType = int

# Policy target from MCTS (visit counts distribution)
# Mapping from action index to its probability (normalized visit count)
PolicyTargetMapping = Mapping[ActionType, float]

# Experience tuple stored in buffer
# Now stores the raw GameState object instead of extracted features.
# Kept as Tuple for performance in buffer operations.
# Use forward reference string "GameState" to avoid runtime import
Experience = Tuple["GameState", PolicyTargetMapping, float]

# Batch of experiences for training
ExperienceBatch = List[Experience]

# Output type from the neural network's evaluate method
# (Policy Mapping, Value Estimate)
# Kept as Tuple for performance.
PolicyValueOutput = Tuple[Mapping[ActionType, float], float]

# Type alias for the data structure holding collected statistics
# Maps metric name to a deque of (step, value) tuples
# Kept as Dict[Deque] internally in StatsCollectorActor, type alias is sufficient here.
StatsCollectorData = Dict[str, Deque[Tuple[int, float]]]

# --- Pydantic Models for Data Transfer ---
# SelfPlayResult moved to src/rl/types.py to resolve circular import

# No model_rebuild needed here anymore

File: src/utils/README.md
# File: src/utils/README.md
# Utilities Module (`src.utils`)

## Purpose and Architecture

This module provides common utility functions and type definitions used across various parts of the AlphaTriangle project. Its goal is to avoid code duplication and provide central definitions for shared concepts.

-   **Helper Functions (`helpers.py`):** Contains miscellaneous helper functions:
    -   `get_device`: Determines the appropriate PyTorch device (CPU, CUDA, MPS) based on availability and preference.
    -   `set_random_seeds`: Initializes random number generators for Python, NumPy, and PyTorch for reproducibility.
    -   `format_eta`: Converts a time duration (in seconds) into a human-readable string (HH:MM:SS).
-   **Type Definitions (`types.py`):** Defines common type aliases and `TypedDict`s used throughout the codebase, particularly for data structures passed between modules (like RL components, NN, and environment). This improves code readability and enables better static analysis. Examples include:
    -   `StateType`: A `TypedDict` defining the structure of the state representation passed to the NN and stored in the buffer (e.g., `{'grid': np.ndarray, 'other_features': np.ndarray}`).
    -   `ActionType`: An alias for `int`, representing encoded actions.
    -   `PolicyTargetMapping`: A mapping from `ActionType` to `float`, representing the policy target from MCTS.
    -   `Experience`: A tuple representing `(GameState, PolicyTargetMapping, float)` stored in the replay buffer.
    -   `ExperienceBatch`: A list of `Experience` tuples.
    -   `PolicyValueOutput`: A tuple representing `(PolicyTargetMapping, float)` returned by the NN's `evaluate` method.
-   **Geometry Utilities (`geometry.py`):** Contains geometric helper functions.
    -   `is_point_in_polygon`: Checks if a 2D point lies inside a given polygon.

## Exposed Interfaces

-   **Functions:**
    -   `get_device(device_preference: str = "auto") -> torch.device`
    -   `set_random_seeds(seed: int = 42)`
    -   `format_eta(seconds: Optional[float]) -> str`
    -   `is_point_in_polygon(point: Tuple[float, float], polygon: List[Tuple[float, float]]) -> bool`
-   **Types:**
    -   `StateType` (TypedDict)
    -   `ActionType` (TypeAlias for `int`)
    -   `PolicyTargetMapping` (TypeAlias for `Mapping[ActionType, float]`)
    -   `Experience` (TypeAlias for `Tuple[GameState, PolicyTargetMapping, float]`)
    -   `ExperienceBatch` (TypeAlias for `List[Experience]`)
    -   `PolicyValueOutput` (TypeAlias for `Tuple[Mapping[ActionType, float], float]`)

## Dependencies

-   **`torch`**:
    -   Used by `get_device` and `set_random_seeds`.
-   **`numpy`**:
    -   Used by `set_random_seeds` and potentially in type definitions (`np.ndarray`).
-   **`src.environment`**:
    -   `GameState` (used in `Experience` type hint via TYPE_CHECKING).
-   **Standard Libraries:** `typing`, `random`, `os`, `math`, `logging`.

---

**Note:** Please keep this README updated when adding or modifying utility functions or type definitions, especially those used as interfaces between different modules. Accurate documentation is crucial for maintainability.

File: src/utils/geometry.py
# File: src/utils/geometry.py
from typing import List, Tuple


def is_point_in_polygon(
    point: Tuple[float, float], polygon: List[Tuple[float, float]]
) -> bool:
    """
    Checks if a point is inside a polygon using the ray casting algorithm.

    Args:
        point: Tuple (x, y) representing the point coordinates.
        polygon: List of tuples [(x1, y1), (x2, y2), ...] representing polygon vertices in order.

    Returns:
        True if the point is inside the polygon, False otherwise.
    """
    x, y = point
    n = len(polygon)
    inside = False

    p1x, p1y = polygon[0]
    for i in range(n + 1):
        p2x, p2y = polygon[i % n]
        if y > min(p1y, p2y):
            if y <= max(p1y, p2y):
                if x <= max(p1x, p2x):
                    if p1y != p2y:
                        xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x
                    # Check if point is on the polygon boundary horizontally
                    if abs(p1x - p2x) < 1e-9:  # Treat vertical lines carefully
                        if abs(x - p1x) < 1e-9:
                            return True  # Point is on a vertical boundary segment
                    elif abs(x - xinters) < 1e-9:
                        return True  # Point is on a non-vertical boundary segment
                    elif p1x == p2x or x <= xinters:
                        inside = not inside
        p1x, p1y = p2x, p2y

    # Additionally check if point is exactly on a vertex
    for px, py in polygon:
        if abs(x - px) < 1e-9 and abs(y - py) < 1e-9:
            return True

    return inside


File: src/utils/helpers.py
# File: src/utils/helpers.py
import torch
import numpy as np
import random
import os
import math
from typing import Optional
import logging

logger = logging.getLogger(__name__)


def get_device(device_preference: str = "auto") -> torch.device:
    """Gets the appropriate torch device based on preference and availability."""
    if device_preference == "cuda" and torch.cuda.is_available():
        logger.info("Using CUDA device.")
        return torch.device("cuda")
    # Note: MPS backend check might differ slightly across torch versions
    if (
        device_preference == "mps"
        and hasattr(torch.backends, "mps")
        and torch.backends.mps.is_available()
    ):
        logger.info("Using MPS device.")
        return torch.device("mps")
    if device_preference == "cpu":
        logger.info("Using CPU device.")
        return torch.device("cpu")

    # Auto selection priority: CUDA > MPS > CPU
    if torch.cuda.is_available():
        logger.info("Auto-selected CUDA device.")
        return torch.device("cuda")
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        logger.info("Auto-selected MPS device.")
        return torch.device("mps")

    logger.info("Auto-selected CPU device.")
    return torch.device("cpu")


def set_random_seeds(seed: int = 42):
    """Sets random seeds for Python, NumPy, and PyTorch."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # for multi-GPU
    # Add MPS seed setting if needed and available
    # if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    #     torch.mps.manual_seed(seed) # Check correct function if needed
    logger.info(f"Set random seeds to {seed}")


def format_eta(seconds: Optional[float]) -> str:
    """Formats seconds into a human-readable HH:MM:SS or MM:SS string."""
    if seconds is None or not np.isfinite(seconds) or seconds < 0:
        return "N/A"
    if seconds > 3600 * 24 * 30:  # Arbitrary limit for very long times
        return ">1 month"

    seconds = int(seconds)
    h, rem = divmod(seconds, 3600)
    m, s = divmod(rem, 60)

    if h > 0:
        return f"{h}h {m:02d}m {s:02d}s"
    if m > 0:
        return f"{m}m {s:02d}s"
    return f"{s}s"


File: src/structs/constants.py
# File: src/structs/constants.py
from typing import List, Tuple

# Colors for randomly generated shapes (moved from EnvConfig)
SHAPE_COLORS: List[Tuple[int, int, int]] = [
    (220, 40, 40),  # Red
    (60, 60, 220),  # Blue
    (40, 200, 40),  # Green
    (230, 230, 40),  # Yellow
    (240, 150, 20),  # Orange
    (140, 40, 140),  # Purple
    (40, 200, 200),  # Cyan
]


File: src/structs/__init__.py
# File: src/structs/__init__.py
"""
Module for core data structures used across different parts of the application,
like environment, visualization, and features. Helps avoid circular dependencies.
"""
from .triangle import Triangle
from .shape import Shape
from .constants import SHAPE_COLORS

__all__ = [
    "Triangle",
    "Shape",
    "SHAPE_COLORS",
]


File: src/structs/README.md
# File: src/structs/README.md
# Core Structures Module (`src.structs`)

## Purpose and Architecture

This module defines fundamental data structures and constants that are shared across multiple major components of the application (like `environment`, `visualization`, `features`). Its primary purpose is to break potential circular dependencies that arise when these components need to know about the same basic building blocks.

-   **`triangle.py`:** Defines the `Triangle` class, representing a single cell on the game grid.
-   **`shape.py`:** Defines the `Shape` class, representing a placeable piece composed of triangles.
-   **`constants.py`:** Defines shared constants, such as the list of possible `SHAPE_COLORS`.

By placing these core definitions in a low-level module with minimal dependencies, other modules can import them without creating import cycles.

## Exposed Interfaces

-   **Classes:**
    -   `Triangle`: Represents a grid cell.
    -   `Shape`: Represents a placeable piece.
-   **Constants:**
    -   `SHAPE_COLORS`: A list of RGB tuples for shape generation.

## Dependencies

This module has minimal dependencies, primarily relying on standard Python libraries (`typing`). It should **not** import from higher-level modules like `environment`, `visualization`, `nn`, `rl`, etc.

---

**Note:** This module should only contain widely shared, fundamental data structures and constants. More complex logic or structures specific to a particular domain (like game rules or rendering details) should remain in their respective modules.

File: src/structs/triangle.py
# File: src/structs/triangle.py
from __future__ import annotations  # For type hinting Triangle within Triangle
from typing import Tuple, Optional, List


class Triangle:
    """Represents a single triangular cell on the grid."""

    def __init__(self, row: int, col: int, is_up: bool, is_death: bool = False):
        self.row = row
        self.col = col
        self.is_up = is_up  # Orientation: True=points up, False=points down
        self.is_death = is_death  # Is this cell part of the non-playable border?
        self.is_occupied = is_death  # Occupied if it's a death cell initially
        self.color: Optional[Tuple[int, int, int]] = (
            None  # Color if occupied by a shape
        )

        # Neighbor references (set by grid_neighbors.link_neighbors)
        self.neighbor_left: Optional["Triangle"] = None
        self.neighbor_right: Optional["Triangle"] = None
        self.neighbor_vert: Optional["Triangle"] = (
            None  # The one sharing the horizontal base/tip
        )

    def get_points(
        self, ox: float, oy: float, cw: float, ch: float
    ) -> List[Tuple[float, float]]:
        """Calculates vertex points for drawing, relative to origin (ox, oy)."""
        # x, y is the top-left corner of the bounding box for this triangle
        x = ox + self.col * (cw * 0.75)  # Effective width is 0.75 * cell width
        y = oy + self.row * ch
        # Vertices depend on orientation
        if self.is_up:
            # Points up: base is at bottom
            return [(x, y + ch), (x + cw, y + ch), (x + cw / 2, y)]
        else:
            # Points down: base is at top
            return [(x, y), (x + cw, y), (x + cw / 2, y + ch)]

    def copy(self) -> "Triangle":
        """Creates a copy of the Triangle object's state (neighbors are not copied)."""
        new_tri = Triangle(self.row, self.col, self.is_up, self.is_death)
        new_tri.is_occupied = self.is_occupied
        new_tri.color = self.color
        # Neighbors must be relinked in the copied grid structure
        return new_tri

    def __repr__(self) -> str:
        state = "D" if self.is_death else ("O" if self.is_occupied else ".")
        orient = "^" if self.is_up else "v"
        return f"T({self.row},{self.col} {orient}{state})"

    # Make hashable based on position for use in sets/dicts if needed
    def __hash__(self):
        return hash((self.row, self.col))

    def __eq__(self, other):
        if not isinstance(other, Triangle):
            return NotImplemented
        return self.row == other.row and self.col == other.col


File: src/structs/shape.py
# File: src/structs/shape.py
from __future__ import annotations  # For type hinting Shape within Shape
from typing import List, Tuple


class Shape:
    """Represents a polyomino-like shape made of triangles."""

    def __init__(
        self, triangles: List[Tuple[int, int, bool]], color: Tuple[int, int, int]
    ):
        # (dr, dc, is_up) relative coords from a reference point (usually 0,0)
        self.triangles: List[Tuple[int, int, bool]] = triangles
        self.color: Tuple[int, int, int] = color
        # Ensure triangles are centered around (0,0) or normalize if needed?
        # For now, assume generator provides relative coords correctly.

    def bbox(self) -> Tuple[int, int, int, int]:
        """Calculates bounding box (min_r, min_c, max_r, max_c) in relative coords."""
        if not self.triangles:
            return (0, 0, 0, 0)
        rows = [t[0] for t in self.triangles]
        cols = [t[1] for t in self.triangles]
        return (min(rows), min(cols), max(rows), max(cols))

    def copy(self) -> "Shape":
        """Creates a shallow copy (triangle list is copied, color is shared)."""
        new_shape = Shape.__new__(Shape)
        new_shape.triangles = list(self.triangles)  # Copy the list of tuples
        new_shape.color = self.color  # Color tuple is immutable, can share
        return new_shape

    def __str__(self) -> str:
        return f"Shape(Color:{self.color}, Tris:{len(self.triangles)})"


File: src/environment/__init__.py
# File: src/environment/__init__.py
"""
Environment module defining the game rules, state, actions, and logic.
This module is now independent of feature extraction for the NN.
"""
# Core components
from .core.game_state import GameState
from .core.action_codec import encode_action, decode_action

# Grid related components
from .grid.grid_data import GridData

# Removed: from .grid.triangle import Triangle
from .grid import logic as GridLogic  # Expose grid logic functions via a namespace

# Shape related components
# Removed: from .shapes.shape import Shape
from .shapes import logic as ShapeLogic  # Expose shape logic functions via a namespace

# Game Logic components (Actions, Step)
from .logic.actions import get_valid_actions
from .logic.step import execute_placement, calculate_reward

# Configuration (often needed alongside environment components)
from src.config import EnvConfig


__all__ = [
    # Core
    "GameState",
    "encode_action",
    "decode_action",
    # Grid
    "GridData",
    # "Triangle", # Removed
    "GridLogic",
    # Shapes
    # "Shape", # Removed
    "ShapeLogic",
    # Logic
    "get_valid_actions",
    "execute_placement",
    "calculate_reward",
    # Config
    "EnvConfig",
]


File: src/environment/README.md
# File: src/environment/README.md
# Environment Module (`src.environment`)

## Purpose and Architecture

This module defines the game world for AlphaTriangle. It encapsulates the rules, state representation, actions, and core game logic. **Crucially, this module is now independent of any feature extraction logic specific to the neural network.** Its sole focus is the simulation of the game itself.

-   **State Representation:** `GameState` holds the current board (`GridData`), available shapes (`List[Shape]`), score, and game status. It represents the canonical state of the game. It uses core structures like `Shape` and `Triangle` defined in `src.structs`.
-   **Core Logic:** Submodules (`grid`, `shapes`, `logic`) handle specific aspects like checking valid placements, clearing lines, managing shape generation, and calculating rewards. These logic modules operate on `GridData`, `Shape`, and `Triangle`.
-   **Action Handling:** `action_codec` provides functions to convert between a structured action (shape index, row, column) and a single integer representation used by the RL agent and MCTS.
-   **Modularity:** Separating grid logic, shape logic, and core state makes the code easier to understand and modify.

**Note:** Feature extraction (converting `GameState` to NN input tensors) is handled by the separate `src/features` module. Core data structures (`Triangle`, `Shape`) are defined in `src/structs`.

## Exposed Interfaces

-   **Core:**
    -   `GameState`: The main class representing the environment state.
        -   `reset()`
        -   `step(action_index: ActionType) -> Tuple[float, float, bool]`
        -   `valid_actions() -> List[ActionType]`
        -   `is_over() -> bool`
        -   `get_outcome() -> float`
        -   `copy() -> GameState`
        -   Public attributes like `grid_data`, `shapes`, `game_score`, `current_step`, etc.
    -   `encode_action(shape_idx: int, r: int, c: int, config: EnvConfig) -> ActionType`
    -   `decode_action(action_index: ActionType, config: EnvConfig) -> Tuple[int, int, int]`
-   **Grid:**
    -   `GridData`: Class holding grid triangle data and line information.
    -   `GridLogic`: Namespace containing functions like `link_neighbors`, `initialize_lines_and_index`, `can_place`, `check_and_clear_lines`.
-   **Shapes:**
    -   `ShapeLogic`: Namespace containing functions like `refill_shape_slots`, `generate_random_shape`.
-   **Logic:**
    -   `get_valid_actions(game_state: GameState) -> List[ActionType]`
    -   `execute_placement(game_state: GameState, shape_idx: int, r: int, c: int, rng: random.Random) -> float`
    -   `calculate_reward(...) -> float` (Used internally by `execute_placement`)
-   **Config:**
    -   `EnvConfig`: Configuration class (re-exported for convenience).

## Dependencies

-   **`src.config`**:
    -   Uses `EnvConfig` extensively to define grid dimensions, shape slots, etc.
-   **`src.structs`**:
    -   Uses `Triangle`, `Shape`, `SHAPE_COLORS`.
-   **`src.utils.types`**:
    -   Uses `ActionType`.
-   **`numpy`**:
    -   Used for grid representation (`GridData`).
-   **Standard Libraries:** `typing`, `numpy`, `logging`, `random`.

---

**Note:** Please keep this README updated when changing game rules, state representation, action space, or the module's internal structure. Accurate documentation is crucial for maintainability.

File: src/environment/core/game_state.py
# File: src/environment/core/game_state.py
from typing import List, Optional, Tuple, Dict, Any
import numpy as np
import logging
import random

# Use relative imports within the environment package
from ...config import EnvConfig
from ...utils.types import ActionType

# Import necessary submodules directly using relative paths
from ..grid.grid_data import GridData
from ..grid import logic as GridLogic
from ..shapes import logic as ShapeLogic
from .action_codec import encode_action, decode_action
from ..logic.actions import get_valid_actions
from ..logic.step import execute_placement

# Import Shape from the new structs module
from src.structs import Shape

logger = logging.getLogger(__name__)


class GameState:
    """Represents the mutable state of the game. Does not handle NN feature extraction."""

    def __init__(
        self, config: Optional[EnvConfig] = None, initial_seed: Optional[int] = None
    ):
        self.env_config = config if config else EnvConfig()
        self._rng = (
            random.Random(initial_seed) if initial_seed is not None else random.Random()
        )

        self.grid_data: GridData = None  # type: ignore Will be initialized
        self.shapes: List[Optional[Shape]] = []  # Uses Shape from structs
        self.game_score: float = 0.0
        self.game_over: bool = False
        self.triangles_cleared_this_episode: int = 0
        self.pieces_placed_this_episode: int = 0
        self.current_step: int = 0  # Track moves within the episode

        # State for interactive/debug modes (kept separate from core game logic)
        self.demo_selected_shape_idx: int = -1
        self.demo_snapped_position: Optional[Tuple[int, int]] = None
        self.debug_highlight_pos: Optional[Tuple[int, int]] = None
        # State for visualization during training (attached by worker)
        self.display_stats: Dict[str, Any] = {}

        self.reset()

    def reset(self):
        """Resets the game to the initial state."""
        self.grid_data = GridData(self.env_config)
        self.shapes = [None] * self.env_config.NUM_SHAPE_SLOTS
        self.game_score = 0.0
        self.triangles_cleared_this_episode = 0
        self.pieces_placed_this_episode = 0
        self.game_over = False
        self.current_step = 0
        self.demo_selected_shape_idx = -1
        self.demo_snapped_position = None
        self.debug_highlight_pos = None
        self.display_stats = {}

        ShapeLogic.refill_shape_slots(self, self._rng)  # Uses ShapeLogic relatively

        if not self.valid_actions():
            logger.warning(
                "Game is over immediately after reset (no valid initial moves)."
            )
            self.game_over = True

    def step(self, action_index: ActionType) -> Tuple[float, float, bool]:
        """Performs one game step. Returns (value_estimate_placeholder, reward, done)."""
        if self.is_over():
            logger.warning("Attempted to step in a game that is already over.")
            return 0.0, 0.0, True

        shape_idx, r, c = decode_action(action_index, self.env_config)
        reward = execute_placement(self, shape_idx, r, c, self._rng)
        self.current_step += 1

        # Check for game over *after* placement and shape refill
        if not self.game_over and not self.valid_actions():
            self.game_over = True
            # Log only once when the state transitions to game over
            logger.info(f"Game over detected after step {self.current_step}.")

        # Placeholder value (0.0) is returned, actual value comes from NN/MCTS later
        return 0.0, reward, self.game_over

    def valid_actions(self) -> List[ActionType]:
        """Returns a list of valid encoded action indices."""
        return get_valid_actions(self)

    def is_over(self) -> bool:
        """Checks if the game is over."""
        return self.game_over

    def get_outcome(self) -> float:
        """Returns the terminal outcome value (e.g., final score). Used by MCTS."""
        if not self.is_over():
            logger.warning("get_outcome() called on a non-terminal state.")
        return self.game_score

    def copy(self) -> "GameState":
        """Creates a deep copy for simulations (e.g., MCTS)."""
        new_state = GameState.__new__(GameState)
        new_state.env_config = self.env_config
        # Correctly copy the RNG state
        new_state._rng = random.Random()  # Create a new instance
        new_state._rng.setstate(self._rng.getstate())  # Set its state
        new_state.grid_data = self.grid_data.deepcopy()
        new_state.shapes = [
            s.copy() if s else None for s in self.shapes
        ]  # Uses Shape from structs
        new_state.game_score = self.game_score
        new_state.game_over = self.game_over
        new_state.triangles_cleared_this_episode = self.triangles_cleared_this_episode
        new_state.pieces_placed_this_episode = self.pieces_placed_this_episode
        new_state.current_step = self.current_step
        new_state.demo_selected_shape_idx = self.demo_selected_shape_idx
        new_state.demo_snapped_position = self.demo_snapped_position
        new_state.debug_highlight_pos = self.debug_highlight_pos
        new_state.display_stats = self.display_stats.copy()
        return new_state

    def __str__(self) -> str:
        shape_strs = [str(s) if s else "None" for s in self.shapes]
        return f"GameState(Step:{self.current_step}, Score:{self.game_score:.1f}, Over:{self.is_over()}, Shapes:[{', '.join(shape_strs)}])"


File: src/environment/core/__init__.py


File: src/environment/core/README.md
# File: src/environment/core/README.md
# Environment Core Submodule (`src.environment.core`)

## Purpose and Architecture

This submodule contains the most fundamental components of the game environment: the `GameState` class and the `action_codec`.

-   **`GameState`:** This class acts as the central hub for the environment's state. It holds references to the `GridData`, the current shapes, score, game status, and other relevant information. It provides the primary interface (`reset`, `step`, `get_state`, `valid_actions`, `is_over`, `get_outcome`, `copy`) for agents (like MCTS or self-play workers) to interact with the game. It delegates specific logic (like placement validation, line clearing, shape generation) to other submodules (`grid`, `shapes`, `logic`, `features`).
-   **`action_codec`:** Provides simple, stateless functions (`encode_action`, `decode_action`) to translate between the agent's integer action representation and the game's internal representation (shape index, row, column). This decouples the agent's action space from the internal game logic.

## Exposed Interfaces

-   **Classes:**
    -   `GameState`: The main state class (see `src/environment/README.md` for methods).
-   **Functions:**
    -   `encode_action(shape_idx: int, r: int, c: int, config: EnvConfig) -> ActionType`
    -   `decode_action(action_index: ActionType, config: EnvConfig) -> Tuple[int, int, int]`

## Dependencies

-   **`src.config`**:
    -   `EnvConfig`: Used by `GameState` and `action_codec`.
-   **`src.utils.types`**:
    -   `StateType`, `ActionType`: Used for method signatures and return types.
-   **`src.environment.grid`**:
    -   `GridData`, `GridLogic`: Used internally by `GameState`.
-   **`src.environment.shapes`**:
    -   `Shape`, `ShapeLogic`: Used internally by `GameState`.
-   **`src.environment.features`**:
    -   `extract_state_features`: Used internally by `GameState.get_state()`.
-   **`src.environment.logic`**:
    -   `get_valid_actions`, `execute_placement`: Used internally by `GameState`.
-   **Standard Libraries:** `typing`, `numpy`, `logging`, `random`.

---

**Note:** Please keep this README updated when modifying the core `GameState` interface or the action encoding/decoding scheme. Accurate documentation is crucial for maintainability.

File: src/environment/core/action_codec.py
# File: src/environment/action_codec.py
from typing import Tuple
from src.config import EnvConfig  # Updated import
from src.utils.types import ActionType  # Updated import


def encode_action(shape_idx: int, r: int, c: int, config: EnvConfig) -> ActionType:
    """Encodes a (shape_idx, r, c) action into a single integer."""
    if not (0 <= shape_idx < config.NUM_SHAPE_SLOTS):
        raise ValueError(
            f"Invalid shape index: {shape_idx}, must be < {config.NUM_SHAPE_SLOTS}"
        )
    if not (0 <= r < config.ROWS):
        raise ValueError(f"Invalid row index: {r}, must be < {config.ROWS}")
    if not (0 <= c < config.COLS):
        raise ValueError(f"Invalid column index: {c}, must be < {config.COLS}")

    # Action = Shape * (GridSize) + Row * (NumCols) + Col
    action_index = shape_idx * (config.ROWS * config.COLS) + r * config.COLS + c
    return action_index


def decode_action(action_index: ActionType, config: EnvConfig) -> Tuple[int, int, int]:
    """Decodes an integer action into (shape_idx, r, c)."""
    if not (0 <= action_index < config.ACTION_DIM):
        raise ValueError(
            f"Invalid action index: {action_index}, must be < {config.ACTION_DIM}"
        )

    # Reverse the encoding process
    grid_size = config.ROWS * config.COLS
    shape_idx = action_index // grid_size
    remainder = action_index % grid_size
    r = remainder // config.COLS
    c = remainder % config.COLS

    return shape_idx, r, c


File: src/environment/shapes/__init__.py


File: src/environment/shapes/README.md
# File: src/environment/shapes/README.md
# Environment Shapes Submodule (`src.environment.shapes`)

## Purpose and Architecture

This submodule defines the logic for managing placeable shapes within the game environment.

-   **Shape Representation:** The `Shape` class (defined in `src.structs`) stores the geometry of a shape as a list of relative triangle coordinates (`(dr, dc, is_up)`) and its color.
-   **Shape Logic:** The `logic.py` module (exposed as `ShapeLogic`) contains functions related to shapes:
    -   `generate_random_shape`: Creates a new `Shape` instance with a random configuration of triangles and a random color (using `SHAPE_COLORS` from `src.structs`). The complexity and variety of shapes are determined here.
    -   `refill_shape_slots`: Manages the available shapes in the `GameState`. When a shape is placed, this function is called to potentially replace the used slot with a new random shape, ensuring the player/agent always has a selection (up to `NUM_SHAPE_SLOTS`).

## Exposed Interfaces

-   **Modules/Namespaces:**
    -   `logic` (often imported as `ShapeLogic`):
        -   `generate_random_shape(rng: random.Random) -> Shape`
        -   `refill_shape_slots(game_state: GameState, rng: random.Random)`

## Dependencies

-   **`src.environment.core`**:
    -   `GameState`: Used by `ShapeLogic.refill_shape_slots` to access and modify the list of available shapes.
-   **`src.config`**:
    -   `EnvConfig`: Accessed via `GameState` (e.g., for `NUM_SHAPE_SLOTS`).
-   **`src.structs`**:
    -   Uses `Shape`, `SHAPE_COLORS`.
-   **Standard Libraries:** `typing`, `random`, `logging`.

---

**Note:** Please keep this README updated when changing the shape generation algorithm or the logic for managing shape slots in the game state. Accurate documentation is crucial for maintainability.

File: src/environment/shapes/shape.py
# File: src/environment/shapes/shape.py
import random
from typing import List, Tuple, Optional

# Removed: from src.visualization import colors


class Shape:
    """Represents a polyomino-like shape made of triangles."""

    def __init__(
        self, triangles: List[Tuple[int, int, bool]], color: Tuple[int, int, int]
    ):
        # (dr, dc, is_up) relative coords from a reference point (usually 0,0)
        self.triangles: List[Tuple[int, int, bool]] = triangles
        self.color: Tuple[int, int, int] = color
        # Ensure triangles are centered around (0,0) or normalize if needed?
        # For now, assume generator provides relative coords correctly.

    def bbox(self) -> Tuple[int, int, int, int]:
        """Calculates bounding box (min_r, min_c, max_r, max_c) in relative coords."""
        if not self.triangles:
            return (0, 0, 0, 0)
        rows = [t[0] for t in self.triangles]
        cols = [t[1] for t in self.triangles]
        return (min(rows), min(cols), max(rows), max(cols))

    def copy(self) -> "Shape":
        """Creates a shallow copy (triangle list is copied, color is shared)."""
        new_shape = Shape.__new__(Shape)
        new_shape.triangles = list(self.triangles)  # Copy the list of tuples
        new_shape.color = self.color  # Color tuple is immutable, can share
        return new_shape

    def __str__(self) -> str:
        return f"Shape(Color:{self.color}, Tris:{len(self.triangles)})"


File: src/environment/grid/grid_data.py
# File: src/environment/grid/grid_data.py
import numpy as np
from typing import List, Tuple, Set, Dict, Optional
import logging

# Use relative imports within environment package
from ...config import EnvConfig
from . import logic as GridLogic  # Import logic module

# Import Triangle from the new structs module
from src.structs import Triangle

logger = logging.getLogger(__name__)


class GridData:
    """Holds the grid state (triangles, occupancy, death zones)."""

    def __init__(self, config: EnvConfig):
        self.rows = config.ROWS
        self.cols = config.COLS
        self.config = config
        self.triangles: List[List[Triangle]] = self._create(
            config
        )  # Uses Triangle from structs
        # Call link_neighbors from the logic module
        GridLogic.link_neighbors(self)

        self._occupied_np = np.array(
            [[t.is_occupied for t in r] for r in self.triangles], dtype=bool
        )
        self._death_np = np.array(
            [[t.is_death for t in r] for r in self.triangles], dtype=bool
        )

        self.potential_lines: Set[frozenset[Triangle]] = (
            set()
        )  # Uses Triangle from structs
        self._triangle_to_lines_map: Dict[Triangle, Set[frozenset[Triangle]]] = (
            {}
        )  # Uses Triangle from structs
        # Call initialize_lines_and_index from the logic module
        GridLogic.initialize_lines_and_index(self)
        logger.info(
            f"GridData initialized ({self.rows}x{self.cols}). Found {len(self.potential_lines)} potential lines."
        )

    def _create(
        self, config: EnvConfig
    ) -> List[List[Triangle]]:  # Uses Triangle from structs
        """Initializes the grid, marking death cells based on COLS_PER_ROW."""
        cols_per_row = config.COLS_PER_ROW
        if len(cols_per_row) != self.rows:
            raise ValueError(
                f"COLS_PER_ROW length mismatch: {len(cols_per_row)} vs {self.rows}"
            )
        if max(cols_per_row, default=0) > self.cols:
            raise ValueError(
                f"Max COLS_PER_ROW exceeds COLS: {max(cols_per_row, default=0)} vs {self.cols}"
            )

        grid = []
        for r in range(self.rows):
            row_tris = []
            intended_width = cols_per_row[r]
            padding = self.cols - intended_width
            pad_left = padding // 2
            start_col = pad_left
            end_col = self.cols - (padding - pad_left)  # Exclusive end

            # Border columns are death cells
            actual_start = start_col + 1
            actual_end = end_col - 1  # Exclusive end

            for c in range(self.cols):
                is_playable = intended_width >= 3 and actual_start <= c < actual_end
                is_death = not is_playable
                is_up = (r + c) % 2 == 0
                row_tris.append(
                    Triangle(r, c, is_up, is_death=is_death)
                )  # Uses Triangle from structs
            grid.append(row_tris)
        return grid

    def valid(self, r: int, c: int) -> bool:
        """Checks if coordinates are within grid bounds."""
        return 0 <= r < self.rows and 0 <= c < self.cols

    def get_occupied_state(self) -> np.ndarray:
        """Returns a copy of the occupancy numpy array."""
        return self._occupied_np.copy()

    def get_death_state(self) -> np.ndarray:
        """Returns a copy of the death zone numpy array."""
        return self._death_np.copy()

    def deepcopy(self) -> "GridData":
        """Creates a deep copy of the grid data."""
        new_grid = GridData.__new__(GridData)
        new_grid.rows = self.rows
        new_grid.cols = self.cols
        new_grid.config = self.config
        new_grid.triangles = [
            [tri.copy() for tri in row] for row in self.triangles
        ]  # Uses Triangle from structs
        # Relink neighbors in the new copy using logic module
        GridLogic.link_neighbors(new_grid)
        new_grid._occupied_np = self._occupied_np.copy()
        new_grid._death_np = self._death_np.copy()
        # Re-initialize line structures using logic module
        new_grid.potential_lines = set()
        new_grid._triangle_to_lines_map = {}
        GridLogic.initialize_lines_and_index(new_grid)
        return new_grid

    def __str__(self) -> str:
        # Simple representation for debugging
        return f"GridData({self.rows}x{self.cols})"


File: src/environment/grid/__init__.py
# File: src/environment/grid/__init__.py
"""
Grid submodule handling the triangular grid structure, data, and logic.
"""
from .grid_data import GridData

# Removed: from .triangle import Triangle
from . import logic

# DO NOT import grid_features here. It has been moved up one level
# to src/environment/grid_features.py to break circular dependencies.

__all__ = [
    "GridData",
    "logic",
]


File: src/environment/grid/README.md

# File: src/environment/grid/README.md
# Environment Grid Submodule (`src.environment.grid`)

## Purpose and Architecture

This submodule manages the game's grid structure and related logic. It defines the triangular cells, their properties, relationships, and operations like placement validation and line clearing.

-   **Cell Representation:** The `Triangle` class (defined in `src.structs`) represents a single cell, storing its position, orientation (`is_up`), state (`is_occupied`, `is_death`), color, and references to its immediate neighbors.
-   **Grid Data Structure:** The `GridData` class holds the 2D array of `Triangle` objects. It also maintains optimized `numpy` arrays (`_occupied_np`, `_death_np`) for faster state access and manages information about potential lines for efficient clearing checks.
-   **Grid Logic:** The `logic.py` module (exposed as `GridLogic`) contains functions operating on `GridData` and `Triangle` objects. This includes:
    -   Initializing the grid based on `EnvConfig` (defining death zones).
    -   Linking triangle neighbors.
    -   Finding and indexing potential lines.
    -   Checking if a shape can be placed (`can_place`).
    -   Checking for and clearing completed lines (`check_and_clear_lines`).
-   **Grid Features:** Note: The `grid_features.py` module, which provided functions to calculate scalar metrics (heights, holes, bumpiness), has been **moved** to the top-level `src/features` module (`src/features/grid_features.py`) as part of decoupling feature extraction from the core environment.

## Exposed Interfaces

-   **Classes:**
    -   `GridData`: Holds the grid state.
        -   `__init__(config: EnvConfig)`
        -   `valid(r: int, c: int) -> bool`
        -   `get_occupied_state() -> np.ndarray`
        -   `get_death_state() -> np.ndarray`
        -   `deepcopy() -> GridData`
-   **Modules/Namespaces:**
    -   `logic` (often imported as `GridLogic`):
        -   `link_neighbors(grid_data: GridData)`
        -   `initialize_lines_and_index(grid_data: GridData)`
        -   `can_place(grid_data: GridData, shape: Shape, r: int, c: int) -> bool`
        -   `check_and_clear_lines(grid_data: GridData, newly_occupied: Set[Triangle]) -> Tuple[int, int, List[Tuple[int, int]]]`

## Dependencies

-   **`src.config`**:
    -   `EnvConfig`: Used by `GridData` initialization and logic functions.
-   **`src.structs`**:
    -   Uses `Triangle`, `Shape`.
-   **`numpy`**:
    -   Used extensively in `GridData`.
-   **Standard Libraries:** `typing`, `logging`, `numpy`.

---

**Note:** Please keep this README updated when changing the grid structure, cell properties, placement rules, or line clearing logic. Accurate documentation is crucial for maintainability.

File: src/environment/grid/triangle.py
# File: src/environment/triangle.py
from typing import Tuple, Optional, List, TYPE_CHECKING

if TYPE_CHECKING:
    # Avoid circular import if Triangle needs GridData or vice-versa later
    # from .grid_data import GridData
    pass


class Triangle:
    """Represents a single triangular cell on the grid."""

    def __init__(self, row: int, col: int, is_up: bool, is_death: bool = False):
        self.row = row
        self.col = col
        self.is_up = is_up  # Orientation: True=points up, False=points down
        self.is_death = is_death  # Is this cell part of the non-playable border?
        self.is_occupied = is_death  # Occupied if it's a death cell initially
        self.color: Optional[Tuple[int, int, int]] = (
            None  # Color if occupied by a shape
        )

        # Neighbor references (set by grid_neighbors.link_neighbors)
        self.neighbor_left: Optional["Triangle"] = None
        self.neighbor_right: Optional["Triangle"] = None
        self.neighbor_vert: Optional["Triangle"] = (
            None  # The one sharing the horizontal base/tip
        )

    def get_points(
        self, ox: float, oy: float, cw: float, ch: float
    ) -> List[Tuple[float, float]]:
        """Calculates vertex points for drawing, relative to origin (ox, oy)."""
        # x, y is the top-left corner of the bounding box for this triangle
        x = ox + self.col * (cw * 0.75)  # Effective width is 0.75 * cell width
        y = oy + self.row * ch
        # Vertices depend on orientation
        if self.is_up:
            # Points up: base is at bottom
            return [(x, y + ch), (x + cw, y + ch), (x + cw / 2, y)]
        else:
            # Points down: base is at top
            return [(x, y), (x + cw, y), (x + cw / 2, y + ch)]

    def copy(self) -> "Triangle":
        """Creates a copy of the Triangle object's state (neighbors are not copied)."""
        new_tri = Triangle(self.row, self.col, self.is_up, self.is_death)
        new_tri.is_occupied = self.is_occupied
        new_tri.color = self.color
        # Neighbors must be relinked in the copied grid structure
        return new_tri

    def __repr__(self) -> str:
        state = "D" if self.is_death else ("O" if self.is_occupied else ".")
        orient = "^" if self.is_up else "v"
        return f"T({self.row},{self.col} {orient}{state})"

    # Make hashable based on position for use in sets/dicts if needed
    def __hash__(self):
        return hash((self.row, self.col))

    def __eq__(self, other):
        if not isinstance(other, Triangle):
            return NotImplemented
        return self.row == other.row and self.col == other.col


File: src/data/__init__.py
# File: src/data/__init__.py
"""
Data management module for handling checkpoints, buffers, and potentially logs.
Uses Pydantic schemas for data structure definition.
"""
from .data_manager import DataManager
from .schemas import CheckpointData, BufferData, LoadedTrainingState # Export schemas

__all__ = [
    "DataManager",
    "CheckpointData", # Export schema
    "BufferData",     # Export schema
    "LoadedTrainingState", # Export schema
]

File: src/data/README.md
# File: src/data/README.md
# Data Management Module (`src.data`)

## Purpose and Architecture

This module is responsible for handling the persistence of training artifacts using structured data schemas defined with Pydantic. It manages:

-   Neural network checkpoints (model weights, optimizer state).
-   Experience replay buffers.
-   Statistics collector state.
-   Run configuration files.

The core component is the `DataManager` class, which centralizes file path management and saving/loading logic based on the `PersistenceConfig` and `TrainConfig`. It uses `cloudpickle` for robust serialization of complex Python objects, including Pydantic models containing tensors and deques.

-   **Schemas (`schemas.py`):** Defines Pydantic models (`CheckpointData`, `BufferData`, `LoadedTrainingState`) to structure the data being saved and loaded, ensuring clarity and enabling validation.
-   **Centralization:** Provides a single point of control for saving/loading operations.
-   **Configuration-Driven:** Uses `PersistenceConfig` and `TrainConfig` to determine save locations, filenames, and loading behavior (e.g., auto-resume).
-   **Serialization:** Uses `cloudpickle` to serialize/deserialize the Pydantic model instances, which effectively handles nested complex objects like tensors and deques within the models.
-   **Run Management:** Organizes saved artifacts into subdirectories based on the `RUN_NAME`.
-   **State Loading:** Provides `load_initial_state` to determine the correct files, deserialize them using `cloudpickle`, validate the structure with Pydantic models, and return a `LoadedTrainingState` object.
-   **State Saving:** Provides `save_training_state` to assemble data into Pydantic models (`CheckpointData`, `BufferData`), serialize them using `cloudpickle`, and save to files.
-   **MLflow Integration:** Logs saved artifacts (checkpoints, buffers, configs) to MLflow after successful local saving.

## Exposed Interfaces

-   **Classes:**
    -   `DataManager`:
        -   `__init__(persist_config: PersistenceConfig, train_config: TrainConfig)`
        -   `load_initial_state() -> LoadedTrainingState`: Loads state, returns Pydantic model.
        -   `save_training_state(...)`: Saves state using Pydantic models and cloudpickle.
        -   `save_run_config(configs: Dict[str, Any])`: Saves config JSON.
        -   `get_checkpoint_path(...) -> str`
        -   `get_buffer_path(...) -> str`
        -   `find_latest_run_dir(...) -> Optional[str]`
    -   `CheckpointData` (from `schemas.py`): Pydantic model for checkpoint structure.
    -   `BufferData` (from `schemas.py`): Pydantic model for buffer structure.
    -   `LoadedTrainingState` (from `schemas.py`): Pydantic model wrapping loaded data.

## Dependencies

-   **`src.config`**: `PersistenceConfig`, `TrainConfig`.
-   **`src.nn`**: `NeuralNetwork`.
-   **`src.rl.core.buffer`**: `ExperienceBuffer`.
-   **`src.stats`**: `StatsCollectorActor`.
-   **`src.utils.types`**: `Experience`.
-   **`torch.optim`**: `Optimizer`.
-   **Standard Libraries:** `os`, `shutil`, `logging`, `glob`, `re`, `json`, `collections.deque`.
-   **Third-Party:** `pydantic`, `cloudpickle`, `torch`, `ray`, `mlflow`.

---

**Note:** Please keep this README updated when changing the Pydantic schemas, the types of artifacts managed, the saving/loading mechanisms, or the responsibilities of the `DataManager`.

File: src/data/schemas.py
# File: src/data/schemas.py
from pydantic import BaseModel, Field, ConfigDict, model_validator
from typing import Dict, Any, List, Optional, Tuple, TYPE_CHECKING
from collections import deque

# Import Experience type hint carefully
from src.utils.types import Experience

# Import GameState OUTSIDE TYPE_CHECKING for Pydantic model_rebuild
from src.environment import GameState

# Pydantic configuration to allow arbitrary types like torch.Tensor and deque
arbitrary_types_config = ConfigDict(arbitrary_types_allowed=True)

class CheckpointData(BaseModel):
    """Pydantic model defining the structure of saved checkpoint data."""
    model_config = arbitrary_types_config

    run_name: str
    global_step: int = Field(..., ge=0)
    episodes_played: int = Field(..., ge=0)
    total_simulations_run: int = Field(..., ge=0)
    model_config_dict: Dict[str, Any]
    env_config_dict: Dict[str, Any]
    model_state_dict: Dict[str, Any] # Placeholder for torch tensors
    optimizer_state_dict: Dict[str, Any] # Placeholder for optimizer state
    stats_collector_state: Dict[str, Any] # Contains lists from deque conversion

class BufferData(BaseModel):
    """Pydantic model defining the structure of saved buffer data."""
    model_config = arbitrary_types_config

    # Store buffer as a list for better serialization compatibility
    # Type hint uses forward reference string 'Experience' if needed,
    # but direct import might work if GameState is handled correctly.
    # Use the Experience type alias which internally uses "GameState" forward ref
    buffer_list: List[Experience]

# Type hint for the combined loaded state, using the Pydantic models
class LoadedTrainingState(BaseModel):
    """Pydantic model representing the fully loaded state."""
    model_config = arbitrary_types_config

    checkpoint_data: Optional[CheckpointData] = None
    buffer_data: Optional[BufferData] = None

# Crucial step: Explicitly rebuild models to resolve forward references
# This is needed because Experience -> GameState is a forward reference
# that Pydantic needs help resolving in this context.
# Now that GameState is imported at runtime, this should work.
BufferData.model_rebuild(force=True)
LoadedTrainingState.model_rebuild(force=True)

File: src/data/data_manager.py
# File: src/data/data_manager.py
import os
import shutil
import logging
import glob
import cloudpickle
import torch
import ray
import re
import json
import mlflow
from typing import TYPE_CHECKING, Optional, Tuple, Dict, Any, List
from collections import deque
from pydantic import ValidationError

# Import Pydantic models and Experience type
from .schemas import CheckpointData, BufferData, LoadedTrainingState
from src.utils.types import Experience

if TYPE_CHECKING:
    from src.nn import NeuralNetwork
    from src.rl.core.buffer import ExperienceBuffer
    from src.config import PersistenceConfig, TrainConfig, MCTSConfig # Added MCTSConfig
    from src.stats import StatsCollectorActor
    from torch.optim import Optimizer

logger = logging.getLogger(__name__)


class DataManager:
    """
    Manages loading and saving of training artifacts using Pydantic schemas
    and cloudpickle for serialization. Handles MLflow artifact logging.
    """

    def __init__(self, persist_config: "PersistenceConfig", train_config: "TrainConfig"):
        self.persist_config = persist_config
        self.train_config = train_config
        if not self.persist_config.RUN_NAME or self.persist_config.RUN_NAME == "default_run":
            logger.warning("DataManager RUN_NAME not set. Using default/current value.")
        os.makedirs(self.persist_config.ROOT_DATA_DIR, exist_ok=True)
        self._update_paths()
        self._create_directories()
        logger.info(f"DataManager initialized. Current Run Name: {self.persist_config.RUN_NAME}. Run directory: {self.run_base_dir}")

    def _update_paths(self):
        """Updates paths based on the current RUN_NAME."""
        self.run_base_dir = self.persist_config.get_run_base_dir()
        self.checkpoint_dir = os.path.join(self.run_base_dir, self.persist_config.CHECKPOINT_SAVE_DIR_NAME)
        self.buffer_dir = os.path.join(self.run_base_dir, self.persist_config.BUFFER_SAVE_DIR_NAME)
        self.log_dir = os.path.join(self.run_base_dir, self.persist_config.LOG_DIR_NAME)
        self.config_path = os.path.join(self.run_base_dir, self.persist_config.CONFIG_FILENAME)

    def _create_directories(self):
        """Creates necessary temporary directories for the current run."""
        os.makedirs(self.run_base_dir, exist_ok=True)
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        if self.persist_config.SAVE_BUFFER:
            os.makedirs(self.buffer_dir, exist_ok=True)

    def get_checkpoint_path(self, run_name: Optional[str] = None, step: Optional[int] = None, is_latest: bool = False, is_best: bool = False, is_final: bool = False) -> str:
        """Constructs the path for a checkpoint file."""
        target_run_name = run_name if run_name else self.persist_config.RUN_NAME
        base_dir = self.persist_config.get_run_base_dir(target_run_name)
        checkpoint_dir = os.path.join(base_dir, self.persist_config.CHECKPOINT_SAVE_DIR_NAME)
        if is_latest: filename = self.persist_config.LATEST_CHECKPOINT_FILENAME
        elif is_best: filename = self.persist_config.BEST_CHECKPOINT_FILENAME
        elif is_final and step is not None: filename = f"checkpoint_final_step_{step}.pkl"
        elif step is not None: filename = f"checkpoint_step_{step}.pkl"
        else: filename = self.persist_config.LATEST_CHECKPOINT_FILENAME
        base, _ = os.path.splitext(filename)
        filename_pkl = base + ".pkl"
        return os.path.join(checkpoint_dir, filename_pkl)

    def get_buffer_path(self, run_name: Optional[str] = None, step: Optional[int] = None, is_final: bool = False) -> str:
        """Constructs the path for the replay buffer file."""
        target_run_name = run_name if run_name else self.persist_config.RUN_NAME
        base_dir = self.persist_config.get_run_base_dir(target_run_name)
        buffer_dir = os.path.join(base_dir, self.persist_config.BUFFER_SAVE_DIR_NAME)
        if is_final and step is not None: filename = f"buffer_final_step_{step}.pkl"
        elif step is not None and self.persist_config.BUFFER_SAVE_FREQ_STEPS > 0: filename = f"buffer_step_{step}.pkl"
        else: filename = self.persist_config.BUFFER_FILENAME
        return os.path.join(buffer_dir, filename)

    def find_latest_run_dir(self, current_run_name: str) -> Optional[str]:
        """Finds the most recent *previous* run directory."""
        runs_root_dir = os.path.join(self.persist_config.ROOT_DATA_DIR, self.persist_config.RUNS_DIR_NAME)
        run_prefix = "train_"
        try:
            if not os.path.exists(runs_root_dir): return None
            potential_dirs = [d for d in os.listdir(runs_root_dir) if os.path.isdir(os.path.join(runs_root_dir, d)) and d.startswith(run_prefix) and d != current_run_name]
            if not potential_dirs: return None
            def extract_timestamp(dir_name):
                match = re.search(r'(\d{8}_\d{6})', dir_name)
                return match.group(1) if match else "0"
            potential_dirs.sort(key=extract_timestamp, reverse=True)
            return potential_dirs[0]
        except Exception as e:
            logger.error(f"Error finding latest run directory: {e}", exc_info=True)
            return None

    def _determine_checkpoint_to_load(self) -> Optional[str]:
        """Determines the absolute path of the checkpoint file to load."""
        load_path_config = self.train_config.LOAD_CHECKPOINT_PATH
        auto_resume = self.train_config.AUTO_RESUME_LATEST
        current_run_name = self.persist_config.RUN_NAME
        checkpoint_to_load = None
        if load_path_config and os.path.exists(load_path_config):
            checkpoint_to_load = os.path.abspath(load_path_config)
            logger.info(f"Using specified checkpoint path: {checkpoint_to_load}")
        elif auto_resume:
            latest_run_name = self.find_latest_run_dir(current_run_name)
            if latest_run_name:
                potential_latest_path = self.get_checkpoint_path(run_name=latest_run_name, is_latest=True)
                if os.path.exists(potential_latest_path):
                    checkpoint_to_load = os.path.abspath(potential_latest_path)
                    logger.info(f"Auto-resuming from latest checkpoint: {checkpoint_to_load}")
        if not checkpoint_to_load:
            logger.info("No checkpoint found to load. Starting training from scratch.")
        return checkpoint_to_load

    def _determine_buffer_to_load(self, checkpoint_path: Optional[str]) -> Optional[str]:
        """Determines the buffer file path to load."""
        if self.train_config.LOAD_BUFFER_PATH and os.path.exists(self.train_config.LOAD_BUFFER_PATH):
            logger.info(f"Using specified buffer path: {self.train_config.LOAD_BUFFER_PATH}")
            return os.path.abspath(self.train_config.LOAD_BUFFER_PATH)
        if checkpoint_path:
            try:
                run_name_loaded_from = os.path.basename(os.path.dirname(os.path.dirname(checkpoint_path)))
                potential_buffer_path = self.get_buffer_path(run_name=run_name_loaded_from)
                if os.path.exists(potential_buffer_path):
                    logger.info(f"Derived buffer path from checkpoint run: {potential_buffer_path}")
                    return os.path.abspath(potential_buffer_path)
            except Exception as e: logger.warning(f"Could not derive run name from checkpoint path {checkpoint_path}: {e}")
        logger.info("No suitable buffer file found to load.")
        return None

    def load_initial_state(self) -> LoadedTrainingState:
        """
        Loads the initial training state using Pydantic models for validation.
        Returns a LoadedTrainingState object containing the deserialized data.
        """
        loaded_state = LoadedTrainingState() # Initialize empty Pydantic model
        checkpoint_to_load = self._determine_checkpoint_to_load()

        if checkpoint_to_load:
            logger.info(f"Loading checkpoint: {checkpoint_to_load}")
            try:
                with open(checkpoint_to_load, "rb") as f:
                    # Cloudpickle loads the CheckpointData Pydantic model instance
                    loaded_checkpoint_model = cloudpickle.load(f)
                # Basic check if loaded object is of expected type
                if isinstance(loaded_checkpoint_model, CheckpointData):
                    loaded_state.checkpoint_data = loaded_checkpoint_model
                    logger.info(f"Checkpoint loaded and validated (Run: {loaded_state.checkpoint_data.run_name}, Step: {loaded_state.checkpoint_data.global_step})")
                else:
                    logger.error(f"Loaded checkpoint file {checkpoint_to_load} did not contain a CheckpointData object (type: {type(loaded_checkpoint_model)}).")
                    checkpoint_to_load = None # Prevent buffer derivation attempt
            except ValidationError as e:
                logger.error(f"Pydantic validation failed for checkpoint {checkpoint_to_load}: {e}", exc_info=True)
                checkpoint_to_load = None
            except Exception as e:
                logger.error(f"Error loading/validating checkpoint from {checkpoint_to_load}: {e}", exc_info=True)
                checkpoint_to_load = None

        if self.persist_config.SAVE_BUFFER:
            buffer_to_load = self._determine_buffer_to_load(checkpoint_to_load)
            if buffer_to_load:
                logger.info(f"Loading buffer: {buffer_to_load}")
                try:
                    with open(buffer_to_load, "rb") as f:
                        # Cloudpickle loads the BufferData Pydantic model instance
                        loaded_buffer_model = cloudpickle.load(f)
                    if isinstance(loaded_buffer_model, BufferData):
                        loaded_state.buffer_data = loaded_buffer_model
                        logger.info(f"Buffer loaded and validated. Size: {len(loaded_state.buffer_data.buffer_list)}")
                    else:
                        logger.error(f"Loaded buffer file {buffer_to_load} did not contain a BufferData object (type: {type(loaded_buffer_model)}).")
                except ValidationError as e:
                    logger.error(f"Pydantic validation failed for buffer {buffer_to_load}: {e}", exc_info=True)
                except Exception as e:
                    logger.error(f"Failed to load/validate experience buffer from {buffer_to_load}: {e}", exc_info=True)

        if not loaded_state.checkpoint_data and not loaded_state.buffer_data:
            logger.info("No checkpoint or buffer loaded. Starting fresh.")

        return loaded_state

    def save_training_state(
        self,
        nn: "NeuralNetwork",
        optimizer: "Optimizer",
        stats_collector_actor: "StatsCollectorActor",
        buffer: "ExperienceBuffer",
        global_step: int,
        episodes_played: int,
        total_simulations_run: int,
        is_best: bool = False,
        is_final: bool = False,
    ):
        """Saves the training state using Pydantic models and cloudpickle."""
        run_name = self.persist_config.RUN_NAME
        logger.info(f"Saving training state for run '{run_name}' at step {global_step}. Final={is_final}, Best={is_best}")

        stats_collector_state = {}
        if stats_collector_actor:
            try:
                stats_state_ref = stats_collector_actor.get_state.remote()
                stats_collector_state = ray.get(stats_state_ref, timeout=5.0)
            except Exception as e: logger.error(f"Error fetching state from StatsCollectorActor for saving: {e}", exc_info=True)

        optimizer_state_cpu = {}
        try:
            optimizer_state_dict = optimizer.state_dict()
            def move_to_cpu(item):
                if isinstance(item, torch.Tensor): return item.cpu()
                elif isinstance(item, dict): return {k: move_to_cpu(v) for k, v in item.items()}
                elif isinstance(item, list): return [move_to_cpu(elem) for elem in item]
                else: return item
            optimizer_state_cpu = move_to_cpu(optimizer_state_dict)
        except Exception as e: logger.error(f"Could not prepare optimizer state for saving: {e}")

        try:
            # Use model_dump() for Pydantic models
            checkpoint_data = CheckpointData(
                run_name=run_name, global_step=global_step, episodes_played=episodes_played,
                total_simulations_run=total_simulations_run,
                model_config_dict=nn.model_config.model_dump(), # Use model_dump
                env_config_dict=nn.env_config.model_dump(),     # Use model_dump
                model_state_dict=nn.get_weights(),
                optimizer_state_dict=optimizer_state_cpu,
                stats_collector_state=stats_collector_state,
            )
        except ValidationError as e:
            logger.error(f"Failed to create CheckpointData model: {e}", exc_info=True)
            return

        step_checkpoint_path = self.get_checkpoint_path(run_name=run_name, step=global_step, is_final=is_final)
        saved_checkpoint_path = None
        try:
            os.makedirs(os.path.dirname(step_checkpoint_path), exist_ok=True)
            with open(step_checkpoint_path, "wb") as f:
                cloudpickle.dump(checkpoint_data, f)
            logger.info(f"Checkpoint temporarily saved to {step_checkpoint_path}")
            saved_checkpoint_path = step_checkpoint_path
            latest_path = self.get_checkpoint_path(run_name=run_name, is_latest=True)
            best_path = self.get_checkpoint_path(run_name=run_name, is_best=True)
            try: shutil.copy2(step_checkpoint_path, latest_path)
            except Exception as e: logger.error(f"Failed to update latest checkpoint link: {e}")
            if is_best:
                try:
                    shutil.copy2(step_checkpoint_path, best_path)
                    logger.info(f"Updated best checkpoint link to step {global_step}")
                except Exception as e: logger.error(f"Failed to update best checkpoint link: {e}")
        except Exception as e: logger.error(f"Failed to save checkpoint file to {step_checkpoint_path}: {e}", exc_info=True)

        saved_buffer_path = None
        if self.persist_config.SAVE_BUFFER:
            buffer_path = self.get_buffer_path(run_name=run_name, step=global_step, is_final=is_final)
            default_buffer_path = self.get_buffer_path(run_name=run_name)
            try:
                buffer_data = BufferData(buffer_list=list(buffer.buffer))
                os.makedirs(os.path.dirname(buffer_path), exist_ok=True)
                with open(buffer_path, "wb") as f:
                    cloudpickle.dump(buffer_data, f)
                logger.info(f"Experience buffer temporarily saved to {buffer_path}")
                saved_buffer_path = buffer_path
                try:
                    with open(default_buffer_path, "wb") as f_default: cloudpickle.dump(buffer_data, f_default)
                    logger.debug(f"Updated default buffer file: {default_buffer_path}")
                except Exception as e_default: logger.error(f"Error updating default buffer file {default_buffer_path}: {e_default}")
            except ValidationError as e: logger.error(f"Failed to create BufferData model: {e}", exc_info=True)
            except Exception as e: logger.error(f"Error saving experience buffer to {buffer_path}: {e}", exc_info=True)

        self._log_artifacts(saved_checkpoint_path, saved_buffer_path, run_name, is_best)

    def _log_artifacts(self, checkpoint_path: Optional[str], buffer_path: Optional[str], run_name: str, is_best: bool):
        """Logs saved checkpoint and buffer files to MLflow."""
        try:
            if checkpoint_path and os.path.exists(checkpoint_path):
                ckpt_artifact_path = self.persist_config.CHECKPOINT_SAVE_DIR_NAME
                mlflow.log_artifact(checkpoint_path, artifact_path=ckpt_artifact_path)
                latest_path = self.get_checkpoint_path(run_name=run_name, is_latest=True)
                if os.path.exists(latest_path): mlflow.log_artifact(latest_path, artifact_path=ckpt_artifact_path)
                if is_best:
                    best_path = self.get_checkpoint_path(run_name=run_name, is_best=True)
                    if os.path.exists(best_path): mlflow.log_artifact(best_path, artifact_path=ckpt_artifact_path)
                logger.info(f"Logged checkpoint artifacts to MLflow path: {ckpt_artifact_path}")
            if buffer_path and os.path.exists(buffer_path):
                buffer_artifact_path = self.persist_config.BUFFER_SAVE_DIR_NAME
                mlflow.log_artifact(buffer_path, artifact_path=buffer_artifact_path)
                default_buffer_path = self.get_buffer_path(run_name=run_name)
                if os.path.exists(default_buffer_path): mlflow.log_artifact(default_buffer_path, artifact_path=buffer_artifact_path)
                logger.info(f"Logged buffer artifacts to MLflow path: {buffer_artifact_path}")
        except Exception as e: logger.error(f"Failed to log artifacts to MLflow: {e}", exc_info=True)

    def save_run_config(self, configs: Dict[str, Any]):
        """Saves the combined configuration dictionary as a JSON artifact."""
        try:
            config_path = self.config_path
            os.makedirs(os.path.dirname(config_path), exist_ok=True)
            # Use Pydantic's model_dump_json for potentially better serialization
            # Need to assemble the full config dict first
            with open(config_path, "w") as f:
                # Simple json dump is likely fine here as configs are basic types
                # Ensure all values are serializable
                def default_serializer(obj):
                    if isinstance(obj, (torch.Tensor, np.ndarray)): return '<tensor/array>'
                    if isinstance(obj, deque): return list(obj)
                    try: return str(obj) # Fallback
                    except: return '<not serializable>'
                json.dump(configs, f, indent=4, default=default_serializer)
            mlflow.log_artifact(config_path, artifact_path="config")
            logger.info("Logged combined config JSON to MLflow.")
        except Exception as e: logger.error(f"Failed to save/log run config JSON: {e}", exc_info=True)

File: src/interaction/event_processor.py
# File: src/interaction/event_processor.py
# Change: Updated imports for visualization.
import pygame
import logging
from typing import TYPE_CHECKING, Generator, Any

# Import specific modules/classes needed
from src import visualization  # Import top-level vis module

if TYPE_CHECKING:
    # Use specific type hint from visualization.core
    from src.visualization.core.visualizer import Visualizer

logger = logging.getLogger(__name__)


def process_pygame_events(
    visualizer: "Visualizer",
) -> Generator[pygame.event.Event, Any, bool]:
    """
    Processes basic Pygame events like QUIT, ESCAPE, VIDEORESIZE.
    Yields other events for mode-specific handlers.
    Returns False via StopIteration value if the application should quit, True otherwise.
    """
    should_quit = False
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            logger.info("Received QUIT event.")
            should_quit = True
            break
        if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
            logger.info("Received ESCAPE key press.")
            should_quit = True
            break
        if event.type == pygame.VIDEORESIZE:
            try:
                w, h = max(320, event.w), max(240, event.h)
                # Update screen directly on the visualizer instance
                visualizer.screen = pygame.display.set_mode((w, h), pygame.RESIZABLE)
                visualizer.layout_rects = (
                    None  # Force layout recalculation on visualizer
                )
                logger.info(f"Window resized to {w}x{h}")
            except pygame.error as e:
                logger.error(f"Error resizing window: {e}")
            yield event  # Yield resize event for potential further handling if needed
        else:
            yield event  # Yield other events

    return not should_quit  # Return True to continue, False to quit


File: src/interaction/__init__.py
# File: src/interaction/__init__.py
from .input_handler import InputHandler
from .event_processor import process_pygame_events
from .play_mode_handler import handle_play_click, update_play_hover
from .debug_mode_handler import handle_debug_click, update_debug_hover

__all__ = [
    "InputHandler",
    "process_pygame_events",
    "handle_play_click",
    "update_play_hover",
    "handle_debug_click",
    "update_debug_hover",
]


File: src/interaction/README.md
# File: src/interaction/README.md
# Interaction Module (`src.interaction`)

## Purpose and Architecture

This module handles user input (keyboard and mouse) for interactive modes of the application, such as "play" and "debug". It bridges the gap between raw Pygame events and actions within the game simulation (`GameState`).

-   **Event Processing:** `event_processor.py` handles common Pygame events like quitting (QUIT, ESC) and window resizing. It acts as a generator, yielding other events for mode-specific processing.
-   **Input Handler:** The `InputHandler` class is the main entry point. It receives Pygame events (via the `event_processor`), determines the current interaction mode ("play" or "debug"), and delegates event handling and hover updates to specific handler functions.
-   **Mode-Specific Handlers:** `play_mode_handler.py` and `debug_mode_handler.py` contain the logic specific to each mode:
    -   `play`: Handles selecting shapes from the preview area and placing them on the grid via clicks. Updates hover previews.
    -   `debug`: Handles toggling the state of individual triangles on the grid via clicks. Updates hover highlights.
-   **Decoupling:** It separates input handling logic from the core game simulation (`environment`) and rendering (`visualization`), although it needs references to both to function.

## Exposed Interfaces

-   **Classes:**
    -   `InputHandler`:
        -   `__init__(game_state: GameState, visualizer: Visualizer, mode: str, env_config: EnvConfig)`
        -   `handle_input() -> bool`: Processes events for one frame, returns `False` if quitting.
-   **Functions:**
    -   `process_pygame_events(visualizer: Visualizer) -> Generator[pygame.event.Event, Any, bool]`: Processes common events, yields others.
    -   `handle_play_click(...)`: Handles clicks in play mode.
    -   `update_play_hover(...)`: Updates hover state in play mode.
    -   `handle_debug_click(...)`: Handles clicks in debug mode.
    -   `update_debug_hover(...)`: Updates hover state in debug mode.

## Dependencies

-   **`src.environment`**:
    -   `GameState`: Modifies the game state based on user actions (placing shapes, toggling debug cells).
    -   `EnvConfig`: Used for coordinate mapping and action encoding.
    -   `GridLogic`, `ActionCodec`: Used by mode-specific handlers.
-   **`src.visualization`**:
    -   `Visualizer`: Used to get layout information (`grid_rect`, `preview_rects`) and for coordinate mapping (`get_grid_coords_from_screen`, `get_preview_index_from_screen`). Also updated directly during resize events.
    -   `VisConfig`: Accessed via `Visualizer`.
-   **`pygame`**:
    -   Relies heavily on Pygame for event handling (`pygame.event`, `pygame.mouse`) and constants (`MOUSEBUTTONDOWN`, `KEYDOWN`, etc.).
-   **Standard Libraries:** `typing`, `logging`.

---

**Note:** Please keep this README updated when adding new interaction modes, changing input handling logic, or modifying the interfaces between interaction, environment, and visualization. Accurate documentation is crucial for maintainability.
Use code with caution.


File: src/interaction/input_handler.py
# File: src/interaction/input_handler.py
# Change: Updated imports for environment, visualization.
import pygame
import logging
from typing import TYPE_CHECKING

# Use relative imports within interaction module
from . import event_processor, play_mode_handler, debug_mode_handler

# Import specific modules/classes needed from other packages
from src import environment
from src import visualization

logger = logging.getLogger(__name__)


class InputHandler:
    """Handles user input, delegating to mode-specific handlers."""

    def __init__(
        self,
        game_state: environment.GameState,  # Use specific type hint
        visualizer: visualization.Visualizer,  # Use specific type hint
        mode: str,
        env_config: environment.EnvConfig,  # Use specific type hint
    ):
        self.game_state = game_state
        self.visualizer = visualizer
        self.mode = mode
        self.env_config = env_config

    def handle_input(self) -> bool:
        """Processes Pygame events and updates state based on mode. Returns False to quit."""
        mouse_pos = pygame.mouse.get_pos()

        # Reset hover states before processing events
        if self.mode == "debug":
            self.game_state.debug_highlight_pos = None
        # Reset snapped position before hover update
        self.game_state.demo_snapped_position = None

        running = True
        # Use the generator to process events
        # Pass the visualizer instance for resize handling
        event_generator = event_processor.process_pygame_events(self.visualizer)
        try:
            while True:
                event = next(event_generator)  # Get next event yielded by processor
                # Pass yielded events to mode-specific handlers
                if self.mode == "play":
                    # Pass necessary components to the handler function
                    play_mode_handler.handle_play_click(
                        event, mouse_pos, self.game_state, self.visualizer
                    )
                elif self.mode == "debug":
                    debug_mode_handler.handle_debug_click(
                        event, mouse_pos, self.game_state, self.visualizer
                    )
        except StopIteration as e:
            # Generator finished, return value indicates if we should continue
            running = e.value

        # Update hover effects after processing all events for this frame
        if running:  # Only update hover if not quitting
            if self.mode == "play":
                # Pass necessary components to the hover update function
                play_mode_handler.update_play_hover(
                    mouse_pos, self.game_state, self.visualizer
                )
            elif self.mode == "debug":
                debug_mode_handler.update_debug_hover(
                    mouse_pos, self.game_state, self.visualizer
                )
            # No else needed, snapped_position was reset earlier

        return running  # Return whether to continue the main loop


File: src/interaction/play_mode_handler.py
# File: src/interaction/play_mode_handler.py
import pygame
import logging
from typing import TYPE_CHECKING, Tuple, Optional

# Import specific modules/functions needed
from src.visualization import core as vis_core  # Import core visualization elements
from src.environment import core as env_core  # Import core environment elements
from src.environment import grid as env_grid  # Import grid logic
from src.environment import logic as env_logic  # Import game logic

# Import Shape from the new structs module
from src.structs import Shape

if TYPE_CHECKING:
    from src.environment.core.game_state import GameState
    from src.visualization.core.visualizer import Visualizer

logger = logging.getLogger(__name__)


def handle_play_click(
    event: pygame.event.Event,
    mouse_pos: Tuple[int, int],
    game_state: "GameState",
    visualizer: "Visualizer",
) -> None:
    """Handles mouse clicks in play mode (select preview, place shape)."""
    if not (event.type == pygame.MOUSEBUTTONDOWN and event.button == 1):
        return  # Only handle left clicks

    # Prevent actions if game is over
    if game_state.is_over():
        logger.info("Game is over, ignoring click.")
        return

    layout_rects = visualizer.ensure_layout()
    grid_rect = layout_rects.get("grid")
    preview_rects = visualizer.preview_rects  # Get cached preview rects from visualizer

    # 1. Check for clicks on shape previews
    # Use coord_mapper from visualization.core
    preview_idx = vis_core.coord_mapper.get_preview_index_from_screen(
        mouse_pos, preview_rects
    )
    if preview_idx is not None:
        if game_state.demo_selected_shape_idx == preview_idx:
            # Clicked selected shape again: deselect
            game_state.demo_selected_shape_idx = -1
            game_state.demo_snapped_position = None  # Clear snap on deselect
            logger.info("Deselected shape.")
        elif (
            0 <= preview_idx < len(game_state.shapes) and game_state.shapes[preview_idx]
        ):
            # Clicked a valid, available shape: select it
            game_state.demo_selected_shape_idx = preview_idx
            logger.info(f"Selected shape index: {preview_idx}")
            # Immediately update hover based on current mouse pos after selection
            update_play_hover(mouse_pos, game_state, visualizer)
        else:
            # Clicked an empty or invalid slot
            logger.info(f"Clicked empty/invalid preview slot: {preview_idx}")
            # Deselect if clicking an empty slot while another is selected
            if game_state.demo_selected_shape_idx != -1:
                game_state.demo_selected_shape_idx = -1
                game_state.demo_snapped_position = None
        return  # Handled preview click (or lack thereof)

    # 2. Check for clicks on the grid (if a shape is selected)
    selected_idx = game_state.demo_selected_shape_idx
    if selected_idx != -1 and grid_rect and grid_rect.collidepoint(mouse_pos):
        # A shape is selected, and the click is within the grid area.
        # Use coord_mapper from visualization.core
        grid_coords = vis_core.coord_mapper.get_grid_coords_from_screen(
            mouse_pos, grid_rect, game_state.env_config
        )
        shape_to_place: Optional[Shape] = game_state.shapes[
            selected_idx
        ]  # Use Shape from structs

        # Check if the placement is valid *at the clicked location*
        # Use grid logic from environment.grid
        if (
            grid_coords
            and shape_to_place
            and env_grid.logic.can_place(  # Use function from grid logic
                game_state.grid_data, shape_to_place, grid_coords[0], grid_coords[1]
            )
        ):
            # Valid placement click!
            r, c = grid_coords
            # Use action codec from environment.core
            action = env_core.action_codec.encode_action(
                selected_idx, r, c, game_state.env_config
            )
            # Execute the step using the game state's method (which uses env logic internally)
            _, reward, done = game_state.step(
                action
            )  # GameState.step returns placeholder_val, reward, done
            logger.info(
                f"Placed shape {selected_idx} at {grid_coords}. R={reward:.1f}, Done={done}"
            )
            # Deselect shape after successful placement
            game_state.demo_selected_shape_idx = -1
            game_state.demo_snapped_position = None  # Clear snap state
        else:
            # Clicked grid, shape selected, but not a valid placement spot for the click
            logger.info(f"Clicked grid at {grid_coords}, but placement invalid.")


def update_play_hover(
    mouse_pos: Tuple[int, int], game_state: "GameState", visualizer: "Visualizer"
) -> None:
    """Updates the snapped position based on mouse hover in play mode."""
    game_state.demo_snapped_position = None

    if game_state.is_over() or game_state.demo_selected_shape_idx == -1:
        return

    layout_rects = visualizer.ensure_layout()
    grid_rect = layout_rects.get("grid")
    if not grid_rect:
        return

    shape_idx = game_state.demo_selected_shape_idx
    if not (0 <= shape_idx < len(game_state.shapes)):
        return
    shape: Optional[Shape] = game_state.shapes[shape_idx]  # Use Shape from structs
    if not shape:
        return

    # Use coord_mapper from visualization.core
    grid_coords = vis_core.coord_mapper.get_grid_coords_from_screen(
        mouse_pos, grid_rect, game_state.env_config
    )

    # Use grid logic from environment.grid
    if grid_coords and env_grid.logic.can_place(  # Use function from grid logic
        game_state.grid_data, shape, grid_coords[0], grid_coords[1]
    ):
        game_state.demo_snapped_position = grid_coords


File: src/interaction/debug_mode_handler.py
# File: src/interaction/debug_mode_handler.py
import pygame
import logging
from typing import TYPE_CHECKING, Tuple

# Import specific modules/functions needed
from src.visualization import core as vis_core  # Import core visualization elements
from src.environment import core as env_core  # Import core environment elements
from src.environment import grid as env_grid  # Import grid logic

# Import Triangle from the new structs module
from src.structs import Triangle

if TYPE_CHECKING:
    from src.environment.core.game_state import GameState
    from src.visualization.core.visualizer import Visualizer

logger = logging.getLogger(__name__)


def handle_debug_click(
    event: pygame.event.Event,
    mouse_pos: Tuple[int, int],
    game_state: "GameState",
    visualizer: "Visualizer",
) -> None:
    """Handles mouse clicks in debug mode (toggle triangle state)."""
    if not (event.type == pygame.MOUSEBUTTONDOWN and event.button == 1):
        return

    layout_rects = visualizer.ensure_layout()
    grid_rect = layout_rects.get("grid")
    if not grid_rect:
        logger.error("Grid layout rectangle not available for debug click.")
        return

    # Use coord_mapper from visualization.core
    grid_coords = vis_core.coord_mapper.get_grid_coords_from_screen(
        mouse_pos, grid_rect, game_state.env_config
    )
    if not grid_coords:
        return

    r, c = grid_coords
    # Use grid_data directly from game_state
    if game_state.grid_data.valid(r, c):
        tri: Triangle = game_state.grid_data.triangles[r][
            c
        ]  # Use Triangle from structs
        if not tri.is_death:
            # Toggle occupancy state
            tri.is_occupied = not tri.is_occupied
            # Update the corresponding numpy array state
            game_state.grid_data._occupied_np[r, c] = tri.is_occupied
            # Update color for visualization using colors from visualization.core
            tri.color = vis_core.colors.DEBUG_TOGGLE_COLOR if tri.is_occupied else None
            logger.info(
                f"DEBUG: Toggled triangle ({r},{c}) -> {'Occupied' if tri.is_occupied else 'Empty'}"
            )

            # Check for line clears AFTER potentially setting to occupied
            if tri.is_occupied:
                # Use grid logic from environment.grid
                # Correct the keyword argument name here:
                lines, tris, coords = env_grid.logic.check_and_clear_lines(
                    game_state.grid_data, newly_occupied_triangles={tri}
                )
                if lines > 0:
                    logger.info(
                        f"DEBUG: Cleared {lines} lines ({tris} tris) at {coords} after toggle."
                    )
        else:
            logger.info(f"Clicked on death cell ({r},{c}). No action.")


def update_debug_hover(
    mouse_pos: Tuple[int, int], game_state: "GameState", visualizer: "Visualizer"
) -> None:
    """Updates the debug highlight position based on mouse hover."""
    layout_rects = visualizer.ensure_layout()
    grid_rect = layout_rects.get("grid")
    if not grid_rect:
        game_state.debug_highlight_pos = None
        return

    # Use coord_mapper from visualization.core
    grid_coords = vis_core.coord_mapper.get_grid_coords_from_screen(
        mouse_pos, grid_rect, game_state.env_config
    )
    valid_hover = False
    if grid_coords:
        r, c = grid_coords
        # Highlight only if hovering over a valid, non-death cell
        # Use grid_data directly from game_state
        if (
            game_state.grid_data.valid(r, c)
            and not game_state.grid_data.triangles[r][c].is_death
        ):
            game_state.debug_highlight_pos = grid_coords
            valid_hover = True

    if not valid_hover:
        game_state.debug_highlight_pos = None


File: src/rl/__init__.py
# File: src/rl/__init__.py
"""
Reinforcement Learning (RL) module.
Contains the core components for training an agent using self-play and MCTS.
"""

# Core RL classes
from .core.orchestrator import TrainingOrchestrator
from .core.trainer import Trainer
from .core.buffer import ExperienceBuffer

# Self-play functionality (now using Ray actor)
from .self_play.worker import SelfPlayWorker
# Import Pydantic model for result type hint from local types module
from .types import SelfPlayResult # Updated import

__all__ = [
    "TrainingOrchestrator",
    "Trainer",
    "ExperienceBuffer",
    "SelfPlayWorker",  # Export the actor class
    "SelfPlayResult",  # Export the Pydantic result type
]

File: src/rl/types.py
# File: src/rl/types.py
# NEW FILE
from typing import List
from pydantic import BaseModel, ConfigDict

# Import types needed for the model definition
from src.utils.types import Experience # Import Experience tuple type
from src.environment import GameState # Import GameState directly

# Pydantic configuration to allow arbitrary types like GameState
arbitrary_types_config = ConfigDict(arbitrary_types_allowed=True)

class SelfPlayResult(BaseModel):
    """Pydantic model for structuring results from a self-play worker."""
    model_config = arbitrary_types_config

    episode_experiences: List[Experience]
    final_score: float
    episode_steps: int
    final_game_state: GameState # Use GameState directly here

# Crucial step: Explicitly rebuild models to resolve forward references
# This is needed because Experience -> GameState is a forward reference
# that Pydantic needs help resolving in this context.
# Now that GameState is imported directly, this should work.
SelfPlayResult.model_rebuild(force=True)

File: src/rl/README.md
# File: src/rl/README.md
# Reinforcement Learning Module (`src.rl`)

## Purpose and Architecture

This module contains the core components and orchestration logic for training the AlphaTriangle agent using reinforcement learning, specifically inspired by the AlphaZero methodology. It combines **parallel self-play data generation using Ray** with centralized neural network training.

-   **Core Components (`src.rl.core`):**
    -   `TrainingOrchestrator`: The main class that manages the entire training process. It initializes and manages Ray actors (`SelfPlayWorker`) for parallel self-play, coordinates data collection into the `ExperienceBuffer`, and triggers training steps via the `Trainer`. It handles loading/saving checkpoints and buffers, logging to MLflow, and the main asynchronous training loop.
    -   `Trainer`: Responsible for performing the neural network update steps on the main process/device. It takes batches of experience from the buffer, uses `src.features` to extract features, calculates losses, and updates the network weights.
    -   `ExperienceBuffer`: A replay buffer that stores experiences (`(GameState, policy_target, value_target)`) generated during self-play.
-   **Self-Play Components (`src.rl.self_play`):**
    -   `worker`: Defines the `SelfPlayWorker` Ray actor. Each actor runs game episodes independently using MCTS and its local copy of the neural network (weights updated periodically by the orchestrator). It collects `GameState` objects, MCTS policy targets, and the final game outcome to generate experiences.

## Exposed Interfaces

-   **Core:**
    -   `TrainingOrchestrator`:
        -   `__init__(...)`
        -   `run_training_loop()`
        -   `request_stop()`
    -   `Trainer`:
        -   `__init__(nn_interface: NeuralNetwork, train_config: TrainConfig, env_config: EnvConfig)`
        -   `train_step(batch: ExperienceBatch) -> Optional[Dict[str, float]]`
        -   `load_optimizer_state(state_dict: dict)`
        -   `get_current_lr() -> float`
    -   `ExperienceBuffer`:
        -   `__init__(config: TrainConfig)`
        -   `add(experience: Experience)`
        -   `add_batch(experiences: List[Experience])`
        -   `sample(batch_size: int) -> Optional[ExperienceBatch]`
        -   `is_ready() -> bool`
        -   `__len__() -> int`
-   **Self-Play:**
    -   `SelfPlayWorker`: Ray actor class (primarily used internally by `TrainingOrchestrator`).
        -   `run_episode() -> SelfPlayResult`
        -   `set_weights(weights: Dict)`

## Dependencies

-   **`src.config`**:
    -   `TrainConfig`, `EnvConfig`, `PersistenceConfig`, `MCTSConfig`, `ModelConfig`: Used extensively by all components.
-   **`src.nn`**:
    -   `NeuralNetwork`: Used by the `Trainer` and instantiated within `SelfPlayWorker`.
-   **`src.features`**:
    -   `extract_state_features`: Used by `Trainer` and `NeuralNetwork`.
-   **`src.mcts`**:
    -   Core MCTS components used by `SelfPlayWorker`.
-   **`src.environment`**:
    -   `GameState`, `EnvConfig`: Used by `SelfPlayWorker` and stored in the buffer.
-   **`src.data`**:
    -   `DataManager`: Used by `TrainingOrchestrator`.
-   **`src.stats`**:
    -   `StatsCollector`: Used by `TrainingOrchestrator` to collect metrics.
-   **`src.utils`**:
    -   `types`: `Experience`, `ExperienceBatch`, etc.
    -   `helpers`.
-   **`src.structs`**:
    -   Implicitly used via `GameState`.
-   **`torch`**:
    -   Used heavily by `Trainer` and `NeuralNetwork`.
-   **`ray`**: # Added dependency
    -   Used by `TrainingOrchestrator` and `SelfPlayWorker` for parallelization.
-   **`mlflow`**:
    -   Used by `TrainingOrchestrator` for logging.
-   **Standard Libraries:** `typing`, `logging`, `os`, `time`, `queue`, `threading`, `random`, `collections.deque`.

---

**Note:** Please keep this README updated when changing the overall training flow, the responsibilities of the orchestrator, trainer, or buffer, or the self-play generation process (especially regarding Ray usage). Accurate documentation is crucial for maintainability.

File: src/rl/core/__init__.py
# File: src/rl/core/__init__.py
"""
Core RL components: Orchestrator, Trainer, Buffer.
"""

# Import the final classes intended for export from their respective modules.
# The orchestrator class itself handles importing its internal helper functions.
from .orchestrator import TrainingOrchestrator
from .trainer import Trainer
from .buffer import ExperienceBuffer
from .visual_state_actor import VisualStateActor  # Export the new actor

__all__ = [
    "TrainingOrchestrator",
    "Trainer",
    "ExperienceBuffer",
    "VisualStateActor",  # Add to exports
]


File: src/rl/core/README.md
# File: src/rl/core/README.md
# RL Core Submodule (`src.rl.core`)

## Purpose and Architecture

This submodule contains the central classes that manage and execute the reinforcement learning training loop, coordinating parallel self-play workers and centralized training updates.

-   **`orchestrator.py`:** Defines the main `TrainingOrchestrator` class.
    -   It initializes all necessary components (NN, Buffer, Trainer, DataManager, Configs, StatsCollector).
    -   It creates and manages a pool of `SelfPlayWorker` Ray actors (`src.rl.self_play.worker`).
    -   It orchestrates the main asynchronous training loop:
        -   Launching self-play episode tasks on remote actors.
        -   Collecting completed episode results (`Experience` data) using `ray.wait()`.
        -   Adding collected data to the `ExperienceBuffer`.
        -   Performing training steps using the `Trainer` when the buffer is ready.
        -   Periodically updating the network weights on the remote `SelfPlayWorker` actors.
        -   Handling checkpoint saving/loading via `DataManager`.
        -   Logging metrics and configurations via `StatsCollector` and MLflow (using helpers).
    -   It manages high-level state like `global_step`, `episodes_played`, `stop_requested`, etc.
    -   It handles graceful shutdown of Ray actors and signaling the visualizer.
-   **`orchestrator_helpers.py`:** Contains helper functions used by the `TrainingOrchestrator` for tasks like:
    -   Loading initial state (checkpoint, buffer, stats).
    -   Saving checkpoints and buffers via `DataManager`.
    -   Logging configurations and metrics to MLflow.
    -   Processing results returned by `SelfPlayWorker` actors.
    -   Running a training step using the `Trainer`.
    -   Logging results from self-play and training steps.
    -   Updating the `visual_state_queue`.
-   **`Trainer`:** This class encapsulates the logic for updating the neural network's weights on the main process/device.
    -   It holds the main `NeuralNetwork` interface, optimizer, and scheduler.
    -   Its `train_step` method takes a batch of experiences, calls `src.features.extract_state_features`, performs forward/backward passes, calculates losses, and updates weights.
-   **`ExperienceBuffer`:** This class implements a replay buffer storing `Experience` tuples (`(GameState, policy_target, value_target)`).

## Exposed Interfaces

-   **Classes:**
    -   `TrainingOrchestrator`: (from `orchestrator.py`)
        -   `__init__(...)`
        -   `run_training_loop()`
        -   `request_stop()`
    -   `Trainer`:
        -   `__init__(nn_interface: NeuralNetwork, train_config: TrainConfig, env_config: EnvConfig)`
        -   `train_step(batch: ExperienceBatch) -> Optional[Dict[str, float]]`
        -   `load_optimizer_state(state_dict: dict)`
        -   `get_current_lr() -> float`
    -   `ExperienceBuffer`:
        -   `__init__(config: TrainConfig)`
        -   `add(experience: Experience)`
        -   `add_batch(experiences: List[Experience])`
        -   `sample(batch_size: int) -> Optional[ExperienceBatch]`
        -   `is_ready() -> bool`
        -   `__len__() -> int`

## Dependencies

-   **`src.config`**:
    -   `TrainConfig`, `EnvConfig`, `PersistenceConfig`, `MCTSConfig`, `ModelConfig`.
-   **`src.nn`**:
    -   `NeuralNetwork`.
-   **`src.features`**:
    -   `extract_state_features`: Used by `Trainer`.
-   **`src.mcts`**:
    -   `MCTSConfig`, `Node`: Used by Orchestrator (passed to self-play workers).
-   **`src.environment`**:
    -   `GameState`, `EnvConfig`: Used by Orchestrator (passed to self-play workers) and stored in Buffer/Experience.
-   **`src.data`**:
    -   `DataManager`.
-   **`src.stats`**:
    -   `StatsCollector`: Used by Orchestrator.
-   **`src.utils`**:
    -   `types`: `Experience`, `ExperienceBatch`, etc.
    -   `helpers`.
-   **`src.rl.self_play`**:
    -   `SelfPlayWorker`, `SelfPlayResult`: Used by Orchestrator.
-   **`torch`**:
    -   Used heavily by `Trainer`.
-   **`ray`**:
    -   Used by `TrainingOrchestrator` to manage actors and tasks.
-   **`mlflow`**:
    -   Used by `orchestrator_helpers`.
-   **Standard Libraries:** `typing`, `logging`, `os`, `time`, `queue`, `threading`, `random`, `collections.deque`, `json`.

---

**Note:** Please keep this README updated when changing the responsibilities or interfaces of the Orchestrator components, Trainer, or Buffer, or how they interact with each other and other modules, especially regarding the use of Ray actors. Accurate documentation is crucial for maintainability.

File: src/rl/core/visual_state_actor.py
# File: src/rl/core/visual_state_actor.py
import ray
import time
from typing import Dict, Optional, Any, TYPE_CHECKING

# Use TYPE_CHECKING to avoid circular import at runtime
if TYPE_CHECKING:
    from src.environment import GameState  # Import GameState for type hint


@ray.remote
class VisualStateActor:
    """A simple Ray actor to hold the latest game states from workers for visualization."""

    def __init__(self):
        self.worker_states: Dict[int, "GameState"] = {}
        self.global_stats: Dict[str, Any] = {}
        self.last_update_times: Dict[int, float] = {}

    def update_state(self, worker_id: int, game_state: "GameState"):
        """Workers call this to update their latest state."""
        # Store a copy to prevent issues with shared references if needed,
        # but direct reference might be faster if visualizer is read-only.
        # Let's store the reference for now as copy was removed from worker.
        self.worker_states[worker_id] = game_state
        self.last_update_times[worker_id] = time.time()

    def update_global_stats(self, stats: Dict[str, Any]):
        """Orchestrator calls this to update global stats."""
        self.global_stats = stats

    def get_all_states(self) -> Dict[int, Any]:
        """Called by the orchestrator to get states for the visual queue."""
        # Return a copy to avoid race conditions in the orchestrator/visualizer
        combined_states = {wid: state for wid, state in self.worker_states.items()}
        # Add global stats under key -1
        combined_states[-1] = self.global_stats.copy()
        return combined_states

    def get_state(self, worker_id: int) -> Optional["GameState"]:
        """Get state for a specific worker (unused currently)."""
        return self.worker_states.get(worker_id)


File: src/rl/core/orchestrator.py
# File: src/rl/core/orchestrator.py
import logging
import time
import threading
import queue
import ray
import torch
from typing import Optional, Dict, Any, List
from collections import deque

# --- Package Imports ---
from src.config import TrainConfig, EnvConfig, PersistenceConfig, ModelConfig
from src.mcts import MCTSConfig # Import Pydantic MCTSConfig
from src.nn import NeuralNetwork
# Import DataManager and the Pydantic schema for loaded state
from src.data import DataManager, LoadedTrainingState
from src.environment import GameState
from src.utils import format_eta, get_device
# Import Experience from utils, SelfPlayResult from local rl types
from src.utils.types import StatsCollectorData, Experience
from ..types import SelfPlayResult # Corrected import location
from src.visualization.ui import ProgressBar
from src.stats import StatsCollectorActor

# Other RL Components
from .buffer import ExperienceBuffer
from .trainer import Trainer
from .visual_state_actor import VisualStateActor
from ..self_play.worker import SelfPlayWorker # Keep worker import

# Import helper functions (logging only now)
from . import orchestrator_helpers as helpers

logger = logging.getLogger(__name__)

LARGE_STEP_COUNT = 10_000_000
VISUAL_UPDATE_INTERVAL = 0.2
STATS_FETCH_INTERVAL = 0.5


class TrainingOrchestrator:
    """
    Manages the overall training loop, coordinating parallel self-play via Ray actors
    and centralized training. Uses DataManager for loading/saving.
    """

    def __init__(
        self,
        nn: NeuralNetwork,
        buffer: ExperienceBuffer,
        trainer: Trainer,
        data_manager: DataManager,
        stats_collector_actor: StatsCollectorActor,
        train_config: TrainConfig,
        env_config: EnvConfig,
        mcts_config: MCTSConfig, # Use Pydantic MCTSConfig
        persist_config: PersistenceConfig,
        visual_state_queue: Optional[queue.Queue[Optional[Dict[int, Any]]]] = None,
    ):
        self.nn = nn
        self.buffer = buffer
        self.trainer = trainer
        self.data_manager = data_manager
        self.stats_collector_actor = stats_collector_actor
        self.train_config = train_config
        self.env_config = env_config
        self.mcts_config = mcts_config # Store Pydantic instance
        self.persist_config = persist_config
        self.visual_state_queue = visual_state_queue

        self.device = nn.device
        self.global_step = 0
        self.episodes_played = 0
        self.total_simulations_run = 0
        self.best_eval_score = -float("inf")
        self.start_time = time.time()
        self.stop_requested = threading.Event()
        self.training_complete = False
        self.target_steps_reached = False
        self.training_exception: Optional[Exception] = None
        self.last_visual_update_time = 0.0
        self.last_stats_fetch_time = 0.0
        self.latest_stats_data: StatsCollectorData = {}

        self.train_step_progress: Optional[ProgressBar] = None
        self.buffer_fill_progress: Optional[ProgressBar] = None

        self.visual_state_actor = VisualStateActor.remote()
        self.workers: List[ray.actor.ActorHandle] = []
        self.worker_tasks: Dict[ray.ObjectRef, int] = {}

        self._load_and_initialize_state()
        self._initialize_workers()
        self._initialize_progress_bars()
        # No need to update workers here, initial weights passed during init
        # self._update_worker_networks()
        helpers.log_configs_to_mlflow(self)
        self._update_visual_states()
        logger.info("Orchestrator initialized.")

    def _load_and_initialize_state(self):
        """Loads initial state using DataManager and applies it."""
        logger.info("Loading initial training state...")
        # DataManager now returns a LoadedTrainingState Pydantic model
        loaded_state: LoadedTrainingState = self.data_manager.load_initial_state()

        # Check if checkpoint data was loaded
        if loaded_state.checkpoint_data:
            cp_data = loaded_state.checkpoint_data
            logger.info(f"Applying loaded checkpoint data (Run: {cp_data.run_name}, Step: {cp_data.global_step})")

            # Config validation (optional, basic check)
            if cp_data.model_config_dict != self.nn.model_config.model_dump(): # Use model_dump
                 logger.warning("Loaded ModelConfig differs from current config!")
            if cp_data.env_config_dict != self.env_config.model_dump(): # Use model_dump
                 logger.warning("Loaded EnvConfig differs from current config!")
            # MCTSConfig check can be added here if saved in checkpoint

            # Apply NN weights
            if cp_data.model_state_dict:
                self.nn.set_weights(cp_data.model_state_dict)
            else: logger.warning("No model state dictionary found in checkpoint.")

            # Apply Optimizer state
            if cp_data.optimizer_state_dict:
                try:
                    self.trainer.optimizer.load_state_dict(cp_data.optimizer_state_dict)
                    for state in self.trainer.optimizer.state.values():
                        for k, v in state.items():
                            if isinstance(v, torch.Tensor): state[k] = v.to(self.nn.device)
                    logger.info("Optimizer state loaded and moved to device.")
                except Exception as opt_load_err: logger.error(f"Could not load optimizer state: {opt_load_err}. Optimizer might reset.")
            else: logger.warning("No optimizer state found in checkpoint.")

            # Apply Stats Collector state
            if cp_data.stats_collector_state and self.stats_collector_actor:
                try:
                    set_state_ref = self.stats_collector_actor.set_state.remote(cp_data.stats_collector_state)
                    ray.get(set_state_ref, timeout=5.0)
                    logger.info("StatsCollectorActor state restored.")
                except Exception as e: logger.error(f"Error restoring StatsCollectorActor state: {e}", exc_info=True)
            elif not cp_data.stats_collector_state: logger.warning("No stats_collector_state found in checkpoint.")

            # Apply Progress Counters
            self.global_step = cp_data.global_step
            self.episodes_played = cp_data.episodes_played
            self.total_simulations_run = cp_data.total_simulations_run

        else:
            logger.info("No checkpoint data loaded. Starting fresh.")
            self.trainer.optimizer.zero_grad(set_to_none=True)
            if self.stats_collector_actor:
                try: ray.get(self.stats_collector_actor.clear.remote())
                except Exception as e: logger.error(f"Failed to clear stats actor state: {e}")

        # Apply Buffer state
        if loaded_state.buffer_data:
            # Re-apply maxlen when loading from the list
            self.buffer.buffer = deque(loaded_state.buffer_data.buffer_list, maxlen=self.buffer.capacity)
            logger.info(f"Experience buffer loaded. Size: {len(self.buffer.buffer)}, Capacity: {self.buffer.capacity}")
        else:
            logger.info("No buffer data loaded.")

        self.nn.model.train() # Ensure model is in training mode after loading

    def _initialize_workers(self):
        """Creates the pool of SelfPlayWorker Ray actors."""
        logger.info(f"Initializing {self.train_config.NUM_SELF_PLAY_WORKERS} self-play workers...")
        initial_weights = self.nn.get_weights() # Get potentially loaded weights
        weights_ref = ray.put(initial_weights)

        for i in range(self.train_config.NUM_SELF_PLAY_WORKERS):
            worker = SelfPlayWorker.options(num_cpus=1).remote(
                actor_id=i,
                env_config=self.env_config,
                mcts_config=self.mcts_config, # Pass Pydantic instance
                model_config=self.nn.model_config,
                train_config=self.train_config,
                initial_weights=weights_ref, # Pass weights ref
                seed=self.train_config.RANDOM_SEED + i,
                worker_device_str=self.train_config.WORKER_DEVICE,
                visual_state_actor_handle=self.visual_state_actor,
            )
            self.workers.append(worker)
        logger.info("Self-play workers initialized.")

    def _update_worker_networks(self):
        """Sends the latest network weights to all workers."""
        if not self.workers: return
        logger.debug("Updating worker networks...")
        current_weights = self.nn.get_weights()
        weights_ref = ray.put(current_weights)
        update_tasks = [worker.set_weights.remote(weights_ref) for worker in self.workers if worker]
        if not update_tasks: return
        try:
            ray.get(update_tasks, timeout=15.0)
            logger.debug("Worker networks updated.")
        except Exception as e:
            logger.error(f"Error updating worker networks: {e}", exc_info=True)
            # Consider if this should stop training

    def _initialize_progress_bars(self):
        """Initializes progress bars after state has been loaded."""
        train_total_steps = self.train_config.MAX_TRAINING_STEPS or LARGE_STEP_COUNT
        self.train_step_progress = ProgressBar("Training Steps", train_total_steps, start_time=self.start_time, initial_steps=self.global_step)
        self.buffer_fill_progress = ProgressBar("Buffer Fill", self.train_config.BUFFER_CAPACITY, start_time=self.start_time, initial_steps=len(self.buffer))
        if self.global_step > 0:
            current_time = time.time()
            self.train_step_progress.start_time = current_time
            self.buffer_fill_progress.start_time = current_time
            logger.info("Reset progress bar start time due to loading checkpoint.")

    def _fetch_latest_stats(self):
        """Fetches the latest stats data from the actor for ETA calculation."""
        current_time = time.time()
        if current_time - self.last_stats_fetch_time < STATS_FETCH_INTERVAL: return
        self.last_stats_fetch_time = current_time
        if self.stats_collector_actor:
            try:
                data_ref = self.stats_collector_actor.get_data.remote()
                self.latest_stats_data = ray.get(data_ref, timeout=1.0)
            except Exception as e: logger.warning(f"Failed to fetch latest stats for ETA: {e}")

    def _log_progress_eta(self):
        """Logs progress and ETA."""
        if self.global_step % 20 != 0 and not self.target_steps_reached: return
        if not self.train_step_progress: return

        elapsed_time = time.time() - self.train_step_progress.start_time
        steps_since_load = self.global_step - self.train_step_progress.initial_steps
        steps_per_sec = steps_since_load / elapsed_time if elapsed_time > 1 else 0
        target_steps = self.train_config.MAX_TRAINING_STEPS
        target_steps_str = str(target_steps) if target_steps else "inf"
        eta_str = format_eta(self.train_step_progress.get_eta_seconds()) if not self.target_steps_reached else "N/A (Target Reached)"
        progress_str = f"Step {self.global_step}/{target_steps_str}"
        if self.target_steps_reached: progress_str += f" (TARGET REACHED +{self.global_step - (target_steps or 0)} extra)"
        buffer_fill_perc = (len(self.buffer) / self.buffer.capacity) * 100 if self.buffer.capacity > 0 else 0.0
        total_sims_str = f"{self.total_simulations_run / 1e6:.2f}M" if self.total_simulations_run >= 1e6 else f"{self.total_simulations_run / 1e3:.1f}k" if self.total_simulations_run >= 1000 else str(self.total_simulations_run)
        num_pending_tasks = len(self.worker_tasks)
        logger.info(
            f"Progress: {progress_str}, Episodes: {self.episodes_played}, Total Sims: {total_sims_str}, "
            f"Buffer: {len(self.buffer)}/{self.buffer.capacity} ({buffer_fill_perc:.1f}%), "
            f"Pending Tasks: {num_pending_tasks}, Speed: {steps_per_sec:.2f} steps/sec, ETA: {eta_str}"
        )

    def request_stop(self):
        """Signals the training loop and workers to stop gracefully."""
        if not self.stop_requested.is_set():
            logger.info("Stop requested.")
            self.stop_requested.set()

    def _update_visual_states(self):
        """Fetches latest states and stats, puts them on the visual queue."""
        if not self.visual_state_queue or not self.visual_state_actor: return
        current_time = time.time()
        if current_time - self.last_visual_update_time < VISUAL_UPDATE_INTERVAL: return
        self.last_visual_update_time = current_time
        try:
            states_ref = self.visual_state_actor.get_all_states.remote()
            stats_ref = self.stats_collector_actor.get_data.remote() if self.stats_collector_actor else None
            refs_to_get = [states_ref] + ([stats_ref] if stats_ref else [])
            results = ray.get(refs_to_get, timeout=1.0)
            combined_states = results[0]
            stats_data = results[1] if stats_ref else {}

            global_stats_for_vis = {
                "global_step": self.global_step, "target_steps_reached": self.target_steps_reached,
                "total_episodes": self.episodes_played, "total_simulations": self.total_simulations_run,
                "train_progress": self.train_step_progress, "buffer_progress": self.buffer_fill_progress,
                "stats_data": stats_data, "num_workers": len(self.workers), "pending_tasks": len(self.worker_tasks),
            }
            combined_states[-1] = global_stats_for_vis

            if len(combined_states) <= 1: return # No worker states
            try:
                while self.visual_state_queue.full(): self.visual_state_queue.get_nowait()
                self.visual_state_queue.put_nowait(combined_states)
            except queue.Full: logger.warning("Visual state queue full, dropping state dictionary.")
            except Exception as qe: logger.error(f"Error putting state dict in visual queue: {qe}")
        except Exception as e: logger.warning(f"Error getting states/stats for visualization: {e}")

    def save_final_state(self):
        """Saves the final training state using DataManager."""
        logger.info("Saving final training state...")
        try:
            self.data_manager.save_training_state(
                nn=self.nn, optimizer=self.trainer.optimizer,
                stats_collector_actor=self.stats_collector_actor, buffer=self.buffer,
                global_step=self.global_step, episodes_played=self.episodes_played,
                total_simulations_run=self.total_simulations_run, is_final=True,
            )
        except Exception as e_save: logger.error(f"Failed to save final training state: {e_save}")

    def _final_cleanup(self):
        """Performs final cleanup of Ray actors and signals visualizer."""
        end_time = time.time()
        logger.info(f"Training loop finished after {format_eta(end_time - self.start_time)}.")
        logger.info("Terminating Ray workers and helper actors...")
        actors_to_kill = self.workers + [self.visual_state_actor, self.stats_collector_actor]
        for actor in actors_to_kill:
            if actor:
                try: ray.kill(actor, no_restart=True)
                except Exception as kill_e: logger.warning(f"Error killing actor: {kill_e}")
        self.workers = []
        self.visual_state_actor = None
        self.stats_collector_actor = None
        self.worker_tasks = {}
        logger.info("Ray actors terminated.")
        if self.visual_state_queue:
            logger.info("Signaling visualizer thread to stop.")
            try: self.visual_state_queue.put(None, timeout=1.0)
            except Exception as qe: logger.error(f"Error signaling visual queue on exit: {qe}")

    def run_training_loop(self):
        """Main training loop coordinating parallel self-play and training."""
        logger.info(f"Starting training loop... Target steps: {self.train_config.MAX_TRAINING_STEPS or 'Infinite'}")
        self.start_time = time.time() # Reset start time after loading

        try:
            # Initial launch of tasks on all workers
            for i, worker in enumerate(self.workers):
                if worker:
                    task_ref = worker.run_episode.remote()
                    self.worker_tasks[task_ref] = i

            while not self.stop_requested.is_set():
                if (not self.target_steps_reached and self.train_config.MAX_TRAINING_STEPS and self.global_step >= self.train_config.MAX_TRAINING_STEPS):
                    logger.info(f"Reached target training steps ({self.train_config.MAX_TRAINING_STEPS}).")
                    self.target_steps_reached = True

                # --- Training Step (only if target not reached) ---
                if self.buffer.is_ready() and not self.target_steps_reached:
                    trained_this_cycle = helpers.run_training_step(self)
                    if trained_this_cycle and (self.global_step % self.train_config.WORKER_UPDATE_FREQ_STEPS == 0):
                        self._update_worker_networks()

                if self.stop_requested.is_set(): break

                # --- Collect Self-Play Results ---
                wait_timeout = 0.1 if self.buffer.is_ready() else 0.5
                ready_refs, _ = ray.wait(list(self.worker_tasks.keys()), num_returns=1, timeout=wait_timeout)

                if ready_refs:
                    for ref in ready_refs:
                        worker_idx = self.worker_tasks.pop(ref, -1)
                        if worker_idx == -1: continue
                        try:
                            # Result is now a SelfPlayResult Pydantic model
                            result: SelfPlayResult = ray.get(ref)
                            helpers.process_self_play_result(self, result, worker_idx)
                        except Exception as e:
                            logger.error(f"Error processing result from worker {worker_idx}: {e}", exc_info=True)
                            if worker_idx < len(self.workers): self.workers[worker_idx] = None # Mark worker as failed
                            # Optionally try to restart the worker? For now, just remove.
                            continue
                        # Relaunch task
                        if (not self.stop_requested.is_set() and worker_idx < len(self.workers) and self.workers[worker_idx]):
                            new_task_ref = self.workers[worker_idx].run_episode.remote()
                            self.worker_tasks[new_task_ref] = worker_idx
                if self.stop_requested.is_set(): break

                self._update_visual_states()

                # --- Periodic Checkpointing (only if target not reached) ---
                if (not self.target_steps_reached and self.global_step > 0 and self.global_step % self.train_config.CHECKPOINT_SAVE_FREQ_STEPS == 0):
                    self.data_manager.save_training_state(
                        nn=self.nn, optimizer=self.trainer.optimizer,
                        stats_collector_actor=self.stats_collector_actor, buffer=self.buffer,
                        global_step=self.global_step, episodes_played=self.episodes_played,
                        total_simulations_run=self.total_simulations_run,
                    )

                self._log_progress_eta()
                if not ready_refs and not self.buffer.is_ready(): time.sleep(0.05)

        except KeyboardInterrupt: logger.warning("KeyboardInterrupt received. Stopping training gracefully."); self.request_stop()
        except Exception as e: logger.critical(f"Unhandled exception in training loop: {e}", exc_info=True); self.training_exception = e; self.request_stop()
        finally:
            if self.training_exception: self.training_complete = False
            elif self.stop_requested.is_set(): self.training_complete = self.target_steps_reached
            else: self.training_complete = self.target_steps_reached
            # Cleanup is called externally

File: src/rl/core/buffer.py
# File: src/rl/core/buffer.py
import random
import logging
from collections import deque
from typing import List, Optional

# Use core types - Experience now contains GameState
from ...utils.types import Experience, ExperienceBatch, PolicyTargetMapping
from ...config import TrainConfig
from ...environment import GameState  # Keep GameState import

logger = logging.getLogger(__name__)


class ExperienceBuffer:
    """Simple FIFO Experience Replay Buffer storing (GameState, PolicyTarget, Value)."""

    def __init__(self, config: TrainConfig):
        self.capacity = config.BUFFER_CAPACITY
        self.min_size_to_train = config.MIN_BUFFER_SIZE_TO_TRAIN
        # Store the deque itself - type hint updated to Experience
        self.buffer: deque[Experience] = deque(maxlen=self.capacity)
        logger.info(f"Experience buffer initialized with capacity {self.capacity}.")

    def add(self, experience: Experience):
        """Adds a single experience tuple (GameState, PolicyTarget, Value) to the buffer."""
        self.buffer.append(experience)

    def add_batch(self, experiences: List[Experience]):
        """Adds a batch of experiences to the buffer."""
        self.buffer.extend(experiences)

    def sample(self, batch_size: int) -> Optional[ExperienceBatch]:
        """Samples a batch of experiences uniformly from the buffer."""
        current_size = len(self.buffer)
        if current_size < batch_size or current_size < self.min_size_to_train:
            return None
        batch = random.sample(self.buffer, batch_size)
        return batch  # Type hint ExperienceBatch already matches List[Experience]

    def __len__(self) -> int:
        """Returns the current number of experiences in the buffer."""
        return len(self.buffer)

    def is_ready(self) -> bool:
        """Checks if the buffer has enough samples to start training."""
        return len(self.buffer) >= self.min_size_to_train


File: src/rl/core/orchestrator_helpers.py
# File: src/rl/core/orchestrator_helpers.py
import logging
import os
import json
import mlflow
import queue
import numpy as np
import ray
import glob
import re
from collections import deque
from typing import TYPE_CHECKING, Optional, Dict, Any

# --- Package Imports ---
from src.environment import GameState
# Import SelfPlayResult Pydantic model from local rl types
from ..types import SelfPlayResult # Updated import
from src.utils.types import StatsCollectorData

if TYPE_CHECKING:
    # Use TYPE_CHECKING to avoid runtime circular import if helpers need the orchestrator type
    from .orchestrator import TrainingOrchestrator

logger = logging.getLogger(__name__)

# --- MLflow Logging ---

def log_configs_to_mlflow(orchestrator: "TrainingOrchestrator"):
    """Logs configuration parameters and saves config JSON to MLflow."""
    try:
        from src.config import APP_NAME

        mlflow.log_param("APP_NAME", APP_NAME)
        # Use model_dump() for Pydantic models
        mlflow.log_params(orchestrator.train_config.model_dump())
        mlflow.log_params(orchestrator.env_config.model_dump())
        mlflow.log_params(orchestrator.nn.model_config.model_dump())
        mlflow.log_params(orchestrator.mcts_config.model_dump()) # Use model_dump
        # Exclude computed field from persistence config logging if desired
        persist_params = orchestrator.persist_config.model_dump(exclude={'MLFLOW_TRACKING_URI'})
        mlflow.log_params(persist_params)

        logger.info("Logged configuration parameters to MLflow.")

        # Log configs as JSON artifact using DataManager
        all_configs = {
            "train_config": orchestrator.train_config.model_dump(),
            "env_config": orchestrator.env_config.model_dump(),
            "model_config": orchestrator.nn.model_config.model_dump(),
            "mcts_config": orchestrator.mcts_config.model_dump(), # Use model_dump
            "persist_config": orchestrator.persist_config.model_dump(),
        }
        orchestrator.data_manager.save_run_config(all_configs)

    except Exception as e:
        logger.error(f"Failed to log parameters/configs to MLflow: {e}", exc_info=True)


def log_metrics_to_mlflow(metrics: dict, step: int):
    """Logs a dictionary of metrics to MLflow."""
    try:
        numeric_metrics = {}
        for k, v in metrics.items():
            if isinstance(v, (int, float, np.number)) and np.isfinite(v):
                numeric_metrics[k] = v
            else: logger.debug(f"Skipping non-numeric metric for MLflow: {k}={v} (type: {type(v)})")
        if numeric_metrics: mlflow.log_metrics(numeric_metrics, step=step)
    except Exception as e: logger.error(f"Failed to log metrics to MLflow: {e}")


# --- State Loading/Saving ---
# REMOVED - Handled by DataManager

# --- Self-Play Result Processing ---

def process_self_play_result(
    orchestrator: "TrainingOrchestrator", result: SelfPlayResult, worker_id: int
):
    """Processes the result (SelfPlayResult Pydantic model) from a completed self-play episode."""
    # Access data using Pydantic model attributes
    if result.episode_experiences:
        episode_total_sims = 0
        episode_root_visits = []
        episode_tree_depths = []
        for exp_state, _, _ in result.episode_experiences:
            if hasattr(exp_state, "display_stats") and exp_state.display_stats:
                episode_total_sims += exp_state.display_stats.get("mcts_simulations", 0)
                episode_root_visits.append(exp_state.display_stats.get("mcts_root_visits", 0))
                episode_tree_depths.append(exp_state.display_stats.get("mcts_tree_depth", 0))

        orchestrator.buffer.add_batch(result.episode_experiences)
        if orchestrator.buffer_fill_progress:
            orchestrator.buffer_fill_progress.set_current_steps(len(orchestrator.buffer))
        orchestrator.episodes_played += 1
        orchestrator.total_simulations_run += episode_total_sims
        _log_self_play_results_async(orchestrator, result.final_score, result.episode_steps, episode_root_visits, episode_tree_depths, worker_id)
    elif not orchestrator.stop_requested.is_set():
        logger.warning(f"Self-play episode from worker {worker_id} produced no experiences.")


def _log_self_play_results_async(
    orchestrator: "TrainingOrchestrator", final_score: float, episode_steps: int,
    root_visits: list, tree_depths: list, worker_id: int
):
    """Logs self-play results asynchronously."""
    avg_root_visits = np.mean(root_visits) if root_visits else 0
    avg_tree_depth = np.mean(tree_depths) if tree_depths else 0
    episode_num = orchestrator.episodes_played
    global_step = orchestrator.global_step
    buffer_size = len(orchestrator.buffer)
    total_sims = orchestrator.total_simulations_run
    buffer_fill_perc = (orchestrator.buffer_fill_progress.get_progress() * 100) if orchestrator.buffer_fill_progress else 0.0

    logger.info(
        f"[W{worker_id}] Ep {episode_num} ({episode_steps} steps, Score: {final_score:.2f}, "
        f"Visits: {avg_root_visits:.1f}, Depth: {avg_tree_depth:.1f}). Buffer: {buffer_size}"
    )

    if orchestrator.stats_collector_actor:
        stats_batch = {
            "SelfPlay/Episode_Score": (final_score, episode_num), "SelfPlay/Episode_Length": (episode_steps, episode_num),
            "MCTS/Avg_Root_Visits": (avg_root_visits, episode_num), "MCTS/Avg_Tree_Depth": (avg_tree_depth, episode_num),
            "Buffer/Size": (buffer_size, global_step), "Progress/Total_Simulations": (total_sims, global_step),
            "Buffer/Fill_Percent": (buffer_fill_perc, global_step),
        }
        orchestrator.stats_collector_actor.log_batch.remote(stats_batch)

    mlflow.log_metric("SelfPlay/Episode_Score", final_score, step=episode_num)
    mlflow.log_metric("SelfPlay/Episode_Length", episode_steps, step=episode_num)
    mlflow.log_metric("MCTS/Avg_Root_Visits", avg_root_visits, step=episode_num)
    mlflow.log_metric("MCTS/Avg_Tree_Depth", avg_tree_depth, step=episode_num)
    mlflow.log_metric("Progress/Episodes_Played", episode_num, step=global_step)
    mlflow.log_metric("Progress/Total_Simulations", total_sims, step=global_step)
    mlflow.log_metric("Buffer/Size", buffer_size, step=global_step)
    mlflow.log_metric("Buffer/Fill_Percent", buffer_fill_perc, step=global_step)


# --- Training Step ---

def run_training_step(orchestrator: "TrainingOrchestrator") -> bool:
    """Runs one step of neural network training."""
    batch = orchestrator.buffer.sample(orchestrator.train_config.BATCH_SIZE)
    if not batch: return False
    loss_info = orchestrator.trainer.train_step(batch)
    if loss_info:
        orchestrator.global_step += 1
        if orchestrator.train_step_progress:
            orchestrator.train_step_progress.set_current_steps(orchestrator.global_step)
        _log_training_results_async(orchestrator, loss_info)
        if orchestrator.global_step % 50 == 0:
            logger.info(f"Step {orchestrator.global_step}: P Loss={loss_info['policy_loss']:.4f}, V Loss={loss_info['value_loss']:.4f}, Ent={loss_info['entropy']:.4f}")
        return True
    else:
        logger.warning(f"Training step {orchestrator.global_step + 1} failed (loss_info is None).")
        return False


def _log_training_results_async(orchestrator: "TrainingOrchestrator", loss_info: dict):
    """Logs training results asynchronously."""
    current_lr = orchestrator.trainer.get_current_lr()
    step = orchestrator.global_step
    train_step_perc = (orchestrator.train_step_progress.get_progress() * 100) if orchestrator.train_step_progress else 0.0

    if orchestrator.stats_collector_actor:
        stats_batch = {
            "Loss/Total": (loss_info["total_loss"], step), "Loss/Policy": (loss_info["policy_loss"], step),
            "Loss/Value": (loss_info["value_loss"], step), "Loss/Entropy": (loss_info["entropy"], step),
            "LearningRate": (current_lr, step), "Progress/Train_Step_Percent": (train_step_perc, step),
        }
        orchestrator.stats_collector_actor.log_batch.remote(stats_batch)

    mlflow_metrics = {
        "Loss/Total": loss_info["total_loss"], "Loss/Policy": loss_info["policy_loss"],
        "Loss/Value": loss_info["value_loss"], "Loss/Entropy": loss_info["entropy"],
        "LearningRate": current_lr, "Progress/Train_Step_Percent": train_step_perc,
    }
    log_metrics_to_mlflow(mlflow_metrics, step=step)

File: src/rl/core/trainer.py
# File: src/rl/core/trainer.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import _LRScheduler
import logging
import numpy as np
from typing import List, Dict, Optional, Tuple

# --- Package Imports ---
from src.config import TrainConfig, EnvConfig, ModelConfig
from src.nn import NeuralNetwork
from src.environment import GameState

# Use core types - ExperienceBatch contains GameState now
from src.utils.types import ExperienceBatch, PolicyTargetMapping, ActionType, StateType
from src.features import extract_state_features

logger = logging.getLogger(__name__)


class Trainer:
    """Handles the neural network training process, including loss calculation and optimizer steps."""

    def __init__(
        self,
        nn_interface: NeuralNetwork,
        train_config: TrainConfig,
        env_config: EnvConfig,
    ):
        self.nn = nn_interface
        self.model = nn_interface.model
        self.train_config = train_config
        self.env_config = env_config
        self.model_config = nn_interface.model_config
        self.device = nn_interface.device
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler(self.optimizer)

    def _create_optimizer(self) -> optim.Optimizer:
        """Creates the optimizer based on TrainConfig."""
        lr = self.train_config.LEARNING_RATE
        wd = self.train_config.WEIGHT_DECAY
        params = self.model.parameters()
        opt_type = self.train_config.OPTIMIZER_TYPE.lower()
        logger.info(f"Creating optimizer: {opt_type}, LR: {lr}, WD: {wd}")
        if opt_type == "adam": return optim.Adam(params, lr=lr, weight_decay=wd)
        elif opt_type == "adamw": return optim.AdamW(params, lr=lr, weight_decay=wd)
        elif opt_type == "sgd": return optim.SGD(params, lr=lr, weight_decay=wd, momentum=0.9)
        else: raise ValueError(f"Unsupported optimizer type: {self.train_config.OPTIMIZER_TYPE}")

    def _create_scheduler(self, optimizer: optim.Optimizer) -> Optional[_LRScheduler]:
        """Creates the learning rate scheduler based on TrainConfig."""
        scheduler_type = self.train_config.LR_SCHEDULER_TYPE
        if not scheduler_type or scheduler_type.lower() == "none":
            logger.info("No LR scheduler configured.")
            return None
        scheduler_type = scheduler_type.lower()
        logger.info(f"Creating LR scheduler: {scheduler_type}")
        if scheduler_type == "steplr":
            step_size = getattr(self.train_config, "LR_SCHEDULER_STEP_SIZE", 100000)
            gamma = getattr(self.train_config, "LR_SCHEDULER_GAMMA", 0.1)
            logger.info(f"  StepLR params: step_size={step_size}, gamma={gamma}")
            return optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
        elif scheduler_type == "cosineannealinglr":
            t_max = self.train_config.LR_SCHEDULER_T_MAX
            eta_min = self.train_config.LR_SCHEDULER_ETA_MIN
            logger.info(f"  CosineAnnealingLR params: T_max={t_max}, eta_min={eta_min}")
            return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)
        else: raise ValueError(f"Unsupported scheduler type: {scheduler_type}")

    def _prepare_batch(
        self, batch: ExperienceBatch
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """Extracts features from GameStates and converts experiences into tensors."""
        batch_size = len(batch)
        grids = []
        other_features = []
        value_targets = []
        policy_target_tensor = torch.zeros((batch_size, self.env_config.ACTION_DIM), dtype=torch.float32, device=self.device)

        for i, (game_state, policy_target_map, value_target) in enumerate(batch):
            state_dict: StateType = extract_state_features(game_state, self.model_config)
            grids.append(state_dict["grid"])
            other_features.append(state_dict["other_features"])
            value_targets.append(value_target)
            for action, prob in policy_target_map.items():
                if 0 <= action < self.env_config.ACTION_DIM:
                    policy_target_tensor[i, action] = prob
                else: logger.warning(f"Action {action} out of bounds in policy target map for sample {i}.")

        grid_tensor = torch.from_numpy(np.stack(grids)).to(self.device)
        other_features_tensor = torch.from_numpy(np.stack(other_features)).to(self.device)
        value_target_tensor = torch.tensor(value_targets, dtype=torch.float32, device=self.device).unsqueeze(1)

        expected_other_dim = self.model_config.OTHER_NN_INPUT_FEATURES_DIM
        if batch_size > 0 and other_features_tensor.shape[1] != expected_other_dim:
            raise ValueError(f"Unexpected other_features tensor shape: {other_features_tensor.shape}, expected dim {expected_other_dim}")

        return grid_tensor, other_features_tensor, policy_target_tensor, value_target_tensor

    def train_step(self, batch: ExperienceBatch) -> Optional[Dict[str, float]]:
        """Performs a single training step on the given batch."""
        if not batch:
            logger.warning("train_step called with empty batch.")
            return None

        self.model.train()
        try:
            grid_t, other_t, policy_target_t, value_target_t = self._prepare_batch(batch)
        except Exception as e:
            logger.error(f"Error preparing batch for training: {e}", exc_info=True)
            return None

        self.optimizer.zero_grad()
        policy_logits, value_pred = self.model(grid_t, other_t)

        value_loss = F.mse_loss(value_pred, value_target_t)
        policy_loss = F.cross_entropy(policy_logits, policy_target_t)

        entropy = 0.0
        entropy_loss = 0.0
        if self.train_config.ENTROPY_BONUS_WEIGHT > 0:
            policy_probs = F.softmax(policy_logits, dim=1)
            entropy_term = -torch.sum(policy_probs * torch.log(policy_probs + 1e-9), dim=1)
            entropy = entropy_term.mean().item()
            entropy_loss = -self.train_config.ENTROPY_BONUS_WEIGHT * entropy_term.mean()

        total_loss = (
            self.train_config.POLICY_LOSS_WEIGHT * policy_loss
            + self.train_config.VALUE_LOSS_WEIGHT * value_loss
            + entropy_loss
        )

        total_loss.backward()

        if self.train_config.GRADIENT_CLIP_VALUE is not None and self.train_config.GRADIENT_CLIP_VALUE > 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.train_config.GRADIENT_CLIP_VALUE)

        self.optimizer.step()
        if self.scheduler:
            self.scheduler.step()

        return {
            "total_loss": total_loss.item(),
            "policy_loss": policy_loss.item(),
            "value_loss": value_loss.item(),
            "entropy": entropy,
        }

    def get_current_lr(self) -> float:
        """Returns the current learning rate from the optimizer."""
        try: return self.optimizer.param_groups[0]["lr"]
        except (IndexError, KeyError):
            logger.warning("Could not retrieve learning rate from optimizer.")
            return 0.0

File: src/rl/self_play/worker.py
# File: src/rl/self_play/worker.py
import logging
import random
import numpy as np
import ray
import torch
import time
from typing import List, Tuple, Optional, Generator, Any, Dict

# --- Package Imports ---
from src.environment import GameState, EnvConfig
from src.mcts import (
    Node,
    MCTSConfig,
    run_mcts_simulations,
    select_action_based_on_visits,
    get_policy_target,
)
from src.nn import NeuralNetwork
from src.config import ModelConfig, TrainConfig
from src.utils import get_device, set_random_seeds

# Experience type now expects GameState
from src.utils.types import Experience, ActionType, PolicyTargetMapping
# Import SelfPlayResult Pydantic model from local rl types
from ..types import SelfPlayResult # Updated import

# Import handle type for VisualStateActor
from ..core.visual_state_actor import VisualStateActor

logger = logging.getLogger(__name__)

@ray.remote
class SelfPlayWorker:
    """
    A Ray actor responsible for running self-play episodes using MCTS and a NN.
    Pushes its state periodically to a central VisualStateActor.
    """

    def __init__(
        self,
        actor_id: int,
        env_config: EnvConfig,
        mcts_config: MCTSConfig,
        model_config: ModelConfig,
        train_config: TrainConfig,
        initial_weights: Optional[Dict] = None,
        seed: Optional[int] = None,
        worker_device_str: str = "cpu",
        visual_state_actor_handle: Optional[
            ray.actor.ActorHandle
        ] = None,  # Handle for vis actor
    ):
        self.actor_id = actor_id
        self.env_config = env_config
        self.mcts_config = mcts_config
        self.model_config = model_config
        self.train_config = train_config
        self.seed = seed if seed is not None else random.randint(0, 1_000_000)
        self.worker_device_str = worker_device_str
        self.visual_state_actor = visual_state_actor_handle  # Store handle

        # --- Configure Logging within the Actor ---
        worker_log_level = logging.DEBUG
        log_format = (
            f"%(asctime)s [%(levelname)s] [W{self.actor_id}] %(name)s: %(message)s"
        )
        logging.basicConfig(level=worker_log_level, format=log_format, force=True)
        global logger
        logger = logging.getLogger(__name__)
        logging.getLogger("src.mcts.core.search").setLevel(logging.DEBUG)
        logging.getLogger("src.mcts.strategy.selection").setLevel(logging.DEBUG)
        logging.getLogger("src.mcts.strategy.expansion").setLevel(logging.DEBUG)
        logging.getLogger("src.mcts.strategy.backpropagation").setLevel(logging.DEBUG)
        # -----------------------------------------

        set_random_seeds(self.seed)

        self.device = get_device(self.worker_device_str)
        self.nn_evaluator = NeuralNetwork(
            model_config=self.model_config,
            env_config=self.env_config,
            train_config=self.train_config,
            device=self.device,
        )
        if initial_weights:
            self.set_weights(initial_weights)
        else:
            self.nn_evaluator.model.eval()

        logger.info(
            f"Worker initialized on device {self.device}. Seed: {self.seed}. LogLevel: {logging.getLevelName(logger.getEffectiveLevel())}"
        )
        logger.debug("Worker init complete.")

    def set_weights(self, weights: Dict):
        """Updates the neural network weights."""
        try:
            self.nn_evaluator.set_weights(weights)
            logger.debug(f"Weights updated.")
        except Exception as e:
            logger.error(f"Failed to set weights: {e}", exc_info=True)

    def _push_visual_state(self, game_state: GameState):
        """Asynchronously pushes the current game state to the visual actor."""
        if self.visual_state_actor:
            logger.debug(
                f"Pushing state (step {game_state.current_step}) to visual actor."
            )
            try:
                state_copy = game_state.copy()
                self.visual_state_actor.update_state.remote(self.actor_id, state_copy)
            except Exception as e:
                logger.error(f"Failed to copy or push visual state: {e}")

    def run_episode(self) -> SelfPlayResult:
        """
        Runs a single episode of self-play using MCTS and the internal neural network.
        Pushes state updates to the visual actor. Returns a SelfPlayResult Pydantic model.
        """
        self.nn_evaluator.model.eval()
        episode_seed = self.seed + random.randint(0, 1000)
        game = GameState(self.env_config, initial_seed=episode_seed)
        self._push_visual_state(game)  # Push initial state
        raw_experiences: List[Tuple[GameState, PolicyTargetMapping, float]] = []
        logger.debug(f"Starting episode with seed {episode_seed}")

        while not game.is_over():
            # Push state update before potentially long MCTS
            self._push_visual_state(game)

            root_node = Node(state=game)
            logger.debug(f"Running MCTS for step {game.current_step}...")
            mcts_max_depth = run_mcts_simulations(
                root_node, self.mcts_config, self.nn_evaluator
            )
            logger.debug(
                f"MCTS finished for step {game.current_step}. Max Depth: {mcts_max_depth}"
            )

            temp = (
                self.mcts_config.temperature_initial
                if game.current_step < self.mcts_config.temperature_anneal_steps
                else self.mcts_config.temperature_final
            )
            policy_target = get_policy_target(root_node, temperature=1.0)
            action = select_action_based_on_visits(root_node, temperature=temp)

            if action is None:
                logger.error(
                    f"MCTS failed to select action at step {game.current_step}. State: {game}. Aborting."
                )
                game.game_over = True
                break

            display_stats: Dict[str, Any] = {
                "game_step": game.current_step + 1,
                "mcts_simulations": self.mcts_config.num_simulations,
                "mcts_root_visits": root_node.visit_count,
                "mcts_temperature": temp,
                "mcts_root_value": root_node.value_estimate,
                "mcts_selected_action": action,
                "mcts_tree_depth": mcts_max_depth,
            }

            state_to_store = game.copy()
            state_to_store.display_stats = display_stats.copy()
            raw_experiences.append((state_to_store, policy_target, 0.0))

            _, _, done = game.step(action)

            # Attach stats to the main game object for the *next* state push
            game.display_stats = display_stats

            if done:
                break

        final_outcome = game.get_outcome()
        logger.debug(
            f"Episode finished. Outcome: {final_outcome}, Steps: {game.current_step}"
        )

        processed_experiences: List[Experience] = [
            (gs, policy, final_outcome) for gs, policy, _ in raw_experiences
        ]

        # Prepare final state for return and push final update
        if not hasattr(game, "display_stats") or not game.display_stats:
            game.display_stats = {}
        game.display_stats["game_step"] = game.current_step
        game.display_stats["final_score"] = final_outcome
        game.display_stats.pop("mcts_selected_action", None)
        last_mcts_depth = (
            raw_experiences[-1][0].display_stats.get("mcts_tree_depth", "?")
            if raw_experiences
            else "?"
        )
        game.display_stats["mcts_tree_depth"] = last_mcts_depth
        self._push_visual_state(game)  # Push final state

        # Return the Pydantic model instance
        return SelfPlayResult(
            episode_experiences=processed_experiences,
            final_score=final_outcome,
            episode_steps=game.current_step,
            final_game_state=game,
        )

File: src/rl/self_play/__init__.py


File: src/rl/self_play/README.md
# File: src/rl/self_play/README.md
# RL Self-Play Submodule (`src.rl.self_play`)

## Purpose and Architecture

This submodule focuses specifically on generating game episodes through self-play, driven by the current neural network and MCTS. It is designed to run in parallel using Ray actors managed by the `TrainingOrchestrator`.

-   **`worker.py`:** Defines the `SelfPlayWorker` class, decorated with `@ray.remote`.
    -   Each `SelfPlayWorker` actor runs independently, typically on a separate CPU core.
    -   It initializes its own `GameState` environment and `NeuralNetwork` instance (usually on the CPU).
    -   It receives configuration objects (`EnvConfig`, `MCTSConfig`, `ModelConfig`, `TrainConfig`) during initialization.
    -   It has a `set_weights` method allowing the `TrainingOrchestrator` to periodically update its local neural network with the latest trained weights from the central model.
    -   Its main method, `run_episode`, simulates a complete game episode:
        -   Uses its local `NeuralNetwork` evaluator and `MCTSConfig` to run MCTS (`src.mcts.run_mcts_simulations`).
        -   Selects actions based on MCTS results (`src.mcts.strategy.policy.select_action_based_on_visits`).
        -   Generates policy targets (`src.mcts.strategy.policy.get_policy_target`).
        -   Stores `(GameState, policy_target, placeholder_value)` tuples.
        -   Steps its local game environment (`GameState.step`).
        -   Backfills the value target after the episode ends.
        -   Returns the collected `Experience` list, final score, episode length, and final `GameState` to the orchestrator.
    -   Optionally, one worker (typically worker 0) can be designated to collect intermediate `GameState` objects with attached statistics, which the orchestrator can then forward to the visualization queue.

## Exposed Interfaces

-   **Classes:**
    -   `SelfPlayWorker`: Ray actor class.
        -   `__init__(...)`
        -   `run_episode() -> SelfPlayResult`: Runs one episode and returns results.
        -   `set_weights(weights: Dict)`: Updates the actor's local network weights.
-   **Types:**
    -   `SelfPlayResult = Tuple[List[Experience], float, int, GameState]` (where `Experience` contains `GameState`).

## Dependencies

-   **`src.config`**:
    -   `EnvConfig`, `MCTSConfig`, `ModelConfig`, `TrainConfig`.
-   **`src.nn`**:
    -   `NeuralNetwork`: Instantiated locally within the actor.
-   **`src.mcts`**:
    -   Core MCTS functions and types.
-   **`src.environment`**:
    -   `GameState`, `EnvConfig`: Used to instantiate and step through the game simulation locally.
-   **`src.utils`**:
    -   `types`: `Experience`, `ActionType`, `PolicyTargetMapping`.
    -   `helpers`: `get_device`, `set_random_seeds`.
-   **`numpy`**:
    -   Used by MCTS strategies.
-   **`ray`**:
    -   The `@ray.remote` decorator makes this a Ray actor.
-   **`torch`**:
    -   Used by the local `NeuralNetwork`.
-   **Standard Libraries:** `typing`, `logging`, `random`.

---

**Note:** Please keep this README updated when changing the self-play episode generation logic within the actor, the data collected (`Experience`), or the interaction with MCTS or the environment. Accurate documentation is crucial for maintainability.


File: src/stats/collector.py
# File: src/stats/collector.py
import logging
import ray
from collections import deque
from typing import Dict, List, Tuple, Optional, Deque, Any
import numpy as np

# Import type alias from utils
from src.utils.types import StatsCollectorData

# Use print for actor logging
# logger = logging.getLogger(__name__)

@ray.remote
class StatsCollectorActor:
    """Ray actor for collecting time-series statistics."""

    def __init__(self, max_history: Optional[int] = 1000):
        self.max_history = max_history
        self._data: StatsCollectorData = {}
        print(f"[StatsCollectorActor] Initialized with max_history={max_history}.")

    def log(self, metric_name: str, value: float, step: int):
        """Logs a single metric value."""
        if not isinstance(metric_name, str):
            print(f"[StatsCollectorActor] ERROR: Invalid metric_name type: {type(metric_name)}")
            return
        if not np.isfinite(value):
            print(f"[StatsCollectorActor] WARNING: Received non-finite value for metric '{metric_name}': {value}. Skipping log.")
            return
        if metric_name not in self._data:
            self._data[metric_name] = deque(maxlen=self.max_history)
        try:
            self._data[metric_name].append((int(step), float(value)))
        except (ValueError, TypeError) as e:
            print(f"[StatsCollectorActor] ERROR: Could not log metric '{metric_name}'. Invalid step/value: {e}")

    def log_batch(self, metrics: Dict[str, Tuple[float, int]]):
        """Logs a batch of metrics."""
        for name, (value, step) in metrics.items():
            self.log(name, value, step)

    def get_data(self) -> StatsCollectorData:
        """Returns a copy of the collected statistics data."""
        return self._data.copy()

    def get_metric_data(self, metric_name: str) -> Optional[Deque[Tuple[int, float]]]:
        """Returns the data deque for a specific metric."""
        return self._data.get(metric_name)

    def clear(self):
        """Clears all collected statistics."""
        self._data = {}
        print("[StatsCollectorActor] Data cleared.")

    def get_state(self) -> Dict[str, Any]:
        """Returns the internal state for saving (converts deques to lists)."""
        serializable_data = {key: list(dq) for key, dq in self._data.items()}
        state = {'max_history': self.max_history, '_data_list': serializable_data}
        # print(f"[StatsCollectorActor] get_state called. Returning state for {len(serializable_data)} metrics.")
        return state

    def set_state(self, state: Dict[str, Any]):
        """Restores the internal state from saved data (expects lists)."""
        # print("[StatsCollectorActor] set_state called.")
        self.max_history = state.get('max_history', self.max_history)
        loaded_data_list = state.get('_data_list', {})
        self._data = {}
        restored_count = 0
        for key, items_list in loaded_data_list.items():
            # Ensure items_list is actually a list of tuples before creating deque
            if isinstance(items_list, list) and all(isinstance(item, tuple) and len(item) == 2 for item in items_list):
                self._data[key] = deque(items_list, maxlen=self.max_history)
                restored_count += 1
            else:
                print(f"[StatsCollectorActor] WARNING: Skipping restore for metric '{key}'. Invalid data format: {type(items_list)}")

        print(f"[StatsCollectorActor] State restored. Restored {restored_count} metrics. Max history: {self.max_history}")

File: src/stats/__init__.py
# File: src/stats/__init__.py
"""
Statistics collection and plotting module.
"""
# Removed: from .collector import StatsCollector
from .collector import StatsCollectorActor # Import the new actor
from .plotter import Plotter
from . import plot_utils  # Expose plot_utils if needed externally
from src.utils.types import StatsCollectorData  # Import type alias

__all__ = [
    # Removed: "StatsCollector",
    "StatsCollectorActor", # Export the actor
    "StatsCollectorData",  # Export type alias
    "Plotter",
    "plot_utils",
]


File: src/stats/README.md
# File: src/stats/README.md
# Statistics Module (`src.stats`)

## Purpose and Architecture

This module provides utilities for collecting, storing, and visualizing time-series statistics generated during the reinforcement learning training process using Matplotlib rendered onto Pygame surfaces.

-   **`collector.py`:** Defines the `StatsCollectorActor` class, a **Ray actor**. This actor uses dictionaries of `deque`s to store metric values (like losses, rewards, learning rate) associated with training steps or episodes. It provides **remote methods** (`log`, `log_batch`) for asynchronous logging from multiple sources (e.g., orchestrator, workers) and methods (`get_data`, `get_metric_data`) for fetching the stored data (e.g., by the visualizer). It supports limiting the history size for performance and memory management. It also includes `get_state` and `set_state` methods for checkpointing.
-   **`plot_utils.py`:** Contains helper functions for Matplotlib plotting, including calculating rolling averages, trend lines, formatting values, and the core `render_single_plot` function which draws a single metric onto a Matplotlib `Axes` object.
-   **`plotter.py`:** Defines the `Plotter` class which manages the overall Matplotlib figure and axes.
    -   It orchestrates the plotting of multiple metrics onto a grid within the figure using `render_single_plot`.
    -   It handles rendering the Matplotlib figure to an in-memory buffer and then converting it to a `pygame.Surface`.
    -   It implements caching logic to avoid regenerating the plot surface on every frame if the data or target size hasn't changed significantly, improving performance.

## Exposed Interfaces

-   **Classes:**
    -   `StatsCollectorActor`: Ray actor for collecting stats.
        -   `__init__(max_history: Optional[int] = 1000)`
        -   `log.remote(metric_name: str, value: float, step: int)`
        -   `log_batch.remote(metrics: Dict[str, Tuple[float, int]])`
        -   `get_data.remote() -> StatsCollectorData`
        -   `get_metric_data.remote(metric_name: str) -> Optional[Deque[Tuple[int, float]]]`
        -   `clear.remote()`
        -   `get_state.remote() -> Dict[str, Any]`
        -   `set_state.remote(state: Dict[str, Any])`
    -   `Plotter`:
        -   `__init__(plot_update_interval: float = 2.0)`
        -   `get_plot_surface(plot_data: StatsCollectorData, target_width: int, target_height: int) -> Optional[pygame.Surface]`
-   **Types:**
    -   `StatsCollectorData`: Type alias `Dict[str, Deque[Tuple[int, float]]]` representing the stored data.
-   **Modules:**
    -   `plot_utils`: Contains helper functions like `render_single_plot`.

## Dependencies

-   **`src.visualization.core.colors`**: Used by `plot_utils.py`.
-   **`pygame`**: Used by `plotter.py` to create the final surface.
-   **`matplotlib`**: Used by `plotter.py` and `plot_utils.py` for generating plots.
-   **`numpy`**: Used by `plot_utils.py` for calculations.
-   **`ray`**: Used by `StatsCollectorActor`.
-   **Standard Libraries:** `typing`, `logging`, `collections.deque`, `math`, `time`, `io`.

## Integration

-   The `TrainingOrchestrator` (`src.rl.core.orchestrator`) instantiates `StatsCollectorActor` and calls its remote `log` or `log_batch` methods.
-   The `GameRenderer` (`src.visualization.core.game_renderer`) holds a handle to the `StatsCollectorActor` and calls `get_data.remote()` periodically to fetch data for plotting.
-   The `GameRenderer` instantiates `Plotter` and calls `get_plot_surface` using the fetched stats data and the target plot area dimensions. It then blits the returned surface.
-   The `DataManager` interacts with the `StatsCollectorActor` via `get_state.remote()` and `set_state.remote()` during checkpoint saving and loading.

---

**Note:** Please keep this README updated when changing the data collection methods, the plotting functions, or the way statistics are managed and displayed, especially regarding the actor-based approach.


File: src/stats/plotter.py
# File: src/stats/plotter.py
import pygame
from typing import Dict, Optional, Deque, Tuple, List
from collections import deque
import matplotlib
import time
from io import BytesIO
import logging
import numpy as np

matplotlib.use("Agg")
import matplotlib.pyplot as plt

# Use relative imports within stats module
from .collector import StatsCollectorData
from .plot_utils import render_single_plot, normalize_color_for_matplotlib

# Import colors from visualization module
from src.visualization.core import colors as vis_colors

logger = logging.getLogger(__name__)


class Plotter:
    """Handles creation and caching of the multi-plot Matplotlib surface."""

    def __init__(self, plot_update_interval: float = 0.5):
        self.plot_surface_cache: Optional[pygame.Surface] = None
        self.last_plot_update_time: float = 0.0
        self.plot_update_interval: float = plot_update_interval
        self.rolling_window_sizes: List[int] = [10, 50]
        self.colors = self._init_colors()

        self.fig: Optional[plt.Figure] = None
        self.axes: Optional[np.ndarray] = None
        self.last_target_size: Tuple[int, int] = (0, 0)
        self.last_data_hash: Optional[int] = None

        logger.info(
            f"[Plotter] Initialized with update interval: {self.plot_update_interval}s"
        )

    def _init_colors(self) -> Dict[str, Tuple[float, float, float]]:
        """Initializes plot colors using vis_colors."""
        return {
            "SelfPlay/Episode_Score": normalize_color_for_matplotlib(vis_colors.YELLOW),
            "Loss/Total": normalize_color_for_matplotlib(vis_colors.RED),
            "Loss/Value": normalize_color_for_matplotlib(vis_colors.BLUE),
            "Loss/Policy": normalize_color_for_matplotlib(vis_colors.GREEN),
            "LearningRate": normalize_color_for_matplotlib(vis_colors.CYAN),
            "SelfPlay/Episode_Length": normalize_color_for_matplotlib(
                vis_colors.ORANGE
            ),
            "Buffer/Size": normalize_color_for_matplotlib(vis_colors.PURPLE),
            "MCTS/Avg_Root_Visits": normalize_color_for_matplotlib(
                vis_colors.LIGHT_GRAY
            ),
            "MCTS/Avg_Tree_Depth": normalize_color_for_matplotlib(vis_colors.LIGHTG),
            "placeholder": normalize_color_for_matplotlib(vis_colors.GRAY),
        }

    def _init_figure(self, target_width: int, target_height: int):
        """Initializes the Matplotlib figure and axes."""
        logger.info(
            f"[Plotter] Initializing Matplotlib figure for size {target_width}x{target_height}"
        )
        if self.fig:
            try:
                plt.close(self.fig)
            except Exception as e:
                logger.warning(f"Error closing previous figure: {e}")

        dpi = 96
        fig_width_in = max(1, target_width / dpi)
        fig_height_in = max(1, target_height / dpi)

        try:
            nrows, ncols = 2, 4
            self.fig, self.axes = plt.subplots(
                nrows,
                ncols,
                figsize=(fig_width_in, fig_height_in),
                dpi=dpi,
                sharex=False,
            )
            self.fig.subplots_adjust(
                hspace=0.4, wspace=0.35, left=0.08, right=0.98, bottom=0.15, top=0.92
            )
            self.last_target_size = (target_width, target_height)
            logger.info(
                f"[Plotter] Matplotlib figure initialized ({nrows}x{ncols} grid)."
            )
        except Exception as e:
            logger.error(f"Error creating Matplotlib figure: {e}", exc_info=True)
            self.fig = None
            self.axes = None
            self.last_target_size = (0, 0)

    def _get_data_hash(self, plot_data: StatsCollectorData) -> int:
        """Generates a simple hash based on data lengths and last elements."""
        hash_val = 0
        for key in sorted(plot_data.keys()):
            dq = plot_data[key]
            if not dq:
                continue
            hash_val ^= hash(key)
            hash_val ^= len(dq)
            try:
                last_step, last_val = dq[-1]
                hash_val ^= hash(last_step)
                hash_val ^= hash(f"{last_val:.6f}")
            except IndexError:
                pass
        return hash_val

    def _update_plot_data(self, plot_data: StatsCollectorData):
        """Updates the data on the existing Matplotlib axes."""
        if self.fig is None or self.axes is None:
            logger.warning("[Plotter] Cannot update plot data, figure not initialized.")
            return False

        plot_update_start = time.monotonic()
        try:
            axes_flat = self.axes.flatten()
            plot_defs = [
                ("SelfPlay/Episode_Score", "Ep Score", False),
                ("Loss/Total", "Total Loss", True),
                ("MCTS/Avg_Root_Visits", "Root Visits", False),
                ("LearningRate", "Learn Rate", True),
                ("SelfPlay/Episode_Length", "Ep Length", False),
                ("Loss/Value", "Value Loss", True),
                ("Loss/Policy", "Policy Loss", True),
                ("MCTS/Avg_Tree_Depth", "Tree Depth", False),
            ]

            # Extract steps and values separately
            data_values: Dict[str, List[float]] = {}
            data_steps: Dict[str, List[int]] = {}
            for key, _, _ in plot_defs:
                dq = plot_data.get(key, deque())
                if dq:
                    steps, values = zip(*dq)
                    data_values[key] = list(values)
                    data_steps[key] = list(steps)
                else:
                    data_values[key] = []
                    data_steps[key] = []

            for i, (data_key, label, log_scale) in enumerate(plot_defs):
                if i >= len(axes_flat):
                    break
                ax = axes_flat[i]
                ax.clear()

                current_values = data_values.get(data_key, [])
                current_steps = data_steps.get(data_key, [])  # Get corresponding steps
                color_mpl = self.colors.get(data_key, (0.5, 0.5, 0.5))

                render_single_plot(
                    ax,
                    current_steps,  # Pass steps as x_coords
                    current_values,  # Pass values as y_data
                    label,
                    color_mpl,
                    self.rolling_window_sizes,
                    show_placeholder=(not current_values),
                    placeholder_text=label,
                    y_log_scale=log_scale,
                )
                nrows, ncols = self.axes.shape
                if i < (nrows - 1) * ncols:
                    ax.set_xticklabels([])
                    ax.set_xlabel("")
                ax.tick_params(axis="x", rotation=0)

            plot_update_duration = time.monotonic() - plot_update_start
            return True

        except Exception as e:
            logger.error(f"Error updating plot data: {e}", exc_info=True)
            try:
                if self.axes is not None:
                    for ax in self.axes.flatten():
                        ax.clear()
            except Exception:
                pass
            return False

    def _render_figure_to_surface(
        self, target_width: int, target_height: int
    ) -> Optional[pygame.Surface]:
        """Renders the current Matplotlib figure to a Pygame surface."""
        if self.fig is None:
            logger.warning("[Plotter] Cannot render figure, not initialized.")
            return None

        render_start = time.monotonic()
        try:
            self.fig.canvas.draw()
            buf = BytesIO()
            self.fig.savefig(
                buf,
                format="png",
                transparent=False,
                facecolor=plt.rcParams["figure.facecolor"],
            )
            buf.seek(0)
            plot_img_surface = pygame.image.load(buf, "png").convert()
            buf.close()

            current_size = plot_img_surface.get_size()
            if current_size != (target_width, target_height):
                plot_img_surface = pygame.transform.scale(
                    plot_img_surface, (target_width, target_height)
                )
            render_duration = time.monotonic() - render_start
            return plot_img_surface

        except Exception as e:
            logger.error(f"Error rendering Matplotlib figure: {e}", exc_info=True)
            return None

    def get_plot_surface(
        self, plot_data: StatsCollectorData, target_width: int, target_height: int
    ) -> Optional[pygame.Surface]:
        """Returns the cached plot surface or creates/updates one if needed."""
        current_time = time.time()
        has_data = any(plot_data.values())
        target_size = (target_width, target_height)

        needs_reinit = (
            self.fig is None
            or self.axes is None
            or self.last_target_size != target_size
        )
        current_data_hash = self._get_data_hash(plot_data)
        data_changed = self.last_data_hash != current_data_hash
        time_elapsed = (
            current_time - self.last_plot_update_time
        ) > self.plot_update_interval
        needs_update = data_changed or time_elapsed
        can_create_plot = target_width > 50 and target_height > 50

        if not can_create_plot:
            if self.plot_surface_cache is not None:
                logger.info("[Plotter] Target size too small, clearing cache/figure.")
                self.plot_surface_cache = None
                if self.fig:
                    plt.close(self.fig)
                self.fig, self.axes = None, None
                self.last_target_size = (0, 0)
            return None

        if not has_data:
            if self.plot_surface_cache is not None:
                logger.info("[Plotter] No plot data, clearing cache/figure.")
                self.plot_surface_cache = None
                if self.fig:
                    plt.close(self.fig)
                self.fig, self.axes = None, None
                self.last_target_size = (0, 0)
            return None

        cache_status = "HIT"
        try:
            if needs_reinit:
                cache_status = "MISS (Re-init)"
                self._init_figure(target_width, target_height)
                if self.fig and self._update_plot_data(plot_data):
                    self.plot_surface_cache = self._render_figure_to_surface(
                        target_width, target_height
                    )
                    self.last_plot_update_time = current_time
                    self.last_data_hash = current_data_hash
                else:
                    self.plot_surface_cache = None
            elif needs_update:
                cache_status = (
                    f"MISS (Update - Data: {data_changed}, Time: {time_elapsed})"
                )
                if self._update_plot_data(plot_data):
                    self.plot_surface_cache = self._render_figure_to_surface(
                        target_width, target_height
                    )
                    self.last_plot_update_time = current_time
                    self.last_data_hash = current_data_hash
                else:
                    logger.warning(
                        "[Plotter] Plot update failed, returning stale cache."
                    )
                    cache_status = "ERROR (Update Failed)"
            elif self.plot_surface_cache is None:
                cache_status = "MISS (Cache None)"
                if self._update_plot_data(plot_data):
                    self.plot_surface_cache = self._render_figure_to_surface(
                        target_width, target_height
                    )
                    self.last_plot_update_time = current_time
                    self.last_data_hash = current_data_hash

        except Exception as e:
            logger.error(f"[Plotter] Error in get_plot_surface: {e}", exc_info=True)
            self.plot_surface_cache = None
            if self.fig:
                plt.close(self.fig)
            self.fig, self.axes = None, None
            self.last_target_size = (0, 0)

        return self.plot_surface_cache

    def __del__(self):
        """Ensure Matplotlib figure is closed."""
        if self.fig:
            try:
                plt.close(self.fig)
                logger.info("[Plotter] Matplotlib figure closed.")
            except Exception as e:
                logger.error(f"[Plotter] Error closing figure in destructor: {e}")


File: src/stats/plot_utils.py
# File: src/stats/plot_utils.py
import numpy as np
from typing import Optional, List, Union, Tuple
import matplotlib
import traceback
import math
import logging

matplotlib.use("Agg")
import matplotlib.pyplot as plt

# Import colors from visualization module
from src.visualization.core import colors as vis_colors

logger = logging.getLogger(__name__)

# --- Constants ---
TREND_SLOPE_TOLERANCE = 1e-5
TREND_MIN_LINEWIDTH = 1
TREND_MAX_LINEWIDTH = 2
TREND_COLOR_STABLE = (1.0, 1.0, 0.0)  # Yellow
TREND_COLOR_INCREASING = (0.0, 0.8, 0.0)  # Green
TREND_COLOR_DECREASING = (0.8, 0.0, 0.0)  # Red
TREND_SLOPE_SCALE_FACTOR = 5.0
TREND_BACKGROUND_ALPHA = 0.15
TREND_LINE_COLOR = (1.0, 1.0, 1.0)
TREND_LINE_STYLE = (0, (5, 10))
TREND_LINE_WIDTH = 0.75
TREND_LINE_ALPHA = 0.7
TREND_LINE_ZORDER = 10
MIN_ALPHA = 0.4  # Alpha for averages/raw line
MAX_ALPHA = 1.0  # Alpha for averages
MIN_DATA_AVG_LINEWIDTH = 1
MAX_DATA_AVG_LINEWIDTH = 2
RAW_DATA_LINEWIDTH = 0.8  # Thinner line for raw data
RAW_DATA_ALPHA = 0.3  # Lower alpha for raw data
DEFAULT_ROLLING_WINDOWS = [10, 50]  # Default rolling average windows


# --- Helper Functions ---
def normalize_color_for_matplotlib(
    color_tuple_0_255: Tuple[int, int, int],
) -> Tuple[float, float, float]:
    """Converts RGB tuple (0-255) to Matplotlib format (0.0-1.0)."""
    if isinstance(color_tuple_0_255, tuple) and len(color_tuple_0_255) == 3:
        return tuple(c / 255.0 for c in color_tuple_0_255)
    logger.warning(f"Invalid color format for normalization: {color_tuple_0_255}")
    return (0.0, 0.0, 0.0)  # Default black


# --- Matplotlib Style Setup ---
try:
    plt.style.use("dark_background")
    # Remove invalid legend.bbox_to_anchor parameter
    plt.rcParams.update(
        {
            "font.size": 9,
            "axes.labelsize": 9,
            "axes.titlesize": 10,  # Slightly smaller title
            "xtick.labelsize": 8,
            "ytick.labelsize": 8,
            "legend.fontsize": 7,  # Smaller legend
            "figure.facecolor": "#262626",
            "axes.facecolor": "#303030",
            "axes.edgecolor": "#707070",
            "axes.labelcolor": "#D0D0D0",
            "xtick.color": "#C0C0C0",
            "ytick.color": "#C0C0C0",
            "grid.color": "#505050",
            "grid.linestyle": "--",
            "grid.alpha": 0.5,
            "axes.titlepad": 4,  # Reduced padding
            "legend.frameon": False,  # No frame for legend
            "legend.loc": "best",  # Use 'best' or specify per plot if needed
            # "legend.bbox_to_anchor": (0, 0.5), # REMOVED - Invalid parameter
            "legend.labelspacing": 0.3,
            "legend.handletextpad": 0.5,
            "legend.handlelength": 1.0,
        }
    )
except Exception as e:
    logger.warning(f"Failed to set Matplotlib style: {e}")


# --- Trend Calculation ---
def calculate_trend_line(
    x_coords: np.ndarray, y_data: np.ndarray
) -> Optional[Tuple[float, float]]:
    """Calculates the slope and intercept of the linear regression line."""
    mask = np.isfinite(y_data)
    if np.sum(mask) < 2:
        return None
    try:
        coeffs = np.polyfit(x_coords[mask], y_data[mask], 1)
        if not all(np.isfinite(c) for c in coeffs):
            return None
        return coeffs[0], coeffs[1]  # slope, intercept
    except (np.linalg.LinAlgError, ValueError):
        return None


def get_trend_color(slope: float, lower_is_better: bool) -> Tuple[float, float, float]:
    """Maps slope to color (Red -> Yellow -> Green)."""
    if abs(slope) < TREND_SLOPE_TOLERANCE:
        return TREND_COLOR_STABLE
    eff_slope = -slope if lower_is_better else slope
    norm_slope = np.clip(
        math.atan(eff_slope * TREND_SLOPE_SCALE_FACTOR) / (math.pi / 2.0), -1.0, 1.0
    )
    t = abs(norm_slope)
    base, target = (
        (TREND_COLOR_STABLE, TREND_COLOR_INCREASING)
        if norm_slope > 0
        else (TREND_COLOR_STABLE, TREND_COLOR_DECREASING)
    )
    color = tuple(base[i] * (1 - t) + target[i] * t for i in range(3))
    return tuple(np.clip(c, 0.0, 1.0) for c in color)


def get_trend_linewidth(slope: float) -> float:
    """Maps slope magnitude to border linewidth."""
    if abs(slope) < TREND_SLOPE_TOLERANCE:
        return TREND_MIN_LINEWIDTH
    norm_mag = np.clip(
        abs(math.atan(slope * TREND_SLOPE_SCALE_FACTOR) / (math.pi / 2.0)), 0.0, 1.0
    )
    return TREND_MIN_LINEWIDTH + norm_mag * (TREND_MAX_LINEWIDTH - TREND_MIN_LINEWIDTH)


# --- Visual Property Interpolation ---
def _interpolate_visual_property(
    rank: int, total_ranks: int, min_val: float, max_val: float
) -> float:
    """Linearly interpolates alpha/linewidth based on rank."""
    if total_ranks <= 1:
        return float(max_val)
    # Rank 0 is the longest average (most prominent), higher ranks are shorter averages
    fraction = rank / max(1, total_ranks - 1)
    value = float(max_val) - (float(max_val) - float(min_val)) * fraction
    return float(np.clip(value, min_val, max_val))


# --- Value Formatting ---
def _format_value(value: float, is_loss: bool) -> str:
    """Formats value based on magnitude and whether it's a loss."""
    if not np.isfinite(value):
        return "N/A"
    if abs(value) < 1e-3 and value != 0:
        return f"{value:.1e}"
    if abs(value) >= 1000:
        return f"{value:,.0f}".replace(",", "_")
    if is_loss:
        return f"{value:.3f}"
    if abs(value) < 10:
        return f"{value:.2f}"
    return f"{value:.1f}"


def _format_slope(slope: float) -> str:
    """Formats slope value for display in the legend."""
    if not np.isfinite(slope):
        return "N/A"
    sign = "+" if slope >= 0 else ""
    abs_slope = abs(slope)
    if abs_slope < 1e-4:
        return f"{sign}{slope:.1e}"
    if abs_slope < 0.1:
        return f"{sign}{slope:.3f}"
    return f"{sign}{slope:.2f}"


# --- Main Plotting Function ---
def render_single_plot(
    ax,
    x_coords: List[int],  # Added x_coords (steps)
    y_data: List[Union[float, int]],  # Renamed data to y_data
    label: str,
    color: Tuple[float, float, float],
    rolling_window_sizes: List[int] = DEFAULT_ROLLING_WINDOWS,
    show_placeholder: bool = True,
    placeholder_text: Optional[str] = None,
    y_log_scale: bool = False,
):
    """Renders data with rolling averages, trend line, and informative legend."""
    try:
        x_coords_np = np.array(x_coords, dtype=float)
        y_data_np = np.array(y_data, dtype=float)
        # Ensure x and y have same length after potential conversion errors
        min_len = min(len(x_coords_np), len(y_data_np))
        x_coords_np = x_coords_np[:min_len]
        y_data_np = y_data_np[:min_len]

        valid_mask = np.isfinite(y_data_np)
        valid_y = y_data_np[valid_mask]
        valid_x = x_coords_np[valid_mask]

    except (ValueError, TypeError):
        valid_y = np.array([])
        valid_x = np.array([])

    n_points = len(valid_y)
    is_lower_better = "loss" in label.lower()

    if n_points == 0:  # Handle empty data
        if show_placeholder:
            p_text = placeholder_text if placeholder_text else f"{label}\n(No data)"
            ax.text(
                0.5,
                0.5,
                p_text,
                ha="center",
                va="center",
                transform=ax.transAxes,
                fontsize=8,
                color=normalize_color_for_matplotlib(vis_colors.GRAY),
            )
        ax.set_yticks([])
        ax.set_xticks([])
        ax.set_title(f"{label} (N/A)", loc="left")
        ax.grid(False)
        ax.patch.set_facecolor(plt.rcParams["axes.facecolor"])
        ax.patch.set_edgecolor(plt.rcParams["axes.edgecolor"])
        ax.patch.set_linewidth(0.5)
        return

    trend_params = calculate_trend_line(valid_x, valid_y)  # Use valid_x for trend calc
    trend_slope = trend_params[0] if trend_params else 0.0
    trend_color = get_trend_color(trend_slope, is_lower_better)
    trend_lw = get_trend_linewidth(trend_slope)
    plotted_windows = sorted([w for w in rolling_window_sizes if n_points >= w])
    total_ranks = 1 + len(plotted_windows)
    current_val = valid_y[-1]
    best_val = np.min(valid_y) if is_lower_better else np.max(valid_y)
    best_val_str = f"Best: {_format_value(best_val, is_lower_better)}"
    ax.set_title(label, loc="left")

    try:
        plotted_legend_items = 0
        min_y, max_y = float("inf"), float("-inf")

        # --- Always plot raw data first (less prominent) ---
        raw_label = f"Val: {_format_value(current_val, is_lower_better)}"
        ax.plot(
            valid_x,  # Use valid_x
            valid_y,
            color=color,
            linewidth=RAW_DATA_LINEWIDTH,
            label=raw_label,
            alpha=RAW_DATA_ALPHA,
            zorder=5,
        )
        min_y = min(min_y, np.min(valid_y))
        max_y = max(max_y, np.max(valid_y))
        plotted_legend_items += 1
        # --- End Plot Raw Data ---

        # --- Plot Rolling Averages (more prominent) ---
        for i, avg_win in enumerate(plotted_windows):
            rank = i
            alpha = _interpolate_visual_property(
                rank, len(plotted_windows), MIN_ALPHA, MAX_ALPHA
            )
            lw = _interpolate_visual_property(
                rank,
                len(plotted_windows),
                MIN_DATA_AVG_LINEWIDTH,
                MAX_DATA_AVG_LINEWIDTH,
            )

            weights = np.ones(avg_win) / avg_win
            # Convolve on valid y data
            rolling_avg = np.convolve(valid_y, weights, mode="valid")
            # Calculate corresponding x coordinates for the rolling average
            avg_x = valid_x[avg_win - 1 :]  # Use valid_x

            if len(avg_x) == len(rolling_avg):
                last_avg = rolling_avg[-1] if len(rolling_avg) > 0 else np.nan
                avg_label = f"Avg {avg_win}: {_format_value(last_avg, is_lower_better)}"
                ax.plot(
                    avg_x,
                    rolling_avg,
                    color=color,
                    linewidth=lw,
                    alpha=alpha,
                    linestyle="-",
                    label=avg_label,
                    zorder=6 + i,
                )
                if len(rolling_avg) > 0:
                    min_y = min(min_y, np.min(rolling_avg))
                    max_y = max(max_y, np.max(rolling_avg))
                plotted_legend_items += 1
        # --- End Plot Rolling Averages ---

        # --- Plot Trend Line ---
        if trend_params and n_points >= 2:
            slope, intercept = trend_params
            # Use min/max of valid_x for trend line ends
            x_trend = np.array([valid_x[0], valid_x[-1]])
            y_trend = slope * x_trend + intercept
            trend_label = f"Trend: {_format_slope(slope)}"
            ax.plot(
                x_trend,
                y_trend,
                color=TREND_LINE_COLOR,
                linestyle=TREND_LINE_STYLE,
                linewidth=TREND_LINE_WIDTH,
                alpha=TREND_LINE_ALPHA,
                label=trend_label,
                zorder=TREND_LINE_ZORDER,
            )
            plotted_legend_items += 1
        # --- End Plot Trend Line ---

        ax.tick_params(axis="both", which="major")
        ax.grid(
            True,
            linestyle=plt.rcParams["grid.linestyle"],
            alpha=plt.rcParams["grid.alpha"],
        )

        if np.isfinite(min_y) and np.isfinite(max_y):
            yrange = max(max_y - min_y, 1e-6)
            pad = yrange * 0.05
            ax.set_ylim(min_y - pad, max_y + pad)

        if y_log_scale and min_y > 1e-9:
            ax.set_yscale("log")
            bottom, top = ax.get_ylim()
            new_bottom = max(bottom, 1e-9)
            if new_bottom >= top:
                new_bottom = top / 10
            ax.set_ylim(bottom=new_bottom, top=top)
        else:
            ax.set_yscale("linear")

        # Set x-limits based on actual step values
        if n_points > 1:
            ax.set_xlim(valid_x[0], valid_x[-1])
        elif n_points == 1:
            ax.set_xlim(valid_x[0] - 0.5, valid_x[0] + 0.5)  # Center single point

        # X-axis formatting for large numbers (based on step values)
        max_step = valid_x[-1] if n_points > 0 else 0
        if max_step > 1000:
            ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True, nbins=4))

            def fmt_func(v, _):
                val = int(v)
                return (
                    f"{val/1e6:.1f}M"
                    if val >= 1e6
                    else (f"{val/1e3:.0f}k" if val >= 1e3 else f"{val}")
                )

            ax.xaxis.set_major_formatter(plt.FuncFormatter(fmt_func))
        elif n_points > 10:  # Use n_points check for density, max_step for scale
            ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True, nbins=5))

        # Add legend if items were plotted
        if plotted_legend_items > 0:
            ax.legend(title=best_val_str)

    except Exception as plot_err:
        logger.error(
            f"ERROR during render_single_plot for '{label}': {plot_err}", exc_info=True
        )
        ax.text(
            0.5,
            0.5,
            f"Plot Error\n({label})",
            ha="center",
            va="center",
            transform=ax.transAxes,
            fontsize=8,
            color=normalize_color_for_matplotlib(vis_colors.RED),
        )
        ax.set_yticks([])
        ax.set_xticks([])
        ax.grid(False)

    # Set background and border based on trend
    ax.patch.set_facecolor((*trend_color, TREND_BACKGROUND_ALPHA))
    ax.patch.set_edgecolor(trend_color)
    ax.patch.set_linewidth(trend_lw)


File: src/mcts/__init__.py
# File: src/mcts/__init__.py
"""
Monte Carlo Tree Search (MCTS) module.
Provides the core algorithm and components for game tree search.
"""

# Core MCTS components
from .core.node import Node
from .core.search import run_mcts_simulations
from .core.config import MCTSConfig # Import Pydantic MCTSConfig
from .core.types import ActionPolicyValueEvaluator, ActionPolicyMapping

# Action selection and policy generation strategies
from .strategy.policy import select_action_based_on_visits, get_policy_target

__all__ = [
    # Core
    "Node",
    "run_mcts_simulations",
    "MCTSConfig", # Export Pydantic MCTSConfig
    "ActionPolicyValueEvaluator",
    "ActionPolicyMapping",
    # Strategy
    "select_action_based_on_visits",
    "get_policy_target",
]

File: src/mcts/README.md
# File: src/mcts/README.md
# Monte Carlo Tree Search Module (`src.mcts`)

## Purpose and Architecture

This module implements the Monte Carlo Tree Search algorithm, a core component of the AlphaZero-style reinforcement learning agent. MCTS is used during self-play to explore the game tree and determine the next best move and generate training targets for the neural network.

-   **Core Components (`src.mcts.core`):**
    -   `Node`: Represents a state in the search tree, storing visit counts, value estimates, prior probabilities, and child nodes. Holds a `GameState` object.
    -   `search`: Contains the main `run_mcts_simulations` function orchestrating the selection, expansion, and backpropagation phases. **This version uses batched neural network evaluation for potentially improved performance.**
    -   `config`: Defines the `MCTSConfig` class holding hyperparameters like the number of simulations, PUCT coefficient, temperature settings, and Dirichlet noise parameters.
    -   `types`: Defines necessary type hints and protocols, notably `ActionPolicyValueEvaluator` which specifies the interface required for the neural network evaluator used by MCTS.
-   **Strategy Components (`src.mcts.strategy`):**
    -   `selection`: Implements the tree traversal logic (PUCT calculation, Dirichlet noise addition, leaf selection).
    -   `expansion`: Handles expanding leaf nodes **using pre-computed policy priors** obtained from batched network evaluation.
    -   `backpropagation`: Implements the process of updating node statistics back up the tree after a simulation.
    -   `policy`: Provides functions to select the final action based on visit counts (`select_action_based_on_visits`) and to generate the policy target vector for training (`get_policy_target`).

## Exposed Interfaces

-   **Core:**
    -   `Node`: The tree node class.
    -   `MCTSConfig`: Configuration class (defined in `src.mcts.core.config`).
    -   `run_mcts_simulations(root_node: Node, config: MCTSConfig, network_evaluator: ActionPolicyValueEvaluator)`: The main function to run MCTS (uses batched evaluation).
    -   `ActionPolicyValueEvaluator`: Protocol defining the NN evaluation interface.
    -   `ActionPolicyMapping`: Type alias for the policy dictionary.
-   **Strategy:**
    -   `select_action_based_on_visits(root_node: Node, temperature: float) -> Optional[ActionType]`: Selects the final move.
    -   `get_policy_target(root_node: Node, temperature: float = 1.0) -> ActionPolicyMapping`: Generates the training policy target.

## Dependencies

-   **`src.environment`**:
    -   `GameState`: Represents the state within each `Node`. MCTS interacts heavily with `GameState` methods like `copy()`, `step()`, `is_over()`, `get_outcome()`, `valid_actions()`.
    -   `EnvConfig`: Accessed via `GameState`.
-   **`src.nn`**:
    -   `NeuralNetwork`: An instance conforming to the `ActionPolicyValueEvaluator` protocol is required by `run_mcts_simulations` and `expansion` to evaluate states (specifically `evaluate_batch`).
-   **`src.utils.types`**:
    -   `ActionType`, `PolicyValueOutput`: Used for actions and NN return types.
-   **`numpy`**:
    -   Used for Dirichlet noise generation and potentially in policy calculations.
-   **Standard Libraries:** `typing`, `math`, `logging`, `numpy`.

---

**Note:** Please keep this README updated when changing the MCTS algorithm phases (selection, expansion, backpropagation), the node structure, configuration options, or the interaction with the environment or neural network. Accurate documentation is crucial for maintainability.

File: src/mcts/core/config.py
# File: src/mcts/core/config.py
from pydantic import BaseModel, Field, field_validator

class MCTSConfig(BaseModel):
    """Configuration for Monte Carlo Tree Search (Pydantic model)."""

    num_simulations: int = Field(80, ge=1)
    puct_coefficient: float = Field(1.0, gt=0)
    temperature_initial: float = Field(1.0, ge=0)
    temperature_final: float = Field(0.01, ge=0)
    temperature_anneal_steps: int = Field(10_000, ge=0)
    dirichlet_alpha: float = Field(0.3, gt=0)
    dirichlet_epsilon: float = Field(0.25, ge=0, le=1.0)
    max_search_depth: int = Field(100, ge=1)

    @field_validator('temperature_final')
    @classmethod
    def check_temp_final_le_initial(cls, v: float, info) -> float:
        initial_temp = info.data.get('temperature_initial')
        if initial_temp is not None and v > initial_temp:
            raise ValueError("temperature_final cannot be greater than temperature_initial")
        return v

File: src/mcts/core/__init__.py


File: src/mcts/core/types.py
# File: src/mcts/types_mcts.py
from typing import Dict, Tuple, Protocol, TYPE_CHECKING, Mapping

# Import the missing type hint
from src.utils.types import PolicyValueOutput

if TYPE_CHECKING:
    from src.environment import GameState  # Updated import
    from src.utils.types import ActionType, StateType  # Updated import

# Type alias for the policy dictionary returned by the network
# Maps action index to prior probability
ActionPolicyMapping = Mapping["ActionType", float]


# Protocol defining the required method for a network evaluator used by MCTS
class ActionPolicyValueEvaluator(Protocol):
    """Defines the interface for evaluating a game state using a neural network."""

    def evaluate(self, state: "GameState") -> PolicyValueOutput:
        """
        Evaluates a single game state using the neural network.

        Args:
            state: The GameState object to evaluate.

        Returns:
            A tuple containing:
                - ActionPolicyMapping: A mapping from valid action indices
                  to their prior probabilities (output by the policy head).
                - float: The estimated value of the state (output by the value head).
        """
        ...

    def evaluate_batch(self, states: list["GameState"]) -> list[PolicyValueOutput]:
        """
        Evaluates a batch of game states using the neural network.
        (Optional but recommended for performance if MCTS supports batch evaluation).

        Args:
            states: A list of GameState objects to evaluate.

        Returns:
            A list of tuples, where each tuple corresponds to an input state and contains:
                - ActionPolicyMapping: Action probabilities for that state.
                - float: The estimated value of that state.
        """
        ...


File: src/mcts/core/README.md
# File: src/mcts/core/README.md
# MCTS Core Submodule (`src.mcts.core`)

## Purpose and Architecture

This submodule defines the fundamental building blocks and interfaces for the Monte Carlo Tree Search implementation.

-   **`Node`:** The `Node` class is the cornerstone, representing a single state within the search tree. It stores the associated `GameState`, parent/child relationships, the action that led to it, and crucial MCTS statistics (visit count, total action value, prior probability). It provides properties like `value_estimate` (Q-value) and `is_expanded`.
-   **`search`:** The `search.py` module contains the high-level `run_mcts_simulations` function. This function orchestrates the core MCTS loop for a given root node: repeatedly selecting leaves, batch-evaluating them using the network, expanding them, and backpropagating the results, using helper functions from the `src.mcts.strategy` submodule.
-   **`config`:** The `config.py` module defines the `MCTSConfig` class, encapsulating all hyperparameters specific to the MCTS algorithm (e.g., simulation count, PUCT constant, temperature, Dirichlet noise parameters). This centralizes MCTS tuning parameters.
-   **`types`:** The `types.py` module defines essential type hints and protocols for the MCTS module. Most importantly, it defines the `ActionPolicyValueEvaluator` protocol, which specifies the `evaluate` and `evaluate_batch` methods that any neural network interface must implement to be usable by the MCTS expansion phase. It also defines `ActionPolicyMapping`.

## Exposed Interfaces

-   **Classes:**
    -   `Node`: Represents a node in the search tree.
    -   `MCTSConfig`: Holds MCTS hyperparameters.
-   **Functions:**
    -   `run_mcts_simulations(root_node: Node, config: MCTSConfig, network_evaluator: ActionPolicyValueEvaluator)`: Orchestrates the MCTS process using batched evaluation.
-   **Protocols/Types:**
    -   `ActionPolicyValueEvaluator`: Defines the interface for the NN evaluator.
    -   `ActionPolicyMapping`: Type alias for the policy dictionary (mapping action index to probability).

## Dependencies

-   **`src.environment`**:
    -   `GameState`: Used within `Node` to represent the state. Methods like `is_over`, `get_outcome`, `valid_actions`, `copy`, `step` are used during the MCTS process (selection, expansion).
-   **`src.mcts.strategy`**:
    -   `selection`, `expansion`, `backpropagation`: The `run_mcts_simulations` function delegates the core algorithm phases to functions within this submodule.
-   **`src.utils.types`**:
    -   `ActionType`, `PolicyValueOutput`: Used in type hints and protocols.
-   **Standard Libraries:** `typing`, `math`, `logging`.

---

**Note:** Please keep this README updated when modifying the `Node` structure, the `MCTSConfig` parameters, the `run_mcts_simulations` logic, or the `ActionPolicyValueEvaluator` interface definition. Accurate documentation is crucial for maintainability.

File: src/mcts/core/search.py
# File: src/mcts/core/search.py
import logging
from typing import TYPE_CHECKING, List, Tuple, Set

# Use relative imports within the mcts package
from ..strategy import selection, expansion, backpropagation
from .node import Node
from .config import MCTSConfig
from .types import ActionPolicyValueEvaluator, ActionPolicyMapping

# Use TYPE_CHECKING to avoid circular import at runtime
if TYPE_CHECKING:
    from ...environment import GameState  # Import GameState for type hint

logger = logging.getLogger(__name__)


def run_mcts_simulations(
    root_node: Node,
    config: MCTSConfig,
    network_evaluator: ActionPolicyValueEvaluator,
) -> int:
    """
    Runs the specified number of MCTS simulations from the root node.
    Evaluates nodes individually. Includes more robust logging.

    Returns:
        The maximum tree depth reached during the simulations.
    """
    if root_node.state.is_over():
        logger.warning("MCTS started on a terminal state. No simulations run.")
        return 0

    max_depth_reached = 0
    sim_success_count = 0
    sim_error_count = 0

    # Initial expansion and noise application if root is not expanded
    if not root_node.is_expanded:
        logger.debug("Root node not expanded, performing initial evaluation...")
        try:
            action_policy, root_value = network_evaluator.evaluate(root_node.state)
            expansion.expand_node_with_policy(root_node, action_policy)
            backpropagation.backpropagate_value(root_node, root_value)  # Initial visit
            logger.debug(f"Initial root expansion complete. Value: {root_value:.3f}")
            selection.add_dirichlet_noise(root_node, config)
        except Exception as e:
            logger.error(f"Initial root expansion failed: {e}", exc_info=True)
            return 0
    elif root_node.visit_count == 0:
        logger.warning(
            "Root node expanded but visit_count is 0. Backpropagating current estimate."
        )
        backpropagation.backpropagate_value(root_node, root_node.value_estimate)

    # --- Main Simulation Loop ---
    logger.debug(f"Starting MCTS loop for {config.num_simulations} simulations...")
    for sim_idx in range(config.num_simulations):
        # Log progress every N simulations to avoid flooding
        # if (sim_idx + 1) % 100 == 0:
        #      logger.debug(f"--- Simulation {sim_idx + 1}/{config.num_simulations} ---")
        logger.debug(
            f"--- Simulation {sim_idx + 1}/{config.num_simulations} ---"
        )  # Log every sim start

        leaf_node = None  # Initialize leaf_node for the finally block
        try:
            # 1. Selection
            logger.debug(f"  Sim {sim_idx+1}: Starting selection...")
            leaf_node, depth = selection.traverse_to_leaf(root_node, config)
            max_depth_reached = max(max_depth_reached, depth)
            logger.debug(
                f"  Sim {sim_idx+1}: Selection finished at depth {depth}. Leaf: {leaf_node}"
            )

            # 2. Expansion & Evaluation (if not terminal)
            value = 0.0
            if leaf_node.state.is_over():
                value = leaf_node.state.get_outcome()
                logger.debug(
                    f"  Sim {sim_idx+1}: Leaf is terminal. Outcome: {value:.3f}"
                )
            else:
                logger.debug(f"  Sim {sim_idx+1}: Evaluating/Expanding leaf...")
                if not leaf_node.is_expanded:
                    action_policy, value = network_evaluator.evaluate(leaf_node.state)
                    expansion.expand_node_with_policy(leaf_node, action_policy)
                    logger.debug(
                        f"  Sim {sim_idx+1}: Expanded & Evaluated leaf. Network Value: {value:.3f}"
                    )
                else:
                    value = leaf_node.value_estimate
                    logger.debug(
                        f"  Sim {sim_idx+1}: Leaf already expanded (depth {depth}). Using current value estimate: {value:.3f}"
                    )

            # 3. Backpropagation
            logger.debug(
                f"  Sim {sim_idx+1}: Starting backpropagation with value {value:.4f}..."
            )
            backpropagation.backpropagate_value(leaf_node, value)
            logger.debug(f"  Sim {sim_idx+1}: Backpropagation complete.")
            sim_success_count += 1

        except Exception as e:
            sim_error_count += 1
            logger.error(
                f"Error during MCTS simulation {sim_idx + 1}: {e}", exc_info=True
            )
            # Log the leaf node state if available when error occurred
            if leaf_node:
                logger.error(f"Error occurred at leaf node: {leaf_node}")
            else:
                logger.error("Error occurred before leaf node was determined.")
            continue  # Try next simulation

        # --- End of Simulation Iteration ---
        # logger.debug(f"--- Finished Simulation {sim_idx + 1} ---") # Optional end log

    # --- Loop Finished ---
    final_log_level = logging.INFO if sim_error_count == 0 else logging.WARNING
    logger.log(
        final_log_level,
        f"MCTS loop finished. Ran {sim_success_count}/{config.num_simulations} sims ({sim_error_count} errors). "
        f"Root visits: {root_node.visit_count}. Max depth: {max_depth_reached}",
    )
    expected_visits = 1 + sim_success_count  # 1 for root + 1 per successful sim
    if root_node.visit_count != expected_visits:
        logger.warning(
            f"Root visit count ({root_node.visit_count}) does not match expected ({expected_visits})!"
        )

    return max_depth_reached


File: src/mcts/core/node.py
# File: src/mcts/core/node.py
from __future__ import annotations  # For type hinting Node within Node
import math
from typing import Dict, Optional, TYPE_CHECKING

if TYPE_CHECKING:
    from src.environment import GameState  # Keep GameState import
    from src.utils.types import ActionType


class Node:
    """Represents a node in the Monte Carlo Search Tree."""

    def __init__(
        self,
        state: "GameState",
        parent: Optional[Node] = None,
        action_taken: Optional["ActionType"] = None,  # Action that *led* to this state
        prior_probability: float = 0.0,  # P(action_taken | parent.state)
    ):
        self.state = state  # The game state this node represents
        self.parent = parent  # Parent node in the tree
        self.action_taken = action_taken  # Action taken from parent to reach this node

        # Children nodes, keyed by the action taken *from this node's state*
        self.children: Dict["ActionType", Node] = {}

        # MCTS statistics
        self.visit_count: int = 0
        self.total_action_value: float = (
            0.0  # Sum of values from simulations passing through here
        )
        self.prior_probability: float = (
            prior_probability  # Prior prob of the action *leading* to this node
        )

    @property
    def is_expanded(self) -> bool:
        """Checks if the node has been expanded (i.e., children generated)."""
        return bool(self.children)

    @property
    def is_leaf(self) -> bool:
        """Checks if the node is a leaf (not expanded)."""
        return not self.is_expanded

    @property
    def value_estimate(self) -> float:
        """
        Calculates the Q-value (average action value) estimate for this node's state.
        This is the average value observed from simulations starting from this state.
        """
        if self.visit_count == 0:
            # Return 0 or perhaps an initial estimate? AlphaZero uses 0.
            return 0.0
        # Average value = Total value accumulated / Number of visits
        return self.total_action_value / self.visit_count

    def __repr__(self) -> str:
        parent_action = self.parent.action_taken if self.parent else "Root"
        return (
            f"Node(StateStep={self.state.current_step}, "
            f"FromAction={self.action_taken}, Visits={self.visit_count}, "
            f"Value={self.value_estimate:.3f}, Prior={self.prior_probability:.4f}, "
            f"Children={len(self.children)})"
        )


File: src/mcts/strategy/policy.py
# File: src/mcts/strategy/policy.py
import numpy as np
import logging
from typing import TYPE_CHECKING, Optional

# Relative imports within MCTS strategy
from ..core.node import Node

# Import ActionPolicyMapping from local types, ActionType from central utils
from ..core.types import ActionPolicyMapping
from ...utils.types import ActionType  # Correct import path for ActionType

logger = logging.getLogger(__name__)


def select_action_based_on_visits(
    root_node: Node, temperature: float
) -> Optional[ActionType]:
    """Selects an action from the root node based on visit counts and temperature."""
    if not root_node.children:
        logger.warning("Cannot select action: Root node has no children.")
        return None

    actions = list(root_node.children.keys())
    visit_counts = np.array(
        [root_node.children[action].visit_count for action in actions],
        dtype=np.float64,
    )

    if len(actions) == 0:
        logger.warning("Cannot select action: No actions available in children.")
        return None

    if temperature == 0.0:
        # Greedy selection
        max_visits = np.max(visit_counts)
        if max_visits == 0:
            logger.warning(
                "No visits recorded for any child node, selecting uniformly."
            )
            selected_action = np.random.choice(actions)
        else:
            best_action_indices = np.where(visit_counts == max_visits)[0]
            chosen_index = np.random.choice(best_action_indices)
            selected_action = actions[chosen_index]
        # logger.info(f"Greedy action selection: {selected_action}")
    else:
        # Temperature-based probabilistic selection (using log-space for stability)
        log_visits = np.log(np.maximum(visit_counts, 1e-9))
        scaled_log_visits = log_visits / temperature
        scaled_log_visits -= np.max(scaled_log_visits)  # Stability trick
        probabilities = np.exp(scaled_log_visits)
        sum_probs = np.sum(probabilities)

        if sum_probs < 1e-9 or not np.isfinite(sum_probs):
            logger.warning(
                f"Could not normalize visit probabilities (sum={sum_probs}). Selecting uniformly."
            )
            probabilities = np.ones(len(actions)) / len(actions)
        else:
            probabilities /= sum_probs  # Normalize
            probabilities /= np.sum(
                probabilities
            )  # Re-normalize for floating point safety

        try:
            selected_action = np.random.choice(actions, p=probabilities)
            # logger.info(f"Sampled action (temp={temperature:.2f}): {selected_action}")
        except ValueError as e:
            logger.error(
                f"Error during np.random.choice: {e}. Probs: {probabilities}, Sum: {np.sum(probabilities)}"
            )
            selected_action = np.random.choice(actions)  # Fallback

    return selected_action


def get_policy_target(root_node: Node, temperature: float = 1.0) -> ActionPolicyMapping:
    """Calculates the policy target distribution based on MCTS visit counts."""
    policy_target: ActionPolicyMapping = {}
    if not root_node.children or root_node.visit_count == 0:
        logger.warning(
            "Cannot compute policy target: Root node has no children or zero visits. Falling back to uniform over valid actions."
        )
        try:
            valid_actions = root_node.state.valid_actions()
            if valid_actions:
                prob = 1.0 / len(valid_actions)
                # Return full policy vector for training if needed (zeroes elsewhere)
                full_target = {
                    a: 0.0 for a in range(root_node.state.env_config.ACTION_DIM)
                }
                for a in valid_actions:
                    full_target[a] = prob
                return full_target
            else:
                return {}
        except Exception as e:
            logger.error(f"Error getting valid actions for fallback policy target: {e}")
            return {}

    child_visits = {
        action: child.visit_count for action, child in root_node.children.items()
    }
    actions = list(child_visits.keys())
    visits = np.array(list(child_visits.values()), dtype=np.float64)

    if not actions:
        logger.warning("Cannot compute policy target: No actions found in children.")
        return {}  # Should be caught above, but safety check

    if temperature == 0.0:  # Deterministic target for temp=0
        max_visits = np.max(visits)
        if max_visits == 0:
            prob = 1.0 / len(actions)
            best_actions = actions
        else:
            best_actions = [actions[i] for i, v in enumerate(visits) if v == max_visits]
            prob = 1.0 / len(best_actions)
        policy_target = {
            a: prob if a in best_actions else 0.0
            for a in range(root_node.state.env_config.ACTION_DIM)
        }

    else:  # Proportional target
        visit_probs = visits ** (1.0 / temperature)
        sum_probs = np.sum(visit_probs)

        if sum_probs < 1e-9 or not np.isfinite(sum_probs):
            logger.warning(
                f"Sum of visit probabilities is near zero ({sum_probs}). Using uniform."
            )
            prob = 1.0 / len(actions)
            raw_policy = {action: prob for action in actions}
        else:
            probabilities = visit_probs / sum_probs
            probabilities /= np.sum(probabilities)  # Re-normalize
            raw_policy = {action: probabilities[i] for i, action in enumerate(actions)}

        # Create full policy vector
        policy_target = {a: 0.0 for a in range(root_node.state.env_config.ACTION_DIM)}
        policy_target.update(raw_policy)

    return policy_target


File: src/mcts/strategy/__init__.py


File: src/mcts/strategy/README.md
# File: src/mcts/strategy/README.md
# MCTS Strategy Submodule (`src.mcts.strategy`)

## Purpose and Architecture

This submodule implements the specific algorithms and heuristics used within the different phases of the Monte Carlo Tree Search, as orchestrated by `src.mcts.core.search.run_mcts_simulations`.

-   **`selection`:** Contains the logic for traversing the tree from the root to a leaf node.
    -   `calculate_puct_score`: Implements the PUCT (Polynomial Upper Confidence Trees) formula, balancing exploitation (node value) and exploration (prior probability and visit counts).
    -   `add_dirichlet_noise`: Adds noise to the root node's prior probabilities to encourage exploration early in the search, as done in AlphaZero.
    -   `select_child_node`: Chooses the child with the highest PUCT score.
    -   `traverse_to_leaf`: Repeatedly applies `select_child_node` to navigate down the tree.
-   **`expansion`:** Handles the expansion of a selected leaf node.
    -   `expand_node_with_policy`: Takes a node and a *pre-computed* policy dictionary (obtained from batched network evaluation) and creates child `Node` objects for all valid actions, initializing them with the corresponding prior probabilities.
-   **`backpropagation`:** Implements the update step after a simulation.
    -   `backpropagate_value`: Traverses from the expanded leaf node back up to the root, incrementing the `visit_count` and adding the simulation's resulting `value` to the `total_action_value` of each node along the path.
-   **`policy`:** Provides functions related to action selection and policy target generation after MCTS has run.
    -   `select_action_based_on_visits`: Selects the final action to be played in the game based on the visit counts of the root's children, using a temperature parameter to control exploration vs. exploitation.
    -   `get_policy_target`: Generates the policy target vector (a probability distribution over actions) based on the visit counts, which is used as a training target for the neural network's policy head.

## Exposed Interfaces

-   **Selection:**
    -   `traverse_to_leaf(root_node: Node, config: MCTSConfig) -> Node`
    -   `add_dirichlet_noise(node: Node, config: MCTSConfig)`
    -   `select_child_node(node: Node, config: MCTSConfig) -> Node` (Primarily internal use)
    -   `calculate_puct_score(...) -> float` (Primarily internal use)
-   **Expansion:**
    -   `expand_node_with_policy(node: Node, action_policy: ActionPolicyMapping)`
-   **Backpropagation:**
    -   `backpropagate_value(leaf_node: Node, value: float)`
-   **Policy:**
    -   `select_action_based_on_visits(root_node: Node, temperature: float) -> Optional[ActionType]`
    -   `get_policy_target(root_node: Node, temperature: float = 1.0) -> ActionPolicyMapping`

## Dependencies

-   **`src.mcts.core`**:
    -   `Node`: The primary data structure operated upon.
    -   `MCTSConfig`: Provides hyperparameters (PUCT coeff, noise params, etc.).
    -   `ActionPolicyMapping`: Used in `expansion` and `policy`.
-   **`src.environment`**:
    -   `GameState`: Accessed via `Node.state` for methods like `is_over`, `get_outcome`, `valid_actions`, `step`.
-   **`src.utils.types`**:
    -   `ActionType`: Used for representing actions.
-   **`numpy`**:
    -   Used for Dirichlet noise and potentially in policy/selection calculations.
-   **Standard Libraries:** `typing`, `math`, `logging`, `numpy`.

---

**Note:** Please keep this README updated when modifying the algorithms within selection, expansion, backpropagation, or policy generation, or changing how they interact with the `Node` structure or `MCTSConfig`. Accurate documentation is crucial for maintainability.

File: src/mcts/strategy/expansion.py
# File: src/mcts/strategy/expansion.py
import logging
from typing import TYPE_CHECKING, List

# Use relative imports within mcts package
from ..core.node import Node  # Import Node relatively
from ..core.types import (
    ActionPolicyMapping,
    ActionPolicyValueEvaluator,
)  # Import evaluator
from ...utils.types import ActionType  # Core ActionType

logger = logging.getLogger(__name__)


def expand_node_with_policy(node: Node, action_policy: ActionPolicyMapping):
    """
    Expands a node by creating children for valid actions using the
    pre-computed action policy priors from the network.
    Assumes the node is not terminal and not already expanded.
    """
    if node.is_expanded:
        # This might happen if max_depth is reached, log as debug instead of warning
        logger.debug(f"Attempted to expand an already expanded node: {node}")
        return
    if node.state.is_over():
        logger.warning(f"Attempted to expand a terminal node: {node}")
        return

    valid_actions: List[ActionType] = node.state.valid_actions()

    if not valid_actions:
        logger.warning(
            f"Expanding node at step {node.state.current_step} with no valid actions but not terminal?"
        )
        node.state.game_over = True
        return

    # Create child nodes for valid actions using the provided policy
    children_created = 0
    for action in valid_actions:
        prior = action_policy.get(action, 0.0)
        if prior < 0.0:
            logger.warning(
                f"Received negative prior ({prior}) for action {action}. Clamping to 0."
            )
            prior = 0.0
        elif prior == 0.0:
            logger.debug(f"Valid action {action} received prior=0 from network.")

        # Create child node - state is not strictly needed until selection/expansion of the child
        # Let's generate state here for simplicity.
        next_state_copy = node.state.copy()
        try:
            _, _, _ = next_state_copy.step(action)
        except Exception as e:
            logger.error(
                f"Error stepping state for child node expansion (action {action}): {e}",
                exc_info=True,
            )
            continue  # Skip this child

        child = Node(
            state=next_state_copy,
            parent=node,
            action_taken=action,
            prior_probability=prior,
        )
        node.children[action] = child
        children_created += 1

    logger.debug(f"Expanded node {node} with {children_created} children.")


# --- New function for simplified search ---
def expand_leaf_node(node: Node, network_evaluator: ActionPolicyValueEvaluator):
    """
    Evaluates a leaf node using the network and expands it.
    (Used by the simplified, non-batched search loop).
    """
    if node.is_expanded:
        logger.debug(f"Node already expanded: {node}")
        return
    if node.state.is_over():
        logger.warning(f"Attempted to expand a terminal node: {node}")
        return

    try:
        action_policy, value = network_evaluator.evaluate(node.state)
        expand_node_with_policy(node, action_policy)
        logger.debug(f"Expanded leaf node. Network Value: {value:.3f}")
        # Note: The value is returned by the search loop and backpropagated there.
    except Exception as e:
        logger.error(
            f"Network evaluation failed during leaf expansion: {e}", exc_info=True
        )
        # How to handle? Node remains unexpanded. Search might select it again.
        # Backpropagation in the search loop will use value=0.0 in case of error.


File: src/mcts/strategy/selection.py
# File: src/mcts/strategy/selection.py
import math
import numpy as np
import logging
from typing import TYPE_CHECKING, Tuple, Optional

# Use relative imports within mcts package
from ..core.node import Node
from ..core.config import MCTSConfig

logger = logging.getLogger(__name__)


def calculate_puct_score(
    child_node: Node,
    parent_visit_count: int,
    config: MCTSConfig,
    log_details: bool = False,
) -> Tuple[float, float, float]:  # Return components for logging
    """Calculates the PUCT score and its components for a child node."""
    q_value = child_node.value_estimate
    prior = child_node.prior_probability
    visits = child_node.visit_count

    if parent_visit_count == 0:
        exploration_term = config.puct_coefficient * prior
    else:
        exploration_term = (
            config.puct_coefficient
            * prior
            * (math.sqrt(parent_visit_count) / (1 + visits))
        )

    score = q_value + exploration_term

    # Use logger.debug for detailed logs
    if log_details:
        logger.debug(
            f"    Action {child_node.action_taken}: Q={q_value:.3f}, P={prior:.4f}, N={visits}, ParentN={parent_visit_count} -> ExpTerm={exploration_term:.4f} -> PUCT={score:.4f}"
        )

    return score, q_value, exploration_term


def add_dirichlet_noise(node: Node, config: MCTSConfig):
    """Adds Dirichlet noise to the prior probabilities of the children of this node."""
    if (
        config.dirichlet_alpha <= 0.0
        or config.dirichlet_epsilon <= 0.0
        or not node.children
        or len(node.children) <= 1
    ):
        return

    actions = list(node.children.keys())
    noise = np.random.dirichlet([config.dirichlet_alpha] * len(actions))
    eps = config.dirichlet_epsilon

    for i, action in enumerate(actions):
        child = node.children[action]
        child.prior_probability = (1 - eps) * child.prior_probability + eps * noise[i]

    logger.debug(
        f"Added Dirichlet noise (alpha={config.dirichlet_alpha}, eps={eps}) to node priors."
    )


def select_child_node(node: Node, config: MCTSConfig) -> Node:
    """Selects the child node with the highest PUCT score. Assumes noise already added if root."""
    if not node.children:
        raise ValueError("Cannot select child from a node with no children.")

    best_score = -float("inf")
    best_child: Optional[Node] = None

    is_root = node.parent is None
    log_limit = 5
    logged_count = 0
    # Log details only when selecting from the root node
    if is_root:
        logger.debug(
            f"Selecting child for Node (Step {node.state.current_step}, Visits {node.visit_count}):"
        )

    children_items = list(node.children.items())

    for action, child in children_items:
        log_this_child = is_root and logged_count < log_limit
        score, q, exp_term = calculate_puct_score(
            child, node.visit_count, config, log_details=log_this_child
        )
        if log_this_child:
            logged_count += 1

        if score > best_score:
            best_score = score
            best_child = child

    if best_child is None:
        logger.error(
            f"Could not select best child for node step {node.state.current_step}. Defaulting to random."
        )
        return np.random.choice(list(node.children.values()))

    if is_root:
        # Log the Q-value of the selected child
        logger.debug(
            f"Selected Child: Action {best_child.action_taken}, Score {best_score:.4f}, Q-value {best_child.value_estimate:.3f}"
        )

    return best_child


def traverse_to_leaf(root_node: Node, config: MCTSConfig) -> Tuple[Node, int]:
    """
    Traverses the tree from root to a leaf node using PUCT selection.
    Returns the leaf node and the depth reached.
    """
    current_node = root_node
    depth = 0
    # logger.debug(f"--- Start Traverse (Root Visits: {root_node.visit_count}) ---")
    while current_node.is_expanded:
        if current_node.state.is_over():
            # logger.debug(f"  Traverse hit terminal node at depth {depth}. Node: {current_node}")
            break
        if config.max_search_depth and depth >= config.max_search_depth:
            logger.debug(f"  Traverse hit max depth {config.max_search_depth}.")
            break

        selected_child = select_child_node(current_node, config)
        # logger.debug(f"  Depth {depth}: Selected Action {selected_child.action_taken} -> Node Visits: {selected_child.visit_count}, Q: {selected_child.value_estimate:.3f}")
        current_node = selected_child
        depth += 1

    # logger.debug(f"--- End Traverse: Reached Leaf Depth {depth}. Node: {current_node} ---")
    return current_node, depth


File: src/mcts/strategy/backpropagation.py
# File: src/mcts/strategy/backpropagation.py
import logging
from typing import TYPE_CHECKING

# Use relative imports within mcts package
if TYPE_CHECKING:
    from ..core.node import Node

logger = logging.getLogger(__name__)


def backpropagate_value(leaf_node: "Node", value: float) -> None:
    """
    Propagates the simulation value back up the tree from the leaf node.
    """
    current_node: "Node" | None = leaf_node
    path_str = []  # For debugging path
    depth = 0

    while current_node is not None:
        q_before = current_node.value_estimate
        current_node.visit_count += 1
        current_node.total_action_value += value
        q_after = current_node.value_estimate
        path_str.append(
            f"N(a={current_node.action_taken},v={current_node.visit_count},q={q_after:.3f})"
        )

        # Log value changes, especially near the root
        if depth <= 2:  # Log details for nodes close to the leaf
            logger.debug(
                f"  Backprop Depth {depth}: Node(Act={current_node.action_taken}, V={current_node.visit_count}), Value={value:.3f}, Q_before={q_before:.3f}, Q_after={q_after:.3f}"
            )

        current_node = current_node.parent
        depth += 1

    # logger.debug(f"Backpropagated value {value:.3f} up path: {' <- '.join(reversed(path_str))}")


