File: requirements.txt
pygame>=2.1.0
numpy>=1.20.0
torch>=1.10.0
tensorboard
cloudpickle
torchvision
matplotlib

File: .resumed.txt


File: main_pygame.py
# File: main_pygame.py
import sys
import pygame
import numpy as np
import os
import time
import traceback
import torch
from typing import List, Tuple, Optional, Dict, Any, Deque

# --- Project Imports ---
# Setup & Logging
from logger import TeeLogger
from app_setup import (
    initialize_pygame,
    initialize_directories,
    load_and_validate_configs,
)

# Configs (Import necessary instances/constants)
from config import (
    VisConfig,
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    TensorBoardConfig,
    DemoConfig,
    DEVICE,
    RANDOM_SEED,
    BUFFER_SAVE_PATH,
    MODEL_SAVE_PATH,
    BASE_CHECKPOINT_DIR,
    BASE_LOG_DIR,
    RUN_LOG_DIR,
)

# Core Components & Helpers
from environment.game_state import GameState, StateType
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from training.trainer import Trainer
from stats.stats_recorder import StatsRecorderBase
from ui.renderer import UIRenderer
from ui.input_handler import InputHandler
from utils.helpers import set_random_seeds
from utils.init_checks import run_pre_checks
from init.rl_components import (
    initialize_envs,
    initialize_agent_buffer,
    initialize_stats_recorder,
    initialize_trainer,
)


class MainApp:
    """Main application class orchestrating the Pygame UI and RL training."""

    def __init__(self):
        print("Initializing Application...")
        set_random_seeds(RANDOM_SEED)

        # --- Configuration Loading ---
        # Instantiate config classes needed by the app directly
        self.vis_config = VisConfig()
        self.env_config = EnvConfig()
        self.dqn_config = DQNConfig()
        self.train_config = TrainConfig()
        self.buffer_config = BufferConfig()
        self.model_config = ModelConfig()
        self.stats_config = StatsConfig()
        self.tensorboard_config = TensorBoardConfig()
        self.demo_config = DemoConfig()
        self.reward_config = RewardConfig()  # Although not directly used, good practice

        # Load combined config dict and validate
        self.config_dict = load_and_validate_configs()
        self.num_envs = self.env_config.NUM_ENVS

        # --- Setup Pygame and Directories ---
        initialize_directories()
        self.screen, self.clock = initialize_pygame(self.vis_config)

        # --- App State ---
        self.app_state = "Initializing"
        self.is_training = False
        self.cleanup_confirmation_active = False
        self.last_cleanup_message_time = 0.0
        self.cleanup_message = ""
        self.status = "Initializing Components"

        # --- Initialize Components ---
        self.renderer: Optional[UIRenderer] = None
        self.input_handler: Optional[InputHandler] = None
        self.envs: List[GameState] = []
        self.agent: Optional[DQNAgent] = None
        self.buffer: Optional[ReplayBufferBase] = None
        self.stats_recorder: Optional[StatsRecorderBase] = None
        self.trainer: Optional[Trainer] = None
        self.demo_env: Optional[GameState] = None

        self._initialize_core_components()

        # Transition to Main Menu after successful initialization
        self.app_state = "MainMenu"
        self.status = "Paused"
        print("Initialization Complete. Ready.")
        print(f"--- tensorboard --logdir {os.path.abspath(BASE_LOG_DIR)} ---")

    def _initialize_core_components(self):
        """Initializes Renderer, RL components, Demo Env, and Input Handler."""
        try:
            # Init Renderer FIRST for immediate feedback
            self.renderer = UIRenderer(self.screen, self.vis_config)
            self.renderer.render_all(  # Render initializing screen
                app_state=self.app_state,
                is_training=False,
                status=self.status,
                stats_summary={},
                buffer_capacity=0,
                envs=[],
                num_envs=0,
                env_config=self.env_config,
                cleanup_confirmation_active=False,
                cleanup_message="",
                last_cleanup_message_time=0,
                tensorboard_log_dir=None,
                plot_data={},
                demo_env=None,
            )
            pygame.time.delay(100)  # Brief pause to show initializing screen

            # Init RL components
            self._initialize_rl_components()

            # Init Demo Env
            self._initialize_demo_env()

            # Init Input Handler (pass self methods as callbacks)
            self.input_handler = InputHandler(
                self.screen,
                self.renderer,
                self._toggle_training,
                self._request_cleanup,
                self._cancel_cleanup,
                self._confirm_cleanup,
                self._exit_app,
                self._start_demo_mode,
                self._exit_demo_mode,
                self._handle_demo_input,
            )
        except Exception as init_err:
            print(f"FATAL ERROR during component initialization: {init_err}")
            traceback.print_exc()
            # Attempt to show error on screen if renderer exists
            if self.renderer:
                try:
                    self.app_state = "Error"
                    self.status = "Initialization Failed"
                    self.renderer._render_error_screen(self.status)
                    pygame.display.flip()
                    time.sleep(5)  # Show error for a bit
                except Exception:
                    pass  # Ignore errors during error rendering
            pygame.quit()
            sys.exit(1)

    def _initialize_rl_components(self):
        """Initializes RL components using helper functions."""
        print("Initializing RL components...")
        start_time = time.time()
        try:
            self.envs = initialize_envs(self.num_envs, self.env_config)
            self.agent, self.buffer = initialize_agent_buffer(
                self.model_config, self.dqn_config, self.env_config, self.buffer_config
            )
            if self.buffer is None:
                raise RuntimeError("Buffer initialization failed unexpectedly.")

            self.stats_recorder = initialize_stats_recorder(
                self.stats_config,
                self.tensorboard_config,
                self.config_dict,
                self.agent,
                self.env_config,
            )
            if self.stats_recorder is None:
                raise RuntimeError("Stats Recorder initialization failed unexpectedly.")

            self.trainer = initialize_trainer(
                self.envs,
                self.agent,
                self.buffer,
                self.stats_recorder,
                self.env_config,
                self.dqn_config,
                self.train_config,
                self.buffer_config,
                self.model_config,
            )
            print(f"RL components initialized in {time.time() - start_time:.2f}s")
        except Exception as e:
            # Let the caller handle the fatal error exit
            print(f"Error during RL component initialization: {e}")
            raise e  # Re-raise to be caught by _initialize_core_components

    def _initialize_demo_env(self):
        """Initializes the separate environment for demo mode."""
        print("Initializing Demo Environment...")
        try:
            self.demo_env = GameState()
            self.demo_env.reset()
            print("Demo environment initialized.")
        except Exception as e:
            print(f"ERROR initializing demo environment: {e}")
            traceback.print_exc()
            self.demo_env = None  # Continue without demo mode
            print("Warning: Demo mode may be unavailable.")

    # --- Input Handler Callbacks (Remain in MainApp) ---
    def _toggle_training(self):
        if self.app_state != "MainMenu":
            return
        self.is_training = not self.is_training
        print(f"Training {'STARTED' if self.is_training else 'PAUSED'}")
        if not self.is_training:
            self._try_save_checkpoint()

    def _request_cleanup(self):
        if self.app_state != "MainMenu":
            return
        was_training = self.is_training
        self.is_training = False
        if was_training:
            self._try_save_checkpoint()
        self.cleanup_confirmation_active = True
        print("Cleanup requested. Confirm action.")

    def _cancel_cleanup(self):
        self.cleanup_confirmation_active = False
        self.cleanup_message = "Cleanup cancelled."
        self.last_cleanup_message_time = time.time()
        print("Cleanup cancelled by user.")

    def _confirm_cleanup(self):
        print("Cleanup confirmed by user. Starting process...")
        try:
            self._cleanup_data()
        except Exception as e:
            print(f"FATAL ERROR during cleanup: {e}")
            traceback.print_exc()
            self.status = "Error: Cleanup Failed Critically"
            self.app_state = "Error"
        finally:
            self.cleanup_confirmation_active = False  # Ensure flag is reset
            print(
                f"Cleanup process finished. State: {self.app_state}, Status: {self.status}"
            )

    def _exit_app(self) -> bool:
        print("Exit requested.")
        return False  # Signal exit to main loop

    def _start_demo_mode(self):
        if self.demo_env is None:
            print("Cannot start demo mode: Demo environment failed to initialize.")
            return
        if self.app_state == "MainMenu":
            print("Entering Demo Mode...")
            self.is_training = False
            self._try_save_checkpoint()
            self.app_state = "Playing"
            self.status = "Playing Demo"
            self.demo_env.reset()

    def _exit_demo_mode(self):
        if self.app_state == "Playing":
            print("Exiting Demo Mode...")
            self.app_state = "MainMenu"
            self.status = "Paused"

    def _handle_demo_input(self, event: pygame.event.Event):
        """Handles keyboard input during demo mode."""
        if self.app_state != "Playing" or self.demo_env is None:
            return
        if self.demo_env.is_frozen() or self.demo_env.is_over():
            return

        if event.type == pygame.KEYDOWN:
            action_taken = False
            if event.key == pygame.K_LEFT:
                self.demo_env.move_target(0, -1)
                action_taken = True
            elif event.key == pygame.K_RIGHT:
                self.demo_env.move_target(0, 1)
                action_taken = True
            elif event.key == pygame.K_UP:
                self.demo_env.move_target(-1, 0)
                action_taken = True
            elif event.key == pygame.K_DOWN:
                self.demo_env.move_target(1, 0)
                action_taken = True
            elif event.key == pygame.K_q:
                self.demo_env.cycle_shape(-1)
                action_taken = True
            elif event.key == pygame.K_e:
                self.demo_env.cycle_shape(1)
                action_taken = True
            elif event.key == pygame.K_SPACE:
                action_index = self.demo_env.get_action_for_current_selection()
                if action_index is not None:
                    state_before = self.demo_env.get_state()
                    reward, done = self.demo_env.step(action_index)
                    next_state_after = self.demo_env.get_state()
                    if self.buffer:
                        try:
                            self.buffer.push(
                                state_before,
                                action_index,
                                reward,
                                next_state_after,
                                done,
                            )
                            if (
                                len(self.buffer) % 50 == 0
                                and len(self.buffer)
                                <= self.train_config.LEARN_START_STEP
                            ):
                                print(
                                    f"[Demo] Added experience. Buffer: {len(self.buffer)}/{self.buffer.capacity}"
                                )
                        except Exception as buf_e:
                            print(f"Error pushing demo experience to buffer: {buf_e}")
                    action_taken = True
                else:
                    action_taken = (
                        True  # Still counts as an action (attempted placement)
                    )

            if self.demo_env.is_over():
                print("[Demo] Game Over! Press ESC to exit.")

    # --- Core Logic Methods (Remain in MainApp) ---
    def _cleanup_data(self):
        """Deletes current run's checkpoint/buffer and re-initializes."""
        print("\n--- CLEANUP DATA INITIATED (Current Run Only) ---")
        self.app_state = "Initializing"
        self.is_training = False
        self.status = "Cleaning"
        messages = []

        # Render initializing screen during cleanup
        if self.renderer:
            try:
                self.renderer.render_all(
                    app_state=self.app_state,
                    is_training=False,
                    status=self.status,
                    stats_summary={},
                    buffer_capacity=0,
                    envs=[],
                    num_envs=0,
                    env_config=self.env_config,
                    cleanup_confirmation_active=False,
                    cleanup_message="",
                    last_cleanup_message_time=0,
                    tensorboard_log_dir=None,
                    plot_data={},
                    demo_env=self.demo_env,
                )
                pygame.display.flip()
                pygame.time.delay(100)
            except Exception as render_err:
                print(f"Warning: Error rendering during cleanup start: {render_err}")

        # Close trainer/stats FIRST
        if self.trainer:
            print("[Cleanup] Running trainer cleanup...")
            try:
                self.trainer.cleanup(save_final=False)
            except Exception as e:
                print(f"Error during trainer cleanup: {e}")
        if self.stats_recorder:
            print("[Cleanup] Closing stats recorder...")
            try:
                self.stats_recorder.close()
            except Exception as e:
                print(f"Error closing stats recorder: {e}")

        # Delete files
        print("[Cleanup] Deleting files...")
        for path, desc in [
            (MODEL_SAVE_PATH, "Agent ckpt"),
            (BUFFER_SAVE_PATH, "Buffer state"),
        ]:
            try:
                if os.path.isfile(path):
                    os.remove(path)
                    msg = f"{desc} deleted: {os.path.basename(path)}"
                else:
                    msg = f"{desc} not found (current run)."
                print(f"  - {msg}")
                messages.append(msg)
            except OSError as e:
                msg = f"Error deleting {desc}: {e}"
                print(f"  - {msg}")
                messages.append(msg)

        time.sleep(0.1)

        # Re-initialize RL components
        print("[Cleanup] Re-initializing RL components...")
        try:
            self._initialize_rl_components()
            if self.demo_env:
                self.demo_env.reset()
            print("[Cleanup] RL components re-initialized successfully.")
            messages.append("RL components re-initialized.")
            self.status = "Paused"
            self.app_state = "MainMenu"
        except Exception as e:
            print(f"FATAL ERROR during RL re-initialization after cleanup: {e}")
            traceback.print_exc()
            self.status = "Error: Re-init Failed"
            self.app_state = "Error"
            messages.append("ERROR RE-INITIALIZING RL COMPONENTS!")
            if self.renderer:
                try:
                    self.renderer._render_error_screen(self.status)
                except Exception as render_err_final:
                    print(f"Warning: Failed to render error screen: {render_err_final}")

        self.cleanup_message = "\n".join(messages)
        self.last_cleanup_message_time = time.time()
        print(
            f"--- CLEANUP DATA COMPLETE (Final State: {self.app_state}, Status: {self.status}) ---"
        )

    def _try_save_checkpoint(self):
        """Saves checkpoint if not training and trainer exists."""
        if self.app_state == "MainMenu" and not self.is_training and self.trainer:
            print("Saving checkpoint on pause...")
            try:
                self.trainer.maybe_save_checkpoint(force_save=True)
            except Exception as e:
                print(f"Error saving checkpoint on pause: {e}")

    def _update(self):
        """Updates the application state and performs training steps."""
        should_step_trainer = False

        if self.app_state == "MainMenu":
            if self.cleanup_confirmation_active:
                self.status = "Confirm Cleanup"
            elif not self.is_training and self.status != "Error":
                self.status = "Paused"
            elif not self.trainer:
                if self.status != "Error":
                    self.status = "Error: Trainer Missing"
            elif self.is_training:
                current_fill = len(self.buffer) if self.buffer else 0
                current_step = self.trainer.global_step if self.trainer else 0
                needed = self.train_config.LEARN_START_STEP
                is_buffer_ready = current_fill >= needed and current_step >= needed

                self.status = (
                    f"Buffering ({current_fill / max(1, needed) * 100:.0f}%)"
                    if not is_buffer_ready
                    else "Training"
                )
                should_step_trainer = True
            else:  # Not training
                if not self.status.startswith("Buffering") and self.status != "Error":
                    self.status = "Paused"

            if should_step_trainer:
                if not self.trainer:
                    print("Error: Trainer became unavailable during _update.")
                    self.status = "Error: Trainer Lost"
                    self.is_training = False
                else:
                    try:
                        step_start_time = time.time()
                        self.trainer.step()
                        step_duration = time.time() - step_start_time
                        if self.vis_config.VISUAL_STEP_DELAY > 0:
                            time.sleep(
                                max(
                                    0, self.vis_config.VISUAL_STEP_DELAY - step_duration
                                )
                            )
                    except Exception as e:
                        print(
                            f"\n--- ERROR DURING TRAINING UPDATE (Step: {getattr(self.trainer, 'global_step', 'N/A')}) ---"
                        )
                        traceback.print_exc()
                        print("--- Pausing training due to error. ---")
                        self.is_training = False
                        self.status = "Error: Training Step Failed"
                        self.app_state = "Error"

        elif self.app_state == "Playing":
            if self.demo_env and hasattr(self.demo_env, "_update_timers"):
                self.demo_env._update_timers()
            self.status = "Playing Demo"
        # Other states (Initializing, Error) have status set elsewhere

    def _render(self):
        """Renders the UI based on the current application state."""
        stats_summary = {}
        plot_data: Dict[str, Deque] = {}
        buffer_capacity = 0

        if self.stats_recorder:
            current_step = getattr(self.trainer, "global_step", 0)
            try:
                stats_summary = self.stats_recorder.get_summary(current_step)
            except Exception as e:
                print(f"Error getting stats summary: {e}")
                stats_summary = {"global_step": current_step}
            try:
                plot_data = self.stats_recorder.get_plot_data()
            except Exception as e:
                print(f"Error getting plot data: {e}")
                plot_data = {}
        elif self.app_state == "Error":
            stats_summary = {"global_step": getattr(self.trainer, "global_step", 0)}

        if self.buffer:
            try:
                buffer_capacity = getattr(self.buffer, "capacity", 0)
            except Exception:
                buffer_capacity = 0

        if not self.renderer:
            print("Error: Renderer not initialized in _render.")
            try:  # Basic error render
                self.screen.fill((0, 0, 0))
                font = pygame.font.SysFont(None, 50)
                surf = font.render("Renderer Error!", True, (255, 0, 0))
                self.screen.blit(
                    surf, surf.get_rect(center=self.screen.get_rect().center)
                )
                pygame.display.flip()
            except Exception:
                pass
            return

        try:
            self.renderer.render_all(
                app_state=self.app_state,
                is_training=self.is_training,
                status=self.status,
                stats_summary=stats_summary,
                buffer_capacity=buffer_capacity,
                envs=(self.envs if hasattr(self, "envs") else []),
                num_envs=self.num_envs,
                env_config=self.env_config,
                cleanup_confirmation_active=self.cleanup_confirmation_active,
                cleanup_message=self.cleanup_message,
                last_cleanup_message_time=self.last_cleanup_message_time,
                tensorboard_log_dir=(
                    self.tensorboard_config.LOG_DIR
                    if self.tensorboard_config.LOG_DIR
                    else None
                ),
                plot_data=plot_data,
                demo_env=self.demo_env,
            )
        except Exception as render_all_err:
            print(f"CRITICAL ERROR in renderer.render_all: {render_all_err}")
            traceback.print_exc()
            try:
                self.app_state = "Error"
                self.status = "Render Error"
                self.renderer._render_error_screen(self.status)
            except Exception as e:
                print(f"Error rendering error screen: {e}")

        # Clear transient message
        if time.time() - self.last_cleanup_message_time >= 5.0:
            self.cleanup_message = ""

    def _perform_cleanup(self):
        """Handles final cleanup of resources."""
        print("Exiting application...")
        if self.trainer:
            print("Performing final trainer cleanup...")
            try:
                save_on_exit = self.status != "Cleaning" and self.app_state != "Error"
                self.trainer.cleanup(save_final=save_on_exit)
            except Exception as final_cleanup_err:
                print(f"Error during final trainer cleanup: {final_cleanup_err}")
        elif self.stats_recorder:  # Close stats recorder even if trainer failed/missing
            print("Closing stats recorder...")
            try:
                self.stats_recorder.close()
            except Exception as log_e:
                print(f"Error closing stats recorder on exit: {log_e}")

        pygame.quit()
        print("Application exited.")

    def run(self):
        """Main application loop."""
        print("Starting main application loop...")
        running = True
        try:
            while running:
                start_frame_time = time.perf_counter()

                # Handle Input
                if self.input_handler:
                    try:
                        running = self.input_handler.handle_input(
                            self.app_state, self.cleanup_confirmation_active
                        )
                    except Exception as input_err:
                        print(
                            f"\n--- UNHANDLED ERROR IN INPUT LOOP ({self.app_state}) ---"
                        )
                        traceback.print_exc()
                        running = False  # Exit on unhandled input error
                else:  # Basic exit if handler failed
                    for event in pygame.event.get():
                        if event.type == pygame.QUIT:
                            running = False
                        if (
                            event.type == pygame.KEYDOWN
                            and event.key == pygame.K_ESCAPE
                        ):
                            if self.app_state == "Playing":
                                self._exit_demo_mode()
                            elif not self.cleanup_confirmation_active:
                                running = False
                    if not running:
                        break

                if not running:
                    break

                # Update State
                try:
                    self._update()
                except Exception as update_err:
                    print(
                        f"\n--- UNHANDLED ERROR IN UPDATE LOOP ({self.app_state}) ---"
                    )
                    traceback.print_exc()
                    self.status = "Error: Update Loop Failed"
                    self.app_state = "Error"
                    self.is_training = False

                # Render Frame
                try:
                    self._render()
                except Exception as render_err:
                    print(
                        f"\n--- UNHANDLED ERROR IN RENDER LOOP ({self.app_state}) ---"
                    )
                    traceback.print_exc()
                    self.status = "Error: Render Loop Failed"
                    self.app_state = "Error"

                # Frame Rate Limiting
                frame_time = time.perf_counter() - start_frame_time
                target_frame_time = (
                    1.0 / self.vis_config.FPS if self.vis_config.FPS > 0 else 0
                )
                sleep_time = max(0, target_frame_time - frame_time)
                if sleep_time > 0.001:
                    time.sleep(sleep_time)

        except KeyboardInterrupt:
            print("\nCtrl+C detected. Exiting gracefully...")
        except Exception as e:
            print(f"\n--- UNHANDLED EXCEPTION IN MAIN LOOP ({self.app_state}) ---")
            traceback.print_exc()
            print("--- EXITING ---")
        finally:
            self._perform_cleanup()


# --- Main Execution Block ---
if __name__ == "__main__":
    # Ensure base directories exist before logger setup
    os.makedirs(BASE_CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(BASE_LOG_DIR, exist_ok=True)
    os.makedirs(RUN_LOG_DIR, exist_ok=True)  # Ensure run-specific log dir exists

    log_filepath = os.path.join(RUN_LOG_DIR, "console_output.log")

    # Setup logging
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    logger = TeeLogger(log_filepath, original_stdout)
    sys.stdout = logger
    sys.stderr = logger

    app_instance = None
    exit_code = 0

    try:
        if run_pre_checks():
            app_instance = MainApp()
            app_instance.run()  # run() now handles cleanup internally
    except SystemExit as exit_err:
        print(f"Exiting due to SystemExit (Code: {getattr(exit_err, 'code', 'N/A')}).")
        exit_code = (
            getattr(exit_err, "code", 1)
            if isinstance(getattr(exit_err, "code", 1), int)
            else 1
        )
    except Exception as main_err:
        print("\n--- UNHANDLED EXCEPTION DURING APP INITIALIZATION OR RUN ---")
        traceback.print_exc()
        print("--- EXITING DUE TO ERROR ---")
        exit_code = 1
        # Ensure cleanup is attempted even if run() didn't finish
        if app_instance and hasattr(app_instance, "_perform_cleanup"):
            print("Attempting cleanup after main exception...")
            try:
                app_instance._perform_cleanup()
            except Exception as cleanup_err:
                print(f"Error during cleanup after main exception: {cleanup_err}")
    finally:
        # Restore logging
        if logger:
            final_app_state = getattr(app_instance, "app_state", "UNKNOWN")
            print(
                f"Restoring console output (Final App State: {final_app_state}). Full log saved to: {log_filepath}"
            )
            logger.close()
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        print(f"Console logging restored. Full log should be in: {log_filepath}")
        sys.exit(exit_code)  # Exit with appropriate code


File: app_setup.py
# File: app_setup.py
import os
import pygame
from typing import Tuple, Dict, Any

from config import (
    VisConfig,
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    TensorBoardConfig,
    DemoConfig,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,
    get_config_dict,
    print_config_info_and_validate,
)


def initialize_pygame(
    vis_config: VisConfig,
) -> Tuple[pygame.Surface, pygame.time.Clock]:
    """Initializes Pygame, sets up the screen and clock."""
    print("Initializing Pygame...")
    pygame.init()
    pygame.font.init()
    screen = pygame.display.set_mode(
        (vis_config.SCREEN_WIDTH, vis_config.SCREEN_HEIGHT), pygame.RESIZABLE
    )
    pygame.display.set_caption("TriCrack DQN")
    clock = pygame.time.Clock()
    print("Pygame initialized.")
    return screen, clock


def initialize_directories():
    """Creates necessary runtime directories."""
    os.makedirs(RUN_CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(RUN_LOG_DIR, exist_ok=True)
    print(f"Ensured directories exist: {RUN_CHECKPOINT_DIR}, {RUN_LOG_DIR}")


def load_and_validate_configs() -> Dict[str, Any]:
    """Loads all config classes and returns the combined config dictionary."""
    # Instantiating config classes implicitly loads defaults
    # The get_config_dict function retrieves their values
    config_dict = get_config_dict()
    print_config_info_and_validate()
    return config_dict


File: visualization/__init__.py


File: init/rl_components.py
# File: init/rl_components.py
import sys
import traceback
import numpy as np
import torch
from typing import List, Tuple, Optional, Dict, Any, Callable

# Import configurations
from config import (
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    TensorBoardConfig,
    DEVICE,
    BUFFER_SAVE_PATH,
    MODEL_SAVE_PATH,
    get_config_dict,
)

# Import core components
try:
    from environment.game_state import GameState, StateType
except ImportError as e:
    print(f"Error importing environment: {e}")
    sys.exit(1)
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from agent.replay_buffer.buffer_utils import create_replay_buffer
from training.trainer import Trainer

from stats.stats_recorder import StatsRecorderBase
from stats.aggregator import StatsAggregator
from stats.simple_stats_recorder import SimpleStatsRecorder
from stats.tensorboard_logger import TensorBoardStatsRecorder


def initialize_envs(num_envs: int, env_config: EnvConfig) -> List[GameState]:
    print(f"Initializing {num_envs} game environments...")
    try:
        envs = [GameState() for _ in range(num_envs)]
        s_test_dict = envs[0].reset()

        if not isinstance(s_test_dict, dict):
            raise TypeError("Env reset did not return a dictionary state.")
        if "grid" not in s_test_dict:
            raise KeyError("State dict missing 'grid'")
        grid_state = s_test_dict["grid"]
        expected_grid_shape = env_config.GRID_STATE_SHAPE
        if (
            not isinstance(grid_state, np.ndarray)
            or grid_state.shape != expected_grid_shape
        ):
            raise ValueError(
                f"Initial grid state shape mismatch! Env:{grid_state.shape}, Cfg:{expected_grid_shape}"
            )
        print(f"Initial grid state shape check PASSED: {grid_state.shape}")

        if "shapes" not in s_test_dict:
            raise KeyError("State dict missing 'shapes'")
        shape_state = s_test_dict["shapes"]
        expected_shape_shape = (
            env_config.NUM_SHAPE_SLOTS,
            env_config.SHAPE_FEATURES_PER_SHAPE,
        )
        if (
            not isinstance(shape_state, np.ndarray)
            or shape_state.shape != expected_shape_shape
        ):
            raise ValueError(
                f"Initial shape state shape mismatch! Env:{shape_state.shape}, Cfg:{expected_shape_shape}"
            )
        print(f"Initial shape state shape check PASSED: {shape_state.shape}")

        valid_acts_init = envs[0].valid_actions()
        if valid_acts_init:
            _, _ = envs[0].step(valid_acts_init[0])
        else:
            print(
                "Warning: No valid actions available after initial reset for testing step()."
            )

        print(f"Successfully initialized {num_envs} environments.")
        return envs
    except Exception as e:
        print(f"FATAL ERROR during env init: {e}")
        traceback.print_exc()
        raise e


def initialize_agent_buffer(
    model_config: ModelConfig,
    dqn_config: DQNConfig,
    env_config: EnvConfig,
    buffer_config: BufferConfig,
) -> Tuple[DQNAgent, ReplayBufferBase]:
    print("Initializing Agent and Buffer...")
    agent = DQNAgent(
        config=model_config,
        dqn_config=dqn_config,
        env_config=env_config,
        buffer_config=buffer_config,
    )
    buffer = create_replay_buffer(config=buffer_config, dqn_config=dqn_config)
    print("Agent and Initial Buffer structure initialized.")
    return agent, buffer


def initialize_stats_recorder(
    stats_config: StatsConfig,
    tb_config: TensorBoardConfig,
    config_dict: Dict[str, Any],
    agent: Optional[DQNAgent],
    env_config: EnvConfig,
) -> StatsRecorderBase:
    print(f"Initializing Statistics Components...")
    # --- MODIFIED: Use avg_windows keyword argument ---
    stats_aggregator = StatsAggregator(
        avg_windows=stats_config.STATS_AVG_WINDOW,  # Changed from avg_window
        plot_window=stats_config.PLOT_DATA_WINDOW,
    )
    # --- END MODIFIED ---
    console_recorder = SimpleStatsRecorder(
        aggregator=stats_aggregator,
        console_log_interval=stats_config.CONSOLE_LOG_FREQ,
    )

    dummy_grid_cpu, dummy_shapes_cpu, model_for_graph_cpu = None, None, None
    if agent and agent.online_net:
        try:
            expected_grid_shape = env_config.GRID_STATE_SHAPE
            dummy_grid_np = np.zeros(expected_grid_shape, dtype=np.float32)
            dummy_shapes_np = np.zeros(
                (env_config.NUM_SHAPE_SLOTS, env_config.SHAPE_FEATURES_PER_SHAPE),
                dtype=np.float32,
            )
            dummy_grid_cpu = torch.tensor(dummy_grid_np, device="cpu").unsqueeze(0)
            dummy_shapes_cpu = torch.tensor(dummy_shapes_np, device="cpu").unsqueeze(0)

            if not hasattr(agent, "dqn_config") or not hasattr(
                agent.online_net, "config"
            ):
                raise AttributeError(
                    "Agent or network missing required config attributes."
                )

            model_for_graph_cpu = type(agent.online_net)(
                env_config=env_config,
                action_dim=env_config.ACTION_DIM,
                model_config=agent.online_net.config,
                dqn_config=agent.dqn_config,
                dueling=agent.online_net.dueling,
                use_noisy=agent.online_net.use_noisy,
            ).to("cpu")

            model_for_graph_cpu.load_state_dict(agent.online_net.state_dict())
            model_for_graph_cpu.eval()
            print("[Stats Init] Prepared model copy and dummy input on CPU for graph.")
        except Exception as e:
            print(f"Warning: Failed to prepare model/input for graph logging: {e}")
            traceback.print_exc()
            dummy_grid_cpu, dummy_shapes_cpu, model_for_graph_cpu = None, None, None

    print(f"Using TensorBoard Logger (Log Dir: {tb_config.LOG_DIR})")
    try:
        dummy_input_tuple = (
            (dummy_grid_cpu, dummy_shapes_cpu)
            if dummy_grid_cpu is not None and dummy_shapes_cpu is not None
            else None
        )
        tb_recorder = TensorBoardStatsRecorder(
            aggregator=stats_aggregator,
            console_recorder=console_recorder,
            log_dir=tb_config.LOG_DIR,
            hparam_dict=config_dict,
            model_for_graph=model_for_graph_cpu,
            dummy_input_for_graph=dummy_input_tuple,
            histogram_log_interval=(
                tb_config.HISTOGRAM_LOG_FREQ if tb_config.LOG_HISTOGRAMS else -1
            ),
            image_log_interval=(
                tb_config.IMAGE_LOG_FREQ if tb_config.LOG_IMAGES else -1
            ),
            shape_q_log_interval=(
                tb_config.SHAPE_Q_LOG_FREQ
                if tb_config.LOG_SHAPE_PLACEMENT_Q_VALUES
                else -1
            ),
        )
        print("Statistics Components initialized successfully.")
        return tb_recorder
    except Exception as e:
        print(f"FATAL: Error initializing TensorBoardStatsRecorder: {e}. Exiting.")
        traceback.print_exc()
        raise e


def initialize_trainer(
    envs: List[GameState],
    agent: DQNAgent,
    buffer: ReplayBufferBase,
    stats_recorder: StatsRecorderBase,
    env_config: EnvConfig,
    dqn_config: DQNConfig,
    train_config: TrainConfig,
    buffer_config: BufferConfig,
    model_config: ModelConfig,
) -> Trainer:
    print("Initializing Trainer...")
    trainer = Trainer(
        envs=envs,
        agent=agent,
        buffer=buffer,
        stats_recorder=stats_recorder,
        env_config=env_config,
        dqn_config=dqn_config,
        train_config=train_config,
        buffer_config=buffer_config,
        model_config=model_config,
        model_save_path=MODEL_SAVE_PATH,
        buffer_save_path=BUFFER_SAVE_PATH,
        load_checkpoint_path=train_config.LOAD_CHECKPOINT_PATH,
        load_buffer_path=train_config.LOAD_BUFFER_PATH,
    )
    print("Trainer initialization finished.")
    return trainer


File: init/__init__.py
# File: init/__init__.py
from .rl_components import (
    initialize_envs,
    initialize_agent_buffer,
    initialize_stats_recorder,
    initialize_trainer,
)

__all__ = [
    "initialize_envs",
    "initialize_agent_buffer",
    "initialize_stats_recorder",
    "initialize_trainer",
]


File: ui/tooltips.py
# File: ui/tooltips.py
# (No significant changes needed, this file was already focused)
import pygame
from typing import Tuple, Dict, Optional
from config import VisConfig


class TooltipRenderer:
    """Handles rendering of tooltips when hovering over specific UI elements."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.font_tooltip = self._init_font()
        self.hovered_stat_key: Optional[str] = None
        self.stat_rects: Dict[str, pygame.Rect] = {}  # Rects to check for hover
        self.tooltip_texts: Dict[str, str] = {}  # Text corresponding to each rect key

    def _init_font(self):
        """Initializes the font used for tooltips."""
        try:
            # Smaller font for tooltips
            return pygame.font.SysFont(None, 18)
        except Exception as e:
            print(f"Warning: SysFont error for tooltip font: {e}. Using default.")
            return pygame.font.Font(None, 18)

    def check_hover(self, mouse_pos: Tuple[int, int]):
        """Checks if the mouse is hovering over any registered stat rect."""
        self.hovered_stat_key = None
        # Iterate in reverse order of drawing to prioritize top elements
        for key, rect in reversed(self.stat_rects.items()):
            # Ensure rect is valid before checking collision
            if (
                rect
                and rect.width > 0
                and rect.height > 0
                and rect.collidepoint(mouse_pos)
            ):
                self.hovered_stat_key = key
                return  # Found one, stop checking

    def render_tooltip(self):
        """Renders the tooltip if a stat element is being hovered over. Does not flip display."""
        if not self.hovered_stat_key or self.hovered_stat_key not in self.tooltip_texts:
            return  # No active hover or no text defined for this key

        tooltip_text = self.tooltip_texts[self.hovered_stat_key]
        mouse_pos = pygame.mouse.get_pos()

        # --- Text Wrapping Logic ---
        lines = []
        max_width = 300  # Max tooltip width in pixels
        words = tooltip_text.split(" ")
        current_line = ""
        for word in words:
            test_line = f"{current_line} {word}" if current_line else word
            try:
                test_surf = self.font_tooltip.render(test_line, True, VisConfig.BLACK)
                if test_surf.get_width() <= max_width:
                    current_line = test_line
                else:
                    lines.append(current_line)
                    current_line = word
            except Exception as e:
                print(f"Warning: Font render error during tooltip wrap: {e}")
                lines.append(current_line)  # Add what we had
                current_line = word  # Start new line
        lines.append(current_line)  # Add the last line

        # --- Render Wrapped Text ---
        line_surfs = []
        total_height = 0
        max_line_width = 0
        try:
            for line in lines:
                if not line:
                    continue  # Skip empty lines
                surf = self.font_tooltip.render(line, True, VisConfig.BLACK)
                line_surfs.append(surf)
                total_height += surf.get_height()
                max_line_width = max(max_line_width, surf.get_width())
        except Exception as e:
            print(f"Warning: Font render error creating tooltip surfaces: {e}")
            return  # Cannot render tooltip

        if not line_surfs:
            return  # No valid lines to render

        # --- Calculate Tooltip Rect and Draw ---
        padding = 5
        tooltip_rect = pygame.Rect(
            mouse_pos[0] + 15,  # Offset from cursor x
            mouse_pos[1] + 10,  # Offset from cursor y
            max_line_width + padding * 2,
            total_height + padding * 2,
        )

        # Clamp tooltip rect to stay within screen bounds
        tooltip_rect.clamp_ip(self.screen.get_rect())

        try:
            # Draw background and border
            pygame.draw.rect(
                self.screen, VisConfig.YELLOW, tooltip_rect, border_radius=3
            )
            pygame.draw.rect(
                self.screen, VisConfig.BLACK, tooltip_rect, 1, border_radius=3
            )

            # Draw text lines onto the screen
            current_y = tooltip_rect.y + padding
            for surf in line_surfs:
                self.screen.blit(surf, (tooltip_rect.x + padding, current_y))
                current_y += surf.get_height()
        except Exception as e:
            print(f"Warning: Error drawing tooltip background/text: {e}")

    def update_rects_and_texts(
        self, rects: Dict[str, pygame.Rect], texts: Dict[str, str]
    ):
        """Updates the dictionaries used for hover detection and text lookup. Called by UIRenderer."""
        self.stat_rects = rects
        self.tooltip_texts = texts


File: ui/demo_renderer.py
# File: ui/demo_renderer.py
import pygame
import math
import traceback
from typing import Optional, Tuple

from config import VisConfig, EnvConfig, DemoConfig
from environment.game_state import GameState
from .panels.game_area import GameAreaRenderer


class DemoRenderer:
    """Handles rendering specifically for the interactive Demo Mode."""

    def __init__(
        self,
        screen: pygame.Surface,
        vis_config: VisConfig,
        demo_config: DemoConfig,
        game_area_renderer: GameAreaRenderer,
    ):
        self.screen = screen
        self.vis_config = vis_config
        self.demo_config = demo_config
        self.game_area_renderer = game_area_renderer
        self._init_demo_fonts()
        self.overlay_font = self.game_area_renderer.fonts.get("env_overlay")
        if not self.overlay_font:
            print("Warning: DemoRenderer could not get overlay font. Using default.")
            self.overlay_font = pygame.font.Font(None, 36)
        # --- NEW: Define invalid placement color ---
        self.invalid_placement_color = (0, 0, 0, 150)  # Transparent Gray
        # --- END NEW ---

    def _init_demo_fonts(self):
        try:
            self.demo_hud_font = pygame.font.SysFont(
                None, self.demo_config.HUD_FONT_SIZE
            )
            self.demo_help_font = pygame.font.SysFont(
                None, self.demo_config.HELP_FONT_SIZE
            )
            if not hasattr(
                self.game_area_renderer, "fonts"
            ) or not self.game_area_renderer.fonts.get("ui"):
                self.game_area_renderer._init_fonts()
        except Exception as e:
            print(f"Warning: SysFont error for demo fonts: {e}. Using default.")
            self.demo_hud_font = pygame.font.Font(None, self.demo_config.HUD_FONT_SIZE)
            self.demo_help_font = pygame.font.Font(
                None, self.demo_config.HELP_FONT_SIZE
            )

    def render(self, demo_env: GameState, env_config: EnvConfig):
        if not demo_env:
            print("Error: DemoRenderer called with demo_env=None")
            return

        bg_color = self._determine_background_color(demo_env)
        self.screen.fill(bg_color)

        sw, sh = self.screen.get_size()
        padding = 30
        hud_height = 60
        help_height = 30
        max_game_h = sh - 2 * padding - hud_height - help_height
        max_game_w = sw - 2 * padding

        if max_game_h <= 0 or max_game_w <= 0:
            self._render_too_small_message(
                "Demo Area Too Small", self.screen.get_rect()
            )
            return

        game_rect, clipped_game_rect = self._calculate_game_area_rect(
            sw, sh, padding, hud_height, help_height, env_config
        )

        if clipped_game_rect.width > 10 and clipped_game_rect.height > 10:
            self._render_game_area(demo_env, env_config, clipped_game_rect, bg_color)
        else:
            self._render_too_small_message("Demo Area Too Small", clipped_game_rect)

        self._render_shape_previews_area(demo_env, sw, clipped_game_rect, padding)
        self._render_hud(demo_env, sw, game_rect.bottom + 10)
        self._render_help_text(sw, sh)

    def _determine_background_color(self, demo_env: GameState) -> Tuple[int, int, int]:
        if demo_env.is_line_clearing():
            return VisConfig.LINE_CLEAR_FLASH_COLOR
        elif demo_env.is_game_over_flashing():
            return VisConfig.GAME_OVER_FLASH_COLOR
        elif demo_env.is_over():
            return VisConfig.DARK_RED
        elif demo_env.is_frozen():
            return (30, 30, 100)
        else:
            return self.demo_config.BACKGROUND_COLOR

    def _calculate_game_area_rect(
        self,
        screen_width: int,
        screen_height: int,
        padding: int,
        hud_height: int,
        help_height: int,
        env_config: EnvConfig,
    ) -> Tuple[pygame.Rect, pygame.Rect]:
        max_game_h = screen_height - 2 * padding - hud_height - help_height
        max_game_w = screen_width - 2 * padding
        aspect_ratio = (env_config.COLS * 0.75 + 0.25) / max(1, env_config.ROWS)
        game_w = max_game_w
        game_h = game_w / aspect_ratio if aspect_ratio > 0 else max_game_h
        if game_h > max_game_h:
            game_h = max_game_h
            game_w = game_h * aspect_ratio
        game_w = math.floor(min(game_w, max_game_w))
        game_h = math.floor(min(game_h, max_game_h))
        game_x = (screen_width - game_w) // 2
        game_y = padding
        game_rect = pygame.Rect(game_x, game_y, game_w, game_h)
        clipped_game_rect = game_rect.clip(self.screen.get_rect())
        return game_rect, clipped_game_rect

    def _render_game_area(
        self,
        demo_env: GameState,
        env_config: EnvConfig,
        clipped_game_rect: pygame.Rect,
        bg_color: Tuple[int, int, int],
    ):
        try:
            game_surf = self.screen.subsurface(clipped_game_rect)
            game_surf.fill(bg_color)

            self.game_area_renderer._render_single_env_grid(
                game_surf, demo_env, env_config
            )

            preview_tri_cell_w, preview_tri_cell_h = self._calculate_demo_triangle_size(
                clipped_game_rect.width, clipped_game_rect.height, env_config
            )
            if preview_tri_cell_w > 0 and preview_tri_cell_h > 0:
                grid_ox, grid_oy = self._calculate_grid_offset(
                    clipped_game_rect.width, clipped_game_rect.height, env_config
                )
                self._render_placement_preview(
                    game_surf,
                    demo_env,
                    preview_tri_cell_w,
                    preview_tri_cell_h,
                    grid_ox,
                    grid_oy,
                )

            if demo_env.is_over():
                self._render_demo_overlay_text(game_surf, "GAME OVER", VisConfig.RED)
            elif demo_env.is_line_clearing():
                self._render_demo_overlay_text(game_surf, "Line Clear!", VisConfig.BLUE)

        except ValueError as e:
            print(f"Error subsurface demo game ({clipped_game_rect}): {e}")
            pygame.draw.rect(self.screen, VisConfig.RED, clipped_game_rect, 1)
        except Exception as render_e:
            print(f"Error rendering demo game area: {render_e}")
            traceback.print_exc()
            pygame.draw.rect(self.screen, VisConfig.RED, clipped_game_rect, 1)

    def _render_shape_previews_area(
        self,
        demo_env: GameState,
        screen_width: int,
        clipped_game_rect: pygame.Rect,
        padding: int,
    ):
        preview_area_w = min(150, screen_width - clipped_game_rect.right - padding // 2)
        if preview_area_w > 20:
            preview_area_rect = pygame.Rect(
                clipped_game_rect.right + padding // 2,
                clipped_game_rect.top,
                preview_area_w,
                clipped_game_rect.height,
            )
            clipped_preview_area_rect = preview_area_rect.clip(self.screen.get_rect())
            if (
                clipped_preview_area_rect.width > 0
                and clipped_preview_area_rect.height > 0
            ):
                try:
                    preview_area_surf = self.screen.subsurface(
                        clipped_preview_area_rect
                    )
                    self._render_demo_shape_previews(preview_area_surf, demo_env)
                except ValueError as e:
                    print(f"Error subsurface demo shape preview area: {e}")
                    pygame.draw.rect(
                        self.screen, VisConfig.RED, clipped_preview_area_rect, 1
                    )
                except Exception as e:
                    print(f"Error rendering demo shape previews: {e}")
                    traceback.print_exc()

    def _render_hud(self, demo_env: GameState, screen_width: int, hud_y: int):
        score_text = f"Score: {demo_env.game_score} | Lines: {demo_env.lines_cleared_this_episode}"
        try:
            score_surf = self.demo_hud_font.render(score_text, True, VisConfig.WHITE)
            score_rect = score_surf.get_rect(midtop=(screen_width // 2, hud_y))
            self.screen.blit(score_surf, score_rect)
        except Exception as e:
            print(f"HUD render error: {e}")

    def _render_help_text(self, screen_width: int, screen_height: int):
        try:
            help_surf = self.demo_help_font.render(
                self.demo_config.HELP_TEXT, True, VisConfig.LIGHTG
            )
            help_rect = help_surf.get_rect(
                centerx=screen_width // 2, bottom=screen_height - 10
            )
            self.screen.blit(help_surf, help_rect)
        except Exception as e:
            print(f"Help render error: {e}")

    def _render_demo_overlay_text(
        self, surf: pygame.Surface, text: str, color: Tuple[int, int, int]
    ):
        try:
            if not self.overlay_font:
                print("Error: Overlay font not available for demo overlay.")
                return
            text_surf = self.overlay_font.render(
                text,
                True,
                VisConfig.WHITE,
                (color[0] // 2, color[1] // 2, color[2] // 2, 220),
            )
            text_rect = text_surf.get_rect(center=surf.get_rect().center)
            surf.blit(text_surf, text_rect)
        except Exception as e:
            print(f"Error rendering demo overlay text '{text}': {e}")

    def _calculate_demo_triangle_size(
        self, surf_w: int, surf_h: int, env_config: EnvConfig
    ) -> Tuple[int, int]:
        padding = self.vis_config.ENV_GRID_PADDING
        drawable_w = max(1, surf_w - 2 * padding)
        drawable_h = max(1, surf_h - 2 * padding)
        grid_rows = env_config.ROWS
        grid_cols_eff_width = env_config.COLS * 0.75 + 0.25
        if grid_rows <= 0 or grid_cols_eff_width <= 0:
            return 0, 0
        scale_w = drawable_w / grid_cols_eff_width
        scale_h = drawable_h / grid_rows
        final_scale = min(scale_w, scale_h)
        if final_scale <= 0:
            return 0, 0
        tri_cell_size = max(1, int(final_scale))
        return tri_cell_size, tri_cell_size

    def _calculate_grid_offset(
        self, surf_w: int, surf_h: int, env_config: EnvConfig
    ) -> Tuple[float, float]:
        padding = self.vis_config.ENV_GRID_PADDING
        drawable_w = max(1, surf_w - 2 * padding)
        drawable_h = max(1, surf_h - 2 * padding)
        grid_rows = env_config.ROWS
        grid_cols_eff_width = env_config.COLS * 0.75 + 0.25
        if grid_rows <= 0 or grid_cols_eff_width <= 0:
            return float(padding), float(padding)
        scale_w = drawable_w / grid_cols_eff_width
        scale_h = drawable_h / grid_rows
        final_scale = min(scale_w, scale_h)
        final_grid_pixel_w = max(1, grid_cols_eff_width * final_scale)
        final_grid_pixel_h = max(1, grid_rows * final_scale)
        grid_ox = padding + (drawable_w - final_grid_pixel_w) / 2
        grid_oy = padding + (drawable_h - final_grid_pixel_h) / 2
        return grid_ox, grid_oy

    def _render_placement_preview(
        self,
        surf: pygame.Surface,
        env: GameState,
        cell_w: int,
        cell_h: int,
        offset_x: float,
        offset_y: float,
    ):
        if cell_w <= 0 or cell_h <= 0:
            return
        shp, rr, cc = env.get_current_selection_info()
        if shp is None:
            return

        is_valid = env.grid.can_place(shp, rr, cc)

        preview_alpha = 150
        if is_valid:
            shape_rgb = shp.color
            preview_color_to_use = (
                shape_rgb[0],
                shape_rgb[1],
                shape_rgb[2],
                preview_alpha,
            )
        else:
            # --- MODIFIED: Use the defined gray color for invalid placement ---
            preview_color_to_use = self.invalid_placement_color
            # --- END MODIFIED ---

        temp_surface = pygame.Surface(surf.get_size(), pygame.SRCALPHA)
        temp_surface.fill((0, 0, 0, 0))

        for dr, dc, up in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if (
                env.grid.valid(nr, nc)
                and 0 <= nr < len(env.grid.triangles)
                and 0 <= nc < len(env.grid.triangles[nr])
                and not env.grid.triangles[nr][nc].is_death
            ):
                temp_tri = env.grid.triangles[nr][nc]
                try:
                    pts = temp_tri.get_points(
                        ox=offset_x, oy=offset_y, cw=cell_w, ch=cell_h
                    )
                    pygame.draw.polygon(temp_surface, preview_color_to_use, pts)
                except Exception as e:
                    pass

        surf.blit(temp_surface, (0, 0))

    def _render_demo_shape_previews(self, surf: pygame.Surface, env: GameState):
        surf.fill((25, 25, 25))
        all_slots = env.shapes
        selected_shape_obj = (
            all_slots[env.demo_selected_shape_idx]
            if 0 <= env.demo_selected_shape_idx < len(all_slots)
            else None
        )
        num_slots = env.env_config.NUM_SHAPE_SLOTS
        surf_w, surf_h = surf.get_size()
        preview_padding = 5

        if num_slots <= 0:
            return

        preview_h = max(20, (surf_h - (num_slots + 1) * preview_padding) / num_slots)
        preview_w = max(20, surf_w - 2 * preview_padding)
        current_preview_y = preview_padding

        for i in range(num_slots):
            shp = all_slots[i] if i < len(all_slots) else None
            preview_rect = pygame.Rect(
                preview_padding, current_preview_y, preview_w, preview_h
            )
            clipped_preview_rect = preview_rect.clip(surf.get_rect())

            if clipped_preview_rect.width <= 0 or clipped_preview_rect.height <= 0:
                current_preview_y += preview_h + preview_padding
                continue

            bg_color = (40, 40, 40)
            border_color = VisConfig.GRAY
            border_width = 1
            if shp is not None and shp == selected_shape_obj:
                border_color = self.demo_config.SELECTED_SHAPE_HIGHLIGHT_COLOR
                border_width = 2

            pygame.draw.rect(surf, bg_color, clipped_preview_rect, border_radius=3)
            pygame.draw.rect(
                surf, border_color, clipped_preview_rect, border_width, border_radius=3
            )

            if shp is not None:
                self._render_single_shape_in_preview_box(
                    surf, shp, preview_rect, clipped_preview_rect
                )

            current_preview_y += preview_h + preview_padding

    def _render_single_shape_in_preview_box(
        self,
        surf: pygame.Surface,
        shp,
        preview_rect: pygame.Rect,
        clipped_preview_rect: pygame.Rect,
    ):
        try:
            inner_padding = 2
            shape_render_area_rect = pygame.Rect(
                inner_padding,
                inner_padding,
                clipped_preview_rect.width - 2 * inner_padding,
                clipped_preview_rect.height - 2 * inner_padding,
            )
            if shape_render_area_rect.width > 0 and shape_render_area_rect.height > 0:
                sub_surf_x = preview_rect.left + shape_render_area_rect.left
                sub_surf_y = preview_rect.top + shape_render_area_rect.top
                shape_sub_surf = surf.subsurface(
                    sub_surf_x,
                    sub_surf_y,
                    shape_render_area_rect.width,
                    shape_render_area_rect.height,
                )
                min_r, min_c, max_r, max_c = shp.bbox()
                shape_h = max(1, max_r - min_r + 1)
                shape_w_eff = max(1, (max_c - min_c + 1) * 0.75 + 0.25)

                scale_h = shape_render_area_rect.height / shape_h
                scale_w = shape_render_area_rect.width / shape_w_eff
                cell_size = max(1, min(scale_h, scale_w))

                self.game_area_renderer._render_single_shape(
                    shape_sub_surf, shp, int(cell_size)
                )
        except ValueError as sub_err:
            print(f"Error subsurface shape preview: {sub_err}")
            pygame.draw.rect(surf, VisConfig.RED, clipped_preview_rect, 1)
        except Exception as e:
            print(f"Error rendering demo shape preview: {e}")
            pygame.draw.rect(surf, VisConfig.RED, clipped_preview_rect, 1)

    def _render_too_small_message(self, text: str, area_rect: pygame.Rect):
        try:
            font = self.game_area_renderer.fonts.get("ui") or pygame.font.SysFont(
                None, 24
            )
            err_surf = font.render(text, True, VisConfig.GRAY)
            target_rect = err_surf.get_rect(center=area_rect.center)
            self.screen.blit(err_surf, target_rect)
        except Exception as e:
            print(f"Error rendering 'too small' message: {e}")


File: ui/renderer.py
# File: ui/renderer.py
import pygame
import time
import traceback
from typing import List, Dict, Any, Optional, Tuple, Deque

from config import VisConfig, EnvConfig, TensorBoardConfig, DemoConfig
from environment.game_state import GameState
from .panels import LeftPanelRenderer, GameAreaRenderer
from .overlays import OverlayRenderer
from .tooltips import TooltipRenderer
from .plotter import Plotter
from .demo_renderer import DemoRenderer  # Import the new demo renderer


class UIRenderer:
    """Orchestrates rendering of all UI components."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.plotter = Plotter()
        self.left_panel = LeftPanelRenderer(screen, vis_config, self.plotter)
        self.game_area = GameAreaRenderer(screen, vis_config)
        self.overlays = OverlayRenderer(screen, vis_config)
        self.tooltips = TooltipRenderer(screen, vis_config)
        self.demo_config = DemoConfig()  # Need demo config for demo renderer
        # --- NEW: Instantiate DemoRenderer ---
        self.demo_renderer = DemoRenderer(
            screen, vis_config, self.demo_config, self.game_area
        )
        # --- END NEW ---
        self.last_plot_update_time = 0

    def check_hover(self, mouse_pos: Tuple[int, int], app_state: str):
        """Passes hover check to the tooltip renderer."""
        if app_state == "MainMenu":
            self.tooltips.update_rects_and_texts(
                self.left_panel.get_stat_rects(), self.left_panel.get_tooltip_texts()
            )
            self.tooltips.check_hover(mouse_pos)
        else:
            # Disable tooltips in other states for now
            self.tooltips.hovered_stat_key = None
            self.tooltips.stat_rects.clear()

    def force_redraw(self):
        """Forces components like the plotter to redraw on the next frame."""
        self.plotter.last_plot_update_time = 0

    def render_all(
        self,
        app_state: str,
        is_training: bool,
        status: str,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        envs: List[GameState],
        num_envs: int,
        env_config: EnvConfig,
        cleanup_confirmation_active: bool,
        cleanup_message: str,
        last_cleanup_message_time: float,
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
        demo_env: Optional[GameState] = None,
    ):
        """Renders UI based on the application state."""
        try:
            # 1. Render main content based on state (WITHOUT flipping)
            if app_state == "MainMenu":
                self._render_main_menu(
                    is_training,
                    status,
                    stats_summary,
                    buffer_capacity,
                    envs,
                    num_envs,
                    env_config,
                    cleanup_message,
                    last_cleanup_message_time,
                    tensorboard_log_dir,
                    plot_data,
                )
            elif app_state == "Playing":
                # --- MODIFIED: Call DemoRenderer ---
                if demo_env:
                    self.demo_renderer.render(demo_env, env_config)
                else:
                    # Render error if demo_env is missing
                    print("Error: Attempting to render demo mode without demo_env.")
                    self._render_simple_message("Demo Env Error!", VisConfig.RED)
                # --- END MODIFIED ---
            elif app_state == "Initializing":
                self._render_initializing_screen(status)
            elif app_state == "Error":
                self._render_error_screen(status)

            # 2. Render Overlays ON TOP if needed
            # Cleanup confirmation overlay takes precedence
            if cleanup_confirmation_active and app_state != "Error":
                self.overlays.render_cleanup_confirmation()
            # Render transient status message if cleanup overlay is NOT active
            elif not cleanup_confirmation_active:
                self.overlays.render_status_message(
                    cleanup_message, last_cleanup_message_time
                )

            # 3. Render Tooltip (only in MainMenu and if cleanup not active)
            if app_state == "MainMenu" and not cleanup_confirmation_active:
                # Tooltip rects/texts are updated during check_hover or implicitly by left_panel.render
                # We just need to call render_tooltip here.
                self.tooltips.render_tooltip()

            # 4. Final Flip
            pygame.display.flip()

        except pygame.error as e:
            # Handle specific pygame errors if needed
            print(f"Pygame rendering error in render_all: {e}")
            traceback.print_exc()
        except Exception as e:
            print(f"Unexpected critical rendering error in render_all: {e}")
            traceback.print_exc()
            # Attempt to render a basic error screen on critical failure
            try:
                self._render_simple_message("Critical Render Error!", VisConfig.RED)
                pygame.display.flip()
            except Exception:
                pass  # Ignore errors during error rendering

    def _render_main_menu(
        self,
        is_training: bool,
        status: str,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        envs: List[GameState],
        num_envs: int,
        env_config: EnvConfig,
        cleanup_message: str,  # No longer need cleanup_confirmation_active here
        last_cleanup_message_time: float,
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
    ):
        """Renders the main training dashboard view. Does NOT flip display."""
        self.screen.fill(VisConfig.BLACK)  # Clear screen

        # Render Main Panels (Left Panel updates tooltip rects internally now)
        self.left_panel.render(
            is_training,
            status,
            stats_summary,
            buffer_capacity,
            tensorboard_log_dir,
            plot_data,
            app_state="MainMenu",  # Pass state for conditional rendering
        )
        self.game_area.render(envs, num_envs, env_config)

        # Status message and tooltips are handled by render_all after this

    def _render_initializing_screen(
        self, status_message: str = "Initializing RL Components..."
    ):
        """Renders the initializing screen with a status message."""
        self._render_simple_message(status_message, VisConfig.WHITE)

    def _render_error_screen(self, status_message: str):
        """Renders the error screen. Does NOT flip display."""
        try:
            self.screen.fill((40, 0, 0))  # Dark red background
            font_title = pygame.font.SysFont(None, 70)
            font_msg = pygame.font.SysFont(None, 30)

            title_surf = font_title.render("APPLICATION ERROR", True, VisConfig.RED)
            title_rect = title_surf.get_rect(
                center=(self.screen.get_width() // 2, self.screen.get_height() // 3)
            )

            msg_surf = font_msg.render(
                f"Status: {status_message}", True, VisConfig.YELLOW
            )
            msg_rect = msg_surf.get_rect(
                center=(self.screen.get_width() // 2, title_rect.bottom + 30)
            )

            exit_surf = font_msg.render(
                "Press ESC or close window to exit.", True, VisConfig.WHITE
            )
            exit_rect = exit_surf.get_rect(
                center=(self.screen.get_width() // 2, self.screen.get_height() * 0.8)
            )

            self.screen.blit(title_surf, title_rect)
            self.screen.blit(msg_surf, msg_rect)
            self.screen.blit(exit_surf, exit_rect)

        except Exception as e:
            print(f"Error rendering error screen: {e}")
            # Fallback to simple message
            self._render_simple_message(f"Error State: {status_message}", VisConfig.RED)

    def _render_simple_message(self, message: str, color: Tuple[int, int, int]):
        """Renders a simple centered text message."""
        try:
            self.screen.fill(VisConfig.BLACK)
            font = pygame.font.SysFont(None, 50)
            text_surf = font.render(message, True, color)
            text_rect = text_surf.get_rect(center=self.screen.get_rect().center)
            self.screen.blit(text_surf, text_rect)
        except Exception as e:
            print(f"Error rendering simple message '{message}': {e}")


File: ui/__init__.py
from .renderer import UIRenderer
from .input_handler import InputHandler

__all__ = ["UIRenderer", "InputHandler"]


File: ui/plotter.py
# File: ui/plotter.py
import pygame
import numpy as np
from typing import Dict, Optional, Deque, List, Union, Tuple
from collections import deque
import matplotlib
import time
import warnings
from io import BytesIO

matplotlib.use("Agg")
import matplotlib.pyplot as plt

from config import VisConfig, BufferConfig, StatsConfig, DQNConfig
from .plot_utils import (
    render_single_plot,
    normalize_color_for_matplotlib,
)


class Plotter:
    """Handles creation and caching of the multi-plot Matplotlib surface."""

    def __init__(self):
        self.plot_surface: Optional[pygame.Surface] = None
        self.last_plot_update_time: float = 0.0
        self.plot_update_interval: float = 1.0
        self.rolling_window_sizes = StatsConfig.STATS_AVG_WINDOW
        self.plot_data_window = StatsConfig.PLOT_DATA_WINDOW
        self.default_line_width = 1.0
        self.avg_line_width = 1.5
        self.avg_line_alpha = 0.8  # Slightly increased alpha for averages

        # Define base colors
        self.colors = {
            "rl_score": normalize_color_for_matplotlib(VisConfig.GOOGLE_COLORS[0]),
            "game_score": normalize_color_for_matplotlib(VisConfig.GOOGLE_COLORS[1]),
            "loss": normalize_color_for_matplotlib(VisConfig.GOOGLE_COLORS[3]),
            "len": normalize_color_for_matplotlib(VisConfig.BLUE),
            "sps": normalize_color_for_matplotlib(VisConfig.LIGHTG),
            "best_game": normalize_color_for_matplotlib((255, 165, 0)),  # Orange
            "lr": normalize_color_for_matplotlib((255, 0, 255)),  # Magenta
            "buffer": normalize_color_for_matplotlib(VisConfig.RED),
            "beta": normalize_color_for_matplotlib((100, 100, 255)),  # Light Blue
            "avg_primary": normalize_color_for_matplotlib(VisConfig.YELLOW),
            "placeholder": normalize_color_for_matplotlib(VisConfig.GRAY),
        }
        # --- NEW: Define colors for secondary average lines ---
        self.avg_line_colors_secondary = [
            normalize_color_for_matplotlib((0, 255, 255)),  # Cyan
            normalize_color_for_matplotlib((255, 165, 0)),  # Orange
            normalize_color_for_matplotlib((0, 255, 0)),  # Lime Green
            normalize_color_for_matplotlib((255, 0, 255)),  # Magenta
        ]
        # --- END NEW ---

    def create_plot_surface(
        self, plot_data: Dict[str, Deque], target_width: int, target_height: int
    ) -> Optional[pygame.Surface]:
        if target_width <= 10 or target_height <= 10 or not plot_data:
            return None

        data_keys = [
            "episode_scores",
            "game_scores",
            "losses",
            "episode_lengths",
            "sps_values",
            "best_game_score_history",
            "lr_values",
            "buffer_sizes",
            "beta_values",
        ]
        data_lists = {key: list(plot_data.get(key, deque())) for key in data_keys}

        if not any(data_lists.values()):
            return None

        fig = None
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", category=UserWarning)
                dpi = 90
                fig_width_in = target_width / dpi
                fig_height_in = target_height / dpi
                fig, axes = plt.subplots(
                    3, 3, figsize=(fig_width_in, fig_height_in), dpi=dpi, sharex=False
                )
                fig.subplots_adjust(
                    hspace=0.55,
                    wspace=0.35,
                    left=0.10,
                    right=0.98,
                    bottom=0.12,
                    top=0.95,
                )
                axes_flat = axes.flatten()

                max_len = max((len(d) for d in data_lists.values() if d), default=0)
                plot_window_label = f"Latest {min(self.plot_data_window, max_len)} Pts"

                # --- MODIFIED: Pass primary and secondary avg colors ---
                render_single_plot(
                    axes_flat[0],
                    data_lists["episode_scores"],
                    "RL Score",
                    self.colors["rl_score"],
                    self.colors["avg_primary"],  # Primary avg color
                    self.avg_line_colors_secondary,  # Secondary avg colors
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[1],
                    data_lists["game_scores"],
                    "Game Score",
                    self.colors["game_score"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[2],
                    data_lists["losses"],
                    "Loss",
                    self.colors["loss"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    show_placeholder=True,
                    placeholder_text="Loss data after Learn Start",
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[3],
                    data_lists["episode_lengths"],
                    "Ep Length",
                    self.colors["len"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[4],
                    data_lists["best_game_score_history"],
                    "Best Game Score",
                    self.colors["best_game"],
                    None,  # No average
                    [],  # No average
                    [],  # No average
                    xlabel=plot_window_label,
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[5],
                    data_lists["sps_values"],
                    "Steps/Sec",
                    self.colors["sps"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[6],
                    data_lists["lr_values"],
                    "Learning Rate",
                    self.colors["lr"],
                    None,  # No average
                    [],  # No average
                    [],  # No average
                    xlabel=plot_window_label,
                    y_log_scale=True,
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                buffer_fill_percent = [
                    (s / max(1, BufferConfig.REPLAY_BUFFER_SIZE) * 100)
                    for s in data_lists["buffer_sizes"]
                ]
                render_single_plot(
                    axes_flat[7],
                    buffer_fill_percent,
                    "Buffer Fill %",
                    self.colors["buffer"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                if BufferConfig.USE_PER:
                    render_single_plot(
                        axes_flat[8],
                        data_lists["beta_values"],
                        "PER Beta",
                        self.colors["beta"],
                        self.colors["avg_primary"],
                        self.avg_line_colors_secondary,
                        self.rolling_window_sizes,
                        xlabel=plot_window_label,
                        default_line_width=self.default_line_width,
                        avg_line_width=self.avg_line_width,
                        avg_line_alpha=self.avg_line_alpha,
                    )
                else:
                    axes_flat[8].text(
                        0.5,
                        0.5,
                        "PER Disabled",
                        ha="center",
                        va="center",
                        transform=axes_flat[8].transAxes,
                        fontsize=8,
                        color=self.colors["placeholder"],
                    )
                    axes_flat[8].set_yticks([])
                    axes_flat[8].set_xticks([])
                # --- END MODIFIED ---

                for ax in axes_flat:
                    ax.tick_params(axis="x", rotation=0)

                buf = BytesIO()
                fig.savefig(
                    buf,
                    format="png",
                    transparent=False,
                    facecolor=plt.rcParams["figure.facecolor"],
                )
                buf.seek(0)
                plot_img_surface = pygame.image.load(buf).convert()
                buf.close()

                if (
                    plot_img_surface.get_size() != (target_width, target_height)
                    and target_width > 0
                    and target_height > 0
                ):
                    plot_img_surface = pygame.transform.smoothscale(
                        plot_img_surface, (target_width, target_height)
                    )
                return plot_img_surface

        except Exception as e:
            print(f"Error creating plot surface: {e}")
            import traceback

            traceback.print_exc()
            return None
        finally:
            if fig is not None:
                plt.close(fig)

    def get_cached_or_updated_plot(
        self, plot_data: Dict[str, Deque], target_width: int, target_height: int
    ) -> Optional[pygame.Surface]:
        current_time = time.time()
        has_data = any(plot_data.values())
        needs_update_time = (
            current_time - self.last_plot_update_time > self.plot_update_interval
        )
        size_changed = self.plot_surface and self.plot_surface.get_size() != (
            target_width,
            target_height,
        )
        first_data_received = has_data and self.plot_surface is None
        can_create_plot = target_width > 50 and target_height > 50

        if can_create_plot and (
            needs_update_time or size_changed or first_data_received
        ):
            new_plot_surface = self.create_plot_surface(
                plot_data, target_width, target_height
            )
            if new_plot_surface:
                self.plot_surface = new_plot_surface
                self.last_plot_update_time = current_time

        return self.plot_surface


File: ui/overlays.py
# File: ui/overlays.py
# (No significant changes needed, this file was already focused)
import pygame
import time
import traceback
from typing import Tuple
from config import VisConfig


class OverlayRenderer:
    """Renders overlay elements like confirmation dialogs and status messages."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.fonts = self._init_fonts()

    def _init_fonts(self):
        """Initializes fonts used for overlays."""
        fonts = {}
        try:
            fonts["overlay_title"] = pygame.font.SysFont(None, 36)
            fonts["overlay_text"] = pygame.font.SysFont(None, 24)
        except Exception as e:
            print(f"Warning: SysFont error for overlay fonts: {e}. Using default.")
            fonts["overlay_title"] = pygame.font.Font(None, 36)
            fonts["overlay_text"] = pygame.font.Font(None, 24)
        return fonts

    def render_cleanup_confirmation(self):
        """Renders the confirmation dialog for cleanup. Does not flip display."""
        try:
            current_width, current_height = self.screen.get_size()

            # Semi-transparent background overlay
            overlay_surface = pygame.Surface(
                (current_width, current_height), pygame.SRCALPHA
            )
            overlay_surface.fill((0, 0, 0, 200))  # Black with alpha
            self.screen.blit(overlay_surface, (0, 0))

            center_x, center_y = current_width // 2, current_height // 2

            # --- Render Text Lines ---
            if "overlay_title" not in self.fonts or "overlay_text" not in self.fonts:
                print("ERROR: Overlay fonts not loaded!")
                # Draw basic fallback text
                fallback_font = pygame.font.Font(None, 30)
                err_surf = fallback_font.render("CONFIRM CLEANUP?", True, VisConfig.RED)
                self.screen.blit(
                    err_surf, err_surf.get_rect(center=(center_x, center_y - 30))
                )
                yes_surf = fallback_font.render("YES", True, VisConfig.WHITE)
                no_surf = fallback_font.render("NO", True, VisConfig.WHITE)
                self.screen.blit(
                    yes_surf, yes_surf.get_rect(center=(center_x - 60, center_y + 50))
                )
                self.screen.blit(
                    no_surf, no_surf.get_rect(center=(center_x + 60, center_y + 50))
                )
                return  # Stop here if fonts failed

            # Use loaded fonts
            prompt_l1 = self.fonts["overlay_title"].render(
                "DELETE CURRENT RUN DATA?", True, VisConfig.RED
            )
            prompt_l2 = self.fonts["overlay_text"].render(
                "(Agent Checkpoint & Buffer State)", True, VisConfig.WHITE
            )
            prompt_l3 = self.fonts["overlay_text"].render(
                "This action cannot be undone!", True, VisConfig.YELLOW
            )

            # Position and blit text
            self.screen.blit(
                prompt_l1, prompt_l1.get_rect(center=(center_x, center_y - 60))
            )
            self.screen.blit(
                prompt_l2, prompt_l2.get_rect(center=(center_x, center_y - 25))
            )
            self.screen.blit(prompt_l3, prompt_l3.get_rect(center=(center_x, center_y)))

            # --- Render Buttons ---
            # Recalculate rects based on current screen size for responsiveness
            confirm_yes_rect = pygame.Rect(center_x - 110, center_y + 30, 100, 40)
            confirm_no_rect = pygame.Rect(center_x + 10, center_y + 30, 100, 40)

            pygame.draw.rect(
                self.screen, (0, 150, 0), confirm_yes_rect, border_radius=5
            )  # Green YES
            pygame.draw.rect(
                self.screen, (150, 0, 0), confirm_no_rect, border_radius=5
            )  # Red NO

            yes_text = self.fonts["overlay_text"].render("YES", True, VisConfig.WHITE)
            no_text = self.fonts["overlay_text"].render("NO", True, VisConfig.WHITE)

            self.screen.blit(
                yes_text, yes_text.get_rect(center=confirm_yes_rect.center)
            )
            self.screen.blit(no_text, no_text.get_rect(center=confirm_no_rect.center))

        except pygame.error as pg_err:
            print(f"Pygame Error in render_cleanup_confirmation: {pg_err}")
            traceback.print_exc()
        except Exception as e:
            print(f"Error in render_cleanup_confirmation: {e}")
            traceback.print_exc()

    def render_status_message(self, message: str, last_message_time: float) -> bool:
        """
        Renders a status message (e.g., after cleanup) temporarily at the bottom center.
        Does not flip display. Returns True if a message was rendered.
        """
        # Check if message exists and hasn't timed out
        if not message or (time.time() - last_message_time >= 5.0):
            return False

        try:
            if "overlay_text" not in self.fonts:  # Check if font loaded
                print(
                    "Warning: Cannot render status message, overlay_text font missing."
                )
                return False

            current_width, current_height = self.screen.get_size()
            lines = message.split("\n")
            max_width = 0
            msg_surfs = []

            # Render each line and find max width
            for line in lines:
                msg_surf = self.fonts["overlay_text"].render(
                    line,
                    True,
                    VisConfig.YELLOW,
                    VisConfig.BLACK,  # Yellow text on black bg
                )
                msg_surfs.append(msg_surf)
                max_width = max(max_width, msg_surf.get_width())

            if not msg_surfs:
                return False  # No lines to render

            # Calculate background size and position
            total_height = (
                sum(s.get_height() for s in msg_surfs) + max(0, len(lines) - 1) * 2
            )
            padding = 5
            bg_rect = pygame.Rect(
                0, 0, max_width + padding * 2, total_height + padding * 2
            )
            bg_rect.midbottom = (
                current_width // 2,
                current_height - 10,
            )  # Position at bottom center

            # Draw background and border
            pygame.draw.rect(self.screen, VisConfig.BLACK, bg_rect, border_radius=3)
            pygame.draw.rect(self.screen, VisConfig.YELLOW, bg_rect, 1, border_radius=3)

            # Draw text lines centered within the background
            current_y = bg_rect.top + padding
            for msg_surf in msg_surfs:
                msg_rect = msg_surf.get_rect(midtop=(bg_rect.centerx, current_y))
                self.screen.blit(msg_surf, msg_rect)
                current_y += msg_surf.get_height() + 2  # Move Y for next line

            return True  # Message was rendered
        except Exception as e:
            print(f"Error rendering status message: {e}")
            traceback.print_exc()
            return False  # Message render failed


File: ui/plot_utils.py
# File: ui/plot_utils.py
import pygame
import numpy as np
from typing import Dict, Optional, Deque, List, Union, Tuple
import matplotlib
import warnings
from io import BytesIO

matplotlib.use("Agg")
import matplotlib.pyplot as plt

from config import VisConfig, BufferConfig, StatsConfig, DQNConfig

# --- Matplotlib Style Configuration ---
try:
    plt.style.use("dark_background")
    plt.rcParams.update(
        {
            "font.size": 8,
            "axes.labelsize": 8,
            "axes.titlesize": 9,
            "xtick.labelsize": 7,
            "ytick.labelsize": 7,
            "legend.fontsize": 6,
            "figure.facecolor": "#262626",
            "axes.facecolor": "#303030",
            "axes.edgecolor": "#707070",  # Default edge color
            "axes.labelcolor": "#D0D0D0",
            "xtick.color": "#C0C0C0",
            "ytick.color": "#C0C0C0",
            "grid.color": "#505050",
            "grid.linestyle": "--",
            "grid.alpha": 0.5,
        }
    )
except Exception as e:
    print(f"Warning: Failed to set Matplotlib style: {e}")
# --- End Style Configuration ---


def normalize_color_for_matplotlib(
    color_tuple_0_255: Tuple[int, int, int],
) -> Tuple[float, float, float]:
    if isinstance(color_tuple_0_255, tuple) and len(color_tuple_0_255) == 3:
        return tuple(c / 255.0 for c in color_tuple_0_255)
    else:
        print(f"Warning: Invalid color tuple {color_tuple_0_255}, returning black.")
        return (0.0, 0.0, 0.0)


# --- NEW: Trend Colors and Settings ---
TREND_INCREASING_COLOR = normalize_color_for_matplotlib((0, 180, 0))  # Darker Green
TREND_DECREASING_COLOR = normalize_color_for_matplotlib((180, 0, 0))  # Darker Red
TREND_STABLE_COLOR = normalize_color_for_matplotlib((70, 70, 70))  # Default gray border
TREND_LINEWIDTH = 1.5  # Make trend border slightly thicker
DEFAULT_LINEWIDTH = 0.8  # Default border thickness
TREND_TOLERANCE = 1e-6  # Tolerance for stability check
# --- END NEW ---


# --- MODIFIED: Function signature and logic (added trend border) ---
def render_single_plot(
    ax,
    data: List[Union[float, int]],
    label: str,
    color: Tuple[float, float, float],
    avg_color_primary: Optional[Tuple[float, float, float]],
    avg_colors_secondary: List[Tuple[float, float, float]],
    rolling_window_sizes: List[int],
    xlabel: Optional[str] = None,
    show_placeholder: bool = True,
    placeholder_text: Optional[str] = None,
    y_log_scale: bool = False,
    default_line_width: float = 1.0,
    avg_line_width: float = 1.5,
    avg_line_alpha: float = 0.7,
):
    """Renders data onto a single Matplotlib Axes object, including multiple rolling averages and trend indicator."""
    n_points = len(data)
    latest_val_str = ""
    primary_avg_window = rolling_window_sizes[0] if rolling_window_sizes else 0

    # --- Reset border style initially ---
    for spine in ax.spines.values():
        spine.set_color(TREND_STABLE_COLOR)
        spine.set_linewidth(DEFAULT_LINEWIDTH)
    # --- End Reset ---

    if data:
        current_val = data[-1]
        if (
            n_points >= primary_avg_window
            and avg_color_primary is not None
            and primary_avg_window > 0
        ):
            try:
                latest_avg = np.mean(data[-primary_avg_window:])
                latest_val_str = f" (Now: {current_val:.3g}, Avg{primary_avg_window}: {latest_avg:.3g})"
            except Exception:
                latest_val_str = f" (Now: {current_val:.3g})"
        else:
            latest_val_str = f" (Now: {current_val:.3g})"
    ax.set_title(f"{label}{latest_val_str}", fontsize=plt.rcParams["axes.titlesize"])

    placeholder_text_color = normalize_color_for_matplotlib(VisConfig.GRAY)
    if n_points == 0:
        if show_placeholder:
            p_text = placeholder_text if placeholder_text else f"{label}\n(No data yet)"
            ax.text(
                0.5,
                0.5,
                p_text,
                ha="center",
                va="center",
                transform=ax.transAxes,
                fontsize=8,
                color=placeholder_text_color,
            )
        ax.set_yticks([])
        ax.set_xticks([])
        return

    try:
        x_coords = np.arange(n_points)
        ax.plot(
            x_coords, data, color=color, linewidth=default_line_width, label=f"{label}"
        )

        avg_linestyles = ["-", "--", ":", "-."]
        plotted_averages = False
        longest_avg_rolling = None
        longest_window = 0

        for i, avg_window in enumerate(rolling_window_sizes):
            if n_points >= avg_window:
                if i == 0 and avg_color_primary:
                    current_avg_color = avg_color_primary
                elif avg_colors_secondary:
                    current_avg_color = avg_colors_secondary[
                        (i - 1) % len(avg_colors_secondary)
                    ]
                else:
                    current_avg_color = color

                weights = np.ones(avg_window) / avg_window
                rolling_avg = np.convolve(data, weights, mode="valid")
                avg_x_coords = np.arange(avg_window - 1, n_points)
                linestyle = avg_linestyles[i % len(avg_linestyles)]
                ax.plot(
                    avg_x_coords,
                    rolling_avg,
                    color=current_avg_color,
                    linewidth=avg_line_width,
                    alpha=avg_line_alpha,
                    linestyle=linestyle,
                    label=f"Avg {avg_window}",
                )
                plotted_averages = True
                # --- Store the longest average calculated ---
                if avg_window >= longest_window:
                    longest_window = avg_window
                    longest_avg_rolling = rolling_avg
            # --- End average plotting loop ---

        # --- Determine and apply trend border based on longest average ---
        if longest_avg_rolling is not None and len(longest_avg_rolling) >= 2:
            latest_avg_long = longest_avg_rolling[-1]
            prev_avg_long = longest_avg_rolling[-2]
            trend_color = TREND_STABLE_COLOR
            trend_lw = DEFAULT_LINEWIDTH

            if latest_avg_long > prev_avg_long + TREND_TOLERANCE:
                trend_color = TREND_INCREASING_COLOR
                trend_lw = TREND_LINEWIDTH
            elif latest_avg_long < prev_avg_long - TREND_TOLERANCE:
                trend_color = TREND_DECREASING_COLOR
                trend_lw = TREND_LINEWIDTH

            for spine in ax.spines.values():
                spine.set_color(trend_color)
                spine.set_linewidth(trend_lw)
        # --- End trend border ---

        ax.tick_params(axis="both", which="major")
        if xlabel:
            ax.set_xlabel(xlabel)
        ax.grid(
            True,
            linestyle=plt.rcParams["grid.linestyle"],
            alpha=plt.rcParams["grid.alpha"],
        )

        min_val = np.min(data)
        max_val = np.max(data)
        padding_factor = 0.1
        # --- Prevent y-axis range collapsing to zero ---
        range_val = max_val - min_val
        if range_val < 1e-6:  # Very small range
            padding = max(
                abs(max_val * padding_factor), 0.5
            )  # Use larger default padding
        else:
            padding = range_val * padding_factor

        # Ensure padding is not excessively small
        padding = max(padding, 1e-6)
        # Set y-limits, ensuring lower can be negative if min_val is negative
        ax.set_ylim(min_val - padding, max_val + padding)
        # --- End y-axis range fix ---

        if y_log_scale and min_val > 1e-9:
            # Ensure bottom limit is positive for log scale
            ax.set_yscale("log")
            ax.set_ylim(bottom=max(min_val * 0.9, 1e-9))
        else:
            ax.set_yscale("linear")

        if n_points > 1:
            ax.set_xlim(-0.02 * n_points, n_points - 1 + 0.02 * n_points)
        elif n_points == 1:
            ax.set_xlim(-0.5, 0.5)

        if n_points > 1000:
            ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True, nbins=4))

            def format_func(value, tick_number):
                if value >= 1_000_000:
                    return f"{value/1_000_000:.1f}M"
                if value >= 1_000:
                    return f"{value/1_000:.0f}k"
                return f"{int(value)}"

            ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))
        elif n_points > 10:
            ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True, nbins=5))

        if plotted_averages:
            ax.legend(loc="best", fontsize=plt.rcParams["legend.fontsize"])

    except Exception as plot_err:
        print(f"ERROR during render_single_plot for '{label}': {plot_err}")
        error_text_color = normalize_color_for_matplotlib(VisConfig.RED)
        ax.text(
            0.5,
            0.5,
            f"Plotting Error\n({label})",
            ha="center",
            va="center",
            transform=ax.transAxes,
            fontsize=8,
            color=error_text_color,
        )
        ax.set_yticks([])
        ax.set_xticks([])


# --- END MODIFIED ---


File: ui/input_handler.py
# File: ui/input_handler.py
# (No significant changes needed, this file was already reasonably focused)
import pygame
from typing import Tuple, Callable, Optional

# --- Define Callback Types (for clarity) ---
HandleDemoInputCallback = Callable[[pygame.event.Event], None]
ToggleTrainingCallback = Callable[[], None]
RequestCleanupCallback = Callable[[], None]
CancelCleanupCallback = Callable[[], None]
ConfirmCleanupCallback = Callable[[], None]
ExitAppCallback = Callable[[], bool]  # Returns False to signal exit
StartDemoModeCallback = Callable[[], None]
ExitDemoModeCallback = Callable[[], None]
# --- End Callback Types ---

# Forward declaration for type hinting UIRenderer
if False:  # Prevent circular import at runtime
    from .renderer import UIRenderer


class InputHandler:
    """Handles Pygame events and triggers callbacks based on application state."""

    def __init__(
        self,
        screen: pygame.Surface,
        renderer: "UIRenderer",  # Use forward declaration
        toggle_training_cb: ToggleTrainingCallback,
        request_cleanup_cb: RequestCleanupCallback,
        cancel_cleanup_cb: CancelCleanupCallback,
        confirm_cleanup_cb: ConfirmCleanupCallback,
        exit_app_cb: ExitAppCallback,
        start_demo_mode_cb: StartDemoModeCallback,
        exit_demo_mode_cb: ExitDemoModeCallback,
        handle_demo_input_cb: HandleDemoInputCallback,
    ):
        self.screen = screen  # Keep reference to update size on resize
        self.renderer = renderer
        # Store callbacks provided by MainApp
        self.toggle_training_cb = toggle_training_cb
        self.request_cleanup_cb = request_cleanup_cb
        self.cancel_cleanup_cb = cancel_cleanup_cb
        self.confirm_cleanup_cb = confirm_cleanup_cb
        self.exit_app_cb = exit_app_cb
        self.start_demo_mode_cb = start_demo_mode_cb
        self.exit_demo_mode_cb = exit_demo_mode_cb
        self.handle_demo_input_cb = handle_demo_input_cb

        # Store button rect definitions locally for collision detection
        # Note: These are visually defined in LeftPanelRenderer, coupling exists.
        self._update_button_rects()  # Initialize rects

    def _update_button_rects(self):
        """Calculates button rects based on initial layout assumptions."""
        # These might need adjustment if the layout in LeftPanelRenderer changes significantly.
        self.train_btn_rect = pygame.Rect(10, 10, 100, 40)
        self.cleanup_btn_rect = pygame.Rect(self.train_btn_rect.right + 10, 10, 160, 40)
        self.demo_btn_rect = pygame.Rect(self.cleanup_btn_rect.right + 10, 10, 120, 40)
        # Confirmation buttons are relative to screen center, calculated dynamically
        sw, sh = self.screen.get_size()
        self.confirm_yes_rect = pygame.Rect(sw // 2 - 110, sh // 2 + 30, 100, 40)
        self.confirm_no_rect = pygame.Rect(sw // 2 + 10, sh // 2 + 30, 100, 40)

    def handle_input(self, app_state: str, cleanup_confirmation_active: bool) -> bool:
        """
        Processes Pygame events based on the current app_state and cleanup overlay status.
        Returns True to continue running, False to exit.
        """
        try:
            mouse_pos = pygame.mouse.get_pos()
        except pygame.error:
            mouse_pos = (0, 0)  # Fallback if display not initialized/error

        # Update confirmation button rects in case of resize
        sw, sh = self.screen.get_size()
        self.confirm_yes_rect.center = (sw // 2 - 60, sh // 2 + 50)
        self.confirm_no_rect.center = (sw // 2 + 60, sh // 2 + 50)

        # Check tooltip hover (only if overlay is not active and in MainMenu)
        # Renderer handles tooltip display, input handler just triggers the check.
        if app_state == "MainMenu" and not cleanup_confirmation_active:
            if hasattr(self.renderer, "check_hover"):
                self.renderer.check_hover(mouse_pos, app_state)

        # --- Event Loop ---
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return self.exit_app_cb()  # Signal exit immediately

            # Handle Window Resizing
            if event.type == pygame.VIDEORESIZE:
                try:
                    # Ensure minimum size
                    new_w, new_h = max(320, event.w), max(240, event.h)
                    self.screen = pygame.display.set_mode(
                        (new_w, new_h), pygame.RESIZABLE
                    )
                    # Update screen references in all UI components that hold one
                    self._update_ui_screen_references(self.screen)
                    # Update local button rects that depend on screen size
                    self._update_button_rects()
                    # Force plotter redraw as its size changed
                    if hasattr(self.renderer, "force_redraw"):
                        self.renderer.force_redraw()
                    print(f"Window resized: {new_w}x{new_h}")
                except pygame.error as e:
                    print(f"Error resizing window: {e}")
                continue  # Skip other event processing for this frame after resize

            # --- State-Specific Input Handling ---

            # 1. Cleanup Confirmation Overlay (Highest Priority)
            if cleanup_confirmation_active:
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                    self.cancel_cleanup_cb()
                elif (
                    event.type == pygame.MOUSEBUTTONDOWN and event.button == 1
                ):  # Left click
                    if self.confirm_yes_rect.collidepoint(mouse_pos):
                        self.confirm_cleanup_cb()
                    elif self.confirm_no_rect.collidepoint(mouse_pos):
                        self.cancel_cleanup_cb()
                # Consume event if overlay is active, preventing other actions
                continue

            # 2. Playing State (Demo Mode)
            elif app_state == "Playing":
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        self.exit_demo_mode_cb()
                    else:
                        # Delegate game control input to specific handler
                        self.handle_demo_input_cb(event)
                # No other input handled in this state currently

            # 3. Main Menu State
            elif app_state == "MainMenu":
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        return self.exit_app_cb()  # Exit App
                    elif event.key == pygame.K_p:
                        self.toggle_training_cb()  # Toggle Training

                if (
                    event.type == pygame.MOUSEBUTTONDOWN and event.button == 1
                ):  # Left click
                    if self.train_btn_rect.collidepoint(mouse_pos):
                        self.toggle_training_cb()
                    elif self.cleanup_btn_rect.collidepoint(mouse_pos):
                        self.request_cleanup_cb()  # Show confirmation
                    elif self.demo_btn_rect.collidepoint(mouse_pos):
                        self.start_demo_mode_cb()
                    # No action for clicking elsewhere in main menu

            # 4. Error State
            elif app_state == "Error":
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                    return self.exit_app_cb()  # Allow exit from error state
                # No other input handled in error state

            # 5. Other States (e.g., Initializing, Cleaning) - Ignore input for now
            # else: pass

        return True  # Continue running

    def _update_ui_screen_references(self, new_screen: pygame.Surface):
        """Recursively updates the screen reference in the renderer and its sub-components."""
        # List of components known to hold a screen reference
        components_to_update = [
            self.renderer,
            getattr(self.renderer, "left_panel", None),
            getattr(self.renderer, "game_area", None),
            getattr(self.renderer, "overlays", None),
            getattr(self.renderer, "tooltips", None),
            getattr(
                self.renderer, "demo_renderer", None
            ),  # Update new demo renderer too
        ]
        for component in components_to_update:
            if component and hasattr(component, "screen"):
                component.screen = new_screen


File: ui/panels/__init__.py
from .left_panel import LeftPanelRenderer
from .game_area import GameAreaRenderer

__all__ = ["LeftPanelRenderer", "GameAreaRenderer"]


File: ui/panels/game_area.py
# File: ui/panels/game_area.py
import pygame
import math
import traceback
from typing import List, Tuple
from config import VisConfig, EnvConfig
from environment.game_state import GameState
from environment.shape import Shape
from environment.triangle import Triangle
import numpy as np


class GameAreaRenderer:
    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.fonts = self._init_fonts()

    def _init_fonts(self):
        fonts = {}
        try:
            fonts["env_score"] = pygame.font.SysFont(None, 18)
            fonts["env_overlay"] = pygame.font.SysFont(None, 36)
            fonts["ui"] = pygame.font.SysFont(None, 24)
        except Exception as e:
            print(f"Warning: SysFont error: {e}. Using default.")
            fonts["env_score"] = pygame.font.Font(None, 18)
            fonts["env_overlay"] = pygame.font.Font(None, 36)
            fonts["ui"] = pygame.font.Font(None, 24)
        return fonts

    def render(self, envs: List[GameState], num_envs: int, env_config: EnvConfig):
        current_width, current_height = self.screen.get_size()
        lp_width = min(current_width, max(300, self.vis_config.LEFT_PANEL_WIDTH))
        ga_rect = pygame.Rect(lp_width, 0, current_width - lp_width, current_height)

        if num_envs <= 0 or ga_rect.width <= 0 or ga_rect.height <= 0:
            return

        render_limit = self.vis_config.NUM_ENVS_TO_RENDER
        num_to_render = num_envs if render_limit <= 0 else min(num_envs, render_limit)
        if num_to_render <= 0:
            return

        cols_env, rows_env, cell_w, cell_h = self._calculate_grid_layout(
            ga_rect, num_to_render
        )

        min_cell_dim = 30
        if cell_w > min_cell_dim and cell_h > min_cell_dim:
            self._render_env_grid(
                envs,
                num_to_render,
                env_config,
                ga_rect,
                cols_env,
                rows_env,
                cell_w,
                cell_h,
            )
        else:
            self._render_too_small_message(ga_rect, cell_w, cell_h)

        if num_to_render < num_envs:
            self._render_render_limit_text(ga_rect, num_to_render, num_envs)

    def _calculate_grid_layout(
        self, ga_rect: pygame.Rect, num_to_render: int
    ) -> Tuple[int, int, int, int]:
        aspect_ratio = ga_rect.width / max(1, ga_rect.height)
        cols_env = max(1, int(math.sqrt(num_to_render * aspect_ratio)))
        rows_env = max(1, math.ceil(num_to_render / cols_env))
        total_spacing_w = (cols_env + 1) * self.vis_config.ENV_SPACING
        total_spacing_h = (rows_env + 1) * self.vis_config.ENV_SPACING
        cell_w = max(1, (ga_rect.width - total_spacing_w) // cols_env)
        cell_h = max(1, (ga_rect.height - total_spacing_h) // rows_env)
        return cols_env, rows_env, cell_w, cell_h

    def _render_env_grid(
        self, envs, num_to_render, env_config, ga_rect, cols, rows, cell_w, cell_h
    ):
        env_idx = 0
        for r in range(rows):
            for c in range(cols):
                if env_idx >= num_to_render:
                    break
                env_x = ga_rect.x + self.vis_config.ENV_SPACING * (c + 1) + c * cell_w
                env_y = ga_rect.y + self.vis_config.ENV_SPACING * (r + 1) + r * cell_h
                env_rect = pygame.Rect(env_x, env_y, cell_w, cell_h)

                clipped_env_rect = env_rect.clip(self.screen.get_rect())
                if clipped_env_rect.width <= 0 or clipped_env_rect.height <= 0:
                    env_idx += 1
                    continue

                try:
                    sub_surf = self.screen.subsurface(clipped_env_rect)
                    self._render_single_env(sub_surf, envs[env_idx], env_config)

                except ValueError as subsurface_error:
                    print(
                        f"Warning: Subsurface error env {env_idx} ({clipped_env_rect}): {subsurface_error}"
                    )
                    pygame.draw.rect(self.screen, (0, 0, 50), clipped_env_rect, 1)
                except Exception as e_render_env:
                    print(f"Error rendering env {env_idx}: {e_render_env}")
                    traceback.print_exc()
                    pygame.draw.rect(self.screen, (50, 0, 50), clipped_env_rect, 1)

                env_idx += 1

    def _render_single_env(
        self, surf: pygame.Surface, env: GameState, env_config: EnvConfig
    ):
        cell_w = surf.get_width()
        cell_h = surf.get_height()
        if cell_w <= 0 or cell_h <= 0:
            return

        # --- Background Color Logic (Order Matters!) ---
        bg_color = VisConfig.GRAY  # Default
        if env.is_line_clearing():  # Line clear flash has highest priority
            bg_color = VisConfig.LINE_CLEAR_FLASH_COLOR
        elif env.is_game_over_flashing():  # Game over flash is next
            bg_color = VisConfig.GAME_OVER_FLASH_COLOR
        elif env.is_blinking():  # Generic blink (currently unused but kept)
            bg_color = VisConfig.YELLOW
        elif env.is_over():  # Static game over (no flash)
            bg_color = VisConfig.DARK_RED
        elif env.is_frozen():  # Frozen Blue (only if not game over/line clearing)
            bg_color = (30, 30, 100)
        surf.fill(bg_color)

        shape_area_height_ratio = 0.20
        grid_area_height = math.floor(cell_h * (1.0 - shape_area_height_ratio))
        shape_area_height = cell_h - grid_area_height
        shape_area_y = grid_area_height

        grid_surf = None
        shape_surf = None
        if grid_area_height > 0 and cell_w > 0:
            try:
                grid_rect = pygame.Rect(0, 0, cell_w, grid_area_height)
                grid_surf = surf.subsurface(grid_rect)
            except ValueError as e:
                print(f"Warning: Grid subsurface error ({grid_rect}): {e}")
                pygame.draw.rect(surf, VisConfig.RED, grid_rect, 1)

        if shape_area_height > 0 and cell_w > 0:
            try:
                shape_rect = pygame.Rect(0, shape_area_y, cell_w, shape_area_height)
                shape_surf = surf.subsurface(shape_rect)
                shape_surf.fill((35, 35, 35))
            except ValueError as e:
                print(f"Warning: Shape subsurface error ({shape_rect}): {e}")
                pygame.draw.rect(surf, VisConfig.RED, shape_rect, 1)

        if grid_surf:
            self._render_single_env_grid(grid_surf, env, env_config)

        if shape_surf:
            self._render_shape_previews(shape_surf, env)

        try:
            score_surf = self.fonts["env_score"].render(
                f"GS: {env.game_score} R: {env.score:.1f}",
                True,
                VisConfig.WHITE,
                (0, 0, 0, 180),
            )
            surf.blit(score_surf, (2, 2))
        except Exception as e:
            print(f"Error rendering score: {e}")

        # --- MODIFIED: Specific Overlay Logic ---
        if env.is_over():
            # Always show GAME OVER text if the game is over
            self._render_overlay_text(surf, "GAME OVER", VisConfig.RED)
        elif env.is_line_clearing():
            # Show Line Clear! text only during the line clear flash
            self._render_overlay_text(surf, "Line Clear!", VisConfig.BLUE)
        # No generic "Frozen" message anymore
        # --- END MODIFIED ---

    def _render_overlay_text(
        self, surf: pygame.Surface, text: str, color: Tuple[int, int, int]
    ):
        try:
            overlay_font = self.fonts["env_overlay"]
            text_surf = overlay_font.render(
                text,
                True,
                VisConfig.WHITE,
                (color[0] // 2, color[1] // 2, color[2] // 2, 220),
            )
            text_rect = text_surf.get_rect(center=surf.get_rect().center)
            surf.blit(text_surf, text_rect)
        except Exception as e:
            print(f"Error rendering overlay text '{text}': {e}")

    def _render_single_env_grid(
        self, surf: pygame.Surface, env: GameState, env_config: EnvConfig
    ):
        try:
            padding = self.vis_config.ENV_GRID_PADDING
            drawable_w = max(1, surf.get_width() - 2 * padding)
            drawable_h = max(1, surf.get_height() - 2 * padding)

            grid_rows = env_config.ROWS
            grid_cols_effective_width = env_config.COLS * 0.75 + 0.25

            if grid_rows <= 0 or grid_cols_effective_width <= 0:
                return

            scale_w_based = drawable_w / grid_cols_effective_width
            scale_h_based = drawable_h / grid_rows
            final_scale = min(scale_w_based, scale_h_based)
            if final_scale <= 0:
                return

            final_grid_pixel_w = grid_cols_effective_width * final_scale
            final_grid_pixel_h = grid_rows * final_scale
            tri_cell_h = max(1, final_scale)
            tri_cell_w = max(1, final_scale)

            grid_ox = padding + (drawable_w - final_grid_pixel_w) / 2
            grid_oy = padding + (drawable_h - final_grid_pixel_h) / 2

            is_highlighting = env.is_highlighting_cleared()
            cleared_coords = (
                set(env.get_cleared_triangle_coords()) if is_highlighting else set()
            )
            highlight_color = self.vis_config.LINE_CLEAR_HIGHLIGHT_COLOR

            if hasattr(env, "grid") and hasattr(env.grid, "triangles"):
                for r in range(env.grid.rows):
                    for c in range(env.grid.cols):
                        if not (
                            0 <= r < len(env.grid.triangles)
                            and 0 <= c < len(env.grid.triangles[r])
                        ):
                            continue
                        t = env.grid.triangles[r][c]
                        if not t.is_death:
                            if not hasattr(t, "get_points"):
                                continue
                            try:
                                pts = t.get_points(
                                    ox=grid_ox,
                                    oy=grid_oy,
                                    cw=int(tri_cell_w),
                                    ch=int(tri_cell_h),
                                )
                                if is_highlighting and (r, c) in cleared_coords:
                                    color = highlight_color
                                elif t.is_occupied:
                                    color = t.color if t.color else VisConfig.RED
                                else:
                                    color = VisConfig.LIGHTG

                                pygame.draw.polygon(surf, color, pts)
                                pygame.draw.polygon(surf, VisConfig.GRAY, pts, 1)
                            except Exception as e_render:
                                pass
            else:
                pygame.draw.rect(surf, VisConfig.RED, surf.get_rect(), 2)
                err_txt = self.fonts["ui"].render(
                    "Invalid Grid Data", True, VisConfig.RED
                )
                surf.blit(err_txt, err_txt.get_rect(center=surf.get_rect().center))

        except Exception as e:
            print(f"Unexpected Render Error in _render_single_env_grid: {e}")
            traceback.print_exc()
            pygame.draw.rect(surf, VisConfig.RED, surf.get_rect(), 2)

    def _render_shape_previews(self, surf: pygame.Surface, env: GameState):
        available_shapes = env.get_shapes()
        if not available_shapes:
            return

        surf_w = surf.get_width()
        surf_h = surf.get_height()
        if surf_w <= 0 or surf_h <= 0:
            return

        num_shapes = len(available_shapes)
        padding = 4
        total_padding_needed = (num_shapes + 1) * padding
        available_width_for_shapes = surf_w - total_padding_needed

        if available_width_for_shapes <= 0:
            return

        width_per_shape = available_width_for_shapes / num_shapes
        height_limit = surf_h - 2 * padding
        preview_dim = max(5, min(width_per_shape, height_limit))

        start_x = (
            padding
            + (surf_w - (num_shapes * preview_dim + (num_shapes - 1) * padding)) / 2
        )
        start_y = padding + (surf_h - preview_dim) / 2

        current_x = start_x
        for shape in available_shapes:
            preview_rect = pygame.Rect(current_x, start_y, preview_dim, preview_dim)
            if preview_rect.right > surf_w - padding:
                break

            try:
                temp_shape_surf = pygame.Surface(
                    (preview_dim, preview_dim), pygame.SRCALPHA
                )
                temp_shape_surf.fill((0, 0, 0, 0))

                min_r, min_c, max_r, max_c = shape.bbox()
                shape_h_cells = max(1, max_r - min_r + 1)
                shape_w_cells_eff = max(1, (max_c - min_c + 1) * 0.75 + 0.25)

                scale_h = preview_dim / shape_h_cells
                scale_w = preview_dim / shape_w_cells_eff
                cell_size = max(1, min(scale_h, scale_w))

                self._render_single_shape(temp_shape_surf, shape, int(cell_size))

                surf.blit(temp_shape_surf, preview_rect.topleft)
                current_x += preview_dim + padding

            except Exception as e:
                print(f"Error rendering shape preview: {e}")
                pygame.draw.rect(surf, VisConfig.RED, preview_rect, 1)
                current_x += preview_dim + padding

    def _render_single_shape(self, surf: pygame.Surface, shape: Shape, cell_size: int):
        if not shape or not shape.triangles or cell_size <= 0:
            return
        min_r, min_c, max_r, max_c = shape.bbox()
        shape_h_cells = max_r - min_r + 1
        shape_w_cells_eff = (max_c - min_c + 1) * 0.75 + 0.25
        if shape_w_cells_eff <= 0 or shape_h_cells <= 0:
            return

        total_w_pixels = shape_w_cells_eff * cell_size
        total_h_pixels = shape_h_cells * cell_size

        offset_x = (surf.get_width() - total_w_pixels) / 2 - min_c * (cell_size * 0.75)
        offset_y = (surf.get_height() - total_h_pixels) / 2 - min_r * cell_size

        for dr, dc, up in shape.triangles:
            tri = Triangle(row=dr, col=dc, is_up=up)
            try:
                pts = tri.get_points(
                    ox=offset_x, oy=offset_y, cw=cell_size, ch=cell_size
                )
                pygame.draw.polygon(surf, shape.color, pts)
            except Exception as e:
                print(f"Warning: Error rendering shape preview tri ({dr},{dc}): {e}")

    def _render_too_small_message(self, ga_rect: pygame.Rect, cell_w: int, cell_h: int):
        try:
            err_surf = self.fonts["ui"].render(
                f"Envs Too Small ({cell_w}x{cell_h})", True, VisConfig.GRAY
            )
            self.screen.blit(err_surf, err_surf.get_rect(center=ga_rect.center))
        except Exception as e:
            print(f"Error rendering 'too small' message: {e}")

    def _render_render_limit_text(
        self, ga_rect: pygame.Rect, num_rendered: int, num_total: int
    ):
        try:
            info_surf = self.fonts["ui"].render(
                f"Rendering {num_rendered}/{num_total} Envs",
                True,
                VisConfig.YELLOW,
                VisConfig.BLACK,
            )
            self.screen.blit(
                info_surf,
                info_surf.get_rect(bottomright=(ga_rect.right - 5, ga_rect.bottom - 5)),
            )
        except Exception as e:
            print(f"Error rendering limit text: {e}")


File: ui/panels/left_panel.py
# File: ui/panels/left_panel.py
import pygame
import os
import time
from typing import Dict, Any, Optional, Deque, Tuple

from config import (
    VisConfig,
    BufferConfig,
    StatsConfig,
    DQNConfig,
    DEVICE,
    TensorBoardConfig,
)
from config.general import TOTAL_TRAINING_STEPS
from ui.plotter import Plotter

from .left_panel_components import (
    ButtonStatusRenderer,
    NotificationRenderer,
    InfoTextRenderer,
    TBStatusRenderer,
    PlotAreaRenderer,
)

# --- MODIFIED: Update tooltip text ---
TOOLTIP_TEXTS = {
    "Status": "Current state: Paused, Buffering, Training, Confirm Cleanup, Cleaning, or Error.",
    "Global Steps": "Total environment steps taken / Total planned steps.",
    "Total Episodes": "Total completed episodes across all environments.",
    # Removed specific window size mention
    "Steps/Sec (Current)": "Current avg Steps/Sec. See plot for history and different averaging windows.",
    "Buffer Fill": f"Current replay buffer fill % ({BufferConfig.REPLAY_BUFFER_SIZE / 1e6:.1f}M cap). See plot.",
    "PER Beta": f"Current PER IS exponent ({BufferConfig.PER_BETA_START:.1f}->1.0). See plot.",
    "Learning Rate": "Current learning rate. See plot for history/schedule.",
    "Train Button": "Click to Start/Pause training (or press 'P').",
    "Cleanup Button": "Click to DELETE agent ckpt & buffer for CURRENT run ONLY, then re-init.",
    "Play Demo Button": "Click to enter interactive play mode. Gameplay fills the replay buffer.",
    "Device": f"Computation device detected ({DEVICE.type.upper()}).",
    "Network": f"CNN+MLP Fusion. Dueling={DQNConfig.USE_DUELING}, Noisy={DQNConfig.USE_NOISY_NETS}, C51={DQNConfig.USE_DISTRIBUTIONAL}",
    "TensorBoard Status": "Indicates TB logging status and log directory.",
    "Notification Area": "Displays the latest best achievements (RL Score, Game Score, Loss).",
    "Best RL Score Info": "Best RL Score achieved: Current Value (Previous Value) - Steps Ago",
    "Best Game Score Info": "Best Game Score achieved: Current Value (Previous Value) - Steps Ago",
    "Best Loss Info": "Best (Lowest) Loss achieved: Current Value (Previous Value) - Steps Ago",
}
# --- END MODIFIED ---


class LeftPanelRenderer:
    """Orchestrates rendering of the left panel using sub-components."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig, plotter: Plotter):
        self.screen = screen
        self.vis_config = vis_config
        self.plotter = plotter
        self.fonts = self._init_fonts()
        self.stat_rects: Dict[str, pygame.Rect] = {}

        self.button_status_renderer = ButtonStatusRenderer(self.screen, self.fonts)
        self.notification_renderer = NotificationRenderer(self.screen, self.fonts)
        self.info_text_renderer = InfoTextRenderer(self.screen, self.fonts)
        self.tb_status_renderer = TBStatusRenderer(self.screen, self.fonts)
        self.plot_area_renderer = PlotAreaRenderer(
            self.screen, self.fonts, self.plotter
        )

    def _init_fonts(self):
        fonts = {}
        font_configs = {
            "ui": 24,
            "status": 28,
            "logdir": 16,
            "plot_placeholder": 20,
            "notification": 19,
            "notification_label": 16,
        }
        for key, size in font_configs.items():
            try:
                fonts[key] = pygame.font.SysFont(None, size)
            except Exception:
                fonts[key] = pygame.font.Font(None, size)
            if fonts[key] is None:
                print(f"ERROR: Font '{key}' failed to load.")
        return fonts

    def render(
        self,
        is_training: bool,
        status: str,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
        app_state: str,
    ):
        current_width, current_height = self.screen.get_size()
        lp_width = min(current_width, max(300, self.vis_config.LEFT_PANEL_WIDTH))
        lp_rect = pygame.Rect(0, 0, lp_width, current_height)

        status_color_map = {
            "Paused": (30, 30, 30),
            "Buffering": (30, 40, 30),
            "Training": (40, 30, 30),
            "Confirm Cleanup": (50, 20, 20),
            "Cleaning": (60, 30, 30),
            "Error": (60, 0, 0),
            "Playing Demo": (30, 30, 40),
            "Initializing": (40, 40, 40),
        }
        bg_color = status_color_map.get(status, (30, 30, 30))
        pygame.draw.rect(self.screen, bg_color, lp_rect)
        self.stat_rects.clear()

        current_y = 10
        notification_area_rect = None

        next_y, rects_bs, notification_area_rect = self.button_status_renderer.render(
            current_y, lp_width, app_state, is_training, status
        )
        self.stat_rects.update(rects_bs)
        current_y = next_y

        if notification_area_rect:
            rects_notif = self.notification_renderer.render(
                notification_area_rect, stats_summary
            )
            self.stat_rects.update(rects_notif)

        next_y, rects_info = self.info_text_renderer.render(
            current_y, stats_summary, buffer_capacity, lp_width
        )
        self.stat_rects.update(rects_info)
        current_y = next_y

        next_y, rects_tb = self.tb_status_renderer.render(
            current_y + 10, tensorboard_log_dir, lp_width
        )
        self.stat_rects.update(rects_tb)
        current_y = next_y

        self.plot_area_renderer.render(
            current_y + 15, lp_width, current_height, plot_data, status
        )

    def get_stat_rects(self) -> Dict[str, pygame.Rect]:
        return self.stat_rects.copy()

    def get_tooltip_texts(self) -> Dict[str, str]:
        return TOOLTIP_TEXTS


File: ui/panels/left_panel_components/plot_area_renderer.py
# File: ui/panels/left_panel_components/plot_area_renderer.py
import pygame
from typing import Dict, Deque, Optional
from config import VisConfig
from ui.plotter import Plotter  # Import Plotter


class PlotAreaRenderer:
    """Renders the plot area using a Plotter instance."""

    def __init__(
        self,
        screen: pygame.Surface,
        fonts: Dict[str, pygame.font.Font],
        plotter: Plotter,  # Pass plotter instance
    ):
        self.screen = screen
        self.fonts = fonts
        self.plotter = plotter

    def render(
        self,
        y_start: int,
        panel_width: int,
        screen_height: int,
        plot_data: Dict[str, Deque],
        status: str,
    ):
        """Renders the plot area."""
        plot_area_height = screen_height - y_start - 10
        plot_area_width = panel_width - 20

        if plot_area_width <= 50 or plot_area_height <= 50:
            return

        plot_surface = self.plotter.get_cached_or_updated_plot(
            plot_data, plot_area_width, plot_area_height
        )
        plot_area_rect = pygame.Rect(10, y_start, plot_area_width, plot_area_height)

        if plot_surface:
            self.screen.blit(plot_surface, plot_area_rect.topleft)
        else:
            # Render placeholder
            pygame.draw.rect(self.screen, (40, 40, 40), plot_area_rect, 1)
            placeholder_text = "Waiting for data..."
            if status == "Buffering":
                placeholder_text = "Buffering... Waiting for plot data..."
            elif status == "Error":
                placeholder_text = "Plotting disabled due to error."
            elif not plot_data or not any(plot_data.values()):
                placeholder_text = "No plot data yet..."

            placeholder_font = self.fonts.get("plot_placeholder")
            if placeholder_font:
                placeholder_surf = placeholder_font.render(
                    placeholder_text, True, VisConfig.GRAY
                )
                placeholder_rect = placeholder_surf.get_rect(
                    center=plot_area_rect.center
                )
                blit_pos = (
                    max(plot_area_rect.left, placeholder_rect.left),
                    max(plot_area_rect.top, placeholder_rect.top),
                )
                clip_area_rect = plot_area_rect.clip(placeholder_rect)
                blit_area = clip_area_rect.move(
                    -placeholder_rect.left, -placeholder_rect.top
                )
                if blit_area.width > 0 and blit_area.height > 0:
                    self.screen.blit(placeholder_surf, blit_pos, area=blit_area)
            else:  # Fallback cross
                pygame.draw.line(
                    self.screen,
                    VisConfig.GRAY,
                    plot_area_rect.topleft,
                    plot_area_rect.bottomright,
                )
                pygame.draw.line(
                    self.screen,
                    VisConfig.GRAY,
                    plot_area_rect.topright,
                    plot_area_rect.bottomleft,
                )


File: ui/panels/left_panel_components/notification_renderer.py
# File: ui/panels/left_panel_components/notification_renderer.py
import pygame
import time
from typing import Dict, Any, Tuple
from config import VisConfig, StatsConfig
import numpy as np


class NotificationRenderer:
    """Renders the notification area with best scores/loss."""

    def __init__(self, screen: pygame.Surface, fonts: Dict[str, pygame.font.Font]):
        self.screen = screen
        self.fonts = fonts

    def _format_steps_ago(self, current_step: int, best_step: int) -> str:
        """Formats the difference in steps into a readable string."""
        if best_step <= 0 or current_step <= best_step:
            return "Now"
        diff = current_step - best_step
        if diff < 1000:
            return f"{diff} steps ago"
        elif diff < 1_000_000:
            return f"{diff / 1000:.1f}k steps ago"
        else:
            return f"{diff / 1_000_000:.1f}M steps ago"

    def _render_line(
        self,
        area_rect: pygame.Rect,
        y_pos: int,
        label: str,
        current_val: Any,
        prev_val: Any,
        best_step: int,
        val_format: str,
        current_step: int,
    ) -> pygame.Rect:
        """Renders a single line within the notification area."""
        label_font = self.fonts.get("notification_label")
        value_font = self.fonts.get("notification")
        if not label_font or not value_font:
            return pygame.Rect(0, y_pos, 0, 0)

        padding = 5
        label_color, value_color = VisConfig.LIGHTG, VisConfig.WHITE
        prev_color, time_color = VisConfig.GRAY, (180, 180, 100)

        label_surf = label_font.render(label, True, label_color)
        label_rect = label_surf.get_rect(topleft=(area_rect.left + padding, y_pos))
        self.screen.blit(label_surf, label_rect)
        current_x = label_rect.right + 4

        current_val_str = "N/A"
        # --- Convert to float *before* checking ---
        val_as_float: Optional[float] = None
        if isinstance(
            current_val, (int, float, np.number)
        ):  # Check against np.number too
            try:
                val_as_float = float(current_val)
            except (ValueError, TypeError):
                val_as_float = None  # Conversion failed

        # --- Use the converted float for checks and formatting ---
        if val_as_float is not None and np.isfinite(val_as_float):
            try:
                current_val_str = val_format.format(val_as_float)
            except (ValueError, TypeError) as fmt_err:
                current_val_str = "ErrFmt"

        val_surf = value_font.render(current_val_str, True, value_color)
        val_rect = val_surf.get_rect(topleft=(current_x, y_pos))
        self.screen.blit(val_surf, val_rect)
        current_x = val_rect.right + 4

        prev_val_str = "(N/A)"
        # --- Convert prev_val to float before checking ---
        prev_val_as_float: Optional[float] = None
        if isinstance(prev_val, (int, float, np.number)):
            try:
                prev_val_as_float = float(prev_val)
            except (ValueError, TypeError):
                prev_val_as_float = None

        if prev_val_as_float is not None and np.isfinite(prev_val_as_float):
            try:
                prev_val_str = f"({val_format.format(prev_val_as_float)})"
            except (ValueError, TypeError):
                prev_val_str = "(ErrFmt)"

        prev_surf = label_font.render(prev_val_str, True, prev_color)
        prev_rect = prev_surf.get_rect(topleft=(current_x, y_pos + 1))
        self.screen.blit(prev_surf, prev_rect)
        current_x = prev_rect.right + 6

        steps_ago_str = self._format_steps_ago(current_step, best_step)
        time_surf = label_font.render(steps_ago_str, True, time_color)
        time_rect = time_surf.get_rect(topleft=(current_x, y_pos + 1))

        available_width = area_rect.right - time_rect.left - padding
        clip_rect = pygame.Rect(0, 0, max(0, available_width), time_rect.height)
        if time_rect.width > available_width > 0:
            self.screen.blit(time_surf, time_rect, area=clip_rect)
        elif available_width > 0:
            self.screen.blit(time_surf, time_rect)

        union_rect = label_rect.union(val_rect).union(prev_rect).union(time_rect)
        union_rect.width = min(union_rect.width, area_rect.width - 2 * padding)
        return union_rect

    def render(
        self, area_rect: pygame.Rect, stats_summary: Dict[str, Any]
    ) -> Dict[str, pygame.Rect]:
        """Renders the notification content."""
        stat_rects: Dict[str, pygame.Rect] = {}
        pygame.draw.rect(self.screen, (45, 45, 45), area_rect, border_radius=3)
        pygame.draw.rect(self.screen, VisConfig.LIGHTG, area_rect, 1, border_radius=3)
        stat_rects["Notification Area"] = area_rect

        value_font = self.fonts.get("notification")
        if not value_font:
            return stat_rects

        padding = 5
        line_height = value_font.get_linesize()
        current_step = stats_summary.get("global_step", 0)
        y = area_rect.top + padding

        # --- Pass values from summary, _render_line handles conversion/check ---
        rect_rl = self._render_line(
            area_rect,
            y,
            "RL Score:",
            stats_summary.get("best_score", -float("inf")),
            stats_summary.get("previous_best_score", -float("inf")),
            stats_summary.get("best_score_step", 0),
            "{:.2f}",
            current_step,
        )
        stat_rects["Best RL Score Info"] = rect_rl.clip(area_rect)
        y += line_height

        rect_game = self._render_line(
            area_rect,
            y,
            "Game Score:",
            stats_summary.get("best_game_score", -float("inf")),
            stats_summary.get("previous_best_game_score", -float("inf")),
            stats_summary.get("best_game_score_step", 0),
            "{:.0f}",
            current_step,
        )
        stat_rects["Best Game Score Info"] = rect_game.clip(area_rect)
        y += line_height

        rect_loss = self._render_line(
            area_rect,
            y,
            "Loss:",
            stats_summary.get("best_loss", float("inf")),
            stats_summary.get("previous_best_loss", float("inf")),
            stats_summary.get("best_loss_step", 0),
            "{:.4f}",
            current_step,
        )
        stat_rects["Best Loss Info"] = rect_loss.clip(area_rect)

        return stat_rects


File: ui/panels/left_panel_components/__init__.py
# File: ui/panels/left_panel_components/__init__.py
from .button_status_renderer import ButtonStatusRenderer
from .notification_renderer import NotificationRenderer
from .info_text_renderer import InfoTextRenderer
from .tb_status_renderer import TBStatusRenderer
from .plot_area_renderer import PlotAreaRenderer

__all__ = [
    "ButtonStatusRenderer",
    "NotificationRenderer",
    "InfoTextRenderer",
    "TBStatusRenderer",
    "PlotAreaRenderer",
]


File: ui/panels/left_panel_components/tb_status_renderer.py
# File: ui/panels/left_panel_components/tb_status_renderer.py
import pygame
import os
from typing import Dict, Optional, Tuple
from config import VisConfig, TensorBoardConfig


class TBStatusRenderer:
    """Renders the TensorBoard status line."""

    def __init__(self, screen: pygame.Surface, fonts: Dict[str, pygame.font.Font]):
        self.screen = screen
        self.fonts = fonts

    def _shorten_path(self, path: str, max_chars: int) -> str:
        """Attempts to shorten a path string for display."""
        if len(path) <= max_chars:
            return path
        try:
            rel_path = os.path.relpath(path)
        except ValueError:  # Handle different drives on Windows
            rel_path = path
        if len(rel_path) <= max_chars:
            return rel_path

        parts = path.replace("\\", "/").split("/")
        if len(parts) >= 2:
            short_path = os.path.join("...", *parts[-2:])
            if len(short_path) <= max_chars:
                return short_path
        # Fallback: Ellipsis + end of basename
        basename = os.path.basename(path)
        return (
            "..." + basename[-(max_chars - 3) :]
            if len(basename) > max_chars - 3
            else basename
        )

    def render(
        self, y_start: int, log_dir: Optional[str], panel_width: int
    ) -> Tuple[int, Dict[str, pygame.Rect]]:
        """Renders the TB status. Returns next_y and stat_rects."""
        stat_rects: Dict[str, pygame.Rect] = {}
        ui_font = self.fonts.get("ui")
        logdir_font = self.fonts.get("logdir")
        if not ui_font or not logdir_font:
            return y_start + 30, stat_rects

        tb_active = (
            TensorBoardConfig.LOG_HISTOGRAMS
            or TensorBoardConfig.LOG_IMAGES
            or TensorBoardConfig.LOG_SHAPE_PLACEMENT_Q_VALUES
        )
        tb_color = VisConfig.GOOGLE_COLORS[0] if tb_active else VisConfig.GRAY
        tb_text = f"TensorBoard: {'Logging Active' if tb_active else 'Logging Minimal'}"

        tb_surf = ui_font.render(tb_text, True, tb_color)
        tb_rect = tb_surf.get_rect(topleft=(10, y_start))
        self.screen.blit(tb_surf, tb_rect)
        stat_rects["TensorBoard Status"] = tb_rect
        last_y = tb_rect.bottom

        if log_dir:
            try:
                panel_char_width = max(
                    10, panel_width // max(1, logdir_font.size("A")[0])
                )
                short_log_dir = self._shorten_path(log_dir, panel_char_width)
            except Exception:
                short_log_dir = os.path.basename(log_dir)

            dir_surf = logdir_font.render(
                f"Log Dir: {short_log_dir}", True, VisConfig.LIGHTG
            )
            dir_rect = dir_surf.get_rect(topleft=(10, tb_rect.bottom + 2))

            clip_width = max(0, panel_width - dir_rect.left - 10)
            if dir_rect.width > clip_width:
                self.screen.blit(
                    dir_surf,
                    dir_rect,
                    area=pygame.Rect(0, 0, clip_width, dir_rect.height),
                )
            else:
                self.screen.blit(dir_surf, dir_rect)

            combined_tb_rect = tb_rect.union(dir_rect)
            combined_tb_rect.width = min(
                combined_tb_rect.width, panel_width - 10 - combined_tb_rect.left
            )
            stat_rects["TensorBoard Status"] = combined_tb_rect
            last_y = dir_rect.bottom

        return last_y, stat_rects


File: ui/panels/left_panel_components/button_status_renderer.py
# File: ui/panels/left_panel_components/button_status_renderer.py
import pygame
from typing import Dict, Tuple, Optional
from config import VisConfig


class ButtonStatusRenderer:
    """Renders the top buttons and status line in the left panel."""

    def __init__(self, screen: pygame.Surface, fonts: Dict[str, pygame.font.Font]):
        self.screen = screen
        self.fonts = fonts

    def _draw_button(self, rect: pygame.Rect, text: str, color: Tuple[int, int, int]):
        """Helper to draw a single button."""
        pygame.draw.rect(self.screen, color, rect, border_radius=5)
        ui_font = self.fonts.get("ui")
        if ui_font:
            lbl_surf = ui_font.render(text, True, VisConfig.WHITE)
            self.screen.blit(lbl_surf, lbl_surf.get_rect(center=rect.center))
        else:
            pygame.draw.line(
                self.screen, VisConfig.RED, rect.topleft, rect.bottomright, 2
            )
            pygame.draw.line(
                self.screen, VisConfig.RED, rect.topright, rect.bottomleft, 2
            )

    def render(
        self,
        y_start: int,
        panel_width: int,
        app_state: str,
        is_training: bool,
        status: str,
    ) -> Tuple[int, Dict[str, pygame.Rect], Optional[pygame.Rect]]:
        """Renders buttons and status. Returns next_y, stat_rects, notification_rect."""
        stat_rects: Dict[str, pygame.Rect] = {}
        notification_rect = None
        next_y = y_start

        if app_state == "MainMenu":
            train_btn_rect = pygame.Rect(10, y_start, 100, 40)
            cleanup_btn_rect = pygame.Rect(train_btn_rect.right + 10, y_start, 160, 40)
            demo_btn_rect = pygame.Rect(cleanup_btn_rect.right + 10, y_start, 120, 40)

            self._draw_button(
                train_btn_rect,
                "Pause" if is_training and status == "Training" else "Train",
                (70, 70, 70),
            )
            self._draw_button(cleanup_btn_rect, "Cleanup This Run", (100, 40, 40))
            self._draw_button(demo_btn_rect, "Play Demo", (40, 100, 40))

            stat_rects["Train Button"] = train_btn_rect
            stat_rects["Cleanup Button"] = cleanup_btn_rect
            stat_rects["Play Demo Button"] = demo_btn_rect

            notification_x = demo_btn_rect.right + 15
            notification_w = panel_width - notification_x - 10
            notif_font = self.fonts.get("notification")
            if notification_w > 50 and notif_font:
                line_h = notif_font.get_linesize()
                notification_h = line_h * 3 + 12
                notification_rect = pygame.Rect(
                    notification_x, y_start, notification_w, notification_h
                )

            next_y = train_btn_rect.bottom + 10
        # --- Status Text ---
        status_text = f"Status: {status}"
        if app_state == "Playing":
            status_text = "Status: Playing Demo"
        elif app_state != "MainMenu":
            status_text = f"Status: {app_state}"

        status_font = self.fonts.get("status")
        if status_font:
            status_surf = status_font.render(status_text, True, VisConfig.YELLOW)
            status_rect_top = next_y if app_state == "MainMenu" else y_start
            status_rect = status_surf.get_rect(topleft=(10, status_rect_top))
            self.screen.blit(status_surf, status_rect)
            if app_state == "MainMenu":
                stat_rects["Status"] = status_rect
            next_y = status_rect.bottom + 5
        else:
            next_y += 20

        return next_y, stat_rects, notification_rect


File: ui/panels/left_panel_components/info_text_renderer.py
# File: ui/panels/left_panel_components/info_text_renderer.py
import pygame
from typing import Dict, Any, Tuple
from config import (
    VisConfig,
    BufferConfig,
    StatsConfig,
    DQNConfig,
    DEVICE,
    TOTAL_TRAINING_STEPS,
)


class InfoTextRenderer:
    """Renders the main block of statistics text."""

    def __init__(self, screen: pygame.Surface, fonts: Dict[str, pygame.font.Font]):
        self.screen = screen
        self.fonts = fonts

    def render(
        self,
        y_start: int,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        panel_width: int,
    ) -> Tuple[int, Dict[str, pygame.Rect]]:
        """Renders the info text block. Returns next_y and stat_rects."""
        stat_rects: Dict[str, pygame.Rect] = {}
        ui_font = self.fonts.get("ui")
        if not ui_font:
            return y_start + 100, stat_rects

        line_height = ui_font.get_linesize()
        buffer_size = stats_summary.get("buffer_size", 0)
        buffer_perc = (
            (buffer_size / max(1, buffer_capacity) * 100)
            if buffer_capacity > 0
            else 0.0
        )
        global_step = stats_summary.get("global_step", 0)

        info_lines = [
            (
                "Global Steps",
                f"{global_step/1e6:.2f}M / {TOTAL_TRAINING_STEPS/1e6:.1f}M",
            ),
            ("Total Episodes", f"{stats_summary.get('total_episodes', 0)}"),
            (
                "Steps/Sec (Current)",
                f"{stats_summary.get('steps_per_second', 0.0):.1f}",
            ),
            ("Buffer Fill", f"{buffer_perc:.1f}% ({buffer_size/1e6:.2f}M)"),
            (
                "PER Beta",
                (
                    f"{stats_summary.get('beta', 0.0):.3f}"
                    if BufferConfig.USE_PER
                    else "N/A"
                ),
            ),
            ("Learning Rate", f"{stats_summary.get('current_lr', 0.0):.1e}"),
            ("Device", f"{DEVICE.type.upper()}"),
            (
                "Network",
                f"Duel={DQNConfig.USE_DUELING}, Noisy={DQNConfig.USE_NOISY_NETS}, C51={DQNConfig.USE_DISTRIBUTIONAL}",
            ),
        ]

        last_y = y_start
        x_pos_key, x_pos_val_offset = 10, 5

        for idx, (key, value_str) in enumerate(info_lines):
            current_y = y_start + idx * line_height
            try:
                key_surf = ui_font.render(f"{key}:", True, VisConfig.LIGHTG)
                key_rect = key_surf.get_rect(topleft=(x_pos_key, current_y))
                self.screen.blit(key_surf, key_rect)

                value_surf = ui_font.render(f"{value_str}", True, VisConfig.WHITE)
                value_rect = value_surf.get_rect(
                    topleft=(key_rect.right + x_pos_val_offset, current_y)
                )

                clip_width = max(0, panel_width - value_rect.left - 10)
                if value_rect.width > clip_width:
                    self.screen.blit(
                        value_surf,
                        value_rect,
                        area=pygame.Rect(0, 0, clip_width, value_rect.height),
                    )
                else:
                    self.screen.blit(value_surf, value_rect)

                combined_rect = key_rect.union(value_rect)
                combined_rect.width = min(
                    combined_rect.width, panel_width - x_pos_key - 10
                )
                stat_rects[key] = combined_rect
                last_y = combined_rect.bottom
            except Exception as e:
                print(f"Error rendering stat line '{key}': {e}")
                last_y = current_y + line_height

        return last_y, stat_rects


File: config/__init__.py
# Expose core config classes and general constants/paths
from .core import (
    VisConfig,
    EnvConfig,
    RewardConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
    DemoConfig,  # Add this if you created the class
)
from .general import (
    DEVICE,
    RANDOM_SEED,
    RUN_ID,
    BASE_CHECKPOINT_DIR,
    BASE_LOG_DIR,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,  # Need this path constant
    BUFFER_SAVE_PATH,
    MODEL_SAVE_PATH,
    TOTAL_TRAINING_STEPS,  # Need this constant
)
from .utils import get_config_dict
from .validation import print_config_info_and_validate

# --- MODIFIED: Assign LOG_DIR after imports ---
# This ensures the correct run-specific directory is set in the config class
# before it's potentially used elsewhere (like in the stats recorder init).
TensorBoardConfig.LOG_DIR = RUN_LOG_DIR
# --- END MODIFIED ---

# You can choose to expose everything or just specific items
__all__ = [
    # Core Classes
    "VisConfig",
    "EnvConfig",
    "RewardConfig",
    "DQNConfig",
    "TrainConfig",
    "BufferConfig",
    "ModelConfig",
    "StatsConfig",
    "TensorBoardConfig",
    "DemoConfig",  # Add this
    # General Constants/Paths
    "DEVICE",
    "RANDOM_SEED",
    "RUN_ID",
    "BASE_CHECKPOINT_DIR",
    "BASE_LOG_DIR",
    "RUN_CHECKPOINT_DIR",
    "RUN_LOG_DIR",
    "BUFFER_SAVE_PATH",
    "MODEL_SAVE_PATH",
    "TOTAL_TRAINING_STEPS",
    # Utils/Validation
    "get_config_dict",
    "print_config_info_and_validate",
]


File: config/core.py
# File: config/core.py
import torch
from typing import Deque, Dict, Any, List, Type, Tuple, Optional

from .general import TOTAL_TRAINING_STEPS


# --- Visualization (Pygame) ---
class VisConfig:
    NUM_ENVS_TO_RENDER = 9
    SCREEN_WIDTH = 1600
    SCREEN_HEIGHT = 900
    VISUAL_STEP_DELAY = 0.005
    LEFT_PANEL_WIDTH = (SCREEN_WIDTH // 4) * 3
    ENV_SPACING = 1
    ENV_GRID_PADDING = 1
    FPS = 0
    WHITE = (255, 255, 255)
    BLACK = (0, 0, 0)
    LIGHTG = (140, 140, 140)
    GRAY = (50, 50, 50)
    RED = (255, 50, 50)
    DARK_RED = (80, 10, 10)
    BLUE = (50, 50, 255)
    YELLOW = (255, 255, 100)
    GOOGLE_COLORS = [(15, 157, 88), (244, 180, 0), (66, 133, 244), (219, 68, 55)]
    LINE_CLEAR_FLASH_COLOR = (180, 180, 220)
    LINE_CLEAR_HIGHLIGHT_COLOR = (255, 255, 0, 180)
    GAME_OVER_FLASH_COLOR = (255, 0, 0)  # Bright Red


# --- Environment ---
class EnvConfig:
    NUM_ENVS = 4096
    ROWS = 8
    COLS = 15
    GRID_FEATURES_PER_CELL = 2  # Occupied, Is_Up
    SHAPE_FEATURES_PER_SHAPE = 5
    NUM_SHAPE_SLOTS = 3

    @property
    def GRID_STATE_SHAPE(self) -> Tuple[int, int, int]:
        return (self.GRID_FEATURES_PER_CELL, self.ROWS, self.COLS)

    @property
    def SHAPE_STATE_DIM(self) -> int:
        return self.NUM_SHAPE_SLOTS * self.SHAPE_FEATURES_PER_SHAPE

    @property
    def ACTION_DIM(self) -> int:
        return self.NUM_SHAPE_SLOTS * (self.ROWS * self.COLS)


# --- Reward Shaping (RL Reward) ---
class RewardConfig:
    REWARD_PLACE_PER_TRI = 0.0
    REWARD_CLEAR_1 = 1.0
    REWARD_CLEAR_2 = 3.0
    REWARD_CLEAR_3PLUS = 6.0
    PENALTY_INVALID_MOVE = -0.1
    PENALTY_HOLE_PER_HOLE = -0.05
    PENALTY_GAME_OVER = -1.0
    REWARD_ALIVE_STEP = 0.001


# --- DQN Algorithm ---
class DQNConfig:
    GAMMA = 0.99
    TARGET_UPDATE_FREQ = 5_000
    LEARNING_RATE = 5e-5
    ADAM_EPS = 1e-4
    GRADIENT_CLIP_NORM = 10.0
    USE_DOUBLE_DQN = True
    USE_DUELING = True
    USE_NOISY_NETS = True
    USE_DISTRIBUTIONAL = True
    V_MIN = -10.0
    V_MAX = 10.0
    NUM_ATOMS = 51
    USE_LR_SCHEDULER = True
    LR_SCHEDULER_T_MAX: int = TOTAL_TRAINING_STEPS
    LR_SCHEDULER_ETA_MIN: float = 1e-7


# --- Training Loop ---
class TrainConfig:
    BATCH_SIZE = 64
    LEARN_START_STEP = 5_000
    LEARN_FREQ = 4
    CHECKPOINT_SAVE_FREQ = 20_000
    LOAD_CHECKPOINT_PATH: str | None = None
    LOAD_BUFFER_PATH: str | None = None


# --- Replay Buffer ---
class BufferConfig:
    REPLAY_BUFFER_SIZE = 200_000
    USE_N_STEP = True
    N_STEP = 6
    USE_PER = True
    PER_ALPHA = 0.6
    PER_BETA_START = 0.4
    PER_BETA_FRAMES = TOTAL_TRAINING_STEPS // 2
    PER_EPSILON = 1e-6


# --- Model Architecture ---
class ModelConfig:
    class Network:
        _env_cfg_instance = EnvConfig()
        HEIGHT = _env_cfg_instance.ROWS
        WIDTH = _env_cfg_instance.COLS
        del _env_cfg_instance
        CONV_CHANNELS = [32, 64, 64]
        CONV_KERNEL_SIZE = 3
        CONV_STRIDE = 1
        CONV_PADDING = 1
        CONV_ACTIVATION = torch.nn.ReLU
        USE_BATCHNORM_CONV = True
        SHAPE_FEATURE_MLP_DIMS = [64]
        SHAPE_MLP_ACTIVATION = torch.nn.ReLU
        COMBINED_FC_DIMS = [256, 128]
        COMBINED_ACTIVATION = torch.nn.ReLU
        USE_BATCHNORM_FC = True
        DROPOUT_FC = 0.0


# --- Statistics and Logging ---
class StatsConfig:
    STATS_AVG_WINDOW: List[int] = [10, 100, 1_000, 10_000]
    CONSOLE_LOG_FREQ = 5_000
    PLOT_DATA_WINDOW = 100_000


# --- TensorBoard Logging ---
class TensorBoardConfig:
    LOG_HISTOGRAMS = True
    HISTOGRAM_LOG_FREQ = 10_000
    LOG_IMAGES = True
    IMAGE_LOG_FREQ = 50_000
    LOG_DIR: Optional[str] = None
    LOG_SHAPE_PLACEMENT_Q_VALUES = True
    SHAPE_Q_LOG_FREQ = 5_000


# --- Demo Mode Visuals ---
class DemoConfig:
    BACKGROUND_COLOR = (10, 10, 20)
    SELECTED_SHAPE_HIGHLIGHT_COLOR = VisConfig.BLUE
    HUD_FONT_SIZE = 24
    HELP_FONT_SIZE = 18
    HELP_TEXT = "[Arrows]=Move | [Q/E]=Cycle Shape | [Space]=Place | [ESC]=Exit"


File: config/utils.py
# File: config/utils.py
# --- Configuration Utilities ---
import torch
from typing import Dict, Any
from .core import (
    VisConfig,
    EnvConfig,
    RewardConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
)
from .general import DEVICE, RANDOM_SEED, RUN_ID


def get_config_dict() -> Dict[str, Any]:
    """Returns a flat dictionary of all relevant config values for logging."""
    all_configs = {}

    # Helper to flatten classes (excluding methods, dunders, ClassVars, nested Classes)
    def flatten_class(cls, prefix=""):
        d = {}
        for k, v in cls.__dict__.items():
            # Basic check: not dunder, not callable, not a type (nested class)
            if not k.startswith("__") and not callable(v) and not isinstance(v, type):
                # More robust checking might be needed for complex configs
                d[f"{prefix}{k}"] = v
        return d

    all_configs.update(flatten_class(VisConfig, "Vis."))
    all_configs.update(flatten_class(EnvConfig, "Env."))
    all_configs.update(flatten_class(RewardConfig, "Reward."))
    all_configs.update(flatten_class(DQNConfig, "DQN."))
    all_configs.update(flatten_class(TrainConfig, "Train."))
    all_configs.update(flatten_class(BufferConfig, "Buffer."))
    all_configs.update(
        flatten_class(ModelConfig.Network, "Model.Net.")
    )  # Flatten sub-class explicitly
    all_configs.update(flatten_class(StatsConfig, "Stats."))
    all_configs.update(flatten_class(TensorBoardConfig, "TB."))

    # Add general constants
    all_configs["General.DEVICE"] = str(DEVICE)
    all_configs["General.RANDOM_SEED"] = RANDOM_SEED
    all_configs["General.RUN_ID"] = RUN_ID

    # Filter out None values specifically for paths that might not be set
    # This prevents logging None for load paths if they aren't specified
    all_configs = {
        k: v for k, v in all_configs.items() if not (k.endswith("_PATH") and v is None)
    }

    # Convert torch.nn activation functions to string representation for logging
    for key, value in all_configs.items():
        if isinstance(value, type) and issubclass(value, torch.nn.Module):
            all_configs[key] = value.__name__
        # Handle potential list values (e.g., CONV_CHANNELS) - convert to string?
        # Tensorboard hparams prefers simple types (int, float, bool, str)
        if isinstance(value, list):
            all_configs[key] = str(value)

    return all_configs


File: config/general.py
# File: config/general.py
# --- General Constants and Paths ---
import torch
import os
import time
from utils.helpers import get_device

# --- General ---
DEVICE = get_device()
RANDOM_SEED = 42
RUN_ID = f"run_{time.strftime('%Y%m%d_%H%M%S')}"
BASE_CHECKPOINT_DIR = "checkpoints"
BASE_LOG_DIR = "logs"

# --- Define Total Steps Here ---
# This makes it available for default LR scheduler T_max
TOTAL_TRAINING_STEPS = 10_000_000

# --- Derived Paths (using RUN_ID) ---
RUN_CHECKPOINT_DIR = os.path.join(BASE_CHECKPOINT_DIR, RUN_ID)
RUN_LOG_DIR = os.path.join(BASE_LOG_DIR, "tensorboard", RUN_ID)

BUFFER_SAVE_PATH = os.path.join(RUN_CHECKPOINT_DIR, "replay_buffer_state.pkl")
MODEL_SAVE_PATH = os.path.join(RUN_CHECKPOINT_DIR, "dqn_agent_state.pth")

# --- REMOVED Assign derived paths to Config classes ---
# from .core import TensorBoardConfig # <<< REMOVE THIS IMPORT
# TensorBoardConfig.LOG_DIR = RUN_LOG_DIR # <<< REMOVE THIS ASSIGNMENT
# --- END REMOVED ---


File: config/validation.py
# File: config/validation.py
import os, torch
from .core import (
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
    VisConfig,
    DemoConfig,
)
from .general import (
    RUN_ID,
    DEVICE,
    MODEL_SAVE_PATH,
    BUFFER_SAVE_PATH,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,
    TOTAL_TRAINING_STEPS,
)


def print_config_info_and_validate():
    env_config_instance = EnvConfig()

    print("-" * 70)
    print(f"RUN ID: {RUN_ID}")
    print(f"Log Directory: {RUN_LOG_DIR}")
    print(f"Checkpoint Directory: {RUN_CHECKPOINT_DIR}")
    print(f"Device: {DEVICE}")
    print(
        f"TB Logging: Histograms={'ON' if TensorBoardConfig.LOG_HISTOGRAMS else 'OFF'}, "
        f"Images={'ON' if TensorBoardConfig.LOG_IMAGES else 'OFF'}, "
        f"ShapeQs={'ON' if TensorBoardConfig.LOG_SHAPE_PLACEMENT_Q_VALUES else 'OFF'}"
    )
    if env_config_instance.GRID_FEATURES_PER_CELL != 2:
        print(
            f"Warning: Network expects {EnvConfig.GRID_FEATURES_PER_CELL} features per cell (Occupied, Is_Up)."
            f" Config has: {env_config_instance.GRID_FEATURES_PER_CELL}"
        )
    if TrainConfig.LOAD_CHECKPOINT_PATH:
        print(
            "*" * 70
            + f"\n*** Warning: LOAD CHECKPOINT from: {TrainConfig.LOAD_CHECKPOINT_PATH} ***\n*** Ensure ckpt matches current Model/DQN Config (distributional, noisy, dueling, scheduler). ***\n"
            + "*" * 70
        )
    else:
        print("--- Starting training from scratch (no checkpoint specified). ---")
    if TrainConfig.LOAD_BUFFER_PATH:
        print(
            "*" * 70
            + f"\n*** Warning: LOAD BUFFER from: {TrainConfig.LOAD_BUFFER_PATH} ***\n*** Ensure buffer matches current Buffer Config (PER, N-Step). ***\n"
            + "*" * 70
        )
    else:
        print("--- Starting with an empty replay buffer (no buffer specified). ---")
    print(f"--- Using Noisy Nets: {DQNConfig.USE_NOISY_NETS} ---")
    print(
        f"--- Using Distributional (C51): {DQNConfig.USE_DISTRIBUTIONAL} (Atoms: {DQNConfig.NUM_ATOMS}, Vmin: {DQNConfig.V_MIN}, Vmax: {DQNConfig.V_MAX}) ---"
    )
    print(
        f"--- Using LR Scheduler: {DQNConfig.USE_LR_SCHEDULER}"
        + (
            f" (CosineAnnealingLR, T_max={DQNConfig.LR_SCHEDULER_T_MAX/1e6:.1f}M steps, eta_min={DQNConfig.LR_SCHEDULER_ETA_MIN:.1e})"
            if DQNConfig.USE_LR_SCHEDULER
            else ""
        )
        + " ---"
    )
    print(
        f"Config: Env=(R={env_config_instance.ROWS}, C={env_config_instance.COLS}), "
        f"GridState={env_config_instance.GRID_STATE_SHAPE}, "
        f"ShapeState={env_config_instance.SHAPE_STATE_DIM}, "
        f"ActionDim={env_config_instance.ACTION_DIM}"
    )
    cnn_str = str(ModelConfig.Network.CONV_CHANNELS).replace(" ", "")
    mlp_str = str(ModelConfig.Network.COMBINED_FC_DIMS).replace(" ", "")
    shape_mlp_cfg_str = str(ModelConfig.Network.SHAPE_FEATURE_MLP_DIMS).replace(" ", "")
    print(
        f"Network: CNN={cnn_str}, ShapeMLP={shape_mlp_cfg_str}, Fusion={mlp_str}, Dueling={DQNConfig.USE_DUELING}"
    )

    print(
        f"Training: NUM_ENVS={env_config_instance.NUM_ENVS}, TOTAL_STEPS={TOTAL_TRAINING_STEPS/1e6:.1f}M, BUFFER={BufferConfig.REPLAY_BUFFER_SIZE/1e6:.1f}M, BATCH={TrainConfig.BATCH_SIZE}, LEARN_START={TrainConfig.LEARN_START_STEP/1e3:.1f}k"
    )
    print(
        f"Buffer: PER={BufferConfig.USE_PER}, N-Step={BufferConfig.N_STEP if BufferConfig.USE_N_STEP else 'N/A'}"
    )
    # --- MODIFIED: Print list of window sizes ---
    print(
        f"Stats: AVG_WINDOWS={StatsConfig.STATS_AVG_WINDOW}, Console Log Freq={StatsConfig.CONSOLE_LOG_FREQ}"
    )
    # --- END MODIFIED ---
    if env_config_instance.NUM_ENVS >= 1024:
        print(
            "*" * 70
            + f"\n*** Warning: NUM_ENVS={env_config_instance.NUM_ENVS}. Monitor system resources. ***"
            + (
                "\n*** Using MPS device. Performance varies. Force CPU via env var if needed. ***"
                if DEVICE.type == "mps"
                else ""
            )
            + "\n"
            + "*" * 70
        )
    print(
        f"--- Rendering {VisConfig.NUM_ENVS_TO_RENDER if VisConfig.NUM_ENVS_TO_RENDER > 0 else 'ALL'} of {env_config_instance.NUM_ENVS} environments ---"
    )
    print("-" * 70)


File: training/experience_collector.py
# File: training/experience_collector.py
import time
import torch
import numpy as np
import random
import traceback
from typing import List, Dict, Any, Tuple

from config import EnvConfig, RewardConfig, TensorBoardConfig, DEVICE

# --- MODIFIED: Import GameState directly for type hinting ---
from environment.game_state import GameState, StateType

# --- END MODIFIED ---
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from stats.stats_recorder import StatsRecorderBase
from utils.types import ActionType


class ExperienceCollector:
    """
    Handles interaction with parallel environments to collect experience.
    Introduces environments sequentially to avoid synchronized starts.
    """

    def __init__(
        self,
        envs: List[GameState],  # Keep GameState type hint
        agent: DQNAgent,
        buffer: ReplayBufferBase,
        stats_recorder: StatsRecorderBase,
        env_config: EnvConfig,
        reward_config: RewardConfig,
        tb_config: TensorBoardConfig,
    ):
        self.envs = envs
        self.agent = agent
        self.buffer = buffer
        self.stats_recorder = stats_recorder
        self.num_envs = env_config.NUM_ENVS
        self.env_config = env_config
        self.reward_config = reward_config
        self.tb_config = tb_config
        self.device = DEVICE

        # Initialize all states, but only a subset will be active initially
        self.current_states: List[StateType] = [env.reset() for env in self.envs]
        self.current_episode_scores = np.zeros(self.num_envs, dtype=np.float32)
        self.current_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)
        self.current_episode_game_scores = np.zeros(self.num_envs, dtype=np.int32)
        self.current_episode_lines_cleared = np.zeros(self.num_envs, dtype=np.int32)
        self.episode_count = 0

        # --- NEW: Track active environments ---
        self.num_active_envs: int = 0
        print(
            f"[ExperienceCollector] Initialized. Staggering start for {self.num_envs} environments."
        )
        # --- END NEW ---

    def collect(self, current_global_step: int) -> int:
        """
        Collects one step of experience from each *active* environment.
        Activates one new environment per call until all are active.
        Returns the number of environment steps actually taken in this call.
        """
        # --- Activate one new environment if not all are active ---
        if self.num_active_envs < self.num_envs:
            self.num_active_envs += 1
            print(
                f"[ExperienceCollector] Activating environment {self.num_active_envs}/{self.num_envs} at global step ~{current_global_step}"
            )
        # --- END ---

        if self.num_active_envs == 0:
            return 0  # Should not happen if num_envs > 0

        # --- Only interact with the active subset ---
        num_to_process = self.num_active_envs
        actions, batch_chosen_slots, batch_shape_qs = self._select_actions(
            num_to_process
        )
        next_states, rewards, dones, step_rewards = self._step_environments(
            actions, num_to_process
        )
        self._store_transitions(actions, rewards, next_states, dones, num_to_process)
        self._handle_episode_ends(dones, current_global_step, num_to_process)
        self._update_current_states(next_states, num_to_process)
        self._log_step_stats(
            actions,
            step_rewards,
            batch_chosen_slots,
            batch_shape_qs,
            current_global_step,
            num_to_process,  # Pass the number processed
        )
        # --- Return the number of environments processed in this step ---
        return num_to_process

    def _select_actions(
        self, num_to_select: int
    ) -> Tuple[List[ActionType], np.ndarray, List[np.ndarray]]:
        """Selects actions for the first `num_to_select` environments."""
        actions = [-1] * num_to_select
        chosen_slots = np.full(num_to_select, -1, dtype=np.int32)
        shape_qs_list = []
        num_slots = self.env_config.NUM_SHAPE_SLOTS

        for i in range(num_to_select):  # Loop only over active envs
            state = self.current_states[i]
            valid_actions = self.envs[i].valid_actions()
            if not valid_actions:
                actions[i] = 0  # Default action if none valid
                shape_qs_list.append(np.full(num_slots, -np.inf, dtype=np.float32))
            else:
                try:
                    actions[i] = self.agent.select_action(state, 0.0, valid_actions)
                    slot, shape_qs, _ = self.agent.get_last_shape_selection_info()
                    chosen_slots[i] = slot if slot is not None else -1
                    qs_to_log = (
                        shape_qs if shape_qs is not None else [-np.inf] * num_slots
                    )
                    # Ensure logged array has correct size
                    shape_qs_list.append(
                        np.array(qs_to_log[:num_slots], dtype=np.float32)
                    )
                except Exception as e:
                    print(f"ERROR: Agent select_action failed env {i}: {e}")
                    traceback.print_exc()
                    actions[i] = random.choice(valid_actions)
                    shape_qs_list.append(np.full(num_slots, -np.inf, dtype=np.float32))
        return actions, chosen_slots, shape_qs_list

    def _step_environments(
        self, actions: List[ActionType], num_to_step: int
    ) -> Tuple[List[StateType], np.ndarray, np.ndarray, np.ndarray]:
        """Steps the first `num_to_step` environments with the selected actions."""
        # Initialize results for the number of envs being stepped
        next_states = [{} for _ in range(num_to_step)]
        rewards = np.zeros(num_to_step, dtype=np.float32)
        dones = np.zeros(num_to_step, dtype=bool)
        step_rewards = np.zeros(num_to_step, dtype=np.float32)

        for i in range(num_to_step):  # Loop only over active envs
            env = self.envs[i]
            action = actions[i]
            try:
                reward, done = env.step(action)
                next_state = env.get_state()
                rewards[i] = reward
                dones[i] = done
                step_rewards[i] = reward
                next_states[i] = next_state
                # Update internal tracking for this env
                self.current_episode_scores[i] += reward
                self.current_episode_lengths[i] += 1
                if not done:
                    self.current_episode_game_scores[i] = env.game_score
                    self.current_episode_lines_cleared[i] = (
                        env.lines_cleared_this_episode
                    )
            except Exception as e:
                print(f"ERROR: Env {i} step failed (Action: {action}): {e}")
                traceback.print_exc()
                rewards[i] = self.reward_config.PENALTY_GAME_OVER
                dones[i] = True
                step_rewards[i] = rewards[i]
                next_states[i] = self._handle_env_crash(env, i)  # Reset and get state
                # Record episode immediately on crash
                self._record_episode_end(
                    i, rewards[i], current_global_step=0  # Step will be updated later
                )
                self._reset_episode_stats(i)

        return next_states, rewards, dones, step_rewards

    def _handle_env_crash(self, env: GameState, env_index: int) -> StateType:
        """Attempts to reset a crashed environment, returns a fallback state if needed."""
        print(f"Attempting reset for crashed env {env_index}...")
        try:
            return env.reset()
        except Exception as reset_e:
            print(
                f"FATAL: Env {env_index} failed reset after crash: {reset_e}. Creating zero state."
            )
            traceback.print_exc()
            grid_zeros = np.zeros(self.env_config.GRID_STATE_SHAPE, dtype=np.float32)
            shape_zeros = np.zeros(
                (
                    self.env_config.NUM_SHAPE_SLOTS,
                    self.env_config.SHAPE_FEATURES_PER_SHAPE,
                ),
                dtype=np.float32,
            )
            return {"grid": grid_zeros, "shapes": shape_zeros}

    def _store_transitions(
        self,
        actions: List[ActionType],
        rewards: np.ndarray,
        next_states: List[StateType],
        dones: np.ndarray,
        num_to_store: int,
    ):
        """Pushes transitions for the first `num_to_store` environments to the replay buffer."""
        for i in range(num_to_store):  # Loop only over active envs
            # Use the state *before* the step was taken
            state_before_step = self.current_states[i]
            self.buffer.push(
                state_before_step, actions[i], rewards[i], next_states[i], dones[i]
            )

    def _handle_episode_ends(
        self, dones: np.ndarray, current_global_step: int, num_to_check: int
    ):
        """Records stats for finished episodes among the first `num_to_check` environments."""
        for i in range(num_to_check):  # Loop only over active envs
            if dones[i]:
                # Pass the correct global step for this env's termination
                step_at_termination = current_global_step + i + 1
                self._record_episode_end(i, 0, step_at_termination)
                self.current_states[i] = self.envs[i].reset()  # Reset env state
                self._reset_episode_stats(i)

    def _record_episode_end(
        self, env_index: int, final_reward_adjustment: float, current_global_step: int
    ):
        """Records stats for a single finished episode."""
        self.episode_count += 1
        final_score = self.current_episode_scores[env_index] + final_reward_adjustment
        final_length = self.current_episode_lengths[env_index]
        final_game_score = self.current_episode_game_scores[env_index]
        final_lines = self.current_episode_lines_cleared[env_index]

        self.stats_recorder.record_episode(
            episode_score=final_score,
            episode_length=final_length,
            episode_num=self.episode_count,
            global_step=current_global_step,  # Use the passed step
            game_score=final_game_score,
            lines_cleared=final_lines,
        )

    def _reset_episode_stats(self, env_index: int):
        """Resets tracking stats for a specific environment index."""
        self.current_episode_scores[env_index] = 0.0
        self.current_episode_lengths[env_index] = 0
        self.current_episode_game_scores[env_index] = 0
        self.current_episode_lines_cleared[env_index] = 0

    def _update_current_states(self, next_states: List[StateType], num_to_update: int):
        """Updates the current states for the first `num_to_update` environments."""
        for i in range(num_to_update):  # Loop only over active envs
            self.current_states[i] = next_states[i]

    def _log_step_stats(
        self,
        actions: List[ActionType],
        step_rewards: np.ndarray,
        chosen_slots: np.ndarray,
        shape_qs_list: List[np.ndarray],
        current_global_step: int,
        num_envs_processed: int,  # Number of envs processed in this step
    ):
        """Logs statistics related to the collection step for the active environments."""
        step_log_data = {
            "buffer_size": len(self.buffer),
            # The global step *after* this collection step
            "global_step": current_global_step + num_envs_processed,
        }
        if self.tb_config.LOG_HISTOGRAMS:
            # Log data only from the processed environments
            step_log_data["step_rewards_batch"] = step_rewards[:num_envs_processed]
            step_log_data["action_batch"] = np.array(
                actions[:num_envs_processed], dtype=np.int32
            )

            valid_chosen_slots = chosen_slots[:num_envs_processed]
            valid_chosen_slots = valid_chosen_slots[valid_chosen_slots != -1]
            if len(valid_chosen_slots) > 0:
                step_log_data["chosen_shape_slot_batch"] = valid_chosen_slots

            if shape_qs_list:  # shape_qs_list already has length num_envs_processed
                flat_shape_qs = np.concatenate(shape_qs_list)
                valid_flat_shape_qs = flat_shape_qs[np.isfinite(flat_shape_qs)]
                if len(valid_flat_shape_qs) > 0:
                    step_log_data["shape_slot_max_q_batch"] = valid_flat_shape_qs

        if self.tb_config.LOG_SHAPE_PLACEMENT_Q_VALUES:
            # Agent's last batch Q values correspond to the last call to select_action,
            # which processed num_envs_processed environments.
            batch_q_vals = self.agent.get_last_batch_q_values_for_actions()
            if batch_q_vals is not None and len(batch_q_vals) == num_envs_processed:
                step_log_data["batch_q_values_actions_taken"] = batch_q_vals
            # else: Handle potential mismatch if needed

        self.stats_recorder.record_step(step_log_data)

    def get_episode_count(self) -> int:
        return self.episode_count


File: training/__init__.py
# File: training/__init__.py
from .trainer import Trainer
from .experience_collector import ExperienceCollector
from .checkpoint_manager import CheckpointManager

__all__ = [
    "Trainer",
    "ExperienceCollector",
    "CheckpointManager",
]


File: training/training_utils.py
# File: training/training_utils.py
import pygame
import numpy as np
from typing import Optional
from config import EnvConfig, VisConfig


def get_env_image_as_numpy(
    env, env_config: EnvConfig, vis_config: VisConfig
) -> Optional[np.ndarray]:
    """Renders a single environment state to a NumPy array for logging."""
    img_h = 300
    aspect_ratio = (env_config.COLS * 0.75 + 0.25) / max(1, env_config.ROWS)
    img_w = int(img_h * aspect_ratio)
    if img_w <= 0 or img_h <= 0:
        return None
    try:
        temp_surf = pygame.Surface((img_w, img_h))
        cell_w_px = img_w / (env_config.COLS * 0.75 + 0.25)
        cell_h_px = img_h / max(1, env_config.ROWS)
        temp_surf.fill(vis_config.BLACK)
        if hasattr(env, "grid") and hasattr(env.grid, "triangles"):
            for r in range(env.grid.rows):
                for c in range(env.grid.cols):
                    if r < len(env.grid.triangles) and c < len(env.grid.triangles[r]):
                        t = env.grid.triangles[r][c]
                        if t.is_death:
                            continue
                        pts = t.get_points(
                            ox=0, oy=0, cw=int(cell_w_px), ch=int(cell_h_px)
                        )
                        color = vis_config.GRAY
                        if t.is_occupied:
                            color = t.color if t.color else vis_config.RED
                        pygame.draw.polygon(temp_surf, color, pts)
        img_array = pygame.surfarray.array3d(temp_surf)
        return np.transpose(img_array, (1, 0, 2))
    except Exception as e:
        print(f"Error generating environment image for TB: {e}")
        return None


File: training/trainer.py
# File: training/trainer.py
import time
import torch
import numpy as np
import traceback
import random
from typing import List, Optional, Dict, Any, Union

from config import (
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    DEVICE,
    TensorBoardConfig,
    VisConfig,
    RewardConfig,
    TOTAL_TRAINING_STEPS,
)

# --- MODIFIED: Import GameState directly for type hinting ---
from environment.game_state import GameState, StateType

# --- END MODIFIED ---
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from stats.stats_recorder import StatsRecorderBase
from utils.helpers import ensure_numpy
from utils.types import (
    PrioritizedNumpyBatch,
    PrioritizedNumpyNStepBatch,
    NumpyBatch,
    NumpyNStepBatch,
)
from .experience_collector import ExperienceCollector
from .checkpoint_manager import CheckpointManager
from .training_utils import get_env_image_as_numpy


class Trainer:
    """Orchestrates the DQN training process."""

    def __init__(
        self,
        envs: List[GameState],  # Keep GameState type hint
        agent: DQNAgent,
        buffer: ReplayBufferBase,
        stats_recorder: StatsRecorderBase,
        env_config: EnvConfig,
        dqn_config: DQNConfig,
        train_config: TrainConfig,
        buffer_config: BufferConfig,
        model_config: ModelConfig,
        model_save_path: str,
        buffer_save_path: str,
        load_checkpoint_path: Optional[str] = None,
        load_buffer_path: Optional[str] = None,
    ):
        print("[Trainer] Initializing...")
        self.envs = envs
        self.agent = agent
        self.stats_recorder = stats_recorder
        # --- MODIFIED: num_envs is max, collector tracks active ---
        self.max_num_envs = env_config.NUM_ENVS
        # --- END MODIFIED ---
        self.device = DEVICE
        self.env_config = env_config
        self.dqn_config = dqn_config
        self.train_config = train_config
        self.buffer_config = buffer_config
        self.model_config = model_config
        self.reward_config = RewardConfig()
        self.tb_config = TensorBoardConfig()
        self.vis_config = VisConfig()

        self.checkpoint_manager = CheckpointManager(
            agent=self.agent,
            buffer=buffer,
            model_save_path=model_save_path,
            buffer_save_path=buffer_save_path,
            load_checkpoint_path=load_checkpoint_path,
            load_buffer_path=load_buffer_path,
            buffer_config=self.buffer_config,
            dqn_config=self.dqn_config,
            device=self.device,
        )
        self.buffer = self.checkpoint_manager.get_buffer()
        self.global_step, initial_episode_count = (
            self.checkpoint_manager.get_initial_state()
        )

        self.experience_collector = ExperienceCollector(
            envs=self.envs,
            agent=self.agent,
            buffer=self.buffer,
            stats_recorder=self.stats_recorder,
            env_config=self.env_config,
            reward_config=self.reward_config,
            tb_config=self.tb_config,
        )
        # Initialize collector's episode count from checkpoint if loaded
        self.experience_collector.episode_count = initial_episode_count

        self.last_image_log_step = -self.tb_config.IMAGE_LOG_FREQ
        self.is_n_step_buffer = (
            self.buffer_config.USE_N_STEP and self.buffer_config.N_STEP > 1
        )

        self._log_initial_state()
        print("[Trainer] Initialization complete.")

    def _log_initial_state(self):
        """Logs the state after initialization and potential loading."""
        initial_beta = self._update_beta()
        initial_lr = self._get_current_lr()
        self.stats_recorder.record_step(
            {
                "buffer_size": len(self.buffer),
                "beta": initial_beta,
                "lr": initial_lr,
                "global_step": self.global_step,
                "episode_count": self.experience_collector.get_episode_count(),
            }
        )
        print(
            f"  -> Start Step={self.global_step}, Ep={self.experience_collector.get_episode_count()}, Buf={len(self.buffer)}, Beta={initial_beta:.4f}, LR={initial_lr:.1e}"
        )

    def _get_current_lr(self) -> float:
        """Retrieves the current learning rate from the optimizer."""
        return (
            self.agent.optimizer.param_groups[0]["lr"]
            if hasattr(self.agent, "optimizer") and self.agent.optimizer.param_groups
            else 0.0
        )

    def step(self):
        """Performs one full step: collect experience, train, update target net."""
        step_start_time = time.perf_counter()

        # --- MODIFIED: Use steps_collected from collector ---
        steps_collected = self.experience_collector.collect(self.global_step)
        if steps_collected == 0:  # Handle case where no envs are active yet
            time.sleep(0.01)  # Small sleep to prevent busy-waiting
            return

        step_before_collection = self.global_step
        self.global_step += steps_collected
        # --- END MODIFIED ---

        if (
            self.global_step >= self.train_config.LEARN_START_STEP
            and self.global_step % self.train_config.LEARN_FREQ == 0
        ):
            if len(self.buffer) >= self.train_config.BATCH_SIZE:
                self._train_batch()

        step_end_time = time.perf_counter()
        step_duration = step_end_time - step_start_time

        self.stats_recorder.record_step(
            {
                "step_time": step_duration,
                "num_steps_processed": steps_collected,  # Log actual steps processed
                "global_step": self.global_step,
            }
        )

        target_freq = self.dqn_config.TARGET_UPDATE_FREQ
        if target_freq > 0 and self.global_step > 0:
            # --- MODIFIED: Use step_before_collection for comparison ---
            if step_before_collection // target_freq < self.global_step // target_freq:
                # --- END MODIFIED ---
                print(f"[Trainer] Updating target network at step {self.global_step}")
                self.agent.update_target_network()

        self.maybe_save_checkpoint(
            step_before_collection
        )  # Pass previous step for freq check
        self._maybe_log_image()

    def _train_batch(self):
        """Samples a batch, computes loss, updates agent, and updates priorities (for PER)."""
        beta = self._update_beta()
        batch_sample = None
        try:
            batch_sample = self.buffer.sample(self.train_config.BATCH_SIZE)
            if batch_sample is None:
                # print(f"[Trainer] Buffer sample returned None (Size: {len(self.buffer)}). Skipping train step.")
                return  # Not enough samples or other issue
        except Exception as e:
            print(f"ERROR sampling buffer: {e}")
            traceback.print_exc()
            return

        # Unpack sample based on whether it's prioritized
        indices, is_weights_np = None, None
        if self.buffer_config.USE_PER:
            if (
                batch_sample
                and isinstance(batch_sample, tuple)
                and len(batch_sample) == 3
            ):
                batch_tuple, indices, is_weights_np = batch_sample
            else:
                print(
                    f"Warning: Expected prioritized batch (tuple of 3), got {type(batch_sample)}. Skipping training step."
                )
                return
        else:  # Uniform sampling
            # Batch tuple is the whole sample for uniform (length 5 or 6)
            if (
                batch_sample
                and isinstance(batch_sample, tuple)
                and len(batch_sample) in [5, 6]
            ):
                batch_tuple = batch_sample
            else:
                print(
                    f"Warning: Expected uniform batch (tuple of 5 or 6), got {type(batch_sample)}. Skipping training step."
                )
                return

        loss, td_errors = torch.tensor(0.0), None
        try:
            # Agent's compute_loss handles checking buffer_config for is_n_step
            loss, td_errors = self.agent.compute_loss(batch_tuple, is_weights_np)
        except Exception as e:
            print(f"ERROR computing loss: {e}")
            traceback.print_exc()
            return

        grad_norm = None
        try:
            grad_norm = self.agent.update(loss)
            if grad_norm is None and self.dqn_config.GRADIENT_CLIP_NORM > 0:
                print("[Trainer] Skipping PER update due to gradient clipping error.")
                return  # Skip PER update if agent update failed
        except Exception as e:
            print(f"ERROR updating agent: {e}")
            traceback.print_exc()
            return

        if self.buffer_config.USE_PER and indices is not None and td_errors is not None:
            try:
                td_errors_np = ensure_numpy(td_errors)
                self.buffer.update_priorities(indices, td_errors_np)
            except Exception as e:
                print(f"ERROR updating PER priorities: {e}")
                traceback.print_exc()

        train_log_data = {
            "loss": loss.item(),
            "grad_norm": grad_norm if grad_norm is not None else 0.0,
            "avg_max_q": self.agent.get_last_avg_max_q(),
            "lr": self._get_current_lr(),
            "global_step": self.global_step,
        }
        if self.tb_config.LOG_HISTOGRAMS and td_errors is not None:
            train_log_data["batch_td_errors"] = (
                td_errors.detach() if td_errors.requires_grad else td_errors
            )

        self.stats_recorder.record_step(train_log_data)

    def _update_beta(self) -> float:
        """Updates PER beta based on annealing schedule and sets it in the buffer."""
        beta = 1.0  # Default for uniform
        if self.buffer_config.USE_PER:
            start, end, frames = (
                self.buffer_config.PER_BETA_START,
                1.0,
                self.buffer_config.PER_BETA_FRAMES,
            )
            fraction = min(1.0, float(self.global_step) / max(1, frames))
            beta = start + fraction * (end - start)
            self.buffer.set_beta(beta)  # Update buffer's beta

        # Always record beta for consistency
        self.stats_recorder.record_step({"beta": beta, "global_step": self.global_step})
        return beta

    def _maybe_log_image(self):
        """Logs a sample environment state image to TensorBoard periodically."""
        if not self.tb_config.LOG_IMAGES:
            return
        img_freq = self.tb_config.IMAGE_LOG_FREQ
        if img_freq <= 0:
            return

        steps_since_last = self.global_step - self.last_image_log_step
        if steps_since_last >= img_freq:
            try:
                # --- MODIFIED: Sample from active envs ---
                num_currently_active = self.experience_collector.num_active_envs
                if num_currently_active == 0:
                    return
                env_idx = random.randint(0, num_currently_active - 1)
                # --- END MODIFIED ---
                img_array = get_env_image_as_numpy(
                    self.envs[env_idx], self.env_config, self.vis_config
                )
                if img_array is not None:
                    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)
                    self.stats_recorder.record_image(
                        f"Environment/Sample State Env {env_idx}",
                        img_tensor,
                        self.global_step,
                    )
                    self.last_image_log_step = self.global_step
            except Exception as e:
                print(f"Error logging environment image: {e}")
                traceback.print_exc()

    # --- MODIFIED: Pass previous step for frequency check ---
    def maybe_save_checkpoint(self, step_before_collection: int, force_save=False):
        # --- END MODIFIED ---
        """Saves agent and buffer state based on frequency or if forced."""
        save_freq = self.train_config.CHECKPOINT_SAVE_FREQ
        if save_freq <= 0 and not force_save:
            return

        # --- MODIFIED: Use step_before_collection for frequency check ---
        should_save_freq = (
            save_freq > 0
            and self.global_step > 0
            and (step_before_collection // save_freq < self.global_step // save_freq)
        )
        # --- END MODIFIED ---

        if force_save or should_save_freq:
            self.checkpoint_manager.save_checkpoint(
                self.global_step, self.experience_collector.get_episode_count()
            )

    def train_loop(self):
        """Main training loop until max steps."""
        print("[Trainer] Starting training loop...")
        try:
            while self.global_step < TOTAL_TRAINING_STEPS:
                self.step()
        except KeyboardInterrupt:
            print("\n[Trainer] Training loop interrupted by user (Ctrl+C).")
        except Exception as e:
            print(f"\n[Trainer] CRITICAL ERROR in training loop: {e}")
            traceback.print_exc()
        finally:
            print("[Trainer] Training loop finished or terminated.")
            self.cleanup(save_final=True)

    def cleanup(self, save_final: bool = True):
        """Performs cleanup actions like saving final state and closing logger."""
        print("[Trainer] Cleaning up resources...")
        if save_final:
            print("[Trainer] Saving final checkpoint...")
            if hasattr(self.buffer, "flush_pending"):
                print("[Trainer] Flushing pending buffer transitions...")
                try:
                    self.buffer.flush_pending()
                except Exception as flush_err:
                    print(f"ERROR flushing buffer: {flush_err}")
                    traceback.print_exc()
            self.checkpoint_manager.save_checkpoint(
                self.global_step,
                self.experience_collector.get_episode_count(),
                is_final=True,
            )
        else:
            print("[Trainer] Skipping final save as requested.")

        if hasattr(self.stats_recorder, "close"):
            try:
                self.stats_recorder.close()
            except Exception as e:
                print(f"Error closing stats recorder: {e}")

        print("[Trainer] Cleanup complete.")


File: training/checkpoint_manager.py
# File: training/checkpoint_manager.py
import os
import torch
import traceback
from typing import Optional, Dict, Any, Tuple

from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from agent.replay_buffer.buffer_utils import create_replay_buffer
from config import BufferConfig, DQNConfig


class CheckpointManager:
    """Handles loading and saving of agent and buffer states."""

    def __init__(
        self,
        agent: DQNAgent,
        buffer: ReplayBufferBase,
        model_save_path: str,
        buffer_save_path: str,
        load_checkpoint_path: Optional[str],
        load_buffer_path: Optional[str],
        buffer_config: BufferConfig,
        dqn_config: DQNConfig,
        device: torch.device,
    ):
        self.agent = agent
        self.buffer = buffer  # Store initial reference
        self.model_save_path = model_save_path
        self.buffer_save_path = buffer_save_path
        self.buffer_config = buffer_config
        self.dqn_config = dqn_config
        self.device = device

        self.global_step = 0
        self.episode_count = 0

        if load_checkpoint_path:
            self.load_agent_checkpoint(load_checkpoint_path)
        else:
            print("[CheckpointManager] No agent checkpoint specified, starting fresh.")

        # --- Load or create buffer ---
        if load_buffer_path:
            self.load_buffer_state(load_buffer_path)
        else:
            print("[CheckpointManager] No buffer specified, creating new empty buffer.")
            # Create a new buffer if none was loaded
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)

        # --- Ensure buffer is initialized ---
        if self.buffer is None:
            print(
                "[CheckpointManager] ERROR: Buffer is None after initialization/loading attempt. Creating empty."
            )
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)

    def load_agent_checkpoint(self, path_to_load: str):
        # Logic remains the same
        if not os.path.isfile(path_to_load):
            print(
                f"[CheckpointManager] LOAD WARNING: Agent ckpt not found: {path_to_load}"
            )
            return
        print(f"[CheckpointManager] Loading agent checkpoint from: {path_to_load}")
        try:
            # Load checkpoint to the specified device
            checkpoint = torch.load(path_to_load, map_location=self.device)
            # --- Pass buffer_config to agent's load_state_dict ---
            self.agent.load_state_dict(checkpoint)
            # --- Get state from checkpoint AFTER loading agent ---
            self.global_step = checkpoint.get("global_step", 0)
            self.episode_count = checkpoint.get("episode_count", 0)
            print(
                f"  -> Resuming from Step: {self.global_step}, Ep: {self.episode_count}"
            )
        except KeyError as e:
            print(
                f"  -> ERROR loading agent checkpoint: Missing key '{e}'. Check compatibility."
            )
            self.global_step = 0
            self.episode_count = 0
        except Exception as e:
            print(f"  -> ERROR loading agent checkpoint ('{e}'). Check compatibility.")
            traceback.print_exc()
            self.global_step = 0
            self.episode_count = 0

    def load_buffer_state(self, path_to_load: str):
        # Logic remains the same, but recreate buffer first
        if not os.path.isfile(path_to_load):
            print(
                f"[CheckpointManager] LOAD WARNING: Buffer file not found: {path_to_load}"
            )
            # Create new buffer if load path invalid
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)
            return

        print(
            f"[CheckpointManager] Recreating buffer structure before loading state from: {path_to_load}"
        )
        try:
            # Create buffer instance based on config FIRST
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)
            # Then load the state into the newly created instance
            self.buffer.load_state(path_to_load)
            print(f"  -> Buffer loaded. Size: {len(self.buffer)}")
        except Exception as e:
            print(f"  -> ERROR loading buffer state ('{e}'). Creating empty buffer.")
            traceback.print_exc()
            # Create new buffer on error
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)

    def save_checkpoint(
        self, global_step: int, episode_count: int, is_final: bool = False
    ):
        # Logic remains the same for agent
        prefix = "FINAL" if is_final else f"step_{global_step}"
        save_dir = os.path.dirname(self.model_save_path)
        os.makedirs(save_dir, exist_ok=True)

        print(f"[CheckpointManager] Saving agent checkpoint ({prefix})...")
        try:
            agent_save_data = self.agent.get_state_dict()
            # Add step/episode info directly to the agent state dict
            agent_save_data["global_step"] = global_step
            agent_save_data["episode_count"] = episode_count
            torch.save(agent_save_data, self.model_save_path)
            print(
                f"  -> Agent checkpoint saved: {os.path.basename(self.model_save_path)}"
            )
        except Exception as e:
            print(f"  -> ERROR saving agent checkpoint: {e}")
            traceback.print_exc()

        print(f"[CheckpointManager] Saving buffer state ({prefix})...")
        try:
            # --- MODIFIED: Flush pending transitions before saving buffer ---
            if hasattr(self.buffer, "flush_pending"):
                print("  -> Flushing pending buffer transitions before saving...")
                self.buffer.flush_pending()
            # --- END MODIFIED ---
            self.buffer.save_state(self.buffer_save_path)
            print(
                f"  -> Buffer state saved: {os.path.basename(self.buffer_save_path)} (Size: {len(self.buffer)})"
            )
        except Exception as e:
            print(f"  -> ERROR saving buffer state: {e}")
            traceback.print_exc()

    def get_initial_state(self) -> Tuple[int, int]:
        # Logic remains the same
        return self.global_step, self.episode_count

    def get_buffer(self) -> ReplayBufferBase:
        # Logic remains the same
        if self.buffer is None:
            raise RuntimeError(
                "Buffer accessed before initialization in CheckpointManager."
            )
        return self.buffer


File: utils/__init__.py


File: utils/init_checks.py
# File: utils/init_checks.py
# --- Pre-Run Sanity Checks ---
import sys
import traceback
import numpy as np
from config import EnvConfig  # Import config class
from environment.game_state import GameState


def run_pre_checks() -> bool:
    """Performs basic checks on GameState and configuration compatibility."""
    print("--- Pre-Run Checks ---")
    try:
        print("Checking GameState and Configuration Compatibility...")
        env_config_instance = EnvConfig()  # Instantiate

        gs_test = GameState()
        gs_test.reset()
        s_test_dict = gs_test.get_state()

        if not isinstance(s_test_dict, dict):
            raise TypeError(
                f"GameState.get_state() should return a dict, but got {type(s_test_dict)}"
            )
        print("GameState state type check PASSED (returned dict).")

        # --- MODIFIED: Check grid shape (2 channels) ---
        if "grid" not in s_test_dict:
            raise KeyError("State dictionary missing 'grid' key.")
        grid_state = s_test_dict["grid"]
        expected_grid_shape = (
            env_config_instance.GRID_STATE_SHAPE
        )  # Uses property (2, H, W)
        if not isinstance(grid_state, np.ndarray):
            raise TypeError(
                f"State 'grid' component should be numpy array, but got {type(grid_state)}"
            )
        if grid_state.shape != expected_grid_shape:
            raise ValueError(
                f"State 'grid' shape mismatch! GameState:{grid_state.shape}, EnvConfig:{expected_grid_shape}"
            )
        print(f"GameState 'grid' state shape check PASSED (Shape: {grid_state.shape}).")
        # --- END MODIFIED ---

        # Check 'shapes' component (unchanged)
        if "shapes" not in s_test_dict:
            raise KeyError("State dictionary missing 'shapes' key.")
        shape_state = s_test_dict["shapes"]
        expected_shape_shape = (
            env_config_instance.NUM_SHAPE_SLOTS,
            env_config_instance.SHAPE_FEATURES_PER_SHAPE,
        )
        if not isinstance(shape_state, np.ndarray):
            raise TypeError(
                f"State 'shapes' component should be numpy array, but got {type(shape_state)}"
            )
        if shape_state.shape != expected_shape_shape:
            raise ValueError(
                f"State 'shapes' shape mismatch! GameState:{shape_state.shape}, EnvConfig:{expected_shape_shape}"
            )
        print(
            f"GameState 'shapes' state shape check PASSED (Shape: {shape_state.shape})."
        )

        _ = gs_test.valid_actions()
        print("GameState valid_actions check PASSED.")
        if not hasattr(gs_test, "game_score"):
            raise AttributeError("GameState missing 'game_score' attribute!")
        print("GameState 'game_score' attribute check PASSED.")
        if not hasattr(gs_test, "lines_cleared_this_episode"):
            raise AttributeError(
                "GameState missing 'lines_cleared_this_episode' attribute!"
            )
        print("GameState 'lines_cleared_this_episode' attribute check PASSED.")

        del gs_test
        print("--- Pre-Run Checks Complete ---")
        return True
    except (NameError, ImportError) as e:
        print(f"FATAL ERROR: Import/Name error: {e}")
    except (ValueError, AttributeError, TypeError, KeyError) as e:
        print(f"FATAL ERROR during pre-run checks: {e}")
    except Exception as e:
        print(f"FATAL ERROR during GameState pre-check: {e}")
        traceback.print_exc()
    sys.exit(1)


File: utils/types.py
# File: utils/types.py
from typing import NamedTuple, Union, Tuple, List, Dict, Any, Optional
import numpy as np
import torch

# --- State represented as a dictionary ---
StateType = Dict[str, np.ndarray]  # e.g., {"grid": ndarray, "shapes": ndarray}

# Type alias for action
ActionType = int


# --- Transition Tuple ---
class Transition(NamedTuple):
    state: StateType
    action: ActionType
    reward: float
    next_state: StateType
    done: bool
    n_step_discount: Optional[float] = None  # Store gamma^n for N-step


# --- Batch Types (Numpy) ---
# State batches are now lists of dictionaries
NumpyStateBatch = List[StateType]
NumpyActionBatch = np.ndarray
NumpyRewardBatch = np.ndarray
NumpyDoneBatch = np.ndarray
NumpyDiscountBatch = np.ndarray  # For N-step discount factor (gamma^n)

# Standard 1-step batch
NumpyBatch = Tuple[
    NumpyStateBatch, NumpyActionBatch, NumpyRewardBatch, NumpyStateBatch, NumpyDoneBatch
]

# N-step batch
NumpyNStepBatch = Tuple[
    NumpyStateBatch,
    NumpyActionBatch,
    NumpyRewardBatch,  # N-step rewards
    NumpyStateBatch,  # State after N steps
    NumpyDoneBatch,  # Done flag after N steps
    NumpyDiscountBatch,  # Effective discount gamma^n
]

# Prioritized 1-step batch
PrioritizedNumpyBatch = Tuple[
    NumpyBatch,  # (states_dicts, actions, rewards, next_states_dicts, dones)
    np.ndarray,  # indices (tree indices for PER update)
    np.ndarray,  # is_weights (importance sampling weights)
]

# Prioritized N-step batch
PrioritizedNumpyNStepBatch = Tuple[
    NumpyNStepBatch,  # (states_dicts, actions, rewards_n, next_states_dicts_n, dones_n, discounts_n)
    np.ndarray,  # indices
    np.ndarray,  # is_weights
]


# --- Batch Types (Tensor) ---
# State tensors are tuples (grid_tensor, shape_tensor) after conversion
TensorStateBatch = Tuple[torch.Tensor, torch.Tensor]
TensorActionBatch = torch.Tensor
TensorRewardBatch = torch.Tensor
TensorDoneBatch = torch.Tensor
TensorDiscountBatch = torch.Tensor  # For N-step discount factor
TensorWeightsBatch = torch.Tensor  # For PER IS weights

# Standard 1-step batch (Tensor)
TensorBatch = Tuple[
    TensorStateBatch,
    TensorActionBatch,
    TensorRewardBatch,
    TensorStateBatch,
    TensorDoneBatch,
]

# N-step batch (Tensor)
TensorNStepBatch = Tuple[
    TensorStateBatch,
    TensorActionBatch,
    TensorRewardBatch,
    TensorStateBatch,
    TensorDoneBatch,
    TensorDiscountBatch,
]


# --- Agent State Dictionary ---
AgentStateDict = Dict[str, Any]  # For saving/loading agent checkpoints


File: utils/helpers.py
# File: utils/helpers.py
import torch
import numpy as np
import random
import os
import pickle
import cloudpickle
from typing import Union, Any


def get_device() -> torch.device:
    """Gets the appropriate torch device (MPS, CUDA, or CPU)."""
    force_cpu = os.environ.get("FORCE_CPU", "false").lower() == "true"
    if force_cpu:
        print("Forcing CPU device based on environment variable.")
        return torch.device("cpu")

    if torch.backends.mps.is_available():
        device_str = "mps"
    elif torch.cuda.is_available():
        device_str = "cuda"
    else:
        device_str = "cpu"

    print(f"Using device: {device_str.upper()}")
    if device_str == "cuda":
        print(f"CUDA Device Name: {torch.cuda.get_device_name(0)}")
    elif device_str == "mps":
        print("MPS device found on MacOS.")
    return torch.device(device_str)


def set_random_seeds(seed: int = 42):
    """Sets random seeds for Python, NumPy, and PyTorch."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # Note: Setting deterministic algorithms can impact performance
        # torch.backends.cudnn.deterministic = True
        # torch.backends.cudnn.benchmark = False
    print(f"Set random seeds to {seed}")


def ensure_numpy(data: Union[np.ndarray, list, tuple, torch.Tensor]) -> np.ndarray:
    """Ensures the input data is a numpy array with float32 type."""
    try:
        if isinstance(data, np.ndarray):
            if data.dtype != np.float32:
                return data.astype(np.float32)
            return data
        elif isinstance(data, torch.Tensor):
            return data.detach().cpu().numpy().astype(np.float32)
        elif isinstance(data, (list, tuple)):
            arr = np.array(data, dtype=np.float32)
            if arr.dtype == np.object_:  # Indicates ragged array
                raise ValueError(
                    "Cannot convert ragged list/tuple to float32 numpy array."
                )
            return arr
        else:
            # Attempt conversion for single numbers or other types
            return np.array([data], dtype=np.float32)
    except (ValueError, TypeError, RuntimeError) as e:
        print(
            f"CRITICAL ERROR in ensure_numpy conversion: {e}. Input type: {type(data)}. Data (partial): {str(data)[:100]}"
        )
        raise ValueError(f"ensure_numpy failed: {e}") from e


def save_object(obj: Any, filepath: str):
    """Saves an arbitrary Python object to a file using cloudpickle."""
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, "wb") as f:
            cloudpickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)
    except Exception as e:
        print(f"Error saving object to {filepath}: {e}")
        raise e  # Re-raise after logging


def load_object(filepath: str) -> Any:
    """Loads a Python object from a file using cloudpickle."""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found for loading: {filepath}")
    try:
        with open(filepath, "rb") as f:
            obj = cloudpickle.load(f)
        return obj
    except Exception as e:
        print(f"Error loading object from {filepath}: {e}")
        raise e  # Re-raise after logging


File: agent/action_selector.py
# File: agent/action_selector.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import random
from typing import List, Dict, Optional, Tuple

from config import EnvConfig, DQNConfig, DEVICE
from environment.game_state import StateType
from utils.types import ActionType
from utils.helpers import ensure_numpy


class ActionSelector:
    """Handles the two-stage action selection process for the DQNAgent."""

    def __init__(
        self,
        online_net: nn.Module,
        env_config: EnvConfig,
        dqn_config: DQNConfig,
        device: torch.device,
    ):
        self.online_net = online_net
        self.env_config = env_config
        self.dqn_config = dqn_config
        self.device = device
        self.num_shape_slots = env_config.NUM_SHAPE_SLOTS
        self.locations_per_shape = env_config.ROWS * env_config.COLS
        self.use_distributional = dqn_config.USE_DISTRIBUTIONAL
        self.use_noisy_nets = dqn_config.USE_NOISY_NETS

        if self.use_distributional:
            self.support = torch.linspace(
                dqn_config.V_MIN,
                dqn_config.V_MAX,
                dqn_config.NUM_ATOMS,
                device=self.device,
            )

        self._last_avg_max_q: float = -float("inf")
        self._last_chosen_shape_slot: Optional[int] = None
        self._last_shape_slot_max_q_values: List[float] = [
            -float("inf")
        ] * self.num_shape_slots
        self._last_placement_q_values_for_chosen_shape: Optional[List[float]] = None

    @torch.no_grad()
    def select_action(
        self, state: StateType, epsilon: float, valid_actions_indices: List[ActionType]
    ) -> ActionType:
        """Selects action using a two-stage process based on Q-values."""
        self._reset_log_state()

        if not valid_actions_indices:
            self._last_avg_max_q = -float("inf")
            return 0

        # Epsilon-greedy exploration (only if NOT using noisy nets)
        if not self.use_noisy_nets and random.random() < epsilon:
            self._last_avg_max_q = -float("inf")
            return random.choice(valid_actions_indices)

        # Always act greedily based on network output (noise is handled internally if enabled)
        q_values_np = self._get_q_values_for_state(state)
        return self._select_best_action_from_q_values(
            q_values_np, valid_actions_indices
        )

    def _reset_log_state(self):
        self._last_avg_max_q = -float("inf")
        self._last_chosen_shape_slot = None
        self._last_shape_slot_max_q_values = [-float("inf")] * self.num_shape_slots
        self._last_placement_q_values_for_chosen_shape = None

    def _get_q_values_for_state(self, state: StateType) -> np.ndarray:
        """Calculates Q-values (or expected Q-values for C51) for a given state."""
        grid_np = ensure_numpy(state["grid"])
        shapes_np = ensure_numpy(state["shapes"])
        grid_t = torch.tensor(
            grid_np, dtype=torch.float32, device=self.device
        ).unsqueeze(0)
        shapes_t = torch.tensor(
            shapes_np, dtype=torch.float32, device=self.device
        ).unsqueeze(0)

        model_device = next(self.online_net.parameters()).device
        grid_t = grid_t.to(model_device)
        shapes_t = shapes_t.to(model_device)

        # --- MODIFIED: Always use eval() mode for action selection ---
        # This prevents BatchNorm errors with batch size 1 and uses mean
        # weights for NoisyLinear during inference.
        self.online_net.eval()
        # --- END MODIFIED ---

        with torch.no_grad():
            dist_or_q = self.online_net(grid_t, shapes_t)

        if self.use_distributional:
            probabilities = F.softmax(dist_or_q, dim=2)
            q_values = (probabilities * self.support).sum(dim=2)
        else:
            q_values = dist_or_q

        return q_values.squeeze(0).cpu().numpy()

    def _select_best_action_from_q_values(
        self, q_values_np: np.ndarray, valid_actions_indices: List[ActionType]
    ) -> ActionType:
        """Implements the two-stage selection logic."""
        best_overall_q = -float("inf")
        best_shape_slot = -1
        best_placement_idx = -1
        placement_qs_for_best_shape: List[float] = []

        valid_actions_by_slot: Dict[int, List[int]] = {
            i: [] for i in range(self.num_shape_slots)
        }
        for action_idx in valid_actions_indices:
            s_idx = action_idx // self.locations_per_shape
            p_idx = action_idx % self.locations_per_shape
            if 0 <= s_idx < self.num_shape_slots:
                valid_actions_by_slot[s_idx].append(p_idx)

        for s_idx in range(self.num_shape_slots):
            valid_placements_for_slot = valid_actions_by_slot[s_idx]
            if not valid_placements_for_slot:
                self._last_shape_slot_max_q_values[s_idx] = -float(
                    "inf"
                )  # Ensure it's set even if no valid actions
                continue

            global_indices = [
                s_idx * self.locations_per_shape + p_idx
                for p_idx in valid_placements_for_slot
            ]
            # Ensure indices are within bounds of the q_values array
            valid_global_indices = [
                idx for idx in global_indices if 0 <= idx < q_values_np.shape[0]
            ]
            if not valid_global_indices:
                self._last_shape_slot_max_q_values[s_idx] = -float("inf")
                continue

            q_values_for_valid_placements = q_values_np[valid_global_indices]

            if q_values_for_valid_placements.size == 0:
                self._last_shape_slot_max_q_values[s_idx] = -float("inf")
                continue

            max_q_for_slot = np.max(q_values_for_valid_placements)
            # Find the original placement index corresponding to the max Q value
            argmax_local = np.argmax(q_values_for_valid_placements)
            best_placement_idx_for_slot = valid_placements_for_slot[argmax_local]

            self._last_shape_slot_max_q_values[s_idx] = float(max_q_for_slot)

            if max_q_for_slot > best_overall_q:
                best_overall_q = max_q_for_slot
                best_shape_slot = s_idx
                best_placement_idx = best_placement_idx_for_slot
                start_g_idx = s_idx * self.locations_per_shape
                end_g_idx = start_g_idx + self.locations_per_shape
                # Ensure slicing is within bounds
                if 0 <= start_g_idx < end_g_idx <= q_values_np.shape[0]:
                    placement_qs_for_best_shape = q_values_np[
                        start_g_idx:end_g_idx
                    ].tolist()
                else:
                    placement_qs_for_best_shape = [
                        -float("inf")
                    ] * self.locations_per_shape

        if best_shape_slot != -1:
            final_action = (
                best_shape_slot * self.locations_per_shape + best_placement_idx
            )
            self._last_avg_max_q = float(best_overall_q)
            self._last_chosen_shape_slot = best_shape_slot
            self._last_placement_q_values_for_chosen_shape = placement_qs_for_best_shape

            if final_action not in valid_actions_indices:
                print(
                    f"CRITICAL WARNING: Chosen action {final_action} (Slot: {best_shape_slot}, Place: {best_placement_idx}, Q: {best_overall_q:.3f}) not in valid_actions! Falling back."
                )
                # print(f"Q-values shape: {q_values_np.shape}")
                # print(f"Valid actions: {valid_actions_indices}")
                self._reset_log_state()
                return random.choice(valid_actions_indices)

            return final_action
        else:
            # This case should ideally not happen if valid_actions_indices is not empty
            print(
                "Warning: No best action found despite having valid actions. Falling back."
            )
            self._last_avg_max_q = -float("inf")
            return random.choice(valid_actions_indices) if valid_actions_indices else 0

    def get_last_avg_max_q(self) -> float:
        return self._last_avg_max_q

    def get_last_shape_selection_info(
        self,
    ) -> Tuple[Optional[int], Optional[List[float]], Optional[List[float]]]:
        # Return copies to prevent external modification
        shape_qs = (
            self._last_shape_slot_max_q_values[:]
            if self._last_shape_slot_max_q_values
            else None
        )
        placement_qs = (
            self._last_placement_q_values_for_chosen_shape[:]
            if self._last_placement_q_values_for_chosen_shape
            else None
        )
        return (
            self._last_chosen_shape_slot,
            shape_qs,
            placement_qs,
        )


File: agent/agent_utils.py
# File: agent/agent_utils.py
import torch
import numpy as np
from typing import Union, Tuple, List, Dict
from utils.types import (
    StateType,
    NumpyBatch,
    NumpyNStepBatch,
    TensorBatch,
    TensorNStepBatch,
)
from config import EnvConfig


def np_batch_to_tensor(
    batch: Union[NumpyBatch, NumpyNStepBatch],
    is_n_step: bool,
    env_config: EnvConfig,
    device: torch.device,
) -> Union[TensorBatch, TensorNStepBatch]:
    """Converts numpy batch tuple (where states are dicts) to tensor tuple."""

    # --- MODIFIED: Unpack based on is_n_step flag ---
    if is_n_step:
        # Ensure the batch has 6 elements for N-step
        if len(batch) != 6:
            raise ValueError(f"Expected 6 elements in N-step batch, got {len(batch)}")
        states_dicts, actions, rewards, next_states_dicts, dones, discounts = batch
    else:
        # Ensure the batch has 5 elements for 1-step
        if len(batch) != 5:
            raise ValueError(f"Expected 5 elements in 1-step batch, got {len(batch)}")
        states_dicts, actions, rewards, next_states_dicts, dones = batch
        discounts = None  # No discount element in 1-step batch
    # --- END MODIFIED ---

    # --- Input Validation and Conversion (Remains the same) ---
    if not isinstance(states_dicts, list) or not isinstance(next_states_dicts, list):
        raise TypeError("States and next_states must be lists of dictionaries.")
    if not states_dicts or not next_states_dicts:
        raise ValueError("State lists cannot be empty.")

    # Validate first state dictionary structure
    first_state = states_dicts[0]
    if (
        not isinstance(first_state, dict)
        or "grid" not in first_state
        or "shapes" not in first_state
    ):
        raise ValueError("State dictionaries must contain 'grid' and 'shapes' keys.")

    try:
        grid_states = np.array([s["grid"] for s in states_dicts], dtype=np.float32)
        shape_states = np.array([s["shapes"] for s in states_dicts], dtype=np.float32)
        grid_next_states = np.array(
            [ns["grid"] for ns in next_states_dicts], dtype=np.float32
        )
        shape_next_states = np.array(
            [ns["shapes"] for ns in next_states_dicts], dtype=np.float32
        )
    except KeyError as e:
        raise ValueError(
            f"Missing key in state dictionary during batch conversion: {e}"
        )
    except Exception as e:
        raise RuntimeError(f"Error converting states to numpy arrays: {e}")

    expected_channels = env_config.GRID_FEATURES_PER_CELL
    if (
        grid_states.shape[1] != expected_channels
        or grid_next_states.shape[1] != expected_channels
    ):
        raise ValueError(
            f"Batch grid state channel mismatch! Expected {expected_channels}, got {grid_states.shape[1]} and {grid_next_states.shape[1]}."
        )

    try:
        grid_s_t = torch.tensor(grid_states, device=device, dtype=torch.float32)
        shape_s_t = torch.tensor(shape_states, device=device, dtype=torch.float32)
        a_t = torch.tensor(actions, device=device, dtype=torch.long).unsqueeze(1)
        r_t = torch.tensor(rewards, device=device, dtype=torch.float32).unsqueeze(1)
        grid_ns_t = torch.tensor(grid_next_states, device=device, dtype=torch.float32)
        shape_ns_t = torch.tensor(shape_next_states, device=device, dtype=torch.float32)
        # Convert dones to float for multiplication in loss calculation
        d_t = torch.tensor(dones, device=device, dtype=torch.float32).unsqueeze(1)
    except Exception as e:
        raise RuntimeError(f"Error converting numpy arrays to tensors: {e}")

    states_t = (grid_s_t, shape_s_t)
    next_states_t = (grid_ns_t, shape_ns_t)

    # --- MODIFIED: Return based on is_n_step ---
    if is_n_step:
        if discounts is None:
            raise ValueError("Discounts array is missing for N-step batch.")
        try:
            disc_t = torch.tensor(
                discounts, device=device, dtype=torch.float32
            ).unsqueeze(1)
        except Exception as e:
            raise RuntimeError(f"Error converting discounts to tensor: {e}")
        return states_t, a_t, r_t, next_states_t, d_t, disc_t
    else:
        return states_t, a_t, r_t, next_states_t, d_t
    # --- END MODIFIED ---


File: agent/loss_calculator.py
# File: agent/loss_calculator.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Optional, Union, Dict, List  # Added List, Dict
from config import DQNConfig, TensorBoardConfig, EnvConfig
from utils.types import (
    TensorBatch,
    TensorNStepBatch,
    NumpyBatch,
    NumpyNStepBatch,
)  # Added Numpy types
from utils.helpers import ensure_numpy
from .agent_utils import np_batch_to_tensor


class LossCalculator:
    """Handles loss calculation for the DQNAgent."""

    def __init__(
        self,
        online_net: nn.Module,
        target_net: nn.Module,
        env_config: EnvConfig,
        dqn_config: DQNConfig,
        tb_config: TensorBoardConfig,
        device: torch.device,
    ):
        self.online_net = online_net
        self.target_net = target_net
        self.env_config = env_config
        self.dqn_config = dqn_config
        self.tb_config = tb_config
        self.device = device
        self.gamma = (
            dqn_config.GAMMA
        )  # Still keep gamma for potential 1-step use or fallbacks
        self.use_distributional = dqn_config.USE_DISTRIBUTIONAL
        self.num_atoms = dqn_config.NUM_ATOMS
        self.v_min = dqn_config.V_MIN
        self.v_max = dqn_config.V_MAX

        if self.use_distributional:
            if self.num_atoms <= 1:
                raise ValueError("NUM_ATOMS must be >= 2 for Distributional RL")
            self.support = torch.linspace(
                self.v_min, self.v_max, self.num_atoms, device=self.device
            )
            self.delta_z = (self.v_max - self.v_min) / max(1, self.num_atoms - 1)
        else:
            self.loss_fn = nn.SmoothL1Loss(reduction="none", beta=1.0)

        self._last_avg_max_q: float = 0.0
        self._batch_q_values_for_actions_taken: Optional[np.ndarray] = None

    def compute_loss(
        self,
        # --- MODIFIED: Type hint for numpy batch from buffer ---
        batch: Union[NumpyBatch, NumpyNStepBatch],
        is_n_step: bool,
        is_weights: Optional[np.ndarray] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Computes the loss and TD errors, handling PER weights."""
        # --- Conversion now happens inside loss computation ---
        try:
            tensor_batch = np_batch_to_tensor(
                batch, is_n_step, self.env_config, self.device
            )
        except Exception as e:
            print(f"Error converting batch to tensor in compute_loss: {e}")
            # Return zero loss and no errors if conversion fails
            return torch.tensor(0.0, device=self.device, requires_grad=True), None

        if self.use_distributional:
            return self._compute_distributional_loss(
                tensor_batch, is_n_step, is_weights
            )
        else:
            return self._compute_standard_loss(tensor_batch, is_n_step, is_weights)

    def _compute_standard_loss(
        self,
        tensor_batch: Union[TensorBatch, TensorNStepBatch],
        is_n_step: bool,
        is_weights: Optional[np.ndarray],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Computes standard (SmoothL1) loss, applying PER weights."""
        # --- MODIFIED: Unpack based on is_n_step ---
        if is_n_step:
            # Ensure batch has 6 elements for N-step tensor batch
            if len(tensor_batch) != 6:
                raise ValueError(
                    f"Expected 6 elements in N-step tensor batch, got {len(tensor_batch)}"
                )
            states_tuple, actions, rewards, next_states_tuple, dones, discounts = (
                tensor_batch
            )
        else:
            # Ensure batch has 5 elements for 1-step tensor batch
            if len(tensor_batch) != 5:
                raise ValueError(
                    f"Expected 5 elements in 1-step tensor batch, got {len(tensor_batch)}"
                )
            states_tuple, actions, rewards, next_states_tuple, dones = tensor_batch
            # Use standard gamma for 1-step
            discounts = torch.full_like(rewards, self.gamma)
        # --- END MODIFIED ---

        grids, shapes = states_tuple
        next_grids, next_shapes = next_states_tuple
        batch_size = grids.size(0)

        with torch.no_grad():
            self.online_net.eval()
            online_next_q = self.online_net(next_grids, next_shapes)
            best_next_actions = online_next_q.argmax(dim=1, keepdim=True)

            self.target_net.eval()
            target_next_q_values = self.target_net(next_grids, next_shapes)
            target_q_for_best_actions = target_next_q_values.gather(
                1, best_next_actions
            )
            # --- MODIFIED: Use n-step reward and discount ---
            # target_q = R_n + discount_n * Q_target(s_{t+n}, argmax_a Q_online(s_{t+n}, a))
            target_q = rewards + discounts * target_q_for_best_actions * (1.0 - dones)
            # --- END MODIFIED ---

        self.online_net.train()
        current_q_all_actions = self.online_net(grids, shapes)
        current_q = current_q_all_actions.gather(1, actions)

        elementwise_loss = self.loss_fn(current_q, target_q)
        td_errors = (target_q - current_q).abs().detach()

        self._log_q_values(grids, shapes, current_q)

        loss = self._apply_per_weights(elementwise_loss, is_weights)
        td_errors_for_per = td_errors.squeeze() if td_errors is not None else None

        return loss, td_errors_for_per

    def _compute_distributional_loss(
        self,
        tensor_batch: Union[TensorBatch, TensorNStepBatch],
        is_n_step: bool,
        is_weights: Optional[np.ndarray],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Computes distributional (C51) loss using KL divergence, applying PER weights."""
        # --- MODIFIED: Unpack based on is_n_step ---
        if is_n_step:
            if len(tensor_batch) != 6:
                raise ValueError(
                    f"Expected 6 elements in N-step tensor batch, got {len(tensor_batch)}"
                )
            states_tuple, actions, _, _, _, _ = (
                tensor_batch  # Rewards/discounts used in target calculation
            )
        else:
            if len(tensor_batch) != 5:
                raise ValueError(
                    f"Expected 5 elements in 1-step tensor batch, got {len(tensor_batch)}"
                )
            states_tuple, actions, _, _, _ = tensor_batch
        # --- END MODIFIED ---

        grids, shapes = states_tuple
        batch_size = grids.size(0)

        with torch.no_grad():
            # --- Target calculation now uses n-step rewards/discounts internally ---
            target_dist = self._get_target_distribution(tensor_batch, is_n_step)

        self.online_net.train()
        current_dist_logits = self.online_net(grids, shapes)
        current_dist_log_probs = F.log_softmax(current_dist_logits, dim=2)

        actions_expanded = actions.view(batch_size, 1, 1).expand(-1, -1, self.num_atoms)
        current_log_probs_for_actions = current_dist_log_probs.gather(
            1, actions_expanded
        ).squeeze(1)

        elementwise_loss = -(target_dist * current_log_probs_for_actions).sum(dim=1)
        td_errors = elementwise_loss.detach()

        self._log_q_values(grids, shapes)

        loss = self._apply_per_weights(elementwise_loss, is_weights)
        td_errors_for_per = td_errors if td_errors is not None else None

        return loss, td_errors_for_per

    @torch.no_grad()
    def _get_target_distribution(
        self, batch: Union[TensorBatch, TensorNStepBatch], is_n_step: bool
    ) -> torch.Tensor:
        """Calculates the target distribution for C51 using Double DQN logic."""
        if not self.use_distributional:
            raise RuntimeError(
                "_get_target_distribution called when use_distributional is False"
            )

        # --- MODIFIED: Unpack rewards and discounts based on is_n_step ---
        if is_n_step:
            if len(batch) != 6:
                raise ValueError(
                    f"Expected 6 elements in N-step tensor batch, got {len(batch)}"
                )
            _, _, rewards, next_states_tuple, dones, discounts = batch
        else:
            if len(batch) != 5:
                raise ValueError(
                    f"Expected 5 elements in 1-step tensor batch, got {len(batch)}"
                )
            _, _, rewards, next_states_tuple, dones = batch[:5]
            discounts = torch.full_like(rewards, self.gamma)  # Use standard gamma
        # --- END MODIFIED ---

        next_grids, next_shapes = next_states_tuple
        batch_size = next_grids.size(0)

        self.online_net.eval()
        online_next_dist_logits = self.online_net(next_grids, next_shapes)
        online_next_probs = F.softmax(online_next_dist_logits, dim=2)
        online_expected_q = (
            online_next_probs * self.support.unsqueeze(0).unsqueeze(0)
        ).sum(dim=2)
        best_next_actions = online_expected_q.argmax(dim=1)

        self.target_net.eval()
        target_next_dist_logits = self.target_net(next_grids, next_shapes)
        target_next_probs = F.softmax(target_next_dist_logits, dim=2)
        target_next_best_dist_probs = target_next_probs[
            torch.arange(batch_size), best_next_actions
        ]

        # --- MODIFIED: Project using n-step rewards and discounts ---
        # Tz = R_n + discount_n * z
        Tz = rewards + discounts * self.support.unsqueeze(0) * (1.0 - dones)
        # --- END MODIFIED ---

        Tz = Tz.clamp(self.v_min, self.v_max)
        b = (Tz - self.v_min) / self.delta_z
        lower_idx = b.floor().long()
        upper_idx = b.ceil().long()

        # Handle edge cases and integer bins
        lower_idx = torch.where(lower_idx == upper_idx, lower_idx - 1, lower_idx)
        upper_idx = torch.where(lower_idx >= upper_idx, lower_idx + 1, upper_idx)
        lower_idx = lower_idx.clamp(0, self.num_atoms - 1)
        upper_idx = upper_idx.clamp(0, self.num_atoms - 1)

        weight_u = b - b.floor()
        weight_l = 1.0 - weight_u

        target_dist = torch.zeros_like(target_next_best_dist_probs)
        target_dist.scatter_add_(1, lower_idx, target_next_best_dist_probs * weight_l)
        target_dist.scatter_add_(1, upper_idx, target_next_best_dist_probs * weight_u)

        return target_dist

    def _log_q_values(
        self,
        grids: torch.Tensor,
        shapes: torch.Tensor,
        current_q: Optional[torch.Tensor] = None,
    ):
        with torch.no_grad():
            self.online_net.eval()
            q_or_dist = self.online_net(grids, shapes)
            if self.use_distributional:
                probabilities = F.softmax(q_or_dist, dim=2)
                q_values = (probabilities * self.support.unsqueeze(0).unsqueeze(0)).sum(
                    dim=2
                )
            else:
                q_values = q_or_dist
            self._last_avg_max_q = q_values.max(dim=1)[0].mean().item()

        if (
            self.tb_config.LOG_SHAPE_PLACEMENT_Q_VALUES
            and current_q is not None
            and not self.use_distributional
        ):
            self._batch_q_values_for_actions_taken = (
                current_q.detach().squeeze().cpu().numpy()
            )
        else:
            self._batch_q_values_for_actions_taken = None

    def _apply_per_weights(
        self, elementwise_loss: torch.Tensor, is_weights: Optional[np.ndarray]
    ) -> torch.Tensor:
        if is_weights is not None:
            is_weights_t = torch.tensor(
                is_weights, dtype=torch.float32, device=self.device
            )
            # Ensure dimensions match for broadcasting (elementwise_loss is likely [B])
            if (
                elementwise_loss.ndim == 1
                and is_weights_t.ndim == 1
                and elementwise_loss.shape[0] == is_weights_t.shape[0]
            ):
                loss = (is_weights_t * elementwise_loss).mean()
            elif elementwise_loss.ndim > 1 and is_weights_t.ndim == 1:
                # Try unsqueezing weights if loss has extra dimensions
                is_weights_t = is_weights_t.unsqueeze(1)
                loss = (is_weights_t * elementwise_loss).mean()
            else:
                print(
                    f"Warning: Mismatched shapes for loss ({elementwise_loss.shape}) and IS weights ({is_weights_t.shape}). Using unweighted mean."
                )
                loss = elementwise_loss.mean()
        else:
            loss = elementwise_loss.mean()
        return loss

    def get_last_avg_max_q(self) -> float:
        return self._last_avg_max_q

    def get_last_batch_q_values_for_actions(self) -> Optional[np.ndarray]:
        return self._batch_q_values_for_actions_taken


File: agent/__init__.py
# File: agent/__init__.py
from .dqn_agent import DQNAgent
from .action_selector import ActionSelector
from .loss_calculator import LossCalculator
from .model_factory import create_network

__all__ = [
    "DQNAgent",
    "ActionSelector",
    "LossCalculator",
    "create_network",
]


File: agent/dqn_agent.py
# File: agent/dqn_agent.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
import traceback
import copy
from typing import Tuple, List, Dict, Any, Optional, Union

from config import (
    ModelConfig,
    EnvConfig,
    DQNConfig,
    DEVICE,
    TensorBoardConfig,
    BufferConfig,
)
from environment.game_state import StateType
from utils.types import ActionType, AgentStateDict, NumpyBatch, NumpyNStepBatch
from agent.model_factory import create_network
from agent.networks.noisy_layer import NoisyLinear
from .action_selector import ActionSelector
from .loss_calculator import LossCalculator


class DQNAgent:
    """DQN Agent orchestrating network, action selection, and loss calculation."""

    def __init__(
        self,
        config: ModelConfig,
        dqn_config: DQNConfig,
        env_config: EnvConfig,
        buffer_config: BufferConfig,
    ):
        print("[DQNAgent] Initializing...")
        self.device = DEVICE
        self.env_config = env_config
        self.dqn_config = dqn_config
        self.tb_config = TensorBoardConfig()
        self.buffer_config = buffer_config
        self.action_dim = env_config.ACTION_DIM

        self.online_net = create_network(
            env_config=self.env_config,
            action_dim=self.action_dim,
            model_config=config,
            dqn_config=dqn_config,
        ).to(self.device)
        self.target_net = create_network(
            env_config=self.env_config,
            action_dim=self.action_dim,
            model_config=config,
            dqn_config=dqn_config,
        ).to(self.device)

        self.target_net.load_state_dict(self.online_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.AdamW(
            self.online_net.parameters(),
            lr=dqn_config.LEARNING_RATE,
            eps=dqn_config.ADAM_EPS,
            weight_decay=1e-5,
        )
        self.scheduler = self._initialize_scheduler(dqn_config)

        self.action_selector = ActionSelector(
            self.online_net, self.env_config, self.dqn_config, self.device
        )
        self.loss_calculator = LossCalculator(
            self.online_net,
            self.target_net,
            self.env_config,
            self.dqn_config,
            self.tb_config,
            self.device,
        )

        self._print_init_info(dqn_config)

    def _initialize_scheduler(
        self, dqn_config: DQNConfig
    ) -> Optional[CosineAnnealingLR]:
        if dqn_config.USE_LR_SCHEDULER:
            print(
                f"[DQNAgent] Using CosineAnnealingLR scheduler (T_max={dqn_config.LR_SCHEDULER_T_MAX}, eta_min={dqn_config.LR_SCHEDULER_ETA_MIN})"
            )
            return CosineAnnealingLR(
                self.optimizer,
                T_max=dqn_config.LR_SCHEDULER_T_MAX,
                eta_min=dqn_config.LR_SCHEDULER_ETA_MIN,
            )
        return None

    def _print_init_info(self, dqn_config: DQNConfig):
        print(f"[DQNAgent] Using Device: {self.device}")
        print(f"[DQNAgent] Online Network: {type(self.online_net).__name__}")
        print(f"[DQNAgent] Using Double DQN: {dqn_config.USE_DOUBLE_DQN}")
        print(f"[DQNAgent] Using Dueling: {dqn_config.USE_DUELING}")
        print(f"[DQNAgent] Using Noisy Nets: {dqn_config.USE_NOISY_NETS}")
        print(f"[DQNAgent] Using Distributional (C51): {dqn_config.USE_DISTRIBUTIONAL}")
        total_params = sum(
            p.numel() for p in self.online_net.parameters() if p.requires_grad
        )
        print(f"[DQNAgent] Trainable Parameters: {total_params / 1e6:.2f} M")

    def select_action(
        self, state: StateType, epsilon: float, valid_actions_indices: List[ActionType]
    ) -> ActionType:
        self.online_net.to(self.device)
        # --- Epsilon is ignored by ActionSelector if USE_NOISY_NETS is true ---
        return self.action_selector.select_action(state, epsilon, valid_actions_indices)

    def compute_loss(
        self,
        batch: Union[NumpyBatch, NumpyNStepBatch],
        is_weights: Optional[np.ndarray] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        is_n_step = self.buffer_config.USE_N_STEP and self.buffer_config.N_STEP > 1
        # --- Set networks to train() for Noisy Nets before loss calculation ---
        if self.dqn_config.USE_NOISY_NETS:
            self.online_net.train()
            self.target_net.train()  # Target also uses noise during loss calculation
        else:
            self.online_net.train()
            self.target_net.eval()  # Standard eval mode for target net
        loss, td_errors = self.loss_calculator.compute_loss(
            batch, is_n_step, is_weights
        )
        # --- Restore target net eval mode if not noisy ---
        if not self.dqn_config.USE_NOISY_NETS:
            self.target_net.eval()
        return loss, td_errors

    def update(self, loss: torch.Tensor) -> Optional[float]:
        grad_norm = None
        self.optimizer.zero_grad(set_to_none=True)
        loss.backward()
        self.online_net.train()  # Keep online net in train mode

        if self.dqn_config.GRADIENT_CLIP_NORM > 0:
            try:
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    self.online_net.parameters(),
                    max_norm=self.dqn_config.GRADIENT_CLIP_NORM,
                    error_if_nonfinite=True,
                ).item()
            except RuntimeError as clip_err:
                print(
                    f"ERROR: Gradient clipping failed: {clip_err}. Skipping optimizer step."
                )
                self.optimizer.zero_grad(set_to_none=True)
                return None
            except Exception as clip_err:
                print(f"Warning: Unexpected error during gradient clipping: {clip_err}")
                grad_norm = None

        try:
            self.optimizer.step()
        except Exception as optim_err:
            print(f"ERROR: Optimizer step failed: {optim_err}")
            traceback.print_exc()
            return None

        if self.scheduler:
            self.scheduler.step()

        # --- Reset noise after optimizer step ---
        if self.dqn_config.USE_NOISY_NETS:
            self.online_net.reset_noise()
            self.target_net.reset_noise()
        # --- END Reset Noise ---

        return grad_norm

    def update_target_network(self):
        self.target_net.load_state_dict(self.online_net.state_dict())
        self.target_net.eval()  # Standard eval mode after update

    def get_state_dict(self) -> AgentStateDict:
        self.online_net.to(self.device)
        self.target_net.to(self.device)
        self.online_net.cpu()
        self.target_net.cpu()

        cpu_optimizer = optim.AdamW(
            self.online_net.parameters(),
            lr=self.dqn_config.LEARNING_RATE,
            eps=self.dqn_config.ADAM_EPS,
            weight_decay=1e-5,
        )
        cpu_optimizer.load_state_dict(self.optimizer.state_dict())
        optimizer_state_dict_cpu = cpu_optimizer.state_dict()
        del cpu_optimizer

        state = {
            "online_net_state_dict": self.online_net.state_dict(),
            "target_net_state_dict": self.target_net.state_dict(),
            "optimizer_state_dict": optimizer_state_dict_cpu,
            "scheduler_state_dict": (
                self.scheduler.state_dict() if self.scheduler else None
            ),
        }

        self.online_net.to(self.device)
        self.target_net.to(self.device)
        self.online_net.train()
        self.target_net.eval()

        return state

    def load_state_dict(self, state_dict: AgentStateDict):
        print(f"[DQNAgent] Loading state dict. Target device: {self.device}")
        try:
            self.online_net.load_state_dict(
                state_dict["online_net_state_dict"], strict=False
            )
            if "target_net_state_dict" in state_dict:
                self.target_net.load_state_dict(
                    state_dict["target_net_state_dict"], strict=False
                )
            else:
                print("Warning: Target net state missing, copying from online.")
                self.target_net.load_state_dict(self.online_net.state_dict())

            self.online_net.to(self.device)
            self.target_net.to(self.device)

            if "optimizer_state_dict" in state_dict:
                self.optimizer.load_state_dict(state_dict["optimizer_state_dict"])
                print("[DQNAgent] Optimizer state loaded.")
            else:
                print("Warning: Optimizer state not found. Resetting optimizer.")
                self._reset_optimizer()

            if (
                self.scheduler
                and "scheduler_state_dict" in state_dict
                and state_dict["scheduler_state_dict"] is not None
            ):
                try:
                    self.scheduler.load_state_dict(state_dict["scheduler_state_dict"])
                    print("[DQNAgent] Scheduler state loaded.")
                except Exception as e:
                    print(
                        f"Warning: Could not load LR scheduler state ({e}). Resetting."
                    )
                    self.scheduler = self._initialize_scheduler(self.dqn_config)
            elif self.scheduler:
                print("Warning: LR Scheduler state not found. Resetting.")
                self.scheduler = self._initialize_scheduler(self.dqn_config)

            self.online_net.train()
            self.target_net.eval()
            print("[DQNAgent] load_state_dict complete.")

        except Exception as e:
            print(f"CRITICAL ERROR during DQNAgent.load_state_dict: {e}")
            traceback.print_exc()
            print("Attempting to reset agent state after load failure.")
            self._reset_optimizer()
            self.target_net.load_state_dict(self.online_net.state_dict())
            self.online_net.train()
            self.target_net.eval()

    def _reset_optimizer(self):
        self.optimizer = optim.AdamW(
            self.online_net.parameters(),
            lr=self.dqn_config.LEARNING_RATE,
            eps=self.dqn_config.ADAM_EPS,
            weight_decay=1e-5,
        )

    def get_last_avg_max_q(self) -> float:
        return self.loss_calculator.get_last_avg_max_q()

    def get_last_shape_selection_info(
        self,
    ) -> Tuple[Optional[int], Optional[List[float]], Optional[List[float]]]:
        return self.action_selector.get_last_shape_selection_info()

    def get_last_batch_q_values_for_actions(self) -> Optional[np.ndarray]:
        return self.loss_calculator.get_last_batch_q_values_for_actions()


File: agent/model_factory.py
# File: agent/model_factory.py
import torch.nn as nn
from config import ModelConfig, EnvConfig, DQNConfig
from typing import Type

from agent.networks.agent_network import AgentNetwork


def create_network(
    # --- MODIFIED: Pass EnvConfig instance ---
    env_config: EnvConfig,
    # --- END MODIFIED ---
    action_dim: int,
    model_config: ModelConfig,
    dqn_config: DQNConfig,
) -> nn.Module:
    """Creates the AgentNetwork based on configuration."""

    print(
        f"[ModelFactory] Creating AgentNetwork (Dueling: {dqn_config.USE_DUELING}, NoisyNets Heads: {dqn_config.USE_NOISY_NETS})"
    )

    # Pass the specific sub-config ModelConfig.Network
    return AgentNetwork(
        # --- MODIFIED: Pass EnvConfig instance ---
        env_config=env_config,
        # --- END MODIFIED ---
        action_dim=action_dim,
        model_config=model_config.Network,  # Pass the Network sub-config
        dqn_config=dqn_config,
        dueling=dqn_config.USE_DUELING,
        use_noisy=dqn_config.USE_NOISY_NETS,
    )


File: agent/replay_buffer/uniform_buffer.py
# File: agent/replay_buffer/uniform_buffer.py
import random
import numpy as np
from collections import deque
from typing import Deque, Tuple, Optional, Any, Dict, Union, List  # Added List
from .base_buffer import ReplayBufferBase

# --- MODIFIED: Import specific StateType ---
from environment.game_state import StateType  # Use the Dict type

# --- END MODIFIED ---
from utils.types import Transition, ActionType, NumpyBatch, NumpyNStepBatch
from utils.helpers import save_object, load_object


class UniformReplayBuffer(ReplayBufferBase):
    """Standard uniform experience replay buffer."""

    def __init__(self, capacity: int):
        super().__init__(capacity)
        self.buffer: Deque[Transition] = deque(maxlen=capacity)

    def push(
        self,
        state: StateType,  # State is now a Dict
        action: ActionType,
        reward: float,
        next_state: StateType,  # Next state is now a Dict
        done: bool,
        **kwargs,
    ):
        n_step_discount = kwargs.get("n_step_discount")
        # Store the dictionary state directly in the Transition
        transition = Transition(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done,
            n_step_discount=n_step_discount,
        )
        self.buffer.append(transition)

    # --- MODIFIED: Sample unpacks dictionary states ---
    def sample(self, batch_size: int) -> Optional[Union[NumpyBatch, NumpyNStepBatch]]:
        if len(self.buffer) < batch_size:
            return None

        batch_transitions: List[Transition] = random.sample(self.buffer, batch_size)
        is_n_step = batch_transitions[0].n_step_discount is not None

        # Unpack transitions, keeping states as dicts for now
        states_dicts = [t.state for t in batch_transitions]
        actions_np = np.array([t.action for t in batch_transitions], dtype=np.int64)
        rewards_np = np.array([t.reward for t in batch_transitions], dtype=np.float32)
        next_states_dicts = [t.next_state for t in batch_transitions]
        dones_np = np.array([t.done for t in batch_transitions], dtype=np.float32)

        if is_n_step:
            discounts_np = np.array(
                [t.n_step_discount for t in batch_transitions], dtype=np.float32
            )
            # Return states as list of dicts, agent will handle conversion
            return (
                states_dicts,
                actions_np,
                rewards_np,
                next_states_dicts,
                dones_np,
                discounts_np,
            )
        else:
            # Return states as list of dicts
            return states_dicts, actions_np, rewards_np, next_states_dicts, dones_np

    # --- END MODIFIED ---

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        pass

    def set_beta(self, beta: float):
        pass

    def flush_pending(self):
        pass

    def __len__(self) -> int:
        return len(self.buffer)

    def get_state(self) -> Dict[str, Any]:
        return {"buffer": list(self.buffer)}

    def load_state_from_data(self, state: Dict[str, Any]):
        saved_buffer_list = state.get("buffer", [])
        # Ensure loaded items are Transitions (or handle potential errors)
        valid_transitions = [t for t in saved_buffer_list if isinstance(t, Transition)]
        if len(valid_transitions) != len(saved_buffer_list):
            print(
                f"Warning: Filtered out {len(saved_buffer_list) - len(valid_transitions)} invalid items during buffer load."
            )
        self.buffer = deque(valid_transitions, maxlen=self.capacity)
        print(f"[UniformReplayBuffer] Loaded {len(self.buffer)} transitions.")

    def save_state(self, filepath: str):
        state = self.get_state()
        save_object(state, filepath)

    def load_state(self, filepath: str):
        state = load_object(filepath)
        self.load_state_from_data(state)


File: agent/replay_buffer/buffer_utils.py
# File: agent/replay_buffer/buffer_utils.py
from config import BufferConfig, DQNConfig
from .base_buffer import ReplayBufferBase
from .uniform_buffer import UniformReplayBuffer
from .prioritized_buffer import PrioritizedReplayBuffer
from .nstep_buffer import NStepBufferWrapper  # Import the wrapper


def create_replay_buffer(
    config: BufferConfig, dqn_config: DQNConfig
) -> ReplayBufferBase:
    """Factory function to create the replay buffer based on configuration."""

    print("[BufferFactory] Creating replay buffer...")
    print(f"  Type: {'Prioritized' if config.USE_PER else 'Uniform'}")
    print(f"  Capacity: {config.REPLAY_BUFFER_SIZE/1e6:.1f}M")

    if config.USE_PER:
        print(
            f"  PER alpha={config.PER_ALPHA}, eps={config.PER_EPSILON}, beta_start={config.PER_BETA_START}"
        )
        core_buffer = PrioritizedReplayBuffer(
            capacity=config.REPLAY_BUFFER_SIZE,
            alpha=config.PER_ALPHA,
            epsilon=config.PER_EPSILON,
        )
    else:
        core_buffer = UniformReplayBuffer(capacity=config.REPLAY_BUFFER_SIZE)

    # --- MODIFIED: Wrap with NStepBufferWrapper if enabled ---
    if config.USE_N_STEP and config.N_STEP > 1:
        print(
            f"  N-Step Wrapper: Enabled (N={config.N_STEP}, gamma={dqn_config.GAMMA})"
        )
        final_buffer = NStepBufferWrapper(
            wrapped_buffer=core_buffer,
            n_step=config.N_STEP,
            gamma=dqn_config.GAMMA,
        )
    else:
        print(f"  N-Step Wrapper: Disabled")
        final_buffer = core_buffer
    # --- END MODIFIED ---

    print(f"[BufferFactory] Final buffer type: {type(final_buffer).__name__}")
    return final_buffer


File: agent/replay_buffer/__init__.py


File: agent/replay_buffer/prioritized_buffer.py
# File: agent/replay_buffer/prioritized_buffer.py
import random
import numpy as np
from typing import Optional, Tuple, Any, Dict, Union, List
from .base_buffer import ReplayBufferBase
from .sum_tree import SumTree

# --- MODIFIED: Import specific StateType ---
from environment.game_state import StateType  # Use the Dict type

# --- END MODIFIED ---
from utils.types import (
    Transition,
    ActionType,
    NumpyBatch,
    PrioritizedNumpyBatch,  # Added
    NumpyNStepBatch,
    PrioritizedNumpyNStepBatch,  # Added
)
from utils.helpers import save_object, load_object


class PrioritizedReplayBuffer(ReplayBufferBase):
    """Prioritized Experience Replay (PER) buffer using a SumTree."""

    def __init__(self, capacity: int, alpha: float, epsilon: float):
        super().__init__(capacity)
        self.tree = SumTree(capacity)
        self.alpha = alpha
        self.epsilon = epsilon
        self.beta = 0.0  # Initial beta, will be annealed by Trainer
        self.max_priority = 1.0  # Initial max priority

    def push(
        self,
        state: StateType,  # State is Dict
        action: ActionType,
        reward: float,
        next_state: StateType,  # Next state is Dict
        done: bool,
        **kwargs,
    ):
        """Adds new experience with maximum priority."""
        n_step_discount = kwargs.get("n_step_discount")
        transition = Transition(
            state, action, reward, next_state, done, n_step_discount
        )
        # Add with current max priority to ensure new samples get seen
        self.tree.add(self.max_priority, transition)

    def sample(
        self, batch_size: int
    ) -> Optional[Union[PrioritizedNumpyBatch, PrioritizedNumpyNStepBatch]]:
        """Samples batch using priorities, calculates IS weights."""
        if len(self) < batch_size:
            # print(f"PER Sample Warning: Not enough samples ({len(self)} < {batch_size})")
            return None

        batch_data: List[Transition] = []
        indices = np.empty(batch_size, dtype=np.int64)
        priorities = np.empty(batch_size, dtype=np.float64)
        segment = self.tree.total() / batch_size

        for i in range(batch_size):
            # Sample uniformly from each segment
            s = random.uniform(segment * i, segment * (i + 1))
            # Ensure s is within valid range, slightly above 0
            s = max(1e-9, min(s, self.tree.total()))

            try:
                idx, p, data = self.tree.get(s)
            except Exception as e:
                print(
                    f"ERROR during SumTree.get(s={s}, total={self.tree.total()}): {e}"
                )
                return None  # Cannot proceed if sampling fails

            retries = 0
            max_retries = 5
            # Handle cases where data might be invalid (e.g., None during fill)
            while not isinstance(data, Transition) and retries < max_retries:
                # print(f"Warning: PER sample retrieved invalid data (type: {type(data)}). Retrying...")
                # Resample from the entire range if invalid data found
                s_retry = random.uniform(1e-9, self.tree.total())
                try:
                    idx, p, data = self.tree.get(s_retry)
                except Exception as e:
                    print(f"ERROR during SumTree.get() retry: {e}")
                    return None
                retries += 1

            if not isinstance(data, Transition):
                print(
                    f"ERROR: PER sample failed after {max_retries} retries. Skipping batch."
                )
                # This could indicate a persistent issue with the SumTree or data storage
                return None

            priorities[i] = p
            batch_data.append(data)
            indices[i] = idx

        # Calculate Importance Sampling (IS) weights
        sampling_probs = np.maximum(
            priorities / self.tree.total(), 1e-9
        )  # Avoid division by zero
        # is_weights = np.power(len(self) * sampling_probs, -self.beta)
        # Use current number of entries (n_entries) instead of capacity (len(self))
        num_entries = max(
            1, len(self)
        )  # Avoid division by zero if buffer is empty somehow
        is_weights = np.power(num_entries * sampling_probs, -self.beta)

        # Normalize weights by max weight for stability (clip weights for safety)
        is_weights = is_weights / (is_weights.max() + 1e-9)
        is_weights = np.clip(is_weights, 1e-6, 100.0).astype(np.float32)

        # Check if N-step based on first item
        # Note: Assumes all items in batch are either N-step or 1-step consistently
        is_n_step = batch_data[0].n_step_discount is not None
        batch_tuple = self._unpack_batch(batch_data, is_n_step)  # Returns dict states

        return batch_tuple, indices, is_weights

    def _unpack_batch(
        self, batch_data: List[Transition], is_n_step: bool
    ) -> Union[NumpyBatch, NumpyNStepBatch]:
        """Unpacks list of Transitions into numpy arrays, keeping states as dicts."""
        # --- Keeps states as list of dictionaries ---
        states_dicts = [t.state for t in batch_data]
        actions_np = np.array([t.action for t in batch_data], dtype=np.int64)
        rewards_np = np.array([t.reward for t in batch_data], dtype=np.float32)
        next_states_dicts = [t.next_state for t in batch_data]
        dones_np = np.array(
            [t.done for t in batch_data], dtype=np.float32
        )  # Use float32 for dones

        if is_n_step:
            discounts_np = np.array(
                [t.n_step_discount for t in batch_data], dtype=np.float32
            )
            return (
                states_dicts,
                actions_np,
                rewards_np,
                next_states_dicts,
                dones_np,
                discounts_np,
            )
        else:
            return (states_dicts, actions_np, rewards_np, next_states_dicts, dones_np)

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """Updates priorities of experiences at given tree indices."""
        if len(indices) != len(priorities):
            print(
                f"Error: Mismatch indices ({len(indices)}) / priorities ({len(priorities)}) in PER update"
            )
            return

        # Apply alpha transformation to TD errors before updating the tree
        priorities = np.power(np.abs(priorities) + self.epsilon, self.alpha)
        priorities = np.maximum(priorities, 1e-6)  # Ensure positive priorities

        for idx, priority in zip(indices, priorities):
            # Validate index corresponds to a leaf node in the tree structure
            if not (self.tree.capacity - 1 <= idx < 2 * self.tree.capacity - 1):
                # print(f"Warning: Invalid tree index {idx} provided to update_priorities. Skipping.")
                continue
            self.tree.update(idx, priority)
            # Update max priority seen so far
            self.max_priority = max(self.max_priority, priority)

    def set_beta(self, beta: float):
        """Updates the beta exponent for IS weight calculation."""
        self.beta = beta

    def flush_pending(self):
        pass  # No-op for core PER buffer

    def __len__(self) -> int:
        return self.tree.n_entries

    def get_state(self) -> Dict[str, Any]:
        # Ensure data contains serializable items (should be Transitions)
        serializable_data = [
            d if isinstance(d, Transition) else None for d in self.tree.data
        ]
        return {
            "tree_nodes": self.tree.tree.copy(),
            "tree_data": serializable_data,  # Save potentially filtered list
            "tree_write_ptr": self.tree.write_ptr,
            "tree_n_entries": self.tree.n_entries,
            "max_priority": self.max_priority,
            "alpha": self.alpha,
            "epsilon": self.epsilon,
            # Beta is transient, derived from training progress, no need to save
        }

    def load_state_from_data(self, state: Dict[str, Any]):
        if "tree_nodes" not in state or "tree_data" not in state:
            print("Error: Invalid PER state format during load. Skipping.")
            self.tree = SumTree(self.capacity)  # Reinitialize tree
            self.max_priority = 1.0
            return

        loaded_capacity = len(state["tree_data"])
        if loaded_capacity != self.capacity:
            print(
                f"Warning: Loaded PER capacity ({loaded_capacity}) != current ({self.capacity}). Recreating tree."
            )
            self.tree = SumTree(self.capacity)
            num_to_load = min(loaded_capacity, self.capacity)
            # Load only valid transitions
            valid_data = [
                d for d in state["tree_data"][:num_to_load] if isinstance(d, Transition)
            ]
            # Populate the data array directly
            self.tree.data[: len(valid_data)] = valid_data
            self.tree.write_ptr = state.get("tree_write_ptr", 0) % self.capacity
            self.tree.n_entries = len(
                valid_data
            )  # Correct n_entries based on valid data loaded
            # Rebuild tree priorities from loaded data (expensive but necessary if capacity changed)
            print("Rebuilding SumTree priorities from loaded data...")
            self.tree.tree.fill(0)  # Clear existing tree sums
            self.max_priority = 1.0  # Reset max priority
            # Re-calculate priorities based on existing data - assume max priority for loaded data
            for i in range(self.tree.n_entries):
                # Assign default max priority during rebuild
                self.tree.update(i + self.capacity - 1, self.max_priority)
            print(f"[PER] Rebuilt tree with {self.tree.n_entries} transitions.")

        else:  # Capacities match
            self.tree.tree = state["tree_nodes"]
            # Load data, filtering invalid entries
            valid_data = [
                d for d in state["tree_data"] if isinstance(d, Transition) or d is None
            ]  # Allow None placeholders
            if len(valid_data) != len(state["tree_data"]):
                print(
                    f"Warning: Filtered {len(state['tree_data']) - len(valid_data)} invalid items from PER data during load."
                )
            self.tree.data = np.array(valid_data, dtype=object)  # Ensure numpy array

            self.tree.write_ptr = state.get("tree_write_ptr", 0)
            self.tree.n_entries = state.get("tree_n_entries", 0)
            # Ensure n_entries is consistent with actual data after filtering
            actual_entries = sum(1 for d in self.tree.data if isinstance(d, Transition))
            if self.tree.n_entries != actual_entries:
                print(
                    f"Warning: Correcting PER n_entries from {self.tree.n_entries} to {actual_entries}"
                )
                self.tree.n_entries = actual_entries

            self.max_priority = state.get("max_priority", 1.0)
            print(f"[PER] Loaded {self.tree.n_entries} transitions.")

        # Load hyperparameters
        self.alpha = state.get("alpha", self.alpha)
        self.epsilon = state.get("epsilon", self.epsilon)
        # Beta is set by Trainer based on current step, not loaded

    def save_state(self, filepath: str):
        save_object(self.get_state(), filepath)

    def load_state(self, filepath: str):
        state = load_object(filepath)
        self.load_state_from_data(state)


File: agent/replay_buffer/base_buffer.py
# File: agent/replay_buffer/base_buffer.py
# (No changes needed)
from abc import ABC, abstractmethod
from typing import Any, Optional, Tuple, Dict
import numpy as np
from utils.types import StateType, ActionType


class ReplayBufferBase(ABC):
    """Abstract base class for all replay buffers."""

    def __init__(self, capacity: int):
        self.capacity = capacity

    @abstractmethod
    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
        **kwargs  # Allow passing extra info like n_step_discount
    ):
        """Add a new experience to the buffer."""
        pass

    @abstractmethod
    def sample(
        self, batch_size: int
    ) -> Optional[Any]:  # Return type depends on PER/NStep
        """Sample a batch of experiences from the buffer."""
        pass

    @abstractmethod
    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """Update priorities for PER (no-op for uniform buffer)."""
        pass

    @abstractmethod
    def __len__(self) -> int:
        """Return the current size of the buffer."""
        pass

    @abstractmethod
    def set_beta(self, beta: float):
        """Set the beta value for PER IS weights (no-op for uniform buffer)."""
        pass

    @abstractmethod
    def flush_pending(self):
        """Process any pending transitions (e.g., for N-step)."""
        pass

    @abstractmethod
    def get_state(self) -> Dict[str, Any]:
        """Return the buffer's state as a dictionary suitable for saving."""
        pass

    @abstractmethod
    def load_state_from_data(self, state: Dict[str, Any]):
        """Load the buffer's state from a dictionary."""
        pass

    @abstractmethod
    def save_state(self, filepath: str):
        """Save the buffer's state to a file."""
        pass

    @abstractmethod
    def load_state(self, filepath: str):
        """Load the buffer's state from a file."""
        pass


File: agent/replay_buffer/nstep_buffer.py
# File: agent/replay_buffer/nstep_buffer.py
from collections import deque
import numpy as np
from typing import Deque, Tuple, Optional, Any, Dict, List, Union
from .base_buffer import ReplayBufferBase
from environment.game_state import StateType  # Use the Dict type
from utils.types import Transition, ActionType, NumpyBatch, NumpyNStepBatch
from utils.helpers import save_object, load_object


class NStepBufferWrapper(ReplayBufferBase):
    """Wraps another buffer to implement N-step returns."""

    def __init__(self, wrapped_buffer: ReplayBufferBase, n_step: int, gamma: float):
        super().__init__(wrapped_buffer.capacity)
        if n_step <= 0:
            raise ValueError("N-step must be positive")
        self.wrapped_buffer = wrapped_buffer
        self.n_step = n_step
        self.gamma = gamma
        # Deque stores (state_dict, action, reward, next_state_dict, done) tuples
        self.n_step_deque: Deque[
            Tuple[StateType, ActionType, float, StateType, bool]
        ] = deque(maxlen=n_step)
        print(f"[NStepWrapper] Initialized with n={n_step}, gamma={gamma}")

    def _calculate_n_step_transition(
        self, current_deque_list: List[Tuple]
    ) -> Optional[Transition]:
        """Calculates the N-step return from a list copy of the deque."""
        if not current_deque_list:
            return None

        n_step_reward = 0.0
        discount_accum = 1.0
        effective_n = len(current_deque_list)
        state_0, action_0 = (
            current_deque_list[0][0],
            current_deque_list[0][1],
        )

        for i in range(effective_n):
            _s, _a, reward_i, next_state_i, done_i = current_deque_list[i]
            n_step_reward += discount_accum * reward_i
            if done_i:
                # Episode terminated within N steps
                n_step_next_state = next_state_i  # State after the terminal action
                n_step_done = True
                n_step_discount = self.gamma ** (i + 1)
                return Transition(
                    state_0,
                    action_0,
                    n_step_reward,
                    n_step_next_state,
                    n_step_done,
                    n_step_discount,
                )
            discount_accum *= self.gamma

        # Loop completed without terminal state
        n_step_next_state = current_deque_list[-1][3]  # next_state from Nth transition
        n_step_done = current_deque_list[-1][4]  # done flag from Nth transition
        n_step_discount = self.gamma**effective_n
        return Transition(
            state_0,
            action_0,
            n_step_reward,
            n_step_next_state,
            n_step_done,
            n_step_discount,
        )

    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
    ):
        self.n_step_deque.append((state, action, reward, next_state, done))

        if len(self.n_step_deque) < self.n_step:
            if done:
                self._flush_on_done()
            return

        # Deque has N items, calculate N-step transition starting from the oldest
        # Pass a copy to avoid modification if _flush_on_done is called concurrently
        n_step_transition = self._calculate_n_step_transition(list(self.n_step_deque))
        if n_step_transition:
            self.wrapped_buffer.push(
                state=n_step_transition.state,
                action=n_step_transition.action,
                reward=n_step_transition.reward,
                next_state=n_step_transition.next_state,
                done=n_step_transition.done,
                n_step_discount=n_step_transition.n_step_discount,
            )

        if done:
            self._flush_on_done()

    def _flush_on_done(self):
        """Processes remaining partial transitions when an episode ends."""
        temp_deque = list(self.n_step_deque)
        while len(temp_deque) > 1:
            n_step_transition = self._calculate_n_step_transition(temp_deque)
            if n_step_transition:
                self.wrapped_buffer.push(
                    state=n_step_transition.state,
                    action=n_step_transition.action,
                    reward=n_step_transition.reward,
                    next_state=n_step_transition.next_state,
                    done=n_step_transition.done,
                    n_step_discount=n_step_transition.n_step_discount,
                )
            temp_deque.pop(0)
        self.n_step_deque.clear()

    def sample(self, batch_size: int) -> Any:
        # Sampling is delegated, wrapped buffer returns N-step or 1-step batch
        return self.wrapped_buffer.sample(batch_size)

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        self.wrapped_buffer.update_priorities(indices, priorities)

    def set_beta(self, beta: float):
        if hasattr(self.wrapped_buffer, "set_beta"):
            self.wrapped_buffer.set_beta(beta)

    def __len__(self) -> int:
        return len(self.wrapped_buffer)

    def flush_pending(self):
        print(
            f"[NStepWrapper] Flushing {len(self.n_step_deque)} pending transitions..."
        )
        temp_deque = list(self.n_step_deque)
        while len(temp_deque) > 0:
            n_step_transition = self._calculate_n_step_transition(temp_deque)
            if n_step_transition:
                self.wrapped_buffer.push(
                    state=n_step_transition.state,
                    action=n_step_transition.action,
                    reward=n_step_transition.reward,
                    next_state=n_step_transition.next_state,
                    done=n_step_transition.done,
                    n_step_discount=n_step_transition.n_step_discount,
                )
            temp_deque.pop(0)
        self.n_step_deque.clear()
        if hasattr(self.wrapped_buffer, "flush_pending"):
            self.wrapped_buffer.flush_pending()
        print("[NStepWrapper] Flushing complete.")

    def get_state(self) -> Dict[str, Any]:
        return {
            "n_step_deque": list(self.n_step_deque),
            "wrapped_buffer_state": self.wrapped_buffer.get_state(),
        }

    def load_state_from_data(self, state: Dict[str, Any]):
        pending_deque_list = state.get("n_step_deque", [])
        valid_pending = [
            t for t in pending_deque_list if isinstance(t, tuple) and len(t) == 5
        ]
        if len(valid_pending) != len(pending_deque_list):
            print(
                f"Warning: Filtered {len(pending_deque_list) - len(valid_pending)} invalid items during NStep deque load."
            )
        self.n_step_deque = deque(valid_pending, maxlen=self.n_step)
        print(f"[NStepWrapper] Loaded {len(self.n_step_deque)} pending transitions.")

        wrapped_state = state.get("wrapped_buffer_state")
        if wrapped_state:
            self.wrapped_buffer.load_state_from_data(wrapped_state)
        else:
            print("[NStepWrapper] Warning: No wrapped buffer state found during load.")

    def save_state(self, filepath: str):
        save_object(self.get_state(), filepath)

    def load_state(self, filepath: str):
        try:
            state_data = load_object(filepath)
            self.load_state_from_data(state_data)
        except Exception as e:
            print(f"[NStepWrapper] Load failed: {e}. Starting empty.")
            self.n_step_deque.clear()
            # Recreate the wrapped buffer if load fails to ensure clean state
            if hasattr(self.wrapped_buffer, "__class__"):
                try:
                    self.wrapped_buffer = self.wrapped_buffer.__class__(
                        capacity=self.wrapped_buffer.capacity
                    )
                    print(
                        "[NStepWrapper] Recreated empty wrapped buffer after load failure."
                    )
                except Exception as recreat_e:
                    print(
                        f"[NStepWrapper] Failed to recreate wrapped buffer: {recreat_e}"
                    )


File: agent/replay_buffer/sum_tree.py
# File: agent/replay_buffer/sum_tree.py
# (Content as provided by user, assuming it's correct and functional)
import numpy as np


class SumTree:
    """Simple SumTree implementation using numpy arrays for PER."""

    def __init__(self, capacity: int):
        if capacity <= 0 or not isinstance(capacity, int):
            raise ValueError("SumTree capacity must be positive integer")
        # Ensure capacity is power of 2 for simpler implementation, or handle general case
        # For simplicity here, we assume any capacity works with the logic.
        # A more robust implementation might pad capacity to the next power of 2.
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1, dtype=np.float64)  # Use float64 for sums
        self.data = np.zeros(capacity, dtype=object)  # Holds Transition objects
        self.write_ptr = 0
        self.n_entries = 0

    def _propagate(self, idx: int, change: float):
        """Propagate priority change up the tree."""
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)

    def _retrieve(self, idx: int, s: float) -> int:
        """Find leaf index for a given cumulative priority value s."""
        left = 2 * idx + 1
        right = left + 1
        if left >= len(self.tree):
            return idx  # Leaf node

        # Use tolerance for float comparison if needed, but direct comparison often fine
        if s <= self.tree[left] + 1e-8:  # Added tolerance
            return self._retrieve(left, s)
        else:
            # Ensure non-negative s for recursive call
            return self._retrieve(right, max(0.0, s - self.tree[left]))

    def total(self) -> float:
        return self.tree[0]

    def add(self, priority: float, data: object):
        """Add new experience, overwriting oldest if full."""
        priority = max(abs(priority), 1e-6)  # Ensure positive priority
        tree_idx = self.write_ptr + self.capacity - 1

        self.data[self.write_ptr] = data
        self.update(tree_idx, priority)  # Update priorities after inserting data

        self.write_ptr = (self.write_ptr + 1) % self.capacity
        if self.n_entries < self.capacity:
            self.n_entries += 1

    def update(self, tree_idx: int, priority: float):
        """Update priority of an experience at a given tree index."""
        priority = max(abs(priority), 1e-6)  # Ensure positive priority

        # Ensure the index is within the valid range for leaf nodes
        if not (self.capacity - 1 <= tree_idx < 2 * self.capacity - 1):
            # print(f"Warning: Invalid tree index {tree_idx} passed to update.")
            return  # Silently ignore invalid indices or raise error

        change = priority - self.tree[tree_idx]
        self.tree[tree_idx] = priority
        # Propagate change only if it's significant and not the root
        if abs(change) > 1e-9 and tree_idx > 0:
            self._propagate(tree_idx, change)

    def get(self, s: float) -> tuple[int, float, object]:
        """Sample an experience based on cumulative priority s. Returns: (tree_idx, priority, data)"""
        if self.total() <= 0 or self.n_entries == 0:
            # Handle empty tree case gracefully
            return 0, 0.0, None

        # Clip s to be within valid range [epsilon, total_priority]
        s = np.clip(s, 1e-9, self.total())

        idx = self._retrieve(0, s)  # Find leaf node index
        data_idx = (
            idx - self.capacity + 1
        )  # Convert tree leaf index to data array index

        # Validate data_idx before accessing self.data
        if not (0 <= data_idx < self.n_entries):
            # This case might happen if sampling occurs while buffer is filling,
            # or due to floating point inaccuracies. Fallback strategy:
            if self.n_entries > 0:
                # Sample the last valid entry as a fallback
                last_valid_data_idx = (
                    self.write_ptr - 1 + self.capacity
                ) % self.capacity
                last_valid_tree_idx = last_valid_data_idx + self.capacity - 1
                # Ensure fallback index is valid before accessing tree
                priority = (
                    self.tree[last_valid_tree_idx]
                    if (
                        self.capacity - 1 <= last_valid_tree_idx < 2 * self.capacity - 1
                    )
                    else 0.0
                )
                # print(f"Warning: PER get() resulted in invalid data_idx {data_idx} for s={s}. Falling back to index {last_valid_data_idx}.")
                return (last_valid_tree_idx, priority, self.data[last_valid_data_idx])
            else:
                return 0, 0.0, None  # Return nothing if truly empty

        # Return valid data
        return (idx, self.tree[idx], self.data[data_idx])

    def __len__(self) -> int:
        return self.n_entries


File: agent/networks/noisy_layer.py
# File: agent/networks/noisy_layer.py
# (No changes needed, already clean)
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional


class NoisyLinear(nn.Module):
    """
    Noisy Linear Layer for Noisy Network (Factorised Gaussian Noise).
    """

    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        # Learnable weights and biases (mean parameters)
        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_mu = nn.Parameter(torch.empty(out_features))

        # Learnable noise parameters (standard deviation parameters)
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))

        # Non-learnable noise buffers
        self.register_buffer("weight_epsilon", torch.empty(out_features, in_features))
        self.register_buffer("bias_epsilon", torch.empty(out_features))

        self.reset_parameters()
        self.reset_noise()  # Initial noise generation

    def reset_parameters(self):
        """Initialize mean and std parameters."""
        mu_range = 1.0 / math.sqrt(self.in_features)
        nn.init.uniform_(self.weight_mu, -mu_range, mu_range)
        nn.init.uniform_(self.bias_mu, -mu_range, mu_range)

        # Initialize sigma parameters (std dev)
        nn.init.constant_(
            self.weight_sigma, self.std_init / math.sqrt(self.in_features)
        )
        nn.init.constant_(self.bias_sigma, self.std_init / math.sqrt(self.out_features))

    def reset_noise(self):
        """Generate new noise samples using Factorised Gaussian noise."""
        epsilon_in = self._scale_noise(self.in_features)
        epsilon_out = self._scale_noise(self.out_features)

        # Outer product for weight noise, direct sample for bias noise
        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))
        self.bias_epsilon.copy_(epsilon_out)

    def _scale_noise(self, size: int) -> torch.Tensor:
        """Generate noise tensor with sign-sqrt transformation."""
        x = torch.randn(size, device=self.weight_mu.device)  # Noise on same device
        return x.sign().mul(x.abs().sqrt())

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass with noisy parameters if training, mean parameters otherwise."""
        if self.training:
            # Sample noise is implicitly used via weight_epsilon, bias_epsilon
            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon
            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon
            # Reset noise *after* use in forward pass for next iteration?
            # Or reset in train() method? Resetting in train() is common.
        else:
            # Use mean parameters during evaluation
            weight = self.weight_mu
            bias = self.bias_mu

        return F.linear(x, weight, bias)

    def train(self, mode: bool = True):
        """Override train mode to reset noise when entering training."""
        if self.training is False and mode is True:  # If switching from eval to train
            self.reset_noise()
        super().train(mode)


File: agent/networks/__init__.py


File: agent/networks/agent_network.py
# File: agent/networks/agent_network.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

from config import ModelConfig, EnvConfig, DQNConfig, DEVICE
from typing import Tuple, List, Type, Optional

from .noisy_layer import NoisyLinear


class AgentNetwork(nn.Module):
    """
    Agent Network: CNN+MLP -> Fusion -> Dueling Heads -> Optional Distributional Output.
    Accepts separate grid and shape tensors as input.
    Uses NoisyLinear layers in output heads if configured.
    """

    def __init__(
        self,
        env_config: EnvConfig,
        action_dim: int,
        model_config: ModelConfig.Network,
        dqn_config: DQNConfig,
        dueling: bool,
        use_noisy: bool,  # This flag controls NoisyLinear usage
    ):
        super().__init__()
        self.dueling = dueling
        self.action_dim = action_dim
        self.env_config = env_config
        self.config = model_config
        self.use_noisy = use_noisy  # Store the flag
        self.use_distributional = dqn_config.USE_DISTRIBUTIONAL
        self.num_atoms = dqn_config.NUM_ATOMS
        self.device = DEVICE

        print(f"[AgentNetwork] Target device set to: {self.device}")
        print(
            f"[AgentNetwork] Distributional C51: {self.use_distributional} ({self.num_atoms} atoms)"
        )

        self.grid_c, self.grid_h, self.grid_w = self.env_config.GRID_STATE_SHAPE
        self.shape_feat_dim = self.env_config.SHAPE_STATE_DIM
        self.num_shape_slots = self.env_config.NUM_SHAPE_SLOTS
        self.shape_feat_per_slot = self.env_config.SHAPE_FEATURES_PER_SHAPE

        print(f"[AgentNetwork] Initializing (Noisy Heads: {self.use_noisy}):")
        print(f"  Input Grid Shape: [B, {self.grid_c}, {self.grid_h}, {self.grid_w}]")
        print(
            f"  Input Shape Features Dim: {self.shape_feat_dim} ({self.num_shape_slots} slots x {self.shape_feat_per_slot} features)"
        )

        self.conv_base, conv_out_h, conv_out_w, conv_out_c = self._build_cnn_branch()
        self.conv_out_size = self._get_conv_out_size(
            (self.grid_c, self.grid_h, self.grid_w)
        )
        print(
            f"  CNN Output Dim (HxWxC): ({conv_out_h}x{conv_out_w}x{conv_out_c}) -> Flat: {self.conv_out_size}"
        )

        self.shape_mlp, self.shape_mlp_out_dim = self._build_shape_mlp_branch()
        print(f"  Shape MLP Output Dim: {self.shape_mlp_out_dim}")

        combined_features_dim = self.conv_out_size + self.shape_mlp_out_dim
        print(f"  Combined Features Dim: {combined_features_dim}")

        # --- Modify Fusion MLP if needed (keeping it simple for now) ---
        # If Noisy Nets are used, potentially the last fusion layer could also be noisy.
        # Keeping fusion MLP with standard Linear layers unless specifically requested otherwise.
        self.fusion_mlp, self.head_input_dim = self._build_fusion_mlp_branch(
            combined_features_dim
        )
        print(f"  Fusion MLP Output Dim (Input to Heads): {self.head_input_dim}")

        # --- Build heads using NoisyLinear conditionally ---
        self._build_output_heads()
        head_type = NoisyLinear if self.use_noisy else nn.Linear  # Determine head type
        output_type = "Distributional" if self.use_distributional else "Q-Value"
        print(
            f"  Using {'Dueling' if self.dueling else 'Single'} Heads ({head_type.__name__}), Output: {output_type} [{self.action_dim * (self.num_atoms if self.use_distributional else 1)} units]"
        )

        if hasattr(self.conv_base, "0") and hasattr(self.conv_base[0], "weight"):
            print(
                f"[AgentNetwork] Final check - conv_base device: {next(self.conv_base.parameters()).device}"
            )
        else:
            print(
                "[AgentNetwork] Final check - conv_base seems empty or has no weights."
            )

    def _build_cnn_branch(self) -> Tuple[nn.Sequential, int, int, int]:
        conv_layers: List[nn.Module] = []
        current_channels = self.grid_c
        h, w = self.grid_h, self.grid_w
        cfg = self.config
        print(
            f"  Building CNN (Input Channels: {current_channels}) on device: {self.device}"
        )
        for i, out_channels in enumerate(cfg.CONV_CHANNELS):
            conv_layer = nn.Conv2d(
                current_channels,
                out_channels,
                kernel_size=cfg.CONV_KERNEL_SIZE,
                stride=cfg.CONV_STRIDE,
                padding=cfg.CONV_PADDING,
                bias=not cfg.USE_BATCHNORM_CONV,
            ).to(self.device)
            conv_layers.append(conv_layer)
            if cfg.USE_BATCHNORM_CONV:
                conv_layers.append(nn.BatchNorm2d(out_channels).to(self.device))
            conv_layers.append(cfg.CONV_ACTIVATION())
            current_channels = out_channels
            h = (h + 2 * cfg.CONV_PADDING - cfg.CONV_KERNEL_SIZE) // cfg.CONV_STRIDE + 1
            w = (w + 2 * cfg.CONV_PADDING - cfg.CONV_KERNEL_SIZE) // cfg.CONV_STRIDE + 1

        cnn_module = nn.Sequential(*conv_layers)
        if len(cnn_module) > 0 and hasattr(cnn_module[0], "weight"):
            print(f"    CNN Layer 0 device after build: {cnn_module[0].weight.device}")
        return cnn_module, h, w, current_channels

    def _get_conv_out_size(self, shape: Tuple[int, int, int]) -> int:
        self.conv_base.to(self.device)
        with torch.no_grad():
            dummy_input = torch.zeros(1, *shape, device=self.device)
            self.conv_base.eval()
            try:
                output = self.conv_base(dummy_input)
                out_size = int(np.prod(output.size()[1:]))
            except Exception as e:
                print(f"Error calculating conv output size: {e}")
                print(f"Input shape to CNN: {dummy_input.shape}")
                out_size = 1
            return out_size

    def _build_shape_mlp_branch(self) -> Tuple[nn.Sequential, int]:
        shape_mlp_layers: List[nn.Module] = []
        current_dim = self.env_config.SHAPE_STATE_DIM
        cfg = self.config
        for hidden_dim in cfg.SHAPE_FEATURE_MLP_DIMS:
            lin_layer = nn.Linear(current_dim, hidden_dim).to(self.device)
            shape_mlp_layers.append(lin_layer)
            shape_mlp_layers.append(cfg.SHAPE_MLP_ACTIVATION())
            current_dim = hidden_dim
        return nn.Sequential(*shape_mlp_layers), current_dim

    def _build_fusion_mlp_branch(self, input_dim: int) -> Tuple[nn.Sequential, int]:
        fusion_layers: List[nn.Module] = []
        current_fusion_dim = input_dim
        cfg = self.config
        fusion_linear_layer_class = nn.Linear  # Keep fusion linear for now
        for i, hidden_dim in enumerate(cfg.COMBINED_FC_DIMS):
            linear_layer = fusion_linear_layer_class(
                current_fusion_dim, hidden_dim, bias=not cfg.USE_BATCHNORM_FC
            ).to(self.device)
            fusion_layers.append(linear_layer)
            if cfg.USE_BATCHNORM_FC:
                fusion_layers.append(nn.BatchNorm1d(hidden_dim).to(self.device))
            fusion_layers.append(cfg.COMBINED_ACTIVATION())
            if cfg.DROPOUT_FC > 0:
                fusion_layers.append(nn.Dropout(cfg.DROPOUT_FC).to(self.device))
            current_fusion_dim = hidden_dim
        return nn.Sequential(*fusion_layers), current_fusion_dim

    def _build_output_heads(self):
        # Use NoisyLinear if self.use_noisy is True
        head_linear_layer_class = NoisyLinear if self.use_noisy else nn.Linear
        output_units_per_stream = (
            self.action_dim * self.num_atoms
            if self.use_distributional
            else self.action_dim
        )
        value_units = self.num_atoms if self.use_distributional else 1

        if self.dueling:
            self.value_head = head_linear_layer_class(
                self.head_input_dim, value_units
            ).to(self.device)
            self.advantage_head = head_linear_layer_class(
                self.head_input_dim, output_units_per_stream
            ).to(self.device)
        else:
            self.output_head = head_linear_layer_class(
                self.head_input_dim, output_units_per_stream
            ).to(self.device)

    def forward(
        self, grid_tensor: torch.Tensor, shape_tensor: torch.Tensor
    ) -> torch.Tensor:
        model_device = next(self.parameters()).device
        if grid_tensor.device != model_device:
            grid_tensor = grid_tensor.to(model_device)
        if shape_tensor.device != model_device:
            shape_tensor = shape_tensor.to(model_device)

        expected_grid_shape = (self.grid_c, self.grid_h, self.grid_w)
        if grid_tensor.ndim != 4 or grid_tensor.shape[1:] != expected_grid_shape:
            raise ValueError(
                f"AgentNetwork forward: Invalid grid_tensor shape {grid_tensor.shape}. Expected [B, {self.grid_c}, {self.grid_h}, {self.grid_w}]."
            )

        batch_size = grid_tensor.size(0)
        expected_shape_flat_dim = self.num_shape_slots * self.shape_feat_per_slot
        if shape_tensor.ndim == 3 and shape_tensor.shape[1:] == (
            self.num_shape_slots,
            self.shape_feat_per_slot,
        ):
            shape_tensor_flat = shape_tensor.view(batch_size, -1)
        elif (
            shape_tensor.ndim == 2 and shape_tensor.shape[1] == expected_shape_flat_dim
        ):
            shape_tensor_flat = shape_tensor
        else:
            raise ValueError(
                f"AgentNetwork forward: Invalid shape_tensor shape {shape_tensor.shape}. Expected [B, {self.num_shape_slots}, {self.shape_feat_per_slot}] or [B, {expected_shape_flat_dim}]."
            )

        conv_output = self.conv_base(grid_tensor)
        conv_output_flat = conv_output.view(batch_size, -1)
        shape_output = self.shape_mlp(shape_tensor_flat)
        combined_features = torch.cat((conv_output_flat, shape_output), dim=1)
        fused_output = self.fusion_mlp(combined_features)

        if self.dueling:
            value = self.value_head(fused_output)
            advantage = self.advantage_head(fused_output)
            if self.use_distributional:
                value = value.view(batch_size, 1, self.num_atoms)
                advantage = advantage.view(batch_size, self.action_dim, self.num_atoms)
                adv_mean = advantage.mean(dim=1, keepdim=True)
                dist_logits = value + (advantage - adv_mean)
            else:
                adv_mean = advantage.mean(dim=1, keepdim=True)
                dist_logits = value + (advantage - adv_mean)
        else:
            dist_logits = self.output_head(fused_output)
            if self.use_distributional:
                dist_logits = dist_logits.view(
                    batch_size, self.action_dim, self.num_atoms
                )

        return dist_logits

    def reset_noise(self):
        """Resets noise in all NoisyLinear layers within the network."""
        if self.use_noisy:
            for module in self.modules():
                if isinstance(module, NoisyLinear):
                    module.reset_noise()


File: environment/game_state.py
# File: environment/game_state.py
import time
import numpy as np
from typing import List, Optional, Tuple, Dict, Union
from collections import deque
from typing import Deque

from .grid import Grid
from .shape import Shape
from config import EnvConfig, RewardConfig

StateType = Dict[str, np.ndarray]


class GameState:
    def __init__(self):
        self.env_config = EnvConfig()
        self.grid = Grid(self.env_config)
        self.shapes: List[Optional[Shape]] = [
            Shape() for _ in range(self.env_config.NUM_SHAPE_SLOTS)
        ]
        self.score = 0.0
        self.game_score = 0
        self.lines_cleared_this_episode = 0
        self.blink_time = 0.0
        self.last_time = time.time()
        self.freeze_time = 0.0
        self.line_clear_flash_time = 0.0
        self.line_clear_highlight_time: float = 0.0
        self.game_over_flash_time: float = 0.0
        self.cleared_triangles_coords: List[Tuple[int, int]] = []
        self.game_over = False
        self._last_action_valid = True
        self.rewards = RewardConfig()
        self.demo_selected_shape_idx: int = 0
        self.demo_target_row: int = self.env_config.ROWS // 2
        self.demo_target_col: int = self.env_config.COLS // 2

    def reset(self) -> StateType:
        self.grid = Grid(self.env_config)
        self.shapes = [Shape() for _ in range(self.env_config.NUM_SHAPE_SLOTS)]
        self.score = 0.0
        self.game_score = 0
        self.lines_cleared_this_episode = 0
        self.blink_time = 0.0
        self.freeze_time = 0.0
        self.line_clear_flash_time = 0.0
        self.line_clear_highlight_time = 0.0
        self.game_over_flash_time = 0.0
        self.cleared_triangles_coords = []
        self.game_over = False
        self._last_action_valid = True
        self.last_time = time.time()
        self.demo_selected_shape_idx = 0
        self.demo_target_row = self.env_config.ROWS // 2
        self.demo_target_col = self.env_config.COLS // 2
        return self.get_state()

    def valid_actions(self) -> List[int]:
        if self.game_over or self.freeze_time > 0:
            return []
        acts = []
        locations_per_shape = self.grid.rows * self.grid.cols
        for i, sh in enumerate(self.shapes):
            if not sh:
                continue
            for r in range(self.grid.rows):
                for c in range(self.grid.cols):
                    if self.grid.can_place(sh, r, c):
                        action_index = i * locations_per_shape + (
                            r * self.grid.cols + c
                        )
                        acts.append(action_index)
        return acts

    def _check_fundamental_game_over(self) -> bool:
        for sh in self.shapes:
            if not sh:
                continue
            for r in range(self.grid.rows):
                for c in range(self.grid.cols):
                    if self.grid.can_place(sh, r, c):
                        return False
        return True

    def is_over(self) -> bool:
        return self.game_over

    def is_frozen(self) -> bool:
        return self.freeze_time > 0

    def is_line_clearing(self) -> bool:
        return self.line_clear_flash_time > 0

    def is_highlighting_cleared(self) -> bool:
        return self.line_clear_highlight_time > 0

    def is_game_over_flashing(self) -> bool:
        return self.game_over_flash_time > 0

    def get_cleared_triangle_coords(self) -> List[Tuple[int, int]]:
        return self.cleared_triangles_coords

    def decode_act(self, a: int) -> Tuple[int, int, int]:
        locations_per_shape = self.grid.rows * self.grid.cols
        s_idx = a // locations_per_shape
        pos_idx = a % locations_per_shape
        rr = pos_idx // self.grid.cols
        cc = pos_idx % self.grid.cols
        return (s_idx, rr, cc)

    def _update_timers(self):
        now = time.time()
        dt = now - self.last_time
        self.last_time = now
        self.freeze_time = max(0, self.freeze_time - dt)
        self.blink_time = max(0, self.blink_time - dt)
        self.line_clear_flash_time = max(0, self.line_clear_flash_time - dt)
        self.line_clear_highlight_time = max(0, self.line_clear_highlight_time - dt)
        self.game_over_flash_time = max(0, self.game_over_flash_time - dt)
        if self.line_clear_highlight_time <= 0 and self.cleared_triangles_coords:
            self.cleared_triangles_coords = []

    def _handle_invalid_placement(self) -> float:
        self._last_action_valid = False
        reward = self.rewards.PENALTY_INVALID_MOVE
        return reward

    def _handle_game_over_state_change(self) -> float:
        if self.game_over:
            return 0.0
        self.game_over = True
        if self.freeze_time <= 0:
            self.freeze_time = 1.0
        # --- MODIFIED: Increase flash duration ---
        self.game_over_flash_time = 0.6
        # --- END MODIFIED ---
        return self.rewards.PENALTY_GAME_OVER

    def _handle_valid_placement(
        self, shp: Shape, s_idx: int, rr: int, cc: int
    ) -> float:
        self._last_action_valid = True
        reward = self.rewards.REWARD_ALIVE_STEP

        self.grid.place(shp, rr, cc)
        self.shapes[s_idx] = None
        self.game_score += len(shp.triangles)

        num_slots = self.env_config.NUM_SHAPE_SLOTS
        found_next = False
        if num_slots > 0:
            next_idx = (s_idx + 1) % num_slots
            for _ in range(num_slots):
                if (
                    0 <= next_idx < len(self.shapes)
                    and self.shapes[next_idx] is not None
                ):
                    self.demo_selected_shape_idx = next_idx
                    found_next = True
                    break
                next_idx = (next_idx + 1) % num_slots
            if not found_next:
                self.demo_selected_shape_idx = 0

        lines_cleared, triangles_cleared, cleared_coords = self.grid.clear_filled_rows()
        self.lines_cleared_this_episode += lines_cleared

        if lines_cleared == 1:
            reward += self.rewards.REWARD_CLEAR_1
        elif lines_cleared == 2:
            reward += self.rewards.REWARD_CLEAR_2
        elif lines_cleared >= 3:
            reward += self.rewards.REWARD_CLEAR_3PLUS

        if triangles_cleared > 0:
            self.game_score += triangles_cleared * 2
            self.blink_time = 0.5
            self.freeze_time = 0.5
            self.line_clear_flash_time = 0.3
            self.line_clear_highlight_time = 0.5
            self.cleared_triangles_coords = cleared_coords

        num_holes = self.grid.count_holes()
        reward += num_holes * self.rewards.PENALTY_HOLE_PER_HOLE

        if all(x is None for x in self.shapes):
            self.shapes = [Shape() for _ in range(self.env_config.NUM_SHAPE_SLOTS)]
            if not found_next:
                first_available = next(
                    (i for i, s in enumerate(self.shapes) if s is not None), 0
                )
                self.demo_selected_shape_idx = first_available

        if self._check_fundamental_game_over():
            reward += self._handle_game_over_state_change()

        return reward

    def step(self, a: int) -> Tuple[float, bool]:
        self._update_timers()

        if self.game_over:
            return (0.0, True)

        s_idx, rr, cc = self.decode_act(a)
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None

        is_valid_placement = shp is not None and self.grid.can_place(shp, rr, cc)

        if self.is_frozen():
            current_rl_reward = self._handle_invalid_placement()
        elif is_valid_placement:
            current_rl_reward = self._handle_valid_placement(shp, s_idx, rr, cc)
        else:
            current_rl_reward = self._handle_invalid_placement()
            if self._check_fundamental_game_over():
                current_rl_reward += self._handle_game_over_state_change()

        self.score += current_rl_reward
        return (current_rl_reward, self.game_over)

    def get_state(self) -> StateType:
        grid_state = self.grid.get_feature_matrix()
        shape_features_per = self.env_config.SHAPE_FEATURES_PER_SHAPE
        num_shapes_expected = self.env_config.NUM_SHAPE_SLOTS
        shape_rep = np.zeros(
            (num_shapes_expected, shape_features_per), dtype=np.float32
        )

        max_tris_norm = 6.0
        max_h_norm = float(self.grid.rows)
        max_w_norm = float(self.grid.cols)

        for i in range(num_shapes_expected):
            s = self.shapes[i] if i < len(self.shapes) else None
            if s:
                tri = s.triangles
                n = len(tri)
                ups = sum(1 for (_, _, u) in tri if u)
                dns = n - ups
                mnr, mnc, mxr, mxc = s.bbox()
                height = mxr - mnr + 1
                width = mxc - mnc + 1

                shape_rep[i, 0] = np.clip(float(n) / max_tris_norm, 0.0, 1.0)
                shape_rep[i, 1] = np.clip(float(ups) / max_tris_norm, 0.0, 1.0)
                shape_rep[i, 2] = np.clip(float(dns) / max_tris_norm, 0.0, 1.0)
                shape_rep[i, 3] = np.clip(float(height) / max_h_norm, 0.0, 1.0)
                shape_rep[i, 4] = np.clip(float(width) / max_w_norm, 0.0, 1.0)

        state_dict = {
            "grid": grid_state.astype(np.float32),
            "shapes": shape_rep.astype(np.float32),
        }
        return state_dict

    def is_blinking(self) -> bool:
        return self.blink_time > 0

    def get_shapes(self) -> List[Shape]:
        return [s for s in self.shapes if s is not None]

    # --- Demo Mode Methods ---
    def cycle_shape(self, direction: int):
        if self.game_over or self.freeze_time > 0:
            return
        num_slots = self.env_config.NUM_SHAPE_SLOTS
        if num_slots <= 0:
            return

        available_indices = [
            i for i, s in enumerate(self.shapes) if s is not None and 0 <= i < num_slots
        ]
        if not available_indices:
            return

        try:
            current_list_idx = available_indices.index(self.demo_selected_shape_idx)
        except ValueError:
            current_list_idx = 0

        new_list_idx = (current_list_idx + direction) % len(available_indices)
        self.demo_selected_shape_idx = available_indices[new_list_idx]

    def move_target(self, dr: int, dc: int):
        if self.game_over or self.freeze_time > 0:
            return
        self.demo_target_row = np.clip(self.demo_target_row + dr, 0, self.grid.rows - 1)
        self.demo_target_col = np.clip(self.demo_target_col + dc, 0, self.grid.cols - 1)

    def get_action_for_current_selection(self) -> Optional[int]:
        if self.game_over or self.freeze_time > 0:
            return None
        s_idx = self.demo_selected_shape_idx
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None
        if shp is None:
            return None

        rr, cc = self.demo_target_row, self.demo_target_col

        if self.grid.can_place(shp, rr, cc):
            locations_per_shape = self.grid.rows * self.grid.cols
            action_index = s_idx * locations_per_shape + (rr * self.grid.cols + cc)
            return action_index
        else:
            return None

    def get_current_selection_info(self) -> Tuple[Optional[Shape], int, int]:
        s_idx = self.demo_selected_shape_idx
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None
        return shp, self.demo_target_row, self.demo_target_col


File: environment/grid.py
# File: environment/grid.py
import numpy as np
from typing import List, Tuple, Optional
from .triangle import Triangle
from .shape import Shape
from config import EnvConfig


class Grid:
    """Represents the game board composed of Triangles."""

    def __init__(self, env_config: EnvConfig):
        self.rows = env_config.ROWS
        self.cols = env_config.COLS
        self.triangles: List[List[Triangle]] = []
        self._create()

    def _create(self) -> None:
        cols_per_row = [9, 11, 13, 15, 15, 13, 11, 9]

        if len(cols_per_row) != self.rows:
            raise ValueError(
                f"Grid._create error: Length of cols_per_row ({len(cols_per_row)}) must match EnvConfig.ROWS ({self.rows})"
            )
        if max(cols_per_row) > self.cols:
            raise ValueError(
                f"Grid._create error: Max playable columns ({max(cols_per_row)}) exceeds EnvConfig.COLS ({self.cols})"
            )

        self.triangles = []
        for r in range(self.rows):
            rowt = []
            base_playable_cols = cols_per_row[r]

            if base_playable_cols <= 0:
                initial_death_cols_left = self.cols
            elif base_playable_cols >= self.cols:
                initial_death_cols_left = 0
            else:
                initial_death_cols_left = (self.cols - base_playable_cols) // 2
            initial_first_death_col_right = initial_death_cols_left + base_playable_cols

            adjusted_death_cols_left = initial_death_cols_left + 1
            adjusted_first_death_col_right = initial_first_death_col_right - 1

            for c in range(self.cols):
                is_death_cell = (
                    (c < adjusted_death_cols_left)
                    or (
                        c >= adjusted_first_death_col_right
                        and adjusted_first_death_col_right > adjusted_death_cols_left
                    )
                    or (base_playable_cols <= 2)
                )

                is_up_cell = (r + c) % 2 == 0
                tri = Triangle(r, c, is_up=is_up_cell, is_death=is_death_cell)
                rowt.append(tri)
            self.triangles.append(rowt)

    def valid(self, r: int, c: int) -> bool:
        return 0 <= r < self.rows and 0 <= c < self.cols

    def can_place(self, shp: Shape, rr: int, cc: int) -> bool:
        for dr, dc, up in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if not self.valid(nr, nc):
                return False
            if not (
                0 <= nr < len(self.triangles) and 0 <= nc < len(self.triangles[nr])
            ):
                return False
            tri = self.triangles[nr][nc]
            if tri.is_death or tri.is_occupied or (tri.is_up != up):
                return False
        return True

    def place(self, shp: Shape, rr: int, cc: int) -> None:
        for dr, dc, _ in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if self.valid(nr, nc):
                if not (
                    0 <= nr < len(self.triangles) and 0 <= nc < len(self.triangles[nr])
                ):
                    continue
                tri = self.triangles[nr][nc]
                if not tri.is_death and not tri.is_occupied:
                    tri.is_occupied = True
                    tri.color = shp.color

    # --- MODIFIED: clear_filled_rows returns coords ---
    def clear_filled_rows(self) -> Tuple[int, int, List[Tuple[int, int]]]:
        """
        Clears fully occupied rows (ignoring death cells) by marking triangles as
        unoccupied. Does NOT apply gravity.
        Returns: (lines cleared, triangles cleared, list of (r, c) coords cleared).
        """
        lines_cleared = 0
        triangles_cleared = 0
        rows_to_clear_indices = []
        cleared_triangles_coords: List[Tuple[int, int]] = []  # Store coords here

        for r in range(self.rows):
            if not (0 <= r < len(self.triangles)):
                continue
            rowt = self.triangles[r]
            is_row_full = True
            num_placeable_triangles_in_row = 0
            for t in rowt:
                if not t.is_death:
                    num_placeable_triangles_in_row += 1
                    if not t.is_occupied:
                        is_row_full = False
                        break

            if is_row_full and num_placeable_triangles_in_row > 0:
                rows_to_clear_indices.append(r)
                lines_cleared += 1

        for r_idx in rows_to_clear_indices:
            if not (0 <= r_idx < len(self.triangles)):
                continue
            for t in self.triangles[r_idx]:
                if not t.is_death and t.is_occupied:
                    triangles_cleared += 1
                    t.is_occupied = False
                    t.color = None
                    cleared_triangles_coords.append((r_idx, t.col))  # Add coords

        return lines_cleared, triangles_cleared, cleared_triangles_coords

    # --- END MODIFIED ---

    def count_holes(self) -> int:
        holes = 0
        for c in range(self.cols):
            occupied_above = False
            for r in range(self.rows):
                if not (
                    0 <= r < len(self.triangles) and 0 <= c < len(self.triangles[r])
                ):
                    continue
                tri = self.triangles[r][c]
                if tri.is_death:
                    occupied_above = False
                    continue

                if tri.is_occupied:
                    occupied_above = True
                elif not tri.is_occupied and occupied_above:
                    holes += 1
        return holes

    def get_feature_matrix(self) -> np.ndarray:
        grid_state = np.zeros((2, self.rows, self.cols), dtype=np.float32)
        for r in range(self.rows):
            for c in range(self.cols):
                if not (
                    0 <= r < len(self.triangles) and 0 <= c < len(self.triangles[r])
                ):
                    continue
                t = self.triangles[r][c]
                if not t.is_death:
                    grid_state[0, r, c] = 1.0 if t.is_occupied else 0.0
                    grid_state[1, r, c] = 1.0 if t.is_up else 0.0
        return grid_state


File: environment/__init__.py


File: environment/triangle.py
# File: environment/triangle.py
# (No changes needed)
from typing import Tuple, Optional, List


class Triangle:
    """Represents a single triangular cell on the grid."""

    def __init__(self, row: int, col: int, is_up: bool, is_death: bool = False):
        self.row = row
        self.col = col
        self.is_up = is_up  # True if pointing up, False if pointing down
        self.is_death = is_death  # True if part of the unplayable border
        self.is_occupied = is_death  # Occupied if it's a death cell initially
        self.color: Optional[Tuple[int, int, int]] = (
            None  # Color if occupied by a shape
        )

    def get_points(
        self, ox: int, oy: int, cw: int, ch: int
    ) -> List[Tuple[float, float]]:
        """Calculates the vertex points for drawing the triangle."""
        x = ox + self.col * (
            cw * 0.75
        )  # Horizontal position based on column and overlap
        y = oy + self.row * ch  # Vertical position based on row
        if self.is_up:
            # Points for an upward-pointing triangle
            return [(x, y + ch), (x + cw, y + ch), (x + cw / 2, y)]
        else:
            # Points for a downward-pointing triangle
            return [(x, y), (x + cw, y), (x + cw / 2, y + ch)]


File: environment/shape.py
# File: environment/shape.py
# (No changes needed)
import random
from typing import List, Tuple
from config import EnvConfig, VisConfig  # Needs VisConfig only for colors

GOOGLE_COLORS = VisConfig.GOOGLE_COLORS  # Use colors from VisConfig


class Shape:
    """Represents a polyomino-like shape made of triangles."""

    def __init__(self) -> None:
        # List of (relative_row, relative_col, is_up) tuples defining the shape
        self.triangles: List[Tuple[int, int, bool]] = []
        self.color: Tuple[int, int, int] = random.choice(GOOGLE_COLORS)
        self._generate()  # Generate the shape structure

    def _generate(self) -> None:
        """Generates a random shape by adding adjacent triangles."""
        n = random.randint(1, 5)  # Number of triangles in the shape
        first_up = random.choice([True, False])  # Orientation of the root triangle
        self.triangles.append((0, 0, first_up))  # Add the root triangle at (0,0)

        # Add remaining triangles adjacent to existing ones
        for _ in range(n - 1):
            # Find valid neighbors of the *last added* triangle
            lr, lc, lu = self.triangles[-1]
            nbrs = self._find_valid_neighbors(lr, lc, lu)
            if nbrs:
                self.triangles.append(random.choice(nbrs))
            # else: Could break early if no valid neighbors found, shape < n

    def _find_valid_neighbors(
        self, r: int, c: int, up: bool
    ) -> List[Tuple[int, int, bool]]:
        """Finds potential neighbor triangles that are not already part of the shape."""
        if up:  # Neighbors of an UP triangle are DOWN triangles
            ns = [(r, c - 1, False), (r, c + 1, False), (r + 1, c, False)]
        else:  # Neighbors of a DOWN triangle are UP triangles
            ns = [(r, c - 1, True), (r, c + 1, True), (r - 1, c, True)]
        # Return only neighbors that are not already in self.triangles
        return [n for n in ns if n not in self.triangles]

    def bbox(self) -> Tuple[int, int, int, int]:
        """Calculates the bounding box (min_r, min_c, max_r, max_c) of the shape."""
        if not self.triangles:
            return (0, 0, 0, 0)
        rr = [t[0] for t in self.triangles]
        cc = [t[1] for t in self.triangles]
        return (min(rr), min(cc), max(rr), max(cc))


File: stats/aggregator.py
# File: stats/aggregator.py
import time
from collections import deque
from typing import Deque, Dict, Any, Optional, List
import numpy as np
from config import StatsConfig


class StatsAggregator:
    """
    Handles aggregation and storage of training statistics using deques.
    Calculates rolling averages and tracks best values. Does not perform logging.
    """

    def __init__(
        self,
        avg_windows: List[int] = StatsConfig.STATS_AVG_WINDOW,
        plot_window: int = StatsConfig.PLOT_DATA_WINDOW,
    ):
        if not avg_windows or not all(
            isinstance(w, int) and w > 0 for w in avg_windows
        ):
            print("Warning: Invalid avg_windows list. Using default [100].")
            self.avg_windows = [100]
        else:
            self.avg_windows = sorted(list(set(avg_windows)))

        if plot_window <= 0:
            plot_window = 10000
        self.plot_window = plot_window

        self.summary_avg_window = self.avg_windows[0]

        self.losses: Deque[float] = deque(maxlen=plot_window)
        self.grad_norms: Deque[float] = deque(maxlen=plot_window)
        self.avg_max_qs: Deque[float] = deque(maxlen=plot_window)
        self.episode_scores: Deque[float] = deque(maxlen=plot_window)
        self.episode_lengths: Deque[int] = deque(maxlen=plot_window)
        self.game_scores: Deque[int] = deque(maxlen=plot_window)
        self.episode_lines_cleared: Deque[int] = deque(maxlen=plot_window)
        self.sps_values: Deque[float] = deque(maxlen=plot_window)
        self.buffer_sizes: Deque[int] = deque(maxlen=plot_window)
        self.beta_values: Deque[float] = deque(maxlen=plot_window)
        self.best_rl_score_history: Deque[float] = deque(maxlen=plot_window)
        self.best_game_score_history: Deque[int] = deque(maxlen=plot_window)
        self.lr_values: Deque[float] = deque(maxlen=plot_window)
        self.epsilon_values: Deque[float] = deque(maxlen=plot_window)

        self.total_episodes = 0
        self.total_lines_cleared = 0
        self.current_epsilon: float = 0.0
        self.current_beta: float = 0.0
        self.current_buffer_size: int = 0
        self.current_global_step: int = 0
        self.current_sps: float = 0.0
        self.current_lr: float = 0.0

        self.best_score: float = -float("inf")
        self.previous_best_score: float = -float("inf")
        self.best_score_step: int = 0

        self.best_game_score: float = -float("inf")
        self.previous_best_game_score: float = -float("inf")
        self.best_game_score_step: int = 0

        self.best_loss: float = float("inf")
        self.previous_best_loss: float = float("inf")
        self.best_loss_step: int = 0

        print(
            f"[StatsAggregator] Initialized. Avg Windows: {self.avg_windows}, Plot Window: {self.plot_window}"
        )

    def record_episode(
        self,
        episode_score: float,
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,
    ) -> Dict[str, Any]:
        current_step = (
            global_step if global_step is not None else self.current_global_step
        )
        update_info = {"new_best_rl": False, "new_best_game": False}

        self.episode_scores.append(episode_score)
        self.episode_lengths.append(episode_length)
        if game_score is not None:
            self.game_scores.append(game_score)
        if lines_cleared is not None:
            self.episode_lines_cleared.append(lines_cleared)
            self.total_lines_cleared += lines_cleared
        self.total_episodes = episode_num

        if episode_score > self.best_score:
            self.previous_best_score = self.best_score
            self.best_score = episode_score
            self.best_score_step = current_step
            update_info["new_best_rl"] = True

        if game_score is not None and game_score > self.best_game_score:
            self.previous_best_game_score = self.best_game_score
            self.best_game_score = float(game_score)
            self.best_game_score_step = current_step
            update_info["new_best_game"] = True

        # --- MODIFIED: Append actual best_score, even if negative ---
        self.best_rl_score_history.append(self.best_score)
        # --- END MODIFIED ---
        current_best_game = (
            int(self.best_game_score) if self.best_game_score > -float("inf") else 0
        )
        self.best_game_score_history.append(current_best_game)

        return update_info

    def record_step(self, step_data: Dict[str, Any]) -> Dict[str, Any]:
        g_step = step_data.get("global_step", self.current_global_step)
        if g_step > self.current_global_step:
            self.current_global_step = g_step

        update_info = {"new_best_loss": False}

        if "loss" in step_data and step_data["loss"] is not None and g_step > 0:
            current_loss = step_data["loss"]
            self.losses.append(current_loss)
            if current_loss < self.best_loss:
                self.previous_best_loss = self.best_loss
                self.best_loss = current_loss
                self.best_loss_step = g_step
                update_info["new_best_loss"] = True

        if "grad_norm" in step_data and step_data["grad_norm"] is not None:
            self.grad_norms.append(step_data["grad_norm"])
        if "avg_max_q" in step_data and step_data["avg_max_q"] is not None:
            self.avg_max_qs.append(step_data["avg_max_q"])
        if "beta" in step_data and step_data["beta"] is not None:
            self.current_beta = step_data["beta"]
            self.beta_values.append(self.current_beta)
        if "buffer_size" in step_data and step_data["buffer_size"] is not None:
            self.current_buffer_size = step_data["buffer_size"]
            self.buffer_sizes.append(self.current_buffer_size)
        if "lr" in step_data and step_data["lr"] is not None:
            self.current_lr = step_data["lr"]
            self.lr_values.append(self.current_lr)
        if "epsilon" in step_data and step_data["epsilon"] is not None:
            self.current_epsilon = step_data["epsilon"]
            self.epsilon_values.append(self.current_epsilon)

        if "step_time" in step_data and step_data["step_time"] > 1e-9:
            num_steps = step_data.get("num_steps_processed", 1)
            sps = num_steps / step_data["step_time"]
            self.sps_values.append(sps)
            self.current_sps = sps

        return update_info

    def get_summary(self, current_global_step: Optional[int] = None) -> Dict[str, Any]:
        if current_global_step is None:
            current_global_step = self.current_global_step

        summary_window = self.summary_avg_window

        def safe_mean(q: Deque, default=0.0) -> float:
            window_data = list(q)[-summary_window:]
            return float(np.mean(window_data)) if window_data else default

        summary = {
            "avg_score_window": safe_mean(self.episode_scores),
            "avg_length_window": safe_mean(self.episode_lengths),
            "avg_loss_window": safe_mean(self.losses),
            "avg_max_q_window": safe_mean(self.avg_max_qs),
            "avg_game_score_window": safe_mean(self.game_scores),
            "avg_lines_cleared_window": safe_mean(self.episode_lines_cleared),
            "avg_sps_window": safe_mean(self.sps_values, default=self.current_sps),
            "avg_lr_window": safe_mean(self.lr_values, default=self.current_lr),
            "total_episodes": self.total_episodes,
            "beta": self.current_beta,
            "buffer_size": self.current_buffer_size,
            "steps_per_second": self.current_sps,
            "global_step": current_global_step,
            "current_lr": self.current_lr,
            "best_score": self.best_score,
            "previous_best_score": self.previous_best_score,
            "best_score_step": self.best_score_step,
            "best_game_score": self.best_game_score,
            "previous_best_game_score": self.previous_best_game_score,
            "best_game_score_step": self.best_game_score_step,
            "best_loss": self.best_loss,
            "previous_best_loss": self.previous_best_loss,
            "best_loss_step": self.best_loss_step,
            "num_ep_scores": len(self.episode_scores),
            "num_losses": len(self.losses),
            "summary_avg_window_size": summary_window,
        }
        return summary

    def get_plot_data(self) -> Dict[str, Deque]:
        return {
            "episode_scores": self.episode_scores.copy(),
            "episode_lengths": self.episode_lengths.copy(),
            "losses": self.losses.copy(),
            "avg_max_qs": self.avg_max_qs.copy(),
            "game_scores": self.game_scores.copy(),
            "episode_lines_cleared": self.episode_lines_cleared.copy(),
            "sps_values": self.sps_values.copy(),
            "buffer_sizes": self.buffer_sizes.copy(),
            "beta_values": self.beta_values.copy(),
            "best_rl_score_history": self.best_rl_score_history.copy(),
            "best_game_score_history": self.best_game_score_history.copy(),
            "lr_values": self.lr_values.copy(),
            "epsilon_values": self.epsilon_values.copy(),
        }


File: stats/__init__.py
# File: stats/__init__.py
from .stats_recorder import StatsRecorderBase
from .aggregator import StatsAggregator
from .simple_stats_recorder import SimpleStatsRecorder
from .tensorboard_logger import TensorBoardStatsRecorder

__all__ = [
    "StatsRecorderBase",
    "StatsAggregator",
    "SimpleStatsRecorder",
    "TensorBoardStatsRecorder",
]


File: stats/simple_stats_recorder.py
# File: stats/simple_stats_recorder.py
import time
from collections import deque
from typing import Deque, Dict, Any, Optional, Union, List
import numpy as np
import torch
from .stats_recorder import StatsRecorderBase
from .aggregator import StatsAggregator
from config import StatsConfig


class SimpleStatsRecorder(StatsRecorderBase):
    """
    Logs aggregated statistics to the console periodically.
    Delegates data storage and aggregation to a StatsAggregator instance.
    Provides no-op implementations for histogram, image, hparam, graph logging.
    """

    def __init__(
        self,
        aggregator: StatsAggregator,
        console_log_interval: int = StatsConfig.CONSOLE_LOG_FREQ,
    ):
        self.aggregator = aggregator
        self.console_log_interval = (
            max(1, console_log_interval) if console_log_interval > 0 else -1
        )
        self.last_log_time: float = time.time()
        self.last_log_step: int = 0
        self.start_time: float = time.time()
        # --- MODIFIED: Get the window size used for summary ---
        self.summary_avg_window = self.aggregator.summary_avg_window
        # --- END MODIFIED ---
        print(
            f"[SimpleStatsRecorder] Initialized. Console Log Interval: {self.console_log_interval if self.console_log_interval > 0 else 'Disabled'}"
        )
        print(
            f"[SimpleStatsRecorder] Console logs will use Avg Window: {self.summary_avg_window}"
        )

    def record_episode(
        self,
        episode_score: float,
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,
    ):
        update_info = self.aggregator.record_episode(
            episode_score,
            episode_length,
            episode_num,
            global_step,
            game_score,
            lines_cleared,
        )
        current_step = (
            global_step
            if global_step is not None
            else self.aggregator.current_global_step
        )
        step_info = f"at Step ~{current_step/1e6:.1f}M"

        if update_info.get("new_best_rl"):
            prev_str = (
                f"{self.aggregator.previous_best_score:.2f}"
                if self.aggregator.previous_best_score > -float("inf")
                else "N/A"
            )
            print(
                f"\n---  New Best RL: {self.aggregator.best_score:.2f} {step_info} (Prev: {prev_str}) ---"
            )
        if update_info.get("new_best_game"):
            prev_str = (
                f"{self.aggregator.previous_best_game_score:.0f}"
                if self.aggregator.previous_best_game_score > -float("inf")
                else "N/A"
            )
            print(
                f"---  New Best Game: {self.aggregator.best_game_score:.0f} {step_info} (Prev: {prev_str}) ---"
            )

    def record_step(self, step_data: Dict[str, Any]):
        _ = self.aggregator.record_step(step_data)
        g_step = step_data.get("global_step", self.aggregator.current_global_step)
        self.log_summary(g_step)

    def get_summary(self, current_global_step: int) -> Dict[str, Any]:
        return self.aggregator.get_summary(current_global_step)

    def get_plot_data(self) -> Dict[str, Deque]:
        return self.aggregator.get_plot_data()

    def log_summary(self, global_step: int):
        if (
            self.console_log_interval <= 0
            or global_step < self.last_log_step + self.console_log_interval
        ):
            return

        summary = self.get_summary(global_step)
        elapsed_runtime = time.time() - self.start_time
        runtime_hrs = elapsed_runtime / 3600

        best_score_val = (
            f"{summary['best_score']:.2f}"
            if summary["best_score"] > -float("inf")
            else "N/A"
        )
        best_loss_val = (
            f"{summary['best_loss']:.4f}"
            if summary["best_loss"] < float("inf")
            else "N/A"
        )

        # --- MODIFIED: Indicate the window size used for averages ---
        avg_window_size = summary.get("summary_avg_window_size", "?")
        log_str = (
            f"[{runtime_hrs:.1f}h|Console] Step: {global_step/1e6:<6.2f}M | "
            f"Ep: {summary['total_episodes']:<7} | SPS: {summary['steps_per_second']:<5.0f} | "
            f"RLScore(Avg{avg_window_size}): {summary['avg_score_window']:<6.2f} (Best: {best_score_val}) | "
            f"Loss(Avg{avg_window_size}): {summary['avg_loss_window']:.4f} (Best: {best_loss_val}) | "
            f"LR: {summary['current_lr']:.1e} | "
            f"Buf: {summary['buffer_size']/1e6:.2f}M"
        )
        # --- END MODIFIED ---
        if summary["beta"] > 0 and summary["beta"] < 1.0:
            log_str += f" | Beta: {summary['beta']:.3f}"

        print(log_str)

        self.last_log_time = time.time()
        self.last_log_step = global_step

    def record_histogram(
        self,
        tag: str,
        values: Union[np.ndarray, torch.Tensor, List[float]],
        global_step: int,
    ):
        pass

    def record_image(
        self, tag: str, image: Union[np.ndarray, torch.Tensor], global_step: int
    ):
        pass

    def record_hparams(self, hparam_dict: Dict[str, Any], metric_dict: Dict[str, Any]):
        pass

    def record_graph(
        self, model: torch.nn.Module, input_to_model: Optional[Any] = None
    ):
        pass

    def close(self):
        print("[SimpleStatsRecorder] Closed.")


File: stats/stats_recorder.py
# File: stats/stats_recorder.py
import time
from abc import ABC, abstractmethod
from collections import deque
from typing import Deque, List, Dict, Any, Optional, Union
import numpy as np
import torch


class StatsRecorderBase(ABC):
    """Base class for recording training statistics."""

    @abstractmethod
    def record_episode(
        self,
        episode_score: float,
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,
    ):
        """Record stats for a completed episode."""
        pass

    @abstractmethod
    def record_step(self, step_data: Dict[str, Any]):
        """Record stats from a training or environment step."""
        pass

    @abstractmethod
    def record_histogram(
        self,
        tag: str,
        values: Union[np.ndarray, torch.Tensor, List[float]],
        global_step: int,
    ):
        """Record a histogram of values."""
        pass

    @abstractmethod
    def record_image(
        self, tag: str, image: Union[np.ndarray, torch.Tensor], global_step: int
    ):
        """Record an image."""
        pass

    @abstractmethod
    def record_hparams(self, hparam_dict: Dict[str, Any], metric_dict: Dict[str, Any]):
        """Record hyperparameters and final/key metrics."""
        pass

    @abstractmethod
    def record_graph(
        self, model: torch.nn.Module, input_to_model: Optional[Any] = None
    ):
        """Record the model graph."""
        pass

    @abstractmethod
    def get_summary(self, current_global_step: int) -> Dict[str, Any]:
        """Return a dictionary containing summary statistics."""
        pass

    @abstractmethod
    def get_plot_data(self) -> Dict[str, Deque]:
        """Return copies of data deques for plotting."""
        pass

    @abstractmethod
    def log_summary(self, global_step: int):
        """Trigger the logging action (e.g., print to console)."""
        pass

    @abstractmethod
    def close(self):
        """Perform any necessary cleanup."""
        pass


