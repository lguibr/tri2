File: requirements.txt
pygame>=2.1.0
numpy>=1.20.0
torch>=1.10.0
tensorboard
cloudpickle
torchvision
matplotlib

File: main_pygame.py
# File: main_pygame.py
import sys
import pygame
import numpy as np
import os
import time
import traceback
import torch
from typing import List, Tuple, Optional, Dict, Any, Deque

# --- Project Imports ---
from logger import TeeLogger
from app_setup import (
    initialize_pygame,
    initialize_directories,
    load_and_validate_configs,
)

from config import (
    VisConfig,
    EnvConfig,
    PPOConfig,
    RNNConfig,
    TrainConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    TensorBoardConfig,
    DemoConfig,
    DEVICE,
    RANDOM_SEED,
    MODEL_SAVE_PATH,
    BASE_CHECKPOINT_DIR,
    BASE_LOG_DIR,
    RUN_LOG_DIR,
)

from environment.game_state import GameState, StateType
from agent.ppo_agent import PPOAgent
from training.trainer import Trainer
from stats.stats_recorder import StatsRecorderBase
from ui.renderer import UIRenderer
from ui.input_handler import InputHandler
from utils.helpers import set_random_seeds
from utils.init_checks import run_pre_checks

from init.rl_components_ppo import (
    initialize_envs,
    initialize_agent,
    initialize_stats_recorder,
    initialize_trainer,
)


class MainApp:
    """Main application class orchestrating the Pygame UI and RL training."""

    def __init__(self):
        print("Initializing Application...")
        set_random_seeds(RANDOM_SEED)

        self.vis_config = VisConfig()
        self.env_config = EnvConfig()
        self.ppo_config = PPOConfig()
        self.rnn_config = RNNConfig()
        self.train_config = TrainConfig()
        self.model_config = ModelConfig()
        self.stats_config = StatsConfig()
        self.tensorboard_config = TensorBoardConfig()
        self.demo_config = DemoConfig()
        self.reward_config = RewardConfig()

        self.config_dict = load_and_validate_configs()
        self.num_envs = self.env_config.NUM_ENVS

        initialize_directories()
        self.screen, self.clock = initialize_pygame(self.vis_config)

        self.app_state = "Initializing"
        self.is_running = False
        self.cleanup_confirmation_active = False
        self.last_cleanup_message_time = 0.0
        self.cleanup_message = ""
        self.status = "Initializing Components"

        self.renderer: Optional[UIRenderer] = None
        self.input_handler: Optional[InputHandler] = None
        self.envs: List[GameState] = []
        self.agent: Optional[PPOAgent] = None
        self.stats_recorder: Optional[StatsRecorderBase] = None
        self.trainer: Optional[Trainer] = None
        self.demo_env: Optional[GameState] = None

        # Initial component initialization (not a re-init)
        self._initialize_core_components(is_reinit=False)

        self.app_state = "MainMenu"
        self.status = "Ready"
        print("Initialization Complete. Ready.")
        print(f"--- tensorboard --logdir {os.path.abspath(BASE_LOG_DIR)} ---")

    # --- MODIFIED: Added is_reinit flag ---
    def _initialize_core_components(self, is_reinit: bool = False):
        """Initializes Renderer, RL components, Demo Env, and Input Handler."""
        try:
            # Renderer only needs init once
            if not is_reinit:
                self.renderer = UIRenderer(self.screen, self.vis_config)
                self.renderer.render_all(
                    app_state=self.app_state,
                    is_running=self.is_running,
                    status=self.status,
                    stats_summary={},
                    envs=[],
                    num_envs=0,
                    env_config=self.env_config,
                    cleanup_confirmation_active=False,
                    cleanup_message="",
                    last_cleanup_message_time=0,
                    tensorboard_log_dir=None,
                    plot_data={},
                    demo_env=None,
                )
                pygame.time.delay(100)

            # Initialize RL components, passing the reinit flag
            self._initialize_rl_components(is_reinit=is_reinit)

            # Demo env and input handler only need init once
            if not is_reinit:
                self._initialize_demo_env()
                self.input_handler = InputHandler(
                    self.screen,
                    self.renderer,
                    self._toggle_running,
                    self._request_cleanup,
                    self._cancel_cleanup,
                    self._confirm_cleanup,
                    self._exit_app,
                    self._start_demo_mode,
                    self._exit_demo_mode,
                    self._handle_demo_input,
                )
        except Exception as init_err:
            print(f"FATAL ERROR during component initialization: {init_err}")
            traceback.print_exc()
            if self.renderer:
                try:
                    self.app_state = "Error"
                    self.status = "Initialization Failed"
                    self.renderer._render_error_screen(self.status)
                    pygame.display.flip()
                    time.sleep(5)
                except Exception:
                    pass
            pygame.quit()
            sys.exit(1)

    # --- MODIFIED: Added is_reinit flag ---
    def _initialize_rl_components(self, is_reinit: bool = False):
        """Initializes RL components using helper functions (now for PPO)."""
        print(f"Initializing RL components (PPO)... Re-init: {is_reinit}")
        start_time = time.time()
        try:
            # Environments and Agent always need re-initialization
            self.envs = initialize_envs(self.num_envs, self.env_config)
            self.agent = initialize_agent(
                self.model_config, self.ppo_config, self.rnn_config, self.env_config
            )

            # Stats recorder needs the reinit flag to avoid re-logging graph/hparams
            self.stats_recorder = initialize_stats_recorder(
                stats_config=self.stats_config,
                tb_config=self.tensorboard_config,
                config_dict=self.config_dict,
                agent=self.agent,
                env_config=self.env_config,
                rnn_config=self.rnn_config,
                is_reinit=is_reinit,  # Pass the flag here
            )
            if self.stats_recorder is None:
                raise RuntimeError("Stats Recorder initialization failed unexpectedly.")

            # Trainer always needs re-initialization
            self.trainer = initialize_trainer(
                envs=self.envs,
                agent=self.agent,
                stats_recorder=self.stats_recorder,
                env_config=self.env_config,
                ppo_config=self.ppo_config,
                rnn_config=self.rnn_config,
                train_config=self.train_config,
                model_config=self.model_config,
            )
            print(f"RL components initialized in {time.time() - start_time:.2f}s")
        except Exception as e:
            print(f"Error during RL component initialization: {e}")
            raise e

    # --- END MODIFIED ---

    def _initialize_demo_env(self):
        """Initializes the separate environment for demo mode."""
        print("Initializing Demo Environment...")
        try:
            self.demo_env = GameState()
            self.demo_env.reset()
            print("Demo environment initialized.")
        except Exception as e:
            print(f"ERROR initializing demo environment: {e}")
            traceback.print_exc()
            self.demo_env = None
            print("Warning: Demo mode may be unavailable.")

    # --- Input Handler Callbacks ---
    def _toggle_running(self):
        """Starts or stops the PPO training run."""
        if self.app_state != "MainMenu":
            return
        self.is_running = not self.is_running
        print(f"PPO Run {'STARTED' if self.is_running else 'STOPPED'}")
        if not self.is_running:
            self._try_save_checkpoint()

    def _request_cleanup(self):
        if self.app_state != "MainMenu":
            return
        was_running = self.is_running
        self.is_running = False
        if was_running:
            self._try_save_checkpoint()
        self.cleanup_confirmation_active = True
        print("Cleanup requested. Confirm action.")

    def _cancel_cleanup(self):
        self.cleanup_confirmation_active = False
        self.cleanup_message = "Cleanup cancelled."
        self.last_cleanup_message_time = time.time()
        print("Cleanup cancelled by user.")

    def _confirm_cleanup(self):
        print("Cleanup confirmed by user. Starting process...")
        try:
            self._cleanup_data()
        except Exception as e:
            print(f"FATAL ERROR during cleanup: {e}")
            traceback.print_exc()
            self.status = "Error: Cleanup Failed Critically"
            self.app_state = "Error"
        finally:
            self.cleanup_confirmation_active = False
            print(
                f"Cleanup process finished. State: {self.app_state}, Status: {self.status}"
            )

    def _exit_app(self) -> bool:
        print("Exit requested.")
        return False  # Signal to main loop to exit

    def _start_demo_mode(self):
        if self.demo_env is None:
            print("Cannot start demo mode: Demo environment failed to initialize.")
            return
        if self.app_state == "MainMenu":
            print("Entering Demo Mode...")
            self.is_running = False
            self._try_save_checkpoint()
            self.app_state = "Playing"
            self.status = "Playing Demo"
            self.demo_env.reset()

    def _exit_demo_mode(self):
        if self.app_state == "Playing":
            print("Exiting Demo Mode...")
            self.app_state = "MainMenu"
            self.status = "Ready"

    def _handle_demo_input(self, event: pygame.event.Event):
        """Handles keyboard input during demo mode."""
        if self.app_state != "Playing" or self.demo_env is None:
            return
        if self.demo_env.is_frozen() or self.demo_env.is_over():
            return

        if event.type == pygame.KEYDOWN:
            action_taken = False
            if event.key == pygame.K_LEFT:
                self.demo_env.move_target(0, -1)
                action_taken = True
            elif event.key == pygame.K_RIGHT:
                self.demo_env.move_target(0, 1)
                action_taken = True
            elif event.key == pygame.K_UP:
                self.demo_env.move_target(-1, 0)
                action_taken = True
            elif event.key == pygame.K_DOWN:
                self.demo_env.move_target(1, 0)
                action_taken = True
            elif event.key == pygame.K_q:
                self.demo_env.cycle_shape(-1)
                action_taken = True
            elif event.key == pygame.K_e:
                self.demo_env.cycle_shape(1)
                action_taken = True
            elif event.key == pygame.K_SPACE:
                action_index = self.demo_env.get_action_for_current_selection()
                if action_index is not None:
                    reward, done = self.demo_env.step(action_index)
                    action_taken = True
                else:
                    # Indicate invalid placement attempt visually? Maybe later.
                    action_taken = True  # Still counts as input handled

            if self.demo_env.is_over():
                print("[Demo] Game Over! Press ESC to exit.")

    # --- Core Logic Methods ---
    def _cleanup_data(self):
        """Deletes current run's checkpoint and re-initializes."""
        print("\n--- CLEANUP DATA INITIATED (Current Run Only) ---")
        self.app_state = "Initializing"
        self.is_running = False
        self.status = "Cleaning"
        messages = []

        # Render cleaning screen
        if self.renderer:
            try:
                self.renderer.render_all(
                    app_state=self.app_state,
                    is_running=False,
                    status=self.status,
                    stats_summary={},
                    envs=[],
                    num_envs=0,
                    env_config=self.env_config,
                    cleanup_confirmation_active=False,
                    cleanup_message="",
                    last_cleanup_message_time=0,
                    tensorboard_log_dir=None,
                    plot_data={},
                    demo_env=self.demo_env,
                )
                pygame.display.flip()
                pygame.time.delay(100)
            except Exception as render_err:
                print(f"Warning: Error rendering during cleanup start: {render_err}")

        # Close existing components safely
        if self.trainer:
            print("[Cleanup] Running trainer cleanup...")
            try:
                self.trainer.cleanup(save_final=False)  # Don't save during cleanup
            except Exception as e:
                print(f"Error during trainer cleanup: {e}")
        if self.stats_recorder:
            print("[Cleanup] Closing stats recorder...")
            try:
                # Don't close the writer itself, just detach components
                # self.stats_recorder.close() # This might close the writer we need
                pass  # Let the new recorder potentially reuse the writer
            except Exception as e:
                print(f"Error closing stats recorder: {e}")

        # Delete checkpoint file for the current run
        print("[Cleanup] Deleting agent checkpoint file...")
        try:
            if os.path.isfile(MODEL_SAVE_PATH):
                os.remove(MODEL_SAVE_PATH)
                msg = f"Agent ckpt deleted: {os.path.basename(MODEL_SAVE_PATH)}"
            else:
                msg = f"Agent ckpt not found (current run)."
            print(f"  - {msg}")
            messages.append(msg)
        except OSError as e:
            msg = f"Error deleting agent ckpt: {e}"
            print(f"  - {msg}")
            messages.append(msg)

        time.sleep(0.1)

        # Re-initialize RL components
        print("[Cleanup] Re-initializing RL components...")
        try:
            # --- MODIFIED: Pass is_reinit=True ---
            self._initialize_rl_components(is_reinit=True)
            # --- END MODIFIED ---

            if self.demo_env:  # Reset demo env as well
                self.demo_env.reset()
            print("[Cleanup] RL components re-initialized successfully.")
            messages.append("RL components re-initialized.")
            self.status = "Ready"
            self.app_state = "MainMenu"
        except Exception as e:
            print(f"FATAL ERROR during RL re-initialization after cleanup: {e}")
            traceback.print_exc()
            self.status = "Error: Re-init Failed"
            self.app_state = "Error"
            messages.append("ERROR RE-INITIALIZING RL COMPONENTS!")
            if self.renderer:
                try:
                    self.renderer._render_error_screen(self.status)
                except Exception as render_err_final:
                    print(f"Warning: Failed to render error screen: {render_err_final}")

        self.cleanup_message = "\n".join(messages)
        self.last_cleanup_message_time = time.time()
        print(
            f"--- CLEANUP DATA COMPLETE (Final State: {self.app_state}, Status: {self.status}) ---"
        )

    def _try_save_checkpoint(self):
        """Saves checkpoint if run is stopped and trainer exists."""
        if self.app_state == "MainMenu" and not self.is_running and self.trainer:
            print("Saving checkpoint on stop...")
            try:
                self.trainer.maybe_save_checkpoint(force_save=True)
            except Exception as e:
                print(f"Error saving checkpoint on stop: {e}")

    def _update(self):
        """Updates the application state and performs training steps (PPO)."""
        should_perform_training_iteration = False

        if self.app_state == "MainMenu":
            if self.cleanup_confirmation_active:
                self.status = "Confirm Cleanup"
            elif not self.is_running and self.status != "Error":
                self.status = "Ready"
            elif not self.trainer:
                if self.status != "Error":
                    self.status = "Error: Trainer Missing"
            elif self.is_running:
                self.status = "Training"
                should_perform_training_iteration = True
            else:  # Not running
                if self.status != "Error":
                    self.status = "Ready"

            if should_perform_training_iteration:
                if not self.trainer:
                    print("Error: Trainer became unavailable during _update.")
                    self.status = "Error: Trainer Lost"
                    self.is_running = False
                else:
                    try:
                        self.trainer.perform_training_iteration()
                    except Exception as e:
                        print(
                            f"\n--- ERROR DURING TRAINING ITERATION (Step: {getattr(self.trainer, 'global_step', 'N/A')}) ---"
                        )
                        traceback.print_exc()
                        print("--- Stopping training due to error. ---")
                        self.is_running = False
                        self.status = "Error: Training Iteration Failed"
                        # Keep app_state as MainMenu but show error status? Or switch to Error state?
                        # Switching to Error state might be clearer.
                        self.app_state = "Error"

        elif self.app_state == "Playing":
            if self.demo_env and hasattr(self.demo_env, "_update_timers"):
                self.demo_env._update_timers()  # Ensure demo timers update
            self.status = "Playing Demo"
        elif self.app_state == "Initializing":
            self.status = "Initializing..."  # Or more specific status if available
        elif self.app_state == "Error":
            # Status should already be set by the error source
            pass

    def _render(self):
        """Renders the UI based on the current application state."""
        stats_summary = {}
        plot_data: Dict[str, Deque] = {}

        # Get stats unless in pure initializing state
        if self.app_state != "Initializing":
            if self.stats_recorder:
                current_step = getattr(self.trainer, "global_step", 0)
                try:
                    stats_summary = self.stats_recorder.get_summary(current_step)
                except Exception as e:
                    print(f"Error getting stats summary: {e}")
                    stats_summary = {"global_step": current_step}  # Basic fallback
                try:
                    plot_data = self.stats_recorder.get_plot_data()
                except Exception as e:
                    print(f"Error getting plot data: {e}")
                    plot_data = {}
            elif self.app_state == "Error":
                # Try to get step count even if recorder failed
                stats_summary = {"global_step": getattr(self.trainer, "global_step", 0)}

        if not self.renderer:
            print("Error: Renderer not initialized in _render.")
            try:  # Basic error render
                self.screen.fill((0, 0, 0))
                font = pygame.font.SysFont(None, 50)
                surf = font.render("Renderer Error!", True, (255, 0, 0))
                self.screen.blit(
                    surf, surf.get_rect(center=self.screen.get_rect().center)
                )
                pygame.display.flip()
            except Exception:
                pass
            return

        try:
            self.renderer.render_all(
                app_state=self.app_state,
                is_running=self.is_running,
                status=self.status,
                stats_summary=stats_summary,
                envs=(self.envs if hasattr(self, "envs") else []),
                num_envs=self.num_envs,
                env_config=self.env_config,
                cleanup_confirmation_active=self.cleanup_confirmation_active,
                cleanup_message=self.cleanup_message,
                last_cleanup_message_time=self.last_cleanup_message_time,
                tensorboard_log_dir=(
                    self.tensorboard_config.LOG_DIR
                    if self.tensorboard_config.LOG_DIR
                    else None
                ),
                plot_data=plot_data,
                demo_env=self.demo_env,
            )
        except Exception as render_all_err:
            print(f"CRITICAL ERROR in renderer.render_all: {render_all_err}")
            traceback.print_exc()
            # Attempt to render a basic error screen via the renderer if possible
            try:
                self.app_state = "Error"
                self.status = "Render Error"
                self.renderer._render_error_screen(self.status)
                pygame.display.flip()  # Need flip here
            except Exception as e:
                print(f"Error rendering error screen: {e}")

        # Clear temporary cleanup message after timeout
        if time.time() - self.last_cleanup_message_time >= 5.0:
            self.cleanup_message = ""

    def _perform_cleanup(self):
        """Handles final cleanup of resources."""
        print("Exiting application...")
        if self.trainer:
            print("Performing final trainer cleanup...")
            try:
                # Save unless cleanup was just performed or in error state
                save_on_exit = self.status != "Cleaning" and self.app_state != "Error"
                self.trainer.cleanup(save_final=save_on_exit)
            except Exception as final_cleanup_err:
                print(f"Error during final trainer cleanup: {final_cleanup_err}")
        elif self.stats_recorder:  # Close stats recorder even if trainer failed/absent
            print("Closing stats recorder...")
            try:
                self.stats_recorder.close()
            except Exception as log_e:
                print(f"Error closing stats recorder on exit: {log_e}")

        pygame.quit()
        print("Application exited.")

    def run(self):
        """Main application loop."""
        print("Starting main application loop...")
        running_flag = True
        try:
            while running_flag:
                start_frame_time = time.perf_counter()

                # --- Input Handling ---
                if self.input_handler:
                    try:
                        running_flag = self.input_handler.handle_input(
                            self.app_state, self.cleanup_confirmation_active
                        )
                    except Exception as input_err:
                        print(
                            f"\n--- UNHANDLED ERROR IN INPUT LOOP ({self.app_state}) ---"
                        )
                        traceback.print_exc()
                        running_flag = False  # Exit on unhandled input error
                else:  # Basic exit if handler failed init
                    for event in pygame.event.get():
                        if event.type == pygame.QUIT:
                            running_flag = False
                        if (
                            event.type == pygame.KEYDOWN
                            and event.key == pygame.K_ESCAPE
                        ):
                            if self.app_state == "Playing":
                                self._exit_demo_mode()
                            elif not self.cleanup_confirmation_active:
                                running_flag = False
                    if not running_flag:
                        break

                if not running_flag:
                    break  # Exit if input handler requested it

                # --- Update ---
                try:
                    self._update()
                except Exception as update_err:
                    print(
                        f"\n--- UNHANDLED ERROR IN UPDATE LOOP ({self.app_state}) ---"
                    )
                    traceback.print_exc()
                    self.status = "Error: Update Loop Failed"
                    self.app_state = "Error"
                    self.is_running = False  # Stop training on update error

                # --- Render ---
                try:
                    self._render()
                except Exception as render_err:
                    print(
                        f"\n--- UNHANDLED ERROR IN RENDER LOOP ({self.app_state}) ---"
                    )
                    traceback.print_exc()
                    self.status = "Error: Render Loop Failed"
                    self.app_state = "Error"
                    # Continue running to display error? Or exit? Let's try continuing.

                # --- Frame Rate Control (Optional) ---
                # Let Pygame handle drawing timing, avoid manual sleep unless necessary
                # self.clock.tick(self.vis_config.FPS if self.vis_config.FPS > 0 else 0)
                # A tiny sleep can prevent 100% CPU usage in idle states if needed
                if not self.is_running and self.app_state == "MainMenu":
                    time.sleep(0.01)

        except KeyboardInterrupt:
            print("\nCtrl+C detected. Exiting gracefully...")
        except Exception as e:
            print(f"\n--- UNHANDLED EXCEPTION IN MAIN LOOP ({self.app_state}) ---")
            traceback.print_exc()
            print("--- EXITING ---")
        finally:
            self._perform_cleanup()


# --- Main Execution Block ---
if __name__ == "__main__":
    os.makedirs(BASE_CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(BASE_LOG_DIR, exist_ok=True)
    os.makedirs(RUN_LOG_DIR, exist_ok=True)

    log_filepath = os.path.join(RUN_LOG_DIR, "console_output.log")

    original_stdout = sys.stdout
    original_stderr = sys.stderr
    logger = TeeLogger(log_filepath, original_stdout)
    sys.stdout = logger
    sys.stderr = logger

    app_instance = None
    exit_code = 0

    try:
        if run_pre_checks():
            app_instance = MainApp()
            app_instance.run()
    except SystemExit as exit_err:
        print(f"Exiting due to SystemExit (Code: {getattr(exit_err, 'code', 'N/A')}).")
        exit_code = (
            getattr(exit_err, "code", 1)
            if isinstance(getattr(exit_err, "code", 1), int)
            else 1
        )
    except Exception as main_err:
        print("\n--- UNHANDLED EXCEPTION DURING APP INITIALIZATION OR RUN ---")
        traceback.print_exc()
        print("--- EXITING DUE TO ERROR ---")
        exit_code = 1
        # Attempt cleanup even if init failed partially
        if app_instance and hasattr(app_instance, "_perform_cleanup"):
            print("Attempting cleanup after main exception...")
            try:
                app_instance._perform_cleanup()
            except Exception as cleanup_err:
                print(f"Error during cleanup after main exception: {cleanup_err}")
    finally:
        if logger:
            final_app_state = getattr(app_instance, "app_state", "UNKNOWN")
            print(
                f"Restoring console output (Final App State: {final_app_state}). Full log saved to: {log_filepath}"
            )
            logger.close()
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        print(f"Console logging restored. Full log should be in: {log_filepath}")
        sys.exit(exit_code)


File: app_setup.py
# File: app_setup.py
import os
import pygame
from typing import Tuple, Dict, Any

from config import (
    VisConfig,
    EnvConfig,
    RewardConfig,
    PPOConfig,
    RNNConfig,
    TrainConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
    DemoConfig,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,
    get_config_dict,
    print_config_info_and_validate,
)


def initialize_pygame(
    vis_config: VisConfig,
) -> Tuple[pygame.Surface, pygame.time.Clock]:
    """Initializes Pygame, sets up the screen and clock."""
    print("Initializing Pygame...")
    pygame.init()
    pygame.font.init()
    screen = pygame.display.set_mode(
        (vis_config.SCREEN_WIDTH, vis_config.SCREEN_HEIGHT), pygame.RESIZABLE
    )
    pygame.display.set_caption("TriCrack PPO")
    clock = pygame.time.Clock()
    print("Pygame initialized.")
    return screen, clock


def initialize_directories():
    """Creates necessary runtime directories."""
    os.makedirs(RUN_CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(RUN_LOG_DIR, exist_ok=True)
    print(f"Ensured directories exist: {RUN_CHECKPOINT_DIR}, {RUN_LOG_DIR}")


def load_and_validate_configs() -> Dict[str, Any]:
    """Loads all config classes and returns the combined config dictionary."""
    config_dict = get_config_dict()
    print_config_info_and_validate()
    return config_dict


File: visualization/__init__.py


File: init/rl_components_ppo.py
# File: init/rl_components_ppo.py
import sys
import traceback
import numpy as np
import torch
from typing import List, Tuple, Optional, Dict, Any, Callable

from config import (
    EnvConfig,
    PPOConfig,
    RNNConfig,
    TrainConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    TensorBoardConfig,
    DEVICE,
    MODEL_SAVE_PATH,
    get_config_dict,
)

try:
    from environment.game_state import GameState, StateType
except ImportError as e:
    print(f"Error importing environment: {e}")
    sys.exit(1)

from agent.ppo_agent import PPOAgent
from training.trainer import Trainer
from stats.stats_recorder import StatsRecorderBase
from stats.aggregator import StatsAggregator
from stats.simple_stats_recorder import SimpleStatsRecorder
from stats.tensorboard_logger import TensorBoardStatsRecorder


def initialize_envs(num_envs: int, env_config: EnvConfig) -> List[GameState]:
    """Initializes the specified number of game environments."""
    print(f"Initializing {num_envs} game environments...")
    try:
        envs = [GameState() for _ in range(num_envs)]
        # Basic validation on the first environment
        s_test_dict = envs[0].reset()

        if not isinstance(s_test_dict, dict):
            raise TypeError("Env reset did not return a dictionary state.")
        if "grid" not in s_test_dict:
            raise KeyError("State dict missing 'grid'")
        grid_state = s_test_dict["grid"]
        expected_grid_shape = env_config.GRID_STATE_SHAPE
        if (
            not isinstance(grid_state, np.ndarray)
            or grid_state.shape != expected_grid_shape
        ):
            raise ValueError(
                f"Initial grid state shape mismatch! Env:{grid_state.shape}, Cfg:{expected_grid_shape}"
            )
        print(f"Initial grid state shape check PASSED: {grid_state.shape}")

        if "shapes" not in s_test_dict:
            raise KeyError("State dict missing 'shapes'")
        shape_state = s_test_dict["shapes"]
        expected_shape_shape = (
            env_config.NUM_SHAPE_SLOTS,
            env_config.SHAPE_FEATURES_PER_SHAPE,
        )
        if (
            not isinstance(shape_state, np.ndarray)
            or shape_state.shape != expected_shape_shape
        ):
            raise ValueError(
                f"Initial shape state shape mismatch! Env:{shape_state.shape}, Cfg:{expected_shape_shape}"
            )
        print(f"Initial shape state shape check PASSED: {shape_state.shape}")

        # Test step with a valid action if available
        valid_acts_init = envs[0].valid_actions()
        if valid_acts_init:
            _, _ = envs[0].step(valid_acts_init[0])
        else:
            print(
                "Warning: No valid actions available after initial reset for testing step()."
            )

        print(f"Successfully initialized {num_envs} environments.")
        return envs
    except Exception as e:
        print(f"FATAL ERROR during env init: {e}")
        traceback.print_exc()
        raise e


def initialize_agent(
    model_config: ModelConfig,
    ppo_config: PPOConfig,
    rnn_config: RNNConfig,
    env_config: EnvConfig,
) -> PPOAgent:
    """Initializes the PPO Agent."""
    print("Initializing PPO Agent...")
    agent = PPOAgent(
        model_config=model_config,
        ppo_config=ppo_config,
        rnn_config=rnn_config,
        env_config=env_config,
    )
    print("PPO Agent initialized.")
    return agent


# --- MODIFIED: Added is_reinit flag ---
def initialize_stats_recorder(
    stats_config: StatsConfig,
    tb_config: TensorBoardConfig,
    config_dict: Dict[str, Any],
    agent: Optional[PPOAgent],
    env_config: EnvConfig,
    rnn_config: RNNConfig,
    is_reinit: bool = False,  # Added flag
) -> StatsRecorderBase:
    """Initializes the statistics recording components."""
    print(f"Initializing Statistics Components... Re-init: {is_reinit}")
    stats_aggregator = StatsAggregator(
        avg_windows=stats_config.STATS_AVG_WINDOW,
        plot_window=stats_config.PLOT_DATA_WINDOW,
    )
    console_recorder = SimpleStatsRecorder(
        aggregator=stats_aggregator,
        console_log_interval=stats_config.CONSOLE_LOG_FREQ,
    )

    model_for_graph_cpu = None
    dummy_input_tuple = None

    # --- MODIFIED: Only prepare graph model/input on initial setup ---
    if not is_reinit and agent and agent.network:
        print("[Stats Init] Preparing model copy and dummy input for graph...")
        try:
            # Prepare dummy input on CPU
            expected_grid_shape = env_config.GRID_STATE_SHAPE
            dummy_grid_np = np.zeros(expected_grid_shape, dtype=np.float32)
            dummy_shapes_np = np.zeros(env_config.SHAPE_STATE_DIM, dtype=np.float32)
            # Add batch and sequence dimensions if RNN is used (B=1, T=1)
            batch_dim = 1
            seq_dim = 1 if rnn_config.USE_RNN else 0
            grid_dims = ([batch_dim, seq_dim] if seq_dim else [batch_dim]) + list(
                expected_grid_shape
            )
            shape_dims = ([batch_dim, seq_dim] if seq_dim else [batch_dim]) + [
                env_config.SHAPE_STATE_DIM
            ]

            dummy_grid_cpu = torch.tensor(dummy_grid_np).reshape(grid_dims).to("cpu")
            dummy_shapes_cpu = (
                torch.tensor(dummy_shapes_np).reshape(shape_dims).to("cpu")
            )

            # Create a copy of the network on CPU for graph tracing
            # Ensure the network type is correctly inferred or passed
            model_for_graph_cpu = type(agent.network)(
                env_config=env_config,
                action_dim=env_config.ACTION_DIM,
                model_config=agent.network.config,  # Access config from the agent's network instance
                rnn_config=rnn_config,
            ).to("cpu")

            model_for_graph_cpu.load_state_dict(agent.network.state_dict())
            model_for_graph_cpu.eval()

            # Prepare dummy input tuple (grid, shapes) - hidden state is optional for graph
            dummy_input_tuple = (dummy_grid_cpu, dummy_shapes_cpu)

            print("[Stats Init] Prepared model copy and dummy input on CPU for graph.")
        except Exception as e:
            print(f"Warning: Failed to prepare model/input for graph logging: {e}")
            traceback.print_exc()
            model_for_graph_cpu, dummy_input_tuple = None, None
    elif is_reinit:
        print("[Stats Init] Skipping graph model preparation during re-initialization.")
    # --- END MODIFIED ---

    print(f"Using TensorBoard Logger (Log Dir: {tb_config.LOG_DIR})")
    try:
        # Pass potentially None model/input to constructor
        tb_recorder = TensorBoardStatsRecorder(
            aggregator=stats_aggregator,
            console_recorder=console_recorder,
            log_dir=tb_config.LOG_DIR,
            hparam_dict=config_dict if not is_reinit else None,  # Log hparams only once
            model_for_graph=model_for_graph_cpu,  # Will be None if is_reinit
            dummy_input_for_graph=dummy_input_tuple,  # Will be None if is_reinit
            histogram_log_interval=(
                tb_config.HISTOGRAM_LOG_FREQ if tb_config.LOG_HISTOGRAMS else -1
            ),
            image_log_interval=(
                tb_config.IMAGE_LOG_FREQ if tb_config.LOG_IMAGES else -1
            ),
            env_config=env_config,
            rnn_config=rnn_config,
        )
        print("Statistics Components initialized successfully.")
        return tb_recorder
    except Exception as e:
        print(f"FATAL: Error initializing TensorBoardStatsRecorder: {e}. Exiting.")
        traceback.print_exc()
        raise e


# --- END MODIFIED ---


def initialize_trainer(
    envs: List[GameState],
    agent: PPOAgent,
    stats_recorder: StatsRecorderBase,
    env_config: EnvConfig,
    ppo_config: PPOConfig,
    rnn_config: RNNConfig,
    train_config: TrainConfig,
    model_config: ModelConfig,
) -> Trainer:
    """Initializes the PPO Trainer."""
    print("Initializing PPO Trainer...")
    trainer = Trainer(
        envs=envs,
        agent=agent,
        stats_recorder=stats_recorder,
        env_config=env_config,
        ppo_config=ppo_config,
        rnn_config=rnn_config,
        train_config=train_config,
        model_config=model_config,
        model_save_path=MODEL_SAVE_PATH,
        load_checkpoint_path=train_config.LOAD_CHECKPOINT_PATH,
    )
    print("PPO Trainer initialization finished.")
    return trainer


File: init/__init__.py
# File: init/__init__.py
from .rl_components_ppo import (
    initialize_envs,
    initialize_agent,
    initialize_stats_recorder,
    initialize_trainer,
)

__all__ = [
    "initialize_envs",
    "initialize_agent",
    "initialize_stats_recorder",
    "initialize_trainer",
]


File: ui/tooltips.py
# File: ui/tooltips.py
# (No significant changes needed, this file was already focused)
import pygame
from typing import Tuple, Dict, Optional
from config import VisConfig


class TooltipRenderer:
    """Handles rendering of tooltips when hovering over specific UI elements."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.font_tooltip = self._init_font()
        self.hovered_stat_key: Optional[str] = None
        self.stat_rects: Dict[str, pygame.Rect] = {}  # Rects to check for hover
        self.tooltip_texts: Dict[str, str] = {}  # Text corresponding to each rect key

    def _init_font(self):
        """Initializes the font used for tooltips."""
        try:
            # Smaller font for tooltips
            return pygame.font.SysFont(None, 18)
        except Exception as e:
            print(f"Warning: SysFont error for tooltip font: {e}. Using default.")
            return pygame.font.Font(None, 18)

    def check_hover(self, mouse_pos: Tuple[int, int]):
        """Checks if the mouse is hovering over any registered stat rect."""
        self.hovered_stat_key = None
        # Iterate in reverse order of drawing to prioritize top elements
        for key, rect in reversed(self.stat_rects.items()):
            # Ensure rect is valid before checking collision
            if (
                rect
                and rect.width > 0
                and rect.height > 0
                and rect.collidepoint(mouse_pos)
            ):
                self.hovered_stat_key = key
                return  # Found one, stop checking

    def render_tooltip(self):
        """Renders the tooltip if a stat element is being hovered over. Does not flip display."""
        if not self.hovered_stat_key or self.hovered_stat_key not in self.tooltip_texts:
            return  # No active hover or no text defined for this key

        tooltip_text = self.tooltip_texts[self.hovered_stat_key]
        mouse_pos = pygame.mouse.get_pos()

        # --- Text Wrapping Logic ---
        lines = []
        max_width = 300  # Max tooltip width in pixels
        words = tooltip_text.split(" ")
        current_line = ""
        for word in words:
            test_line = f"{current_line} {word}" if current_line else word
            try:
                test_surf = self.font_tooltip.render(test_line, True, VisConfig.BLACK)
                if test_surf.get_width() <= max_width:
                    current_line = test_line
                else:
                    lines.append(current_line)
                    current_line = word
            except Exception as e:
                print(f"Warning: Font render error during tooltip wrap: {e}")
                lines.append(current_line)  # Add what we had
                current_line = word  # Start new line
        lines.append(current_line)  # Add the last line

        # --- Render Wrapped Text ---
        line_surfs = []
        total_height = 0
        max_line_width = 0
        try:
            for line in lines:
                if not line:
                    continue  # Skip empty lines
                surf = self.font_tooltip.render(line, True, VisConfig.BLACK)
                line_surfs.append(surf)
                total_height += surf.get_height()
                max_line_width = max(max_line_width, surf.get_width())
        except Exception as e:
            print(f"Warning: Font render error creating tooltip surfaces: {e}")
            return  # Cannot render tooltip

        if not line_surfs:
            return  # No valid lines to render

        # --- Calculate Tooltip Rect and Draw ---
        padding = 5
        tooltip_rect = pygame.Rect(
            mouse_pos[0] + 15,  # Offset from cursor x
            mouse_pos[1] + 10,  # Offset from cursor y
            max_line_width + padding * 2,
            total_height + padding * 2,
        )

        # Clamp tooltip rect to stay within screen bounds
        tooltip_rect.clamp_ip(self.screen.get_rect())

        try:
            # Draw background and border
            pygame.draw.rect(
                self.screen, VisConfig.YELLOW, tooltip_rect, border_radius=3
            )
            pygame.draw.rect(
                self.screen, VisConfig.BLACK, tooltip_rect, 1, border_radius=3
            )

            # Draw text lines onto the screen
            current_y = tooltip_rect.y + padding
            for surf in line_surfs:
                self.screen.blit(surf, (tooltip_rect.x + padding, current_y))
                current_y += surf.get_height()
        except Exception as e:
            print(f"Warning: Error drawing tooltip background/text: {e}")

    def update_rects_and_texts(
        self, rects: Dict[str, pygame.Rect], texts: Dict[str, str]
    ):
        """Updates the dictionaries used for hover detection and text lookup. Called by UIRenderer."""
        self.stat_rects = rects
        self.tooltip_texts = texts


File: ui/demo_renderer.py
# File: ui/demo_renderer.py
import pygame
import math
import traceback
from typing import Optional, Tuple

from config import VisConfig, EnvConfig, DemoConfig
from environment.game_state import GameState
from .panels.game_area import GameAreaRenderer


class DemoRenderer:
    """Handles rendering specifically for the interactive Demo Mode."""

    def __init__(
        self,
        screen: pygame.Surface,
        vis_config: VisConfig,
        demo_config: DemoConfig,
        game_area_renderer: GameAreaRenderer,
    ):
        self.screen = screen
        self.vis_config = vis_config
        self.demo_config = demo_config
        self.game_area_renderer = game_area_renderer
        self._init_demo_fonts()
        self.overlay_font = self.game_area_renderer.fonts.get("env_overlay")
        if not self.overlay_font:
            print("Warning: DemoRenderer could not get overlay font. Using default.")
            self.overlay_font = pygame.font.Font(None, 36)
        # --- NEW: Define invalid placement color ---
        self.invalid_placement_color = (0, 0, 0, 150)  # Transparent Gray
        # --- END NEW ---

    def _init_demo_fonts(self):
        try:
            self.demo_hud_font = pygame.font.SysFont(
                None, self.demo_config.HUD_FONT_SIZE
            )
            self.demo_help_font = pygame.font.SysFont(
                None, self.demo_config.HELP_FONT_SIZE
            )
            if not hasattr(
                self.game_area_renderer, "fonts"
            ) or not self.game_area_renderer.fonts.get("ui"):
                self.game_area_renderer._init_fonts()
        except Exception as e:
            print(f"Warning: SysFont error for demo fonts: {e}. Using default.")
            self.demo_hud_font = pygame.font.Font(None, self.demo_config.HUD_FONT_SIZE)
            self.demo_help_font = pygame.font.Font(
                None, self.demo_config.HELP_FONT_SIZE
            )

    def render(self, demo_env: GameState, env_config: EnvConfig):
        if not demo_env:
            print("Error: DemoRenderer called with demo_env=None")
            return

        bg_color = self._determine_background_color(demo_env)
        self.screen.fill(bg_color)

        sw, sh = self.screen.get_size()
        padding = 30
        hud_height = 60
        help_height = 30
        max_game_h = sh - 2 * padding - hud_height - help_height
        max_game_w = sw - 2 * padding

        if max_game_h <= 0 or max_game_w <= 0:
            self._render_too_small_message(
                "Demo Area Too Small", self.screen.get_rect()
            )
            return

        game_rect, clipped_game_rect = self._calculate_game_area_rect(
            sw, sh, padding, hud_height, help_height, env_config
        )

        if clipped_game_rect.width > 10 and clipped_game_rect.height > 10:
            self._render_game_area(demo_env, env_config, clipped_game_rect, bg_color)
        else:
            self._render_too_small_message("Demo Area Too Small", clipped_game_rect)

        self._render_shape_previews_area(demo_env, sw, clipped_game_rect, padding)
        self._render_hud(demo_env, sw, game_rect.bottom + 10)
        self._render_help_text(sw, sh)

    def _determine_background_color(self, demo_env: GameState) -> Tuple[int, int, int]:
        if demo_env.is_line_clearing():
            return VisConfig.LINE_CLEAR_FLASH_COLOR
        elif demo_env.is_game_over_flashing():
            return VisConfig.GAME_OVER_FLASH_COLOR
        elif demo_env.is_over():
            return VisConfig.DARK_RED
        elif demo_env.is_frozen():
            return (30, 30, 100)
        else:
            return self.demo_config.BACKGROUND_COLOR

    def _calculate_game_area_rect(
        self,
        screen_width: int,
        screen_height: int,
        padding: int,
        hud_height: int,
        help_height: int,
        env_config: EnvConfig,
    ) -> Tuple[pygame.Rect, pygame.Rect]:
        max_game_h = screen_height - 2 * padding - hud_height - help_height
        max_game_w = screen_width - 2 * padding
        aspect_ratio = (env_config.COLS * 0.75 + 0.25) / max(1, env_config.ROWS)
        game_w = max_game_w
        game_h = game_w / aspect_ratio if aspect_ratio > 0 else max_game_h
        if game_h > max_game_h:
            game_h = max_game_h
            game_w = game_h * aspect_ratio
        game_w = math.floor(min(game_w, max_game_w))
        game_h = math.floor(min(game_h, max_game_h))
        game_x = (screen_width - game_w) // 2
        game_y = padding
        game_rect = pygame.Rect(game_x, game_y, game_w, game_h)
        clipped_game_rect = game_rect.clip(self.screen.get_rect())
        return game_rect, clipped_game_rect

    def _render_game_area(
        self,
        demo_env: GameState,
        env_config: EnvConfig,
        clipped_game_rect: pygame.Rect,
        bg_color: Tuple[int, int, int],
    ):
        try:
            game_surf = self.screen.subsurface(clipped_game_rect)
            game_surf.fill(bg_color)

            self.game_area_renderer._render_single_env_grid(
                game_surf, demo_env, env_config
            )

            preview_tri_cell_w, preview_tri_cell_h = self._calculate_demo_triangle_size(
                clipped_game_rect.width, clipped_game_rect.height, env_config
            )
            if preview_tri_cell_w > 0 and preview_tri_cell_h > 0:
                grid_ox, grid_oy = self._calculate_grid_offset(
                    clipped_game_rect.width, clipped_game_rect.height, env_config
                )
                self._render_placement_preview(
                    game_surf,
                    demo_env,
                    preview_tri_cell_w,
                    preview_tri_cell_h,
                    grid_ox,
                    grid_oy,
                )

            if demo_env.is_over():
                self._render_demo_overlay_text(game_surf, "GAME OVER", VisConfig.RED)
            elif demo_env.is_line_clearing():
                self._render_demo_overlay_text(game_surf, "Line Clear!", VisConfig.BLUE)

        except ValueError as e:
            print(f"Error subsurface demo game ({clipped_game_rect}): {e}")
            pygame.draw.rect(self.screen, VisConfig.RED, clipped_game_rect, 1)
        except Exception as render_e:
            print(f"Error rendering demo game area: {render_e}")
            traceback.print_exc()
            pygame.draw.rect(self.screen, VisConfig.RED, clipped_game_rect, 1)

    def _render_shape_previews_area(
        self,
        demo_env: GameState,
        screen_width: int,
        clipped_game_rect: pygame.Rect,
        padding: int,
    ):
        preview_area_w = min(150, screen_width - clipped_game_rect.right - padding // 2)
        if preview_area_w > 20:
            preview_area_rect = pygame.Rect(
                clipped_game_rect.right + padding // 2,
                clipped_game_rect.top,
                preview_area_w,
                clipped_game_rect.height,
            )
            clipped_preview_area_rect = preview_area_rect.clip(self.screen.get_rect())
            if (
                clipped_preview_area_rect.width > 0
                and clipped_preview_area_rect.height > 0
            ):
                try:
                    preview_area_surf = self.screen.subsurface(
                        clipped_preview_area_rect
                    )
                    self._render_demo_shape_previews(preview_area_surf, demo_env)
                except ValueError as e:
                    print(f"Error subsurface demo shape preview area: {e}")
                    pygame.draw.rect(
                        self.screen, VisConfig.RED, clipped_preview_area_rect, 1
                    )
                except Exception as e:
                    print(f"Error rendering demo shape previews: {e}")
                    traceback.print_exc()

    def _render_hud(self, demo_env: GameState, screen_width: int, hud_y: int):
        score_text = f"Score: {demo_env.game_score} | Lines: {demo_env.lines_cleared_this_episode}"
        try:
            score_surf = self.demo_hud_font.render(score_text, True, VisConfig.WHITE)
            score_rect = score_surf.get_rect(midtop=(screen_width // 2, hud_y))
            self.screen.blit(score_surf, score_rect)
        except Exception as e:
            print(f"HUD render error: {e}")

    def _render_help_text(self, screen_width: int, screen_height: int):
        try:
            help_surf = self.demo_help_font.render(
                self.demo_config.HELP_TEXT, True, VisConfig.LIGHTG
            )
            help_rect = help_surf.get_rect(
                centerx=screen_width // 2, bottom=screen_height - 10
            )
            self.screen.blit(help_surf, help_rect)
        except Exception as e:
            print(f"Help render error: {e}")

    def _render_demo_overlay_text(
        self, surf: pygame.Surface, text: str, color: Tuple[int, int, int]
    ):
        try:
            if not self.overlay_font:
                print("Error: Overlay font not available for demo overlay.")
                return
            text_surf = self.overlay_font.render(
                text,
                True,
                VisConfig.WHITE,
                (color[0] // 2, color[1] // 2, color[2] // 2, 220),
            )
            text_rect = text_surf.get_rect(center=surf.get_rect().center)
            surf.blit(text_surf, text_rect)
        except Exception as e:
            print(f"Error rendering demo overlay text '{text}': {e}")

    def _calculate_demo_triangle_size(
        self, surf_w: int, surf_h: int, env_config: EnvConfig
    ) -> Tuple[int, int]:
        padding = self.vis_config.ENV_GRID_PADDING
        drawable_w = max(1, surf_w - 2 * padding)
        drawable_h = max(1, surf_h - 2 * padding)
        grid_rows = env_config.ROWS
        grid_cols_eff_width = env_config.COLS * 0.75 + 0.25
        if grid_rows <= 0 or grid_cols_eff_width <= 0:
            return 0, 0
        scale_w = drawable_w / grid_cols_eff_width
        scale_h = drawable_h / grid_rows
        final_scale = min(scale_w, scale_h)
        if final_scale <= 0:
            return 0, 0
        tri_cell_size = max(1, int(final_scale))
        return tri_cell_size, tri_cell_size

    def _calculate_grid_offset(
        self, surf_w: int, surf_h: int, env_config: EnvConfig
    ) -> Tuple[float, float]:
        padding = self.vis_config.ENV_GRID_PADDING
        drawable_w = max(1, surf_w - 2 * padding)
        drawable_h = max(1, surf_h - 2 * padding)
        grid_rows = env_config.ROWS
        grid_cols_eff_width = env_config.COLS * 0.75 + 0.25
        if grid_rows <= 0 or grid_cols_eff_width <= 0:
            return float(padding), float(padding)
        scale_w = drawable_w / grid_cols_eff_width
        scale_h = drawable_h / grid_rows
        final_scale = min(scale_w, scale_h)
        final_grid_pixel_w = max(1, grid_cols_eff_width * final_scale)
        final_grid_pixel_h = max(1, grid_rows * final_scale)
        grid_ox = padding + (drawable_w - final_grid_pixel_w) / 2
        grid_oy = padding + (drawable_h - final_grid_pixel_h) / 2
        return grid_ox, grid_oy

    def _render_placement_preview(
        self,
        surf: pygame.Surface,
        env: GameState,
        cell_w: int,
        cell_h: int,
        offset_x: float,
        offset_y: float,
    ):
        if cell_w <= 0 or cell_h <= 0:
            return
        shp, rr, cc = env.get_current_selection_info()
        if shp is None:
            return

        is_valid = env.grid.can_place(shp, rr, cc)

        preview_alpha = 150
        if is_valid:
            shape_rgb = shp.color
            preview_color_to_use = (
                shape_rgb[0],
                shape_rgb[1],
                shape_rgb[2],
                preview_alpha,
            )
        else:
            # --- MODIFIED: Use the defined gray color for invalid placement ---
            preview_color_to_use = self.invalid_placement_color
            # --- END MODIFIED ---

        temp_surface = pygame.Surface(surf.get_size(), pygame.SRCALPHA)
        temp_surface.fill((0, 0, 0, 0))

        for dr, dc, up in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if (
                env.grid.valid(nr, nc)
                and 0 <= nr < len(env.grid.triangles)
                and 0 <= nc < len(env.grid.triangles[nr])
                and not env.grid.triangles[nr][nc].is_death
            ):
                temp_tri = env.grid.triangles[nr][nc]
                try:
                    pts = temp_tri.get_points(
                        ox=offset_x, oy=offset_y, cw=cell_w, ch=cell_h
                    )
                    pygame.draw.polygon(temp_surface, preview_color_to_use, pts)
                except Exception as e:
                    pass

        surf.blit(temp_surface, (0, 0))

    def _render_demo_shape_previews(self, surf: pygame.Surface, env: GameState):
        surf.fill((25, 25, 25))
        all_slots = env.shapes
        selected_shape_obj = (
            all_slots[env.demo_selected_shape_idx]
            if 0 <= env.demo_selected_shape_idx < len(all_slots)
            else None
        )
        num_slots = env.env_config.NUM_SHAPE_SLOTS
        surf_w, surf_h = surf.get_size()
        preview_padding = 5

        if num_slots <= 0:
            return

        preview_h = max(20, (surf_h - (num_slots + 1) * preview_padding) / num_slots)
        preview_w = max(20, surf_w - 2 * preview_padding)
        current_preview_y = preview_padding

        for i in range(num_slots):
            shp = all_slots[i] if i < len(all_slots) else None
            preview_rect = pygame.Rect(
                preview_padding, current_preview_y, preview_w, preview_h
            )
            clipped_preview_rect = preview_rect.clip(surf.get_rect())

            if clipped_preview_rect.width <= 0 or clipped_preview_rect.height <= 0:
                current_preview_y += preview_h + preview_padding
                continue

            bg_color = (40, 40, 40)
            border_color = VisConfig.GRAY
            border_width = 1
            if shp is not None and shp == selected_shape_obj:
                border_color = self.demo_config.SELECTED_SHAPE_HIGHLIGHT_COLOR
                border_width = 2

            pygame.draw.rect(surf, bg_color, clipped_preview_rect, border_radius=3)
            pygame.draw.rect(
                surf, border_color, clipped_preview_rect, border_width, border_radius=3
            )

            if shp is not None:
                self._render_single_shape_in_preview_box(
                    surf, shp, preview_rect, clipped_preview_rect
                )

            current_preview_y += preview_h + preview_padding

    def _render_single_shape_in_preview_box(
        self,
        surf: pygame.Surface,
        shp,
        preview_rect: pygame.Rect,
        clipped_preview_rect: pygame.Rect,
    ):
        try:
            inner_padding = 2
            shape_render_area_rect = pygame.Rect(
                inner_padding,
                inner_padding,
                clipped_preview_rect.width - 2 * inner_padding,
                clipped_preview_rect.height - 2 * inner_padding,
            )
            if shape_render_area_rect.width > 0 and shape_render_area_rect.height > 0:
                sub_surf_x = preview_rect.left + shape_render_area_rect.left
                sub_surf_y = preview_rect.top + shape_render_area_rect.top
                shape_sub_surf = surf.subsurface(
                    sub_surf_x,
                    sub_surf_y,
                    shape_render_area_rect.width,
                    shape_render_area_rect.height,
                )
                min_r, min_c, max_r, max_c = shp.bbox()
                shape_h = max(1, max_r - min_r + 1)
                shape_w_eff = max(1, (max_c - min_c + 1) * 0.75 + 0.25)

                scale_h = shape_render_area_rect.height / shape_h
                scale_w = shape_render_area_rect.width / shape_w_eff
                cell_size = max(1, min(scale_h, scale_w))

                self.game_area_renderer._render_single_shape(
                    shape_sub_surf, shp, int(cell_size)
                )
        except ValueError as sub_err:
            print(f"Error subsurface shape preview: {sub_err}")
            pygame.draw.rect(surf, VisConfig.RED, clipped_preview_rect, 1)
        except Exception as e:
            print(f"Error rendering demo shape preview: {e}")
            pygame.draw.rect(surf, VisConfig.RED, clipped_preview_rect, 1)

    def _render_too_small_message(self, text: str, area_rect: pygame.Rect):
        try:
            font = self.game_area_renderer.fonts.get("ui") or pygame.font.SysFont(
                None, 24
            )
            err_surf = font.render(text, True, VisConfig.GRAY)
            target_rect = err_surf.get_rect(center=area_rect.center)
            self.screen.blit(err_surf, target_rect)
        except Exception as e:
            print(f"Error rendering 'too small' message: {e}")


File: ui/renderer.py
# File: ui/renderer.py
import pygame
import time
import traceback
from typing import List, Dict, Any, Optional, Tuple, Deque

from config import VisConfig, EnvConfig, TensorBoardConfig, DemoConfig
from environment.game_state import GameState
from .panels import LeftPanelRenderer, GameAreaRenderer
from .overlays import OverlayRenderer
from .tooltips import TooltipRenderer
from .plotter import Plotter
from .demo_renderer import DemoRenderer


class UIRenderer:
    """Orchestrates rendering of all UI components."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.plotter = Plotter()
        self.left_panel = LeftPanelRenderer(screen, vis_config, self.plotter)
        self.game_area = GameAreaRenderer(screen, vis_config)
        self.overlays = OverlayRenderer(screen, vis_config)
        self.tooltips = TooltipRenderer(screen, vis_config)
        self.demo_config = DemoConfig()
        self.demo_renderer = DemoRenderer(
            screen, vis_config, self.demo_config, self.game_area
        )
        self.last_plot_update_time = 0

    def check_hover(self, mouse_pos: Tuple[int, int], app_state: str):
        """Passes hover check to the tooltip renderer."""
        if app_state == "MainMenu":
            self.tooltips.update_rects_and_texts(
                self.left_panel.get_stat_rects(), self.left_panel.get_tooltip_texts()
            )
            self.tooltips.check_hover(mouse_pos)
        else:
            self.tooltips.hovered_stat_key = None
            self.tooltips.stat_rects.clear()

    def force_redraw(self):
        """Forces components like the plotter to redraw on the next frame."""
        self.plotter.last_plot_update_time = 0

    def render_all(
        self,
        app_state: str,
        is_running: bool,  # Renamed from is_training
        status: str,
        stats_summary: Dict[str, Any],
        envs: List[GameState],
        num_envs: int,
        env_config: EnvConfig,
        cleanup_confirmation_active: bool,
        cleanup_message: str,
        last_cleanup_message_time: float,
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
        demo_env: Optional[GameState] = None,
    ):
        """Renders UI based on the application state."""
        try:
            if app_state == "MainMenu":
                self._render_main_menu(
                    is_running,
                    status,
                    stats_summary,
                    envs,
                    num_envs,
                    env_config,
                    cleanup_message,
                    last_cleanup_message_time,
                    tensorboard_log_dir,
                    plot_data,
                )
            elif app_state == "Playing":
                if demo_env:
                    self.demo_renderer.render(demo_env, env_config)
                else:
                    print("Error: Attempting to render demo mode without demo_env.")
                    self._render_simple_message("Demo Env Error!", VisConfig.RED)
            elif app_state == "Initializing":
                self._render_initializing_screen(status)
            elif app_state == "Error":
                self._render_error_screen(status)

            if cleanup_confirmation_active and app_state != "Error":
                self.overlays.render_cleanup_confirmation()
            elif not cleanup_confirmation_active:
                self.overlays.render_status_message(
                    cleanup_message, last_cleanup_message_time
                )

            if app_state == "MainMenu" and not cleanup_confirmation_active:
                self.tooltips.render_tooltip()

            pygame.display.flip()

        except pygame.error as e:
            print(f"Pygame rendering error in render_all: {e}")
            traceback.print_exc()
        except Exception as e:
            print(f"Unexpected critical rendering error in render_all: {e}")
            traceback.print_exc()
            try:
                self._render_simple_message("Critical Render Error!", VisConfig.RED)
                pygame.display.flip()
            except Exception:
                pass

    def _render_main_menu(
        self,
        is_running: bool,
        status: str,
        stats_summary: Dict[str, Any],
        envs: List[GameState],
        num_envs: int,
        env_config: EnvConfig,
        cleanup_message: str,
        last_cleanup_message_time: float,
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
    ):
        """Renders the main training dashboard view."""
        self.screen.fill(VisConfig.BLACK)

        self.left_panel.render(
            is_running,
            status,
            stats_summary,
            tensorboard_log_dir,
            plot_data,
            app_state="MainMenu",
        )
        self.game_area.render(envs, num_envs, env_config)

    def _render_initializing_screen(
        self, status_message: str = "Initializing RL Components..."
    ):
        """Renders the initializing screen with a status message."""
        self._render_simple_message(status_message, VisConfig.WHITE)

    def _render_error_screen(self, status_message: str):
        """Renders the error screen."""
        try:
            self.screen.fill((40, 0, 0))
            font_title = pygame.font.SysFont(None, 70)
            font_msg = pygame.font.SysFont(None, 30)

            title_surf = font_title.render("APPLICATION ERROR", True, VisConfig.RED)
            title_rect = title_surf.get_rect(
                center=(self.screen.get_width() // 2, self.screen.get_height() // 3)
            )

            msg_surf = font_msg.render(
                f"Status: {status_message}", True, VisConfig.YELLOW
            )
            msg_rect = msg_surf.get_rect(
                center=(self.screen.get_width() // 2, title_rect.bottom + 30)
            )

            exit_surf = font_msg.render(
                "Press ESC or close window to exit.", True, VisConfig.WHITE
            )
            exit_rect = exit_surf.get_rect(
                center=(self.screen.get_width() // 2, self.screen.get_height() * 0.8)
            )

            self.screen.blit(title_surf, title_rect)
            self.screen.blit(msg_surf, msg_rect)
            self.screen.blit(exit_surf, exit_rect)

        except Exception as e:
            print(f"Error rendering error screen: {e}")
            self._render_simple_message(f"Error State: {status_message}", VisConfig.RED)

    def _render_simple_message(self, message: str, color: Tuple[int, int, int]):
        """Renders a simple centered text message."""
        try:
            self.screen.fill(VisConfig.BLACK)
            font = pygame.font.SysFont(None, 50)
            text_surf = font.render(message, True, color)
            text_rect = text_surf.get_rect(center=self.screen.get_rect().center)
            self.screen.blit(text_surf, text_rect)
        except Exception as e:
            print(f"Error rendering simple message '{message}': {e}")


File: ui/__init__.py
from .renderer import UIRenderer
from .input_handler import InputHandler

__all__ = ["UIRenderer", "InputHandler"]


File: ui/plotter.py
# File: ui/plotter.py
import pygame
import numpy as np
from typing import Dict, Optional, Deque, List, Union, Tuple
from collections import deque
import matplotlib
import time
import warnings
from io import BytesIO
import traceback

matplotlib.use("Agg")
import matplotlib.pyplot as plt

from config import VisConfig, StatsConfig
from .plot_utils import (
    render_single_plot,
    normalize_color_for_matplotlib,
)


class Plotter:
    """Handles creation and caching of the multi-plot Matplotlib surface."""

    def __init__(self):
        self.plot_surface: Optional[pygame.Surface] = None
        self.last_plot_update_time: float = 0.0
        self.plot_update_interval: float = 0.5
        self.rolling_window_sizes = StatsConfig.STATS_AVG_WINDOW
        self.plot_data_window = StatsConfig.PLOT_DATA_WINDOW
        self.default_line_width = 1.0
        self.avg_line_width = 1.5
        self.avg_line_alpha = 0.8

        self.colors = {
            "rl_score": normalize_color_for_matplotlib(VisConfig.GOOGLE_COLORS[0]),
            "game_score": normalize_color_for_matplotlib(VisConfig.GOOGLE_COLORS[1]),
            "policy_loss": normalize_color_for_matplotlib(VisConfig.GOOGLE_COLORS[3]),
            "value_loss": normalize_color_for_matplotlib(VisConfig.BLUE),
            "entropy": normalize_color_for_matplotlib((150, 150, 150)),
            "len": normalize_color_for_matplotlib(VisConfig.BLUE),
            "sps": normalize_color_for_matplotlib(VisConfig.LIGHTG),
            "best_game": normalize_color_for_matplotlib((255, 165, 0)),
            "lr": normalize_color_for_matplotlib((255, 0, 255)),
            "avg_primary": normalize_color_for_matplotlib(VisConfig.YELLOW),
            "placeholder": normalize_color_for_matplotlib(VisConfig.GRAY),
        }
        self.avg_line_colors_secondary = [
            normalize_color_for_matplotlib((0, 255, 255)),
            normalize_color_for_matplotlib((255, 165, 0)),
            normalize_color_for_matplotlib((0, 255, 0)),
            normalize_color_for_matplotlib((255, 0, 255)),
        ]

    def create_plot_surface(
        self, plot_data: Dict[str, Deque], target_width: int, target_height: int
    ) -> Optional[pygame.Surface]:

        if target_width <= 10 or target_height <= 10 or not plot_data:
            return None

        data_keys = [
            "episode_scores",
            "game_scores",
            "policy_loss",
            "value_loss",
            "entropy",
            "episode_lengths",
            "sps_values",
            "best_game_score_history",
            "lr_values",
        ]
        data_lists = {key: list(plot_data.get(key, deque())) for key in data_keys}

        # --- DEBUG LOGGING ---
        # print(f"[Plotter Debug] Data lengths: "
        #       f"RLScore={len(data_lists['episode_scores'])}, GameScore={len(data_lists['game_scores'])}, EpLen={len(data_lists['episode_lengths'])}, "
        #       f"PLoss={len(data_lists['policy_loss'])}, VLoss={len(data_lists['value_loss'])}, Ent={len(data_lists['entropy'])}")
        # --- END DEBUG LOGGING ---

        if not any(len(d) > 0 for d in data_lists.values()):
            return None

        fig = None
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", category=UserWarning)
                dpi = 90
                fig_width_in = max(1, target_width / dpi)
                fig_height_in = max(1, target_height / dpi)

                fig, axes = plt.subplots(
                    3, 3, figsize=(fig_width_in, fig_height_in), dpi=dpi, sharex=False
                )
                fig.subplots_adjust(
                    hspace=0.55,
                    wspace=0.35,
                    left=0.10,
                    right=0.98,
                    bottom=0.12,
                    top=0.95,
                )
                axes_flat = axes.flatten()

                max_len = max((len(d) for d in data_lists.values() if d), default=0)
                plot_window_label = (
                    f"Latest {min(self.plot_data_window, max_len)} Updates"
                )

                render_single_plot(
                    axes_flat[0],
                    data_lists["episode_scores"],
                    "RL Score",
                    self.colors["rl_score"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    placeholder_text="RL Score",
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[1],
                    data_lists["game_scores"],
                    "Game Score",
                    self.colors["game_score"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    placeholder_text="Game Score",
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[2],
                    data_lists["policy_loss"],
                    "Policy Loss",
                    self.colors["policy_loss"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    placeholder_text="Policy Loss",
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[3],
                    data_lists["value_loss"],
                    "Value Loss",
                    self.colors["value_loss"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    placeholder_text="Value Loss",
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[4],
                    data_lists["entropy"],
                    "Entropy",
                    self.colors["entropy"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    placeholder_text="Entropy",
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[5],
                    data_lists["episode_lengths"],
                    "Ep Length",
                    self.colors["len"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    placeholder_text="Episode Length",
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[6],
                    data_lists["sps_values"],
                    "Steps/Sec",
                    self.colors["sps"],
                    self.colors["avg_primary"],
                    self.avg_line_colors_secondary,
                    self.rolling_window_sizes,
                    xlabel=plot_window_label,
                    placeholder_text="SPS",
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[7],
                    data_lists["best_game_score_history"],
                    "Best Game Score",
                    self.colors["best_game"],
                    None,
                    [],
                    [],
                    xlabel=plot_window_label,
                    placeholder_text="Best Game Score",
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )
                render_single_plot(
                    axes_flat[8],
                    data_lists["lr_values"],
                    "Learning Rate",
                    self.colors["lr"],
                    None,
                    [],
                    [],
                    xlabel=plot_window_label,
                    y_log_scale=True,
                    placeholder_text="Learning Rate",
                    default_line_width=self.default_line_width,
                    avg_line_width=self.avg_line_width,
                    avg_line_alpha=self.avg_line_alpha,
                )

                for ax in axes_flat:
                    ax.tick_params(axis="x", rotation=0)

                buf = BytesIO()
                fig.savefig(
                    buf,
                    format="png",
                    transparent=False,
                    facecolor=plt.rcParams["figure.facecolor"],
                )
                buf.seek(0)
                plot_img_surface = pygame.image.load(buf).convert()
                buf.close()

                current_size = plot_img_surface.get_size()
                if current_size != (target_width, target_height):
                    plot_img_surface = pygame.transform.smoothscale(
                        plot_img_surface, (target_width, target_height)
                    )

                return plot_img_surface

        except Exception as e:
            print(f"Error creating plot surface: {e}")
            traceback.print_exc()
            return None
        finally:
            if fig is not None:
                plt.close(fig)

    def get_cached_or_updated_plot(
        self, plot_data: Dict[str, Deque], target_width: int, target_height: int
    ) -> Optional[pygame.Surface]:
        """Returns the cached plot surface or generates a new one if needed."""
        current_time = time.time()
        has_data = any(d for d in plot_data.values())
        needs_update_time = (
            current_time - self.last_plot_update_time > self.plot_update_interval
        )
        size_changed = self.plot_surface and self.plot_surface.get_size() != (
            target_width,
            target_height,
        )
        first_plot_needed = has_data and self.plot_surface is None
        can_create_plot = target_width > 50 and target_height > 50

        if can_create_plot and (needs_update_time or size_changed or first_plot_needed):
            new_plot_surface = self.create_plot_surface(
                plot_data, target_width, target_height
            )
            if new_plot_surface:
                self.plot_surface = new_plot_surface
                self.last_plot_update_time = current_time

        return self.plot_surface


File: ui/overlays.py
# File: ui/overlays.py
# (No significant changes needed, this file was already focused)
import pygame
import time
import traceback
from typing import Tuple
from config import VisConfig


class OverlayRenderer:
    """Renders overlay elements like confirmation dialogs and status messages."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.fonts = self._init_fonts()

    def _init_fonts(self):
        """Initializes fonts used for overlays."""
        fonts = {}
        try:
            fonts["overlay_title"] = pygame.font.SysFont(None, 36)
            fonts["overlay_text"] = pygame.font.SysFont(None, 24)
        except Exception as e:
            print(f"Warning: SysFont error for overlay fonts: {e}. Using default.")
            fonts["overlay_title"] = pygame.font.Font(None, 36)
            fonts["overlay_text"] = pygame.font.Font(None, 24)
        return fonts

    def render_cleanup_confirmation(self):
        """Renders the confirmation dialog for cleanup. Does not flip display."""
        try:
            current_width, current_height = self.screen.get_size()

            # Semi-transparent background overlay
            overlay_surface = pygame.Surface(
                (current_width, current_height), pygame.SRCALPHA
            )
            overlay_surface.fill((0, 0, 0, 200))  # Black with alpha
            self.screen.blit(overlay_surface, (0, 0))

            center_x, center_y = current_width // 2, current_height // 2

            # --- Render Text Lines ---
            if "overlay_title" not in self.fonts or "overlay_text" not in self.fonts:
                print("ERROR: Overlay fonts not loaded!")
                # Draw basic fallback text
                fallback_font = pygame.font.Font(None, 30)
                err_surf = fallback_font.render("CONFIRM CLEANUP?", True, VisConfig.RED)
                self.screen.blit(
                    err_surf, err_surf.get_rect(center=(center_x, center_y - 30))
                )
                yes_surf = fallback_font.render("YES", True, VisConfig.WHITE)
                no_surf = fallback_font.render("NO", True, VisConfig.WHITE)
                self.screen.blit(
                    yes_surf, yes_surf.get_rect(center=(center_x - 60, center_y + 50))
                )
                self.screen.blit(
                    no_surf, no_surf.get_rect(center=(center_x + 60, center_y + 50))
                )
                return  # Stop here if fonts failed

            # Use loaded fonts
            prompt_l1 = self.fonts["overlay_title"].render(
                "DELETE CURRENT RUN DATA?", True, VisConfig.RED
            )
            prompt_l2 = self.fonts["overlay_text"].render(
                "(Agent Checkpoint & Buffer State)", True, VisConfig.WHITE
            )
            prompt_l3 = self.fonts["overlay_text"].render(
                "This action cannot be undone!", True, VisConfig.YELLOW
            )

            # Position and blit text
            self.screen.blit(
                prompt_l1, prompt_l1.get_rect(center=(center_x, center_y - 60))
            )
            self.screen.blit(
                prompt_l2, prompt_l2.get_rect(center=(center_x, center_y - 25))
            )
            self.screen.blit(prompt_l3, prompt_l3.get_rect(center=(center_x, center_y)))

            # --- Render Buttons ---
            # Recalculate rects based on current screen size for responsiveness
            confirm_yes_rect = pygame.Rect(center_x - 110, center_y + 30, 100, 40)
            confirm_no_rect = pygame.Rect(center_x + 10, center_y + 30, 100, 40)

            pygame.draw.rect(
                self.screen, (0, 150, 0), confirm_yes_rect, border_radius=5
            )  # Green YES
            pygame.draw.rect(
                self.screen, (150, 0, 0), confirm_no_rect, border_radius=5
            )  # Red NO

            yes_text = self.fonts["overlay_text"].render("YES", True, VisConfig.WHITE)
            no_text = self.fonts["overlay_text"].render("NO", True, VisConfig.WHITE)

            self.screen.blit(
                yes_text, yes_text.get_rect(center=confirm_yes_rect.center)
            )
            self.screen.blit(no_text, no_text.get_rect(center=confirm_no_rect.center))

        except pygame.error as pg_err:
            print(f"Pygame Error in render_cleanup_confirmation: {pg_err}")
            traceback.print_exc()
        except Exception as e:
            print(f"Error in render_cleanup_confirmation: {e}")
            traceback.print_exc()

    def render_status_message(self, message: str, last_message_time: float) -> bool:
        """
        Renders a status message (e.g., after cleanup) temporarily at the bottom center.
        Does not flip display. Returns True if a message was rendered.
        """
        # Check if message exists and hasn't timed out
        if not message or (time.time() - last_message_time >= 5.0):
            return False

        try:
            if "overlay_text" not in self.fonts:  # Check if font loaded
                print(
                    "Warning: Cannot render status message, overlay_text font missing."
                )
                return False

            current_width, current_height = self.screen.get_size()
            lines = message.split("\n")
            max_width = 0
            msg_surfs = []

            # Render each line and find max width
            for line in lines:
                msg_surf = self.fonts["overlay_text"].render(
                    line,
                    True,
                    VisConfig.YELLOW,
                    VisConfig.BLACK,  # Yellow text on black bg
                )
                msg_surfs.append(msg_surf)
                max_width = max(max_width, msg_surf.get_width())

            if not msg_surfs:
                return False  # No lines to render

            # Calculate background size and position
            total_height = (
                sum(s.get_height() for s in msg_surfs) + max(0, len(lines) - 1) * 2
            )
            padding = 5
            bg_rect = pygame.Rect(
                0, 0, max_width + padding * 2, total_height + padding * 2
            )
            bg_rect.midbottom = (
                current_width // 2,
                current_height - 10,
            )  # Position at bottom center

            # Draw background and border
            pygame.draw.rect(self.screen, VisConfig.BLACK, bg_rect, border_radius=3)
            pygame.draw.rect(self.screen, VisConfig.YELLOW, bg_rect, 1, border_radius=3)

            # Draw text lines centered within the background
            current_y = bg_rect.top + padding
            for msg_surf in msg_surfs:
                msg_rect = msg_surf.get_rect(midtop=(bg_rect.centerx, current_y))
                self.screen.blit(msg_surf, msg_rect)
                current_y += msg_surf.get_height() + 2  # Move Y for next line

            return True  # Message was rendered
        except Exception as e:
            print(f"Error rendering status message: {e}")
            traceback.print_exc()
            return False  # Message render failed


File: ui/plot_utils.py
# File: ui/plot_utils.py
import pygame
import numpy as np
from typing import Dict, Optional, Deque, List, Union, Tuple
import matplotlib
import warnings
from io import BytesIO
import traceback

matplotlib.use("Agg")
import matplotlib.pyplot as plt

from config import VisConfig, StatsConfig

try:
    plt.style.use("dark_background")
    plt.rcParams.update(
        {
            "font.size": 8,
            "axes.labelsize": 8,
            "axes.titlesize": 9,
            "xtick.labelsize": 7,
            "ytick.labelsize": 7,
            "legend.fontsize": 6,
            "figure.facecolor": "#262626",
            "axes.facecolor": "#303030",
            "axes.edgecolor": "#707070",
            "axes.labelcolor": "#D0D0D0",
            "xtick.color": "#C0C0C0",
            "ytick.color": "#C0C0C0",
            "grid.color": "#505050",
            "grid.linestyle": "--",
            "grid.alpha": 0.5,
        }
    )
except Exception as e:
    print(f"Warning: Failed to set Matplotlib style: {e}")


def normalize_color_for_matplotlib(
    color_tuple_0_255: Tuple[int, int, int],
) -> Tuple[float, float, float]:
    """Converts RGB tuple (0-255) to Matplotlib format (0.0-1.0)."""
    if isinstance(color_tuple_0_255, tuple) and len(color_tuple_0_255) == 3:
        return tuple(c / 255.0 for c in color_tuple_0_255)
    else:
        return (0.0, 0.0, 0.0)


TREND_INCREASING_COLOR = normalize_color_for_matplotlib((0, 180, 0))
TREND_DECREASING_COLOR = normalize_color_for_matplotlib((180, 0, 0))
TREND_STABLE_COLOR = normalize_color_for_matplotlib((70, 70, 70))
TREND_LINEWIDTH = 1.5
DEFAULT_LINEWIDTH = 0.8
TREND_TOLERANCE = 1e-6


def render_single_plot(
    ax,
    data: List[Union[float, int]],
    label: str,
    color: Tuple[float, float, float],
    avg_color_primary: Optional[Tuple[float, float, float]],
    avg_colors_secondary: List[Tuple[float, float, float]],
    rolling_window_sizes: List[int],
    xlabel: Optional[str] = None,
    show_placeholder: bool = True,
    placeholder_text: Optional[str] = None,
    y_log_scale: bool = False,
    default_line_width: float = 1.0,
    avg_line_width: float = 1.5,
    avg_line_alpha: float = 0.7,
):
    """Renders data onto a single Matplotlib Axes object."""
    # --- MODIFIED: Filter data more robustly ---
    valid_data = [
        d for d in data if isinstance(d, (int, float, np.number)) and np.isfinite(d)
    ]
    n_points = len(valid_data)
    # --- END MODIFIED ---

    # --- DEBUG LOGGING ---
    # if label in ["RL Score", "Game Score", "Ep Length"]:
    #     print(f"[render_single_plot Debug] Plot '{label}': Input data length={len(data)}, Valid points={n_points}")
    # --- END DEBUG LOGGING ---

    latest_val_str = ""
    primary_avg_window = rolling_window_sizes[0] if rolling_window_sizes else 0

    for spine in ax.spines.values():
        spine.set_color(TREND_STABLE_COLOR)
        spine.set_linewidth(DEFAULT_LINEWIDTH)

    placeholder_text_color = normalize_color_for_matplotlib(VisConfig.GRAY)

    # --- MODIFIED: Explicit placeholder handling if NO valid points ---
    if n_points == 0:
        if show_placeholder:
            p_text = placeholder_text if placeholder_text else f"{label}\n(No data)"
            ax.text(
                0.5,
                0.5,
                p_text,
                ha="center",
                va="center",
                transform=ax.transAxes,
                fontsize=8,
                color=placeholder_text_color,
            )
        ax.set_yticks([])
        ax.set_xticks([])
        ax.set_title(f"{label} (N/A)", fontsize=plt.rcParams["axes.titlesize"])
        ax.grid(False)
        return
    # --- END MODIFIED ---

    data_to_plot = np.array(valid_data)
    x_coords = np.arange(n_points)

    current_val = data_to_plot[-1]
    if (
        n_points >= primary_avg_window
        and avg_color_primary is not None
        and primary_avg_window > 0
    ):
        try:
            latest_avg = np.mean(data_to_plot[-primary_avg_window:])
            latest_val_str = (
                f" (Now: {current_val:.3g}, Avg{primary_avg_window}: {latest_avg:.3g})"
            )
        except Exception:
            latest_val_str = f" (Now: {current_val:.3g})"
    else:
        latest_val_str = f" (Now: {current_val:.3g})"
    ax.set_title(f"{label}{latest_val_str}", fontsize=plt.rcParams["axes.titlesize"])

    try:
        ax.plot(
            x_coords,
            data_to_plot,
            color=color,
            linewidth=default_line_width,
            label=f"{label}",
        )

        avg_linestyles = ["-", "--", ":", "-."]
        plotted_averages = False
        longest_avg_rolling = None
        longest_window = 0

        for i, avg_window in enumerate(rolling_window_sizes):
            if n_points >= avg_window:
                current_avg_color = color
                if i == 0 and avg_color_primary:
                    current_avg_color = avg_color_primary
                elif i > 0 and avg_colors_secondary:
                    current_avg_color = avg_colors_secondary[
                        (i - 1) % len(avg_colors_secondary)
                    ]

                weights = np.ones(avg_window) / avg_window
                rolling_avg = np.convolve(data_to_plot, weights, mode="valid")
                avg_x_coords = np.arange(avg_window - 1, n_points)
                linestyle = avg_linestyles[i % len(avg_linestyles)]

                if len(avg_x_coords) == len(rolling_avg):
                    ax.plot(
                        avg_x_coords,
                        rolling_avg,
                        color=current_avg_color,
                        linewidth=avg_line_width,
                        alpha=avg_line_alpha,
                        linestyle=linestyle,
                        label=f"Avg {avg_window}",
                    )
                    plotted_averages = True
                    if avg_window >= longest_window:
                        longest_window = avg_window
                        longest_avg_rolling = rolling_avg
                else:
                    print(
                        f"Warning: Mismatch in rolling avg shapes for {label} (window {avg_window})"
                    )

        if longest_avg_rolling is not None and len(longest_avg_rolling) >= 2:
            latest_avg_long = longest_avg_rolling[-1]
            prev_avg_long = longest_avg_rolling[-2]
            trend_color = TREND_STABLE_COLOR
            trend_lw = DEFAULT_LINEWIDTH
            if latest_avg_long > prev_avg_long + TREND_TOLERANCE:
                trend_color = TREND_INCREASING_COLOR
                trend_lw = TREND_LINEWIDTH
            elif latest_avg_long < prev_avg_long - TREND_TOLERANCE:
                trend_color = TREND_DECREASING_COLOR
                trend_lw = TREND_LINEWIDTH
            for spine in ax.spines.values():
                spine.set_color(trend_color)
                spine.set_linewidth(trend_lw)

        ax.tick_params(axis="both", which="major")
        if xlabel:
            ax.set_xlabel(xlabel)
        ax.grid(
            True,
            linestyle=plt.rcParams["grid.linestyle"],
            alpha=plt.rcParams["grid.alpha"],
        )

        min_val = np.min(data_to_plot)
        max_val = np.max(data_to_plot)
        padding_factor = 0.1
        range_val = max_val - min_val
        if abs(range_val) < 1e-6:
            padding = max(abs(max_val * padding_factor), 0.5) if max_val != 0 else 0.5
        else:
            padding = range_val * padding_factor
        padding = max(padding, 1e-6)
        ax.set_ylim(min_val - padding, max_val + padding)

        if y_log_scale and min_val > 1e-9:
            ax.set_yscale("log")
            ax.set_ylim(bottom=max(min_val * 0.9, 1e-9))
        else:
            ax.set_yscale("linear")

        if n_points > 1:
            ax.set_xlim(-0.02 * n_points, n_points - 1 + 0.02 * n_points)
        elif n_points == 1:
            ax.set_xlim(-0.5, 0.5)

        if n_points > 1000:
            ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True, nbins=4))

            def format_func(value, tick_number):
                val_int = int(value)
                if val_int >= 1_000_000:
                    return f"{val_int/1_000_000:.1f}M"
                if val_int >= 1_000:
                    return f"{val_int/1_000:.0f}k"
                return f"{val_int}"

            ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))
        elif n_points > 10:
            ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True, nbins=5))

        if plotted_averages:
            ax.legend(loc="best", fontsize=plt.rcParams["legend.fontsize"])

    except Exception as plot_err:
        print(f"ERROR during render_single_plot for '{label}': {plot_err}")
        error_text_color = normalize_color_for_matplotlib(VisConfig.RED)
        ax.text(
            0.5,
            0.5,
            f"Plot Error\n({label})",
            ha="center",
            va="center",
            transform=ax.transAxes,
            fontsize=8,
            color=error_text_color,
        )
        ax.set_yticks([])
        ax.set_xticks([])
        ax.grid(False)


File: ui/input_handler.py
# File: ui/input_handler.py
import pygame
from typing import Tuple, Callable, Optional

HandleDemoInputCallback = Callable[[pygame.event.Event], None]
ToggleRunningCallback = Callable[[], None]  # Renamed
RequestCleanupCallback = Callable[[], None]
CancelCleanupCallback = Callable[[], None]
ConfirmCleanupCallback = Callable[[], None]
ExitAppCallback = Callable[[], bool]
StartDemoModeCallback = Callable[[], None]
ExitDemoModeCallback = Callable[[], None]

if False:
    from .renderer import UIRenderer


class InputHandler:
    """Handles Pygame events and triggers callbacks based on application state."""

    def __init__(
        self,
        screen: pygame.Surface,
        renderer: "UIRenderer",
        toggle_running_cb: ToggleRunningCallback,  # Renamed
        request_cleanup_cb: RequestCleanupCallback,
        cancel_cleanup_cb: CancelCleanupCallback,
        confirm_cleanup_cb: ConfirmCleanupCallback,
        exit_app_cb: ExitAppCallback,
        start_demo_mode_cb: StartDemoModeCallback,
        exit_demo_mode_cb: ExitDemoModeCallback,
        handle_demo_input_cb: HandleDemoInputCallback,
    ):
        self.screen = screen
        self.renderer = renderer
        self.toggle_running_cb = toggle_running_cb  # Renamed
        self.request_cleanup_cb = request_cleanup_cb
        self.cancel_cleanup_cb = cancel_cleanup_cb
        self.confirm_cleanup_cb = confirm_cleanup_cb
        self.exit_app_cb = exit_app_cb
        self.start_demo_mode_cb = start_demo_mode_cb
        self.exit_demo_mode_cb = exit_demo_mode_cb
        self.handle_demo_input_cb = handle_demo_input_cb

        self._update_button_rects()

    def _update_button_rects(self):
        """Calculates button rects based on initial layout assumptions."""
        self.run_btn_rect = pygame.Rect(10, 10, 100, 40)  # Renamed from train_btn_rect
        self.cleanup_btn_rect = pygame.Rect(self.run_btn_rect.right + 10, 10, 160, 40)
        self.demo_btn_rect = pygame.Rect(self.cleanup_btn_rect.right + 10, 10, 120, 40)
        sw, sh = self.screen.get_size()
        self.confirm_yes_rect = pygame.Rect(sw // 2 - 110, sh // 2 + 30, 100, 40)
        self.confirm_no_rect = pygame.Rect(sw // 2 + 10, sh // 2 + 30, 100, 40)

    def handle_input(self, app_state: str, cleanup_confirmation_active: bool) -> bool:
        """
        Processes Pygame events. Returns True to continue running, False to exit.
        """
        try:
            mouse_pos = pygame.mouse.get_pos()
        except pygame.error:
            mouse_pos = (0, 0)

        sw, sh = self.screen.get_size()
        self.confirm_yes_rect.center = (sw // 2 - 60, sh // 2 + 50)
        self.confirm_no_rect.center = (sw // 2 + 60, sh // 2 + 50)

        if app_state == "MainMenu" and not cleanup_confirmation_active:
            if hasattr(self.renderer, "check_hover"):
                self.renderer.check_hover(mouse_pos, app_state)

        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return self.exit_app_cb()

            if event.type == pygame.VIDEORESIZE:
                try:
                    new_w, new_h = max(320, event.w), max(240, event.h)
                    self.screen = pygame.display.set_mode(
                        (new_w, new_h), pygame.RESIZABLE
                    )
                    self._update_ui_screen_references(self.screen)
                    self._update_button_rects()
                    if hasattr(self.renderer, "force_redraw"):
                        self.renderer.force_redraw()
                    print(f"Window resized: {new_w}x{new_h}")
                except pygame.error as e:
                    print(f"Error resizing window: {e}")
                continue

            if cleanup_confirmation_active:
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                    self.cancel_cleanup_cb()
                elif event.type == pygame.MOUSEBUTTONDOWN and event.button == 1:
                    if self.confirm_yes_rect.collidepoint(mouse_pos):
                        self.confirm_cleanup_cb()
                    elif self.confirm_no_rect.collidepoint(mouse_pos):
                        self.cancel_cleanup_cb()
                continue

            elif app_state == "Playing":
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        self.exit_demo_mode_cb()
                    else:
                        self.handle_demo_input_cb(event)

            elif app_state == "MainMenu":
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        return self.exit_app_cb()
                    elif event.key == pygame.K_p:
                        self.toggle_running_cb()  # Toggle Run

                if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1:
                    if self.run_btn_rect.collidepoint(mouse_pos):  # Use run_btn_rect
                        self.toggle_running_cb()
                    elif self.cleanup_btn_rect.collidepoint(mouse_pos):
                        self.request_cleanup_cb()
                    elif self.demo_btn_rect.collidepoint(mouse_pos):
                        self.start_demo_mode_cb()

            elif app_state == "Error":
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                    return self.exit_app_cb()

        return True

    def _update_ui_screen_references(self, new_screen: pygame.Surface):
        """Updates the screen reference in the renderer and its sub-components."""
        components_to_update = [
            self.renderer,
            getattr(self.renderer, "left_panel", None),
            getattr(self.renderer, "game_area", None),
            getattr(self.renderer, "overlays", None),
            getattr(self.renderer, "tooltips", None),
            getattr(self.renderer, "demo_renderer", None),
        ]
        for component in components_to_update:
            if component and hasattr(component, "screen"):
                component.screen = new_screen


File: ui/panels/__init__.py
from .left_panel import LeftPanelRenderer
from .game_area import GameAreaRenderer

__all__ = ["LeftPanelRenderer", "GameAreaRenderer"]


File: ui/panels/game_area.py
# File: ui/panels/game_area.py
import pygame
import math
import traceback
from typing import List, Tuple
from config import VisConfig, EnvConfig
from environment.game_state import GameState
from environment.shape import Shape
from environment.triangle import Triangle
import numpy as np


class GameAreaRenderer:
    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.fonts = self._init_fonts()

    def _init_fonts(self):
        fonts = {}
        try:
            fonts["env_score"] = pygame.font.SysFont(None, 18)
            fonts["env_overlay"] = pygame.font.SysFont(None, 36)
            fonts["ui"] = pygame.font.SysFont(None, 24)
        except Exception as e:
            print(f"Warning: SysFont error: {e}. Using default.")
            fonts["env_score"] = pygame.font.Font(None, 18)
            fonts["env_overlay"] = pygame.font.Font(None, 36)
            fonts["ui"] = pygame.font.Font(None, 24)
        return fonts

    def render(self, envs: List[GameState], num_envs: int, env_config: EnvConfig):
        current_width, current_height = self.screen.get_size()
        lp_width = min(current_width, max(300, self.vis_config.LEFT_PANEL_WIDTH))
        ga_rect = pygame.Rect(lp_width, 0, current_width - lp_width, current_height)

        if num_envs <= 0 or ga_rect.width <= 0 or ga_rect.height <= 0:
            return

        render_limit = self.vis_config.NUM_ENVS_TO_RENDER
        num_to_render = num_envs if render_limit <= 0 else min(num_envs, render_limit)
        if num_to_render <= 0:
            return

        cols_env, rows_env, cell_w, cell_h = self._calculate_grid_layout(
            ga_rect, num_to_render
        )

        min_cell_dim = 30
        if cell_w > min_cell_dim and cell_h > min_cell_dim:
            self._render_env_grid(
                envs,
                num_to_render,
                env_config,
                ga_rect,
                cols_env,
                rows_env,
                cell_w,
                cell_h,
            )
        else:
            self._render_too_small_message(ga_rect, cell_w, cell_h)

        if num_to_render < num_envs:
            self._render_render_limit_text(ga_rect, num_to_render, num_envs)

    def _calculate_grid_layout(
        self, ga_rect: pygame.Rect, num_to_render: int
    ) -> Tuple[int, int, int, int]:
        aspect_ratio = ga_rect.width / max(1, ga_rect.height)
        cols_env = max(1, int(math.sqrt(num_to_render * aspect_ratio)))
        rows_env = max(1, math.ceil(num_to_render / cols_env))
        total_spacing_w = (cols_env + 1) * self.vis_config.ENV_SPACING
        total_spacing_h = (rows_env + 1) * self.vis_config.ENV_SPACING
        cell_w = max(1, (ga_rect.width - total_spacing_w) // cols_env)
        cell_h = max(1, (ga_rect.height - total_spacing_h) // rows_env)
        return cols_env, rows_env, cell_w, cell_h

    def _render_env_grid(
        self, envs, num_to_render, env_config, ga_rect, cols, rows, cell_w, cell_h
    ):
        env_idx = 0
        for r in range(rows):
            for c in range(cols):
                if env_idx >= num_to_render:
                    break
                env_x = ga_rect.x + self.vis_config.ENV_SPACING * (c + 1) + c * cell_w
                env_y = ga_rect.y + self.vis_config.ENV_SPACING * (r + 1) + r * cell_h
                env_rect = pygame.Rect(env_x, env_y, cell_w, cell_h)

                clipped_env_rect = env_rect.clip(self.screen.get_rect())
                if clipped_env_rect.width <= 0 or clipped_env_rect.height <= 0:
                    env_idx += 1
                    continue

                try:
                    sub_surf = self.screen.subsurface(clipped_env_rect)
                    self._render_single_env(sub_surf, envs[env_idx], env_config)

                except ValueError as subsurface_error:
                    print(
                        f"Warning: Subsurface error env {env_idx} ({clipped_env_rect}): {subsurface_error}"
                    )
                    pygame.draw.rect(self.screen, (0, 0, 50), clipped_env_rect, 1)
                except Exception as e_render_env:
                    print(f"Error rendering env {env_idx}: {e_render_env}")
                    traceback.print_exc()
                    pygame.draw.rect(self.screen, (50, 0, 50), clipped_env_rect, 1)

                env_idx += 1

    def _render_single_env(
        self, surf: pygame.Surface, env: GameState, env_config: EnvConfig
    ):
        cell_w = surf.get_width()
        cell_h = surf.get_height()
        if cell_w <= 0 or cell_h <= 0:
            return

        # --- Background Color Logic (Order Matters!) ---
        bg_color = VisConfig.GRAY  # Default
        if env.is_line_clearing():  # Line clear flash has highest priority
            bg_color = VisConfig.LINE_CLEAR_FLASH_COLOR
        elif env.is_game_over_flashing():  # Game over flash is next
            bg_color = VisConfig.GAME_OVER_FLASH_COLOR
        elif env.is_blinking():  # Generic blink (currently unused but kept)
            bg_color = VisConfig.YELLOW
        elif env.is_over():  # Static game over (no flash)
            bg_color = VisConfig.DARK_RED
        elif env.is_frozen():  # Frozen Blue (only if not game over/line clearing)
            bg_color = (30, 30, 100)
        surf.fill(bg_color)

        shape_area_height_ratio = 0.20
        grid_area_height = math.floor(cell_h * (1.0 - shape_area_height_ratio))
        shape_area_height = cell_h - grid_area_height
        shape_area_y = grid_area_height

        grid_surf = None
        shape_surf = None
        if grid_area_height > 0 and cell_w > 0:
            try:
                grid_rect = pygame.Rect(0, 0, cell_w, grid_area_height)
                grid_surf = surf.subsurface(grid_rect)
            except ValueError as e:
                print(f"Warning: Grid subsurface error ({grid_rect}): {e}")
                pygame.draw.rect(surf, VisConfig.RED, grid_rect, 1)

        if shape_area_height > 0 and cell_w > 0:
            try:
                shape_rect = pygame.Rect(0, shape_area_y, cell_w, shape_area_height)
                shape_surf = surf.subsurface(shape_rect)
                shape_surf.fill((35, 35, 35))
            except ValueError as e:
                print(f"Warning: Shape subsurface error ({shape_rect}): {e}")
                pygame.draw.rect(surf, VisConfig.RED, shape_rect, 1)

        if grid_surf:
            self._render_single_env_grid(grid_surf, env, env_config)

        if shape_surf:
            self._render_shape_previews(shape_surf, env)

        try:
            score_surf = self.fonts["env_score"].render(
                f"GS: {env.game_score} R: {env.score:.1f}",
                True,
                VisConfig.WHITE,
                (0, 0, 0, 180),
            )
            surf.blit(score_surf, (2, 2))
        except Exception as e:
            print(f"Error rendering score: {e}")

        # --- MODIFIED: Specific Overlay Logic ---
        if env.is_over():
            # Always show GAME OVER text if the game is over
            self._render_overlay_text(surf, "GAME OVER", VisConfig.RED)
        elif env.is_line_clearing():
            # Show Line Clear! text only during the line clear flash
            self._render_overlay_text(surf, "Line Clear!", VisConfig.BLUE)
        # No generic "Frozen" message anymore
        # --- END MODIFIED ---

    def _render_overlay_text(
        self, surf: pygame.Surface, text: str, color: Tuple[int, int, int]
    ):
        try:
            overlay_font = self.fonts["env_overlay"]
            text_surf = overlay_font.render(
                text,
                True,
                VisConfig.WHITE,
                (color[0] // 2, color[1] // 2, color[2] // 2, 220),
            )
            text_rect = text_surf.get_rect(center=surf.get_rect().center)
            surf.blit(text_surf, text_rect)
        except Exception as e:
            print(f"Error rendering overlay text '{text}': {e}")

    def _render_single_env_grid(
        self, surf: pygame.Surface, env: GameState, env_config: EnvConfig
    ):
        try:
            padding = self.vis_config.ENV_GRID_PADDING
            drawable_w = max(1, surf.get_width() - 2 * padding)
            drawable_h = max(1, surf.get_height() - 2 * padding)

            grid_rows = env_config.ROWS
            grid_cols_effective_width = env_config.COLS * 0.75 + 0.25

            if grid_rows <= 0 or grid_cols_effective_width <= 0:
                return

            scale_w_based = drawable_w / grid_cols_effective_width
            scale_h_based = drawable_h / grid_rows
            final_scale = min(scale_w_based, scale_h_based)
            if final_scale <= 0:
                return

            final_grid_pixel_w = grid_cols_effective_width * final_scale
            final_grid_pixel_h = grid_rows * final_scale
            tri_cell_h = max(1, final_scale)
            tri_cell_w = max(1, final_scale)

            grid_ox = padding + (drawable_w - final_grid_pixel_w) / 2
            grid_oy = padding + (drawable_h - final_grid_pixel_h) / 2

            is_highlighting = env.is_highlighting_cleared()
            cleared_coords = (
                set(env.get_cleared_triangle_coords()) if is_highlighting else set()
            )
            highlight_color = self.vis_config.LINE_CLEAR_HIGHLIGHT_COLOR

            if hasattr(env, "grid") and hasattr(env.grid, "triangles"):
                for r in range(env.grid.rows):
                    for c in range(env.grid.cols):
                        if not (
                            0 <= r < len(env.grid.triangles)
                            and 0 <= c < len(env.grid.triangles[r])
                        ):
                            continue
                        t = env.grid.triangles[r][c]
                        if not t.is_death:
                            if not hasattr(t, "get_points"):
                                continue
                            try:
                                pts = t.get_points(
                                    ox=grid_ox,
                                    oy=grid_oy,
                                    cw=int(tri_cell_w),
                                    ch=int(tri_cell_h),
                                )
                                if is_highlighting and (r, c) in cleared_coords:
                                    color = highlight_color
                                elif t.is_occupied:
                                    color = t.color if t.color else VisConfig.RED
                                else:
                                    color = VisConfig.LIGHTG

                                pygame.draw.polygon(surf, color, pts)
                                pygame.draw.polygon(surf, VisConfig.GRAY, pts, 1)
                            except Exception as e_render:
                                pass
            else:
                pygame.draw.rect(surf, VisConfig.RED, surf.get_rect(), 2)
                err_txt = self.fonts["ui"].render(
                    "Invalid Grid Data", True, VisConfig.RED
                )
                surf.blit(err_txt, err_txt.get_rect(center=surf.get_rect().center))

        except Exception as e:
            print(f"Unexpected Render Error in _render_single_env_grid: {e}")
            traceback.print_exc()
            pygame.draw.rect(surf, VisConfig.RED, surf.get_rect(), 2)

    def _render_shape_previews(self, surf: pygame.Surface, env: GameState):
        available_shapes = env.get_shapes()
        if not available_shapes:
            return

        surf_w = surf.get_width()
        surf_h = surf.get_height()
        if surf_w <= 0 or surf_h <= 0:
            return

        num_shapes = len(available_shapes)
        padding = 4
        total_padding_needed = (num_shapes + 1) * padding
        available_width_for_shapes = surf_w - total_padding_needed

        if available_width_for_shapes <= 0:
            return

        width_per_shape = available_width_for_shapes / num_shapes
        height_limit = surf_h - 2 * padding
        preview_dim = max(5, min(width_per_shape, height_limit))

        start_x = (
            padding
            + (surf_w - (num_shapes * preview_dim + (num_shapes - 1) * padding)) / 2
        )
        start_y = padding + (surf_h - preview_dim) / 2

        current_x = start_x
        for shape in available_shapes:
            preview_rect = pygame.Rect(current_x, start_y, preview_dim, preview_dim)
            if preview_rect.right > surf_w - padding:
                break

            try:
                temp_shape_surf = pygame.Surface(
                    (preview_dim, preview_dim), pygame.SRCALPHA
                )
                temp_shape_surf.fill((0, 0, 0, 0))

                min_r, min_c, max_r, max_c = shape.bbox()
                shape_h_cells = max(1, max_r - min_r + 1)
                shape_w_cells_eff = max(1, (max_c - min_c + 1) * 0.75 + 0.25)

                scale_h = preview_dim / shape_h_cells
                scale_w = preview_dim / shape_w_cells_eff
                cell_size = max(1, min(scale_h, scale_w))

                self._render_single_shape(temp_shape_surf, shape, int(cell_size))

                surf.blit(temp_shape_surf, preview_rect.topleft)
                current_x += preview_dim + padding

            except Exception as e:
                print(f"Error rendering shape preview: {e}")
                pygame.draw.rect(surf, VisConfig.RED, preview_rect, 1)
                current_x += preview_dim + padding

    def _render_single_shape(self, surf: pygame.Surface, shape: Shape, cell_size: int):
        if not shape or not shape.triangles or cell_size <= 0:
            return
        min_r, min_c, max_r, max_c = shape.bbox()
        shape_h_cells = max_r - min_r + 1
        shape_w_cells_eff = (max_c - min_c + 1) * 0.75 + 0.25
        if shape_w_cells_eff <= 0 or shape_h_cells <= 0:
            return

        total_w_pixels = shape_w_cells_eff * cell_size
        total_h_pixels = shape_h_cells * cell_size

        offset_x = (surf.get_width() - total_w_pixels) / 2 - min_c * (cell_size * 0.75)
        offset_y = (surf.get_height() - total_h_pixels) / 2 - min_r * cell_size

        for dr, dc, up in shape.triangles:
            tri = Triangle(row=dr, col=dc, is_up=up)
            try:
                pts = tri.get_points(
                    ox=offset_x, oy=offset_y, cw=cell_size, ch=cell_size
                )
                pygame.draw.polygon(surf, shape.color, pts)
            except Exception as e:
                print(f"Warning: Error rendering shape preview tri ({dr},{dc}): {e}")

    def _render_too_small_message(self, ga_rect: pygame.Rect, cell_w: int, cell_h: int):
        try:
            err_surf = self.fonts["ui"].render(
                f"Envs Too Small ({cell_w}x{cell_h})", True, VisConfig.GRAY
            )
            self.screen.blit(err_surf, err_surf.get_rect(center=ga_rect.center))
        except Exception as e:
            print(f"Error rendering 'too small' message: {e}")

    def _render_render_limit_text(
        self, ga_rect: pygame.Rect, num_rendered: int, num_total: int
    ):
        try:
            info_surf = self.fonts["ui"].render(
                f"Rendering {num_rendered}/{num_total} Envs",
                True,
                VisConfig.YELLOW,
                VisConfig.BLACK,
            )
            self.screen.blit(
                info_surf,
                info_surf.get_rect(bottomright=(ga_rect.right - 5, ga_rect.bottom - 5)),
            )
        except Exception as e:
            print(f"Error rendering limit text: {e}")


File: ui/panels/left_panel.py
# File: ui/panels/left_panel.py
import pygame
import os
import time
from typing import Dict, Any, Optional, Deque, Tuple

from config import (
    VisConfig,
    StatsConfig,
    PPOConfig,
    RNNConfig,  # Added
    DEVICE,
    TensorBoardConfig,
)
from config.general import TOTAL_TRAINING_STEPS
from ui.plotter import Plotter

from .left_panel_components import (
    ButtonStatusRenderer,
    NotificationRenderer,
    InfoTextRenderer,
    TBStatusRenderer,
    PlotAreaRenderer,
)

TOOLTIP_TEXTS = {
    "Status": "Current state: Ready, Training, Confirm Cleanup, Cleaning, or Error.",
    "Global Steps": "Total environment steps taken / Total planned steps.",
    "Total Episodes": "Total completed episodes across all environments.",
    "Steps/Sec (Current)": "Current avg Steps/Sec (Collection + Update). See plot for history.",
    "Learning Rate": "Current learning rate. See plot for history/schedule.",
    "Run Button": "Click to Start/Stop training run (or press 'P').",  # Renamed
    "Cleanup Button": "Click to DELETE agent ckpt for CURRENT run ONLY, then re-init.",
    "Play Demo Button": "Click to enter interactive play mode.",
    "Device": f"Computation device detected ({DEVICE.type.upper()}).",
    "Network": f"Actor-Critic (CNN+MLP Fusion -> Optional LSTM:{RNNConfig.USE_RNN})",  # Updated
    "TensorBoard Status": "Indicates TB logging status and log directory.",
    "Notification Area": "Displays the latest best achievements (RL Score, Game Score, Value Loss).",
    "Best RL Score Info": "Best RL Score achieved: Current Value (Previous Value) - Steps Ago",
    "Best Game Score Info": "Best Game Score achieved: Current Value (Previous Value) - Steps Ago",
    "Best Loss Info": "Best (Lowest) Value Loss achieved: Current Value (Previous Value) - Steps Ago",
    "Policy Loss": "Average loss for the policy network during the last update.",
    "Value Loss": "Average loss for the value network during the last update.",
    "Entropy": "Average policy entropy during the last update (encourages exploration).",
}


class LeftPanelRenderer:
    """Orchestrates rendering of the left panel using sub-components."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig, plotter: Plotter):
        self.screen = screen
        self.vis_config = vis_config
        self.plotter = plotter
        self.fonts = self._init_fonts()
        self.stat_rects: Dict[str, pygame.Rect] = {}

        self.button_status_renderer = ButtonStatusRenderer(self.screen, self.fonts)
        self.notification_renderer = NotificationRenderer(self.screen, self.fonts)
        self.info_text_renderer = InfoTextRenderer(self.screen, self.fonts)
        self.tb_status_renderer = TBStatusRenderer(self.screen, self.fonts)
        self.plot_area_renderer = PlotAreaRenderer(
            self.screen, self.fonts, self.plotter
        )

    def _init_fonts(self):
        fonts = {}
        font_configs = {
            "ui": 24,
            "status": 28,
            "logdir": 16,
            "plot_placeholder": 20,
            "notification": 19,
            "notification_label": 16,
        }
        for key, size in font_configs.items():
            try:
                fonts[key] = pygame.font.SysFont(None, size)
            except Exception:
                fonts[key] = pygame.font.Font(None, size)
            if fonts[key] is None:
                print(f"ERROR: Font '{key}' failed to load.")
        return fonts

    def render(
        self,
        is_running: bool,  # Renamed from is_training
        status: str,
        stats_summary: Dict[str, Any],
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
        app_state: str,
    ):
        current_width, current_height = self.screen.get_size()
        lp_width = min(current_width, max(300, self.vis_config.LEFT_PANEL_WIDTH))
        lp_rect = pygame.Rect(0, 0, lp_width, current_height)

        status_color_map = {
            "Ready": (30, 30, 30),
            "Training": (30, 40, 30),
            "Confirm Cleanup": (50, 20, 20),
            "Cleaning": (60, 30, 30),
            "Error": (60, 0, 0),
            "Playing Demo": (30, 30, 40),
            "Initializing": (40, 40, 40),
        }
        bg_color = status_color_map.get(status, (30, 30, 30))
        pygame.draw.rect(self.screen, bg_color, lp_rect)
        self.stat_rects.clear()

        current_y = 10
        notification_area_rect = None

        next_y, rects_bs, notification_area_rect = self.button_status_renderer.render(
            current_y, lp_width, app_state, is_running, status
        )
        self.stat_rects.update(rects_bs)
        current_y = next_y

        if notification_area_rect:
            rects_notif = self.notification_renderer.render(
                notification_area_rect, stats_summary
            )
            self.stat_rects.update(rects_notif)

        next_y, rects_info = self.info_text_renderer.render(
            current_y, stats_summary, lp_width
        )
        self.stat_rects.update(rects_info)
        current_y = next_y

        next_y, rects_tb = self.tb_status_renderer.render(
            current_y + 10, tensorboard_log_dir, lp_width
        )
        self.stat_rects.update(rects_tb)
        current_y = next_y

        self.plot_area_renderer.render(
            current_y + 15, lp_width, current_height, plot_data, status
        )

    def get_stat_rects(self) -> Dict[str, pygame.Rect]:
        return self.stat_rects.copy()

    def get_tooltip_texts(self) -> Dict[str, str]:
        return TOOLTIP_TEXTS


File: ui/panels/left_panel_components/plot_area_renderer.py
# File: ui/panels/left_panel_components/plot_area_renderer.py
import pygame
from typing import Dict, Deque, Optional
from config import VisConfig
from ui.plotter import Plotter  # Import Plotter


class PlotAreaRenderer:
    """Renders the plot area using a Plotter instance."""

    def __init__(
        self,
        screen: pygame.Surface,
        fonts: Dict[str, pygame.font.Font],
        plotter: Plotter,  # Pass plotter instance
    ):
        self.screen = screen
        self.fonts = fonts
        self.plotter = plotter

    def render(
        self,
        y_start: int,
        panel_width: int,
        screen_height: int,
        plot_data: Dict[str, Deque],
        status: str,
    ):
        """Renders the plot area."""
        plot_area_height = screen_height - y_start - 10
        plot_area_width = panel_width - 20

        if plot_area_width <= 50 or plot_area_height <= 50:
            return

        plot_surface = self.plotter.get_cached_or_updated_plot(
            plot_data, plot_area_width, plot_area_height
        )
        plot_area_rect = pygame.Rect(10, y_start, plot_area_width, plot_area_height)

        if plot_surface:
            self.screen.blit(plot_surface, plot_area_rect.topleft)
        else:
            # Render placeholder
            pygame.draw.rect(self.screen, (40, 40, 40), plot_area_rect, 1)
            placeholder_text = "Waiting for data..."
            if status == "Buffering":
                placeholder_text = "Buffering... Waiting for plot data..."
            elif status == "Error":
                placeholder_text = "Plotting disabled due to error."
            elif not plot_data or not any(plot_data.values()):
                placeholder_text = "No plot data yet..."

            placeholder_font = self.fonts.get("plot_placeholder")
            if placeholder_font:
                placeholder_surf = placeholder_font.render(
                    placeholder_text, True, VisConfig.GRAY
                )
                placeholder_rect = placeholder_surf.get_rect(
                    center=plot_area_rect.center
                )
                blit_pos = (
                    max(plot_area_rect.left, placeholder_rect.left),
                    max(plot_area_rect.top, placeholder_rect.top),
                )
                clip_area_rect = plot_area_rect.clip(placeholder_rect)
                blit_area = clip_area_rect.move(
                    -placeholder_rect.left, -placeholder_rect.top
                )
                if blit_area.width > 0 and blit_area.height > 0:
                    self.screen.blit(placeholder_surf, blit_pos, area=blit_area)
            else:  # Fallback cross
                pygame.draw.line(
                    self.screen,
                    VisConfig.GRAY,
                    plot_area_rect.topleft,
                    plot_area_rect.bottomright,
                )
                pygame.draw.line(
                    self.screen,
                    VisConfig.GRAY,
                    plot_area_rect.topright,
                    plot_area_rect.bottomleft,
                )


File: ui/panels/left_panel_components/notification_renderer.py
# File: ui/panels/left_panel_components/notification_renderer.py
import pygame
import time
from typing import Dict, Any, Tuple
from config import VisConfig, StatsConfig
import numpy as np


class NotificationRenderer:
    """Renders the notification area with best scores/loss."""

    def __init__(self, screen: pygame.Surface, fonts: Dict[str, pygame.font.Font]):
        self.screen = screen
        self.fonts = fonts

    def _format_steps_ago(self, current_step: int, best_step: int) -> str:
        """Formats the difference in steps into a readable string."""
        if best_step <= 0 or current_step <= best_step:
            return "Now"
        diff = current_step - best_step
        if diff < 1000:
            return f"{diff} steps ago"
        elif diff < 1_000_000:
            return f"{diff / 1000:.1f}k steps ago"
        else:
            return f"{diff / 1_000_000:.1f}M steps ago"

    def _render_line(
        self,
        area_rect: pygame.Rect,
        y_pos: int,
        label: str,
        current_val: Any,
        prev_val: Any,
        best_step: int,
        val_format: str,
        current_step: int,
    ) -> pygame.Rect:
        """Renders a single line within the notification area."""
        label_font = self.fonts.get("notification_label")
        value_font = self.fonts.get("notification")
        if not label_font or not value_font:
            return pygame.Rect(0, y_pos, 0, 0)

        padding = 5
        label_color, value_color = VisConfig.LIGHTG, VisConfig.WHITE
        prev_color, time_color = VisConfig.GRAY, (180, 180, 100)

        label_surf = label_font.render(label, True, label_color)
        label_rect = label_surf.get_rect(topleft=(area_rect.left + padding, y_pos))
        self.screen.blit(label_surf, label_rect)
        current_x = label_rect.right + 4

        current_val_str = "N/A"
        # --- Convert to float *before* checking ---
        val_as_float: Optional[float] = None
        if isinstance(
            current_val, (int, float, np.number)
        ):  # Check against np.number too
            try:
                val_as_float = float(current_val)
            except (ValueError, TypeError):
                val_as_float = None  # Conversion failed

        # --- Use the converted float for checks and formatting ---
        if val_as_float is not None and np.isfinite(val_as_float):
            try:
                current_val_str = val_format.format(val_as_float)
            except (ValueError, TypeError) as fmt_err:
                current_val_str = "ErrFmt"

        val_surf = value_font.render(current_val_str, True, value_color)
        val_rect = val_surf.get_rect(topleft=(current_x, y_pos))
        self.screen.blit(val_surf, val_rect)
        current_x = val_rect.right + 4

        prev_val_str = "(N/A)"
        # --- Convert prev_val to float before checking ---
        prev_val_as_float: Optional[float] = None
        if isinstance(prev_val, (int, float, np.number)):
            try:
                prev_val_as_float = float(prev_val)
            except (ValueError, TypeError):
                prev_val_as_float = None

        if prev_val_as_float is not None and np.isfinite(prev_val_as_float):
            try:
                prev_val_str = f"({val_format.format(prev_val_as_float)})"
            except (ValueError, TypeError):
                prev_val_str = "(ErrFmt)"

        prev_surf = label_font.render(prev_val_str, True, prev_color)
        prev_rect = prev_surf.get_rect(topleft=(current_x, y_pos + 1))
        self.screen.blit(prev_surf, prev_rect)
        current_x = prev_rect.right + 6

        steps_ago_str = self._format_steps_ago(current_step, best_step)
        time_surf = label_font.render(steps_ago_str, True, time_color)
        time_rect = time_surf.get_rect(topleft=(current_x, y_pos + 1))

        available_width = area_rect.right - time_rect.left - padding
        clip_rect = pygame.Rect(0, 0, max(0, available_width), time_rect.height)
        if time_rect.width > available_width > 0:
            self.screen.blit(time_surf, time_rect, area=clip_rect)
        elif available_width > 0:
            self.screen.blit(time_surf, time_rect)

        union_rect = label_rect.union(val_rect).union(prev_rect).union(time_rect)
        union_rect.width = min(union_rect.width, area_rect.width - 2 * padding)
        return union_rect

    def render(
        self, area_rect: pygame.Rect, stats_summary: Dict[str, Any]
    ) -> Dict[str, pygame.Rect]:
        """Renders the notification content."""
        stat_rects: Dict[str, pygame.Rect] = {}
        pygame.draw.rect(self.screen, (45, 45, 45), area_rect, border_radius=3)
        pygame.draw.rect(self.screen, VisConfig.LIGHTG, area_rect, 1, border_radius=3)
        stat_rects["Notification Area"] = area_rect

        value_font = self.fonts.get("notification")
        if not value_font:
            return stat_rects

        padding = 5
        line_height = value_font.get_linesize()
        current_step = stats_summary.get("global_step", 0)
        y = area_rect.top + padding

        # --- Pass values from summary, _render_line handles conversion/check ---
        rect_rl = self._render_line(
            area_rect,
            y,
            "RL Score:",
            stats_summary.get("best_score", -float("inf")),
            stats_summary.get("previous_best_score", -float("inf")),
            stats_summary.get("best_score_step", 0),
            "{:.2f}",
            current_step,
        )
        stat_rects["Best RL Score Info"] = rect_rl.clip(area_rect)
        y += line_height

        rect_game = self._render_line(
            area_rect,
            y,
            "Game Score:",
            stats_summary.get("best_game_score", -float("inf")),
            stats_summary.get("previous_best_game_score", -float("inf")),
            stats_summary.get("best_game_score_step", 0),
            "{:.0f}",
            current_step,
        )
        stat_rects["Best Game Score Info"] = rect_game.clip(area_rect)
        y += line_height

        rect_loss = self._render_line(
            area_rect,
            y,
            "Loss:",
            stats_summary.get("best_loss", float("inf")),
            stats_summary.get("previous_best_loss", float("inf")),
            stats_summary.get("best_loss_step", 0),
            "{:.4f}",
            current_step,
        )
        stat_rects["Best Loss Info"] = rect_loss.clip(area_rect)

        return stat_rects


File: ui/panels/left_panel_components/__init__.py
from .button_status_renderer import ButtonStatusRenderer
from .notification_renderer import NotificationRenderer
from .info_text_renderer import InfoTextRenderer
from .tb_status_renderer import TBStatusRenderer
from .plot_area_renderer import PlotAreaRenderer

__all__ = [
    "ButtonStatusRenderer",
    "NotificationRenderer",
    "InfoTextRenderer",
    "TBStatusRenderer",
    "PlotAreaRenderer",
]


File: ui/panels/left_panel_components/tb_status_renderer.py
# File: ui/panels/left_panel_components/tb_status_renderer.py
import pygame
import os
from typing import Dict, Optional, Tuple
from config import VisConfig, TensorBoardConfig


class TBStatusRenderer:
    """Renders the TensorBoard status line."""

    def __init__(self, screen: pygame.Surface, fonts: Dict[str, pygame.font.Font]):
        self.screen = screen
        self.fonts = fonts

    def _shorten_path(self, path: str, max_chars: int) -> str:
        """Attempts to shorten a path string for display."""
        if len(path) <= max_chars:
            return path
        try:
            rel_path = os.path.relpath(path)
        except ValueError:  # Handle different drives on Windows
            rel_path = path
        if len(rel_path) <= max_chars:
            return rel_path

        parts = path.replace("\\", "/").split("/")
        if len(parts) >= 2:
            short_path = os.path.join("...", *parts[-2:])
            if len(short_path) <= max_chars:
                return short_path
        # Fallback: Ellipsis + end of basename
        basename = os.path.basename(path)
        return (
            "..." + basename[-(max_chars - 3) :]
            if len(basename) > max_chars - 3
            else basename
        )

    def render(
        self, y_start: int, log_dir: Optional[str], panel_width: int
    ) -> Tuple[int, Dict[str, pygame.Rect]]:
        """Renders the TB status. Returns next_y and stat_rects."""
        stat_rects: Dict[str, pygame.Rect] = {}
        ui_font = self.fonts.get("ui")
        logdir_font = self.fonts.get("logdir")
        if not ui_font or not logdir_font:
            return y_start + 30, stat_rects

        tb_active = (
            TensorBoardConfig.LOG_HISTOGRAMS
            or TensorBoardConfig.LOG_IMAGES
            or TensorBoardConfig.LOG_SHAPE_PLACEMENT_Q_VALUES
        )
        tb_color = VisConfig.GOOGLE_COLORS[0] if tb_active else VisConfig.GRAY
        tb_text = f"TensorBoard: {'Logging Active' if tb_active else 'Logging Minimal'}"

        tb_surf = ui_font.render(tb_text, True, tb_color)
        tb_rect = tb_surf.get_rect(topleft=(10, y_start))
        self.screen.blit(tb_surf, tb_rect)
        stat_rects["TensorBoard Status"] = tb_rect
        last_y = tb_rect.bottom

        if log_dir:
            try:
                panel_char_width = max(
                    10, panel_width // max(1, logdir_font.size("A")[0])
                )
                short_log_dir = self._shorten_path(log_dir, panel_char_width)
            except Exception:
                short_log_dir = os.path.basename(log_dir)

            dir_surf = logdir_font.render(
                f"Log Dir: {short_log_dir}", True, VisConfig.LIGHTG
            )
            dir_rect = dir_surf.get_rect(topleft=(10, tb_rect.bottom + 2))

            clip_width = max(0, panel_width - dir_rect.left - 10)
            if dir_rect.width > clip_width:
                self.screen.blit(
                    dir_surf,
                    dir_rect,
                    area=pygame.Rect(0, 0, clip_width, dir_rect.height),
                )
            else:
                self.screen.blit(dir_surf, dir_rect)

            combined_tb_rect = tb_rect.union(dir_rect)
            combined_tb_rect.width = min(
                combined_tb_rect.width, panel_width - 10 - combined_tb_rect.left
            )
            stat_rects["TensorBoard Status"] = combined_tb_rect
            last_y = dir_rect.bottom

        return last_y, stat_rects


File: ui/panels/left_panel_components/button_status_renderer.py
# File: ui/panels/left_panel_components/button_status_renderer.py
import pygame
from typing import Dict, Tuple, Optional
from config import VisConfig


class ButtonStatusRenderer:
    """Renders the top buttons and status line in the left panel."""

    def __init__(self, screen: pygame.Surface, fonts: Dict[str, pygame.font.Font]):
        self.screen = screen
        self.fonts = fonts

    def _draw_button(self, rect: pygame.Rect, text: str, color: Tuple[int, int, int]):
        """Helper to draw a single button."""
        pygame.draw.rect(self.screen, color, rect, border_radius=5)
        ui_font = self.fonts.get("ui")
        if ui_font:
            lbl_surf = ui_font.render(text, True, VisConfig.WHITE)
            self.screen.blit(lbl_surf, lbl_surf.get_rect(center=rect.center))
        else:
            pygame.draw.line(
                self.screen, VisConfig.RED, rect.topleft, rect.bottomright, 2
            )
            pygame.draw.line(
                self.screen, VisConfig.RED, rect.topright, rect.bottomleft, 2
            )

    def render(
        self,
        y_start: int,
        panel_width: int,
        app_state: str,
        is_running: bool,  # Renamed from is_training
        status: str,
    ) -> Tuple[int, Dict[str, pygame.Rect], Optional[pygame.Rect]]:
        """Renders buttons and status. Returns next_y, stat_rects, notification_rect."""
        stat_rects: Dict[str, pygame.Rect] = {}
        notification_rect = None
        next_y = y_start

        if app_state == "MainMenu":
            run_btn_rect = pygame.Rect(10, y_start, 100, 40)  # Renamed
            cleanup_btn_rect = pygame.Rect(run_btn_rect.right + 10, y_start, 160, 40)
            demo_btn_rect = pygame.Rect(cleanup_btn_rect.right + 10, y_start, 120, 40)

            self._draw_button(
                run_btn_rect,  # Use run_btn_rect
                (
                    "Stop" if is_running and status == "Training" else "Run"
                ),  # Updated text
                (70, 70, 70),
            )
            self._draw_button(cleanup_btn_rect, "Cleanup This Run", (100, 40, 40))
            self._draw_button(demo_btn_rect, "Play Demo", (40, 100, 40))

            stat_rects["Run Button"] = run_btn_rect  # Renamed key
            stat_rects["Cleanup Button"] = cleanup_btn_rect
            stat_rects["Play Demo Button"] = demo_btn_rect

            notification_x = demo_btn_rect.right + 15
            notification_w = panel_width - notification_x - 10
            notif_font = self.fonts.get("notification")
            if notification_w > 50 and notif_font:
                line_h = notif_font.get_linesize()
                notification_h = line_h * 3 + 12
                notification_rect = pygame.Rect(
                    notification_x, y_start, notification_w, notification_h
                )

            next_y = run_btn_rect.bottom + 10  # Use run_btn_rect
        # --- Status Text ---
        status_text = f"Status: {status}"
        if app_state == "Playing":
            status_text = "Status: Playing Demo"
        elif app_state != "MainMenu":
            status_text = f"Status: {app_state}"

        status_font = self.fonts.get("status")
        if status_font:
            status_surf = status_font.render(status_text, True, VisConfig.YELLOW)
            status_rect_top = next_y if app_state == "MainMenu" else y_start
            status_rect = status_surf.get_rect(topleft=(10, status_rect_top))
            self.screen.blit(status_surf, status_rect)
            if app_state == "MainMenu":
                stat_rects["Status"] = status_rect
            next_y = status_rect.bottom + 5
        else:
            next_y += 20

        return next_y, stat_rects, notification_rect


File: ui/panels/left_panel_components/info_text_renderer.py
# File: ui/panels/left_panel_components/info_text_renderer.py
import pygame
from typing import Dict, Any, Tuple
from config import (
    VisConfig,
    StatsConfig,
    PPOConfig,
    RNNConfig,
    DEVICE,
    TOTAL_TRAINING_STEPS,
)


class InfoTextRenderer:
    """Renders the main block of statistics text."""

    def __init__(self, screen: pygame.Surface, fonts: Dict[str, pygame.font.Font]):
        self.screen = screen
        self.fonts = fonts

    def render(
        self,
        y_start: int,
        stats_summary: Dict[str, Any],
        panel_width: int,
    ) -> Tuple[int, Dict[str, pygame.Rect]]:
        """Renders the info text block. Returns next_y and stat_rects."""
        stat_rects: Dict[str, pygame.Rect] = {}
        ui_font = self.fonts.get("ui")
        if not ui_font:
            return y_start + 100, stat_rects

        line_height = ui_font.get_linesize()
        global_step = stats_summary.get("global_step", 0)

        # --- MODIFIED: Use specific PPO metric keys ---
        info_lines = [
            (
                "Global Steps",
                f"{global_step/1e6:.2f}M / {TOTAL_TRAINING_STEPS/1e6:.1f}M",
            ),
            ("Total Episodes", f"{stats_summary.get('total_episodes', 0)}"),
            (
                "Steps/Sec (Current)",
                f"{stats_summary.get('steps_per_second', 0.0):.1f}",
            ),
            (
                "Policy Loss",
                f"{stats_summary.get('policy_loss', 0.0):.4f}",
            ),  # Use 'policy_loss'
            (
                "Value Loss",
                f"{stats_summary.get('value_loss', 0.0):.4f}",
            ),  # Use 'value_loss'
            ("Entropy", f"{stats_summary.get('entropy', 0.0):.4f}"),  # Use 'entropy'
            ("Learning Rate", f"{stats_summary.get('current_lr', 0.0):.1e}"),
            ("Device", f"{DEVICE.type.upper()}"),
            (
                "Network",
                f"Actor-Critic (CNN+MLP->LSTM:{RNNConfig.USE_RNN})",
            ),
        ]
        # --- END MODIFIED ---

        last_y = y_start
        x_pos_key, x_pos_val_offset = 10, 5

        for idx, (key, value_str) in enumerate(info_lines):
            current_y = y_start + idx * line_height
            try:
                key_surf = ui_font.render(f"{key}:", True, VisConfig.LIGHTG)
                key_rect = key_surf.get_rect(topleft=(x_pos_key, current_y))
                self.screen.blit(key_surf, key_rect)

                value_surf = ui_font.render(f"{value_str}", True, VisConfig.WHITE)
                value_rect = value_surf.get_rect(
                    topleft=(key_rect.right + x_pos_val_offset, current_y)
                )

                clip_width = max(0, panel_width - value_rect.left - 10)
                if value_rect.width > clip_width:
                    self.screen.blit(
                        value_surf,
                        value_rect,
                        area=pygame.Rect(0, 0, clip_width, value_rect.height),
                    )
                else:
                    self.screen.blit(value_surf, value_rect)

                combined_rect = key_rect.union(value_rect)
                combined_rect.width = min(
                    combined_rect.width, panel_width - x_pos_key - 10
                )
                stat_rects[key] = combined_rect
                last_y = combined_rect.bottom
            except Exception as e:
                print(f"Error rendering stat line '{key}': {e}")
                last_y = current_y + line_height

        return last_y, stat_rects


File: config/__init__.py
# File: config/__init__.py
from .core import (
    VisConfig,
    EnvConfig,
    RewardConfig,
    PPOConfig,
    RNNConfig,
    TrainConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
    DemoConfig,
)
from .general import (
    DEVICE,
    RANDOM_SEED,
    RUN_ID,
    BASE_CHECKPOINT_DIR,
    BASE_LOG_DIR,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,
    MODEL_SAVE_PATH,
    TOTAL_TRAINING_STEPS,
)
from .utils import get_config_dict
from .validation import print_config_info_and_validate

# Assign RUN_LOG_DIR to TensorBoardConfig after imports
TensorBoardConfig.LOG_DIR = RUN_LOG_DIR

__all__ = [
    # Core Classes
    "VisConfig",
    "EnvConfig",
    "RewardConfig",
    "PPOConfig",
    "RNNConfig",
    "TrainConfig",
    "ModelConfig",
    "StatsConfig",
    "TensorBoardConfig",
    "DemoConfig",
    # General Constants/Paths
    "DEVICE",
    "RANDOM_SEED",
    "RUN_ID",
    "BASE_CHECKPOINT_DIR",
    "BASE_LOG_DIR",
    "RUN_CHECKPOINT_DIR",
    "RUN_LOG_DIR",
    "MODEL_SAVE_PATH",
    "TOTAL_TRAINING_STEPS",
    # Utils/Validation
    "get_config_dict",
    "print_config_info_and_validate",
]


File: config/core.py
import torch
from typing import Deque, Dict, Any, List, Type, Tuple, Optional

from .general import TOTAL_TRAINING_STEPS


class VisConfig:
    NUM_ENVS_TO_RENDER = 9  # Number of environments to display visually
    SCREEN_WIDTH = 1600
    SCREEN_HEIGHT = 900
    VISUAL_STEP_DELAY = (
        0.005  # Small delay for visualization steps (mostly relevant if FPS locked)
    )
    # Adjust left panel for potentially wider plots or more stats
    LEFT_PANEL_WIDTH = int(SCREEN_WIDTH * 0.4)
    ENV_SPACING = 1  # Pixel spacing between rendered envs
    ENV_GRID_PADDING = 1  # Pixel padding inside each rendered env grid
    FPS = 0  # Target FPS for visualization (0 = unlocked, run as fast as possible)

    # Colors
    WHITE = (255, 255, 255)
    BLACK = (0, 0, 0)
    LIGHTG = (140, 140, 140)
    GRAY = (50, 50, 50)
    RED = (255, 50, 50)
    DARK_RED = (80, 10, 10)
    BLUE = (50, 50, 255)
    YELLOW = (255, 255, 100)
    GOOGLE_COLORS = [(15, 157, 88), (244, 180, 0), (66, 133, 244), (219, 68, 55)]
    LINE_CLEAR_FLASH_COLOR = (180, 180, 220)
    LINE_CLEAR_HIGHLIGHT_COLOR = (255, 255, 0, 180)
    GAME_OVER_FLASH_COLOR = (255, 0, 0)


class EnvConfig:
    # --- Performance Tuning Target ---
    NUM_ENVS = 512  # Keep as is, batching will help more than just increasing this
    ROWS = 8
    COLS = 15
    GRID_FEATURES_PER_CELL = 2
    SHAPE_FEATURES_PER_SHAPE = 5
    NUM_SHAPE_SLOTS = 3

    @property
    def GRID_STATE_SHAPE(self) -> Tuple[int, int, int]:
        return (self.GRID_FEATURES_PER_CELL, self.ROWS, self.COLS)

    @property
    def SHAPE_STATE_DIM(self) -> int:
        return self.NUM_SHAPE_SLOTS * self.SHAPE_FEATURES_PER_SHAPE

    @property
    def ACTION_DIM(self) -> int:
        # Total possible placement actions: shape_slot * (row * col)
        return self.NUM_SHAPE_SLOTS * (self.ROWS * self.COLS)


class RewardConfig:
    REWARD_PLACE_PER_TRI = 0.0
    REWARD_CLEAR_1 = 1.0
    REWARD_CLEAR_2 = 3.0
    REWARD_CLEAR_3PLUS = 6.0
    PENALTY_INVALID_MOVE = -0.1
    PENALTY_HOLE_PER_HOLE = -0.05
    PENALTY_GAME_OVER = -1.0
    REWARD_ALIVE_STEP = 0.01  # Small reward for surviving a step


class PPOConfig:
    # --- Core PPO Parameters ---
    LEARNING_RATE = 3e-4  # Initial learning rate
    ADAM_EPS = 1e-5  # Epsilon for Adam optimizer stability
    # --- Performance Tuning Target ---
    # Adjust based on memory/performance trade-off with batching
    NUM_STEPS_PER_ROLLOUT = 256
    PPO_EPOCHS = 4  # Number of optimization epochs per rollout
    # --- Performance Tuning Target ---
    # Can be increased slightly if batching improves throughput
    NUM_MINIBATCHES = 32
    CLIP_PARAM = 0.1  # PPO clipping parameter (0.1 or 0.2 common)
    GAMMA = 0.99  # Discount factor for future rewards
    GAE_LAMBDA = 0.95  # Generalized Advantage Estimation lambda
    VALUE_LOSS_COEF = 0.5  # Coefficient for the value loss
    ENTROPY_COEF = 0.01  # Coefficient for the entropy bonus (exploration)
    MAX_GRAD_NORM = 0.5  # Gradient clipping norm limit (0 disables)
    USE_LR_SCHEDULER = True  # Enable linear learning rate decay
    LR_SCHEDULER_END_FRACTION = (
        0.0  # Fraction of total steps where LR reaches its minimum (0.0 = decay to 0)
    )

    @property
    def MINIBATCH_SIZE(self) -> int:
        total_data_per_update = EnvConfig.NUM_ENVS * self.NUM_STEPS_PER_ROLLOUT
        batch_size = total_data_per_update // self.NUM_MINIBATCHES
        return max(1, batch_size)


class RNNConfig:
    # --- Enable LSTM ---
    USE_RNN = True
    LSTM_HIDDEN_SIZE = 256  # Might need adjustment based on performance/task
    LSTM_NUM_LAYERS = 1


class TrainConfig:
    CHECKPOINT_SAVE_FREQ = (
        50  # Save every N *agent updates* (i.e., after N full rollouts)
    )
    LOAD_CHECKPOINT_PATH: str | None = (
        None  # Path to load a checkpoint from, None to start fresh
    )


class ModelConfig:
    class Network:
        _env_cfg_instance = EnvConfig()  # Temporary instance to access dims
        HEIGHT = _env_cfg_instance.ROWS
        WIDTH = _env_cfg_instance.COLS
        del _env_cfg_instance  # Clean up temporary instance

        # CNN Branch
        CONV_CHANNELS = [32, 64, 64]
        CONV_KERNEL_SIZE = 3
        CONV_STRIDE = 1
        CONV_PADDING = 1
        CONV_ACTIVATION = torch.nn.ReLU
        USE_BATCHNORM_CONV = True

        # Shape MLP Branch
        SHAPE_FEATURE_MLP_DIMS = [64]
        SHAPE_MLP_ACTIVATION = torch.nn.ReLU

        # Fusion MLP Branch
        COMBINED_FC_DIMS = [
            256,
            128,
        ]  # Consider increasing if LSTM is added (e.g., [512, 256])
        COMBINED_ACTIVATION = torch.nn.ReLU
        USE_BATCHNORM_FC = True
        DROPOUT_FC = 0.0  # Dropout rate in fusion layers (0 = disabled)


class StatsConfig:
    STATS_AVG_WINDOW: List[int] = [
        50,
        500,
        5000,
    ]  # Windows for calculating rolling averages
    CONSOLE_LOG_FREQ = 10  # Log summary to console every N agent updates (rollouts)
    PLOT_DATA_WINDOW = 100_000  # Max number of data points to keep for plotting


class TensorBoardConfig:
    LOG_HISTOGRAMS = True  # Log parameter/gradient histograms
    HISTOGRAM_LOG_FREQ = 20  # Log histograms every N agent updates (rollouts)
    LOG_IMAGES = True  # Log sample environment state images
    IMAGE_LOG_FREQ = 100  # Log images every N agent updates (rollouts)
    LOG_DIR: Optional[str] = None  # Set automatically in config/__init__.py


class DemoConfig:
    BACKGROUND_COLOR = (10, 10, 20)
    SELECTED_SHAPE_HIGHLIGHT_COLOR = VisConfig.BLUE
    HUD_FONT_SIZE = 24
    HELP_FONT_SIZE = 18
    HELP_TEXT = "[Arrows]=Move | [Q/E]=Cycle Shape | [Space]=Place | [ESC]=Exit"


File: config/utils.py
# File: config/utils.py
import torch
from typing import Dict, Any
from .core import (
    VisConfig,
    EnvConfig,
    RewardConfig,
    PPOConfig,
    RNNConfig,
    TrainConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
    DemoConfig,
)
from .general import DEVICE, RANDOM_SEED, RUN_ID


def get_config_dict() -> Dict[str, Any]:
    """Returns a flat dictionary of all relevant config values for logging."""
    all_configs = {}

    def flatten_class(cls, prefix=""):
        d = {}
        for k, v in vars(cls).items():
            if (
                not k.startswith("__")
                and not callable(v)
                and not isinstance(v, type)
                and not hasattr(v, "__module__")
            ):
                if isinstance(getattr(cls, k, None), property):
                    try:
                        v = getattr(cls(), k)
                    except Exception:
                        continue
                d[f"{prefix}{k}"] = v
        return d

    all_configs.update(flatten_class(VisConfig, "Vis."))
    all_configs.update(flatten_class(EnvConfig, "Env."))
    all_configs.update(flatten_class(RewardConfig, "Reward."))
    all_configs.update(flatten_class(PPOConfig, "PPO."))
    all_configs.update(flatten_class(RNNConfig, "RNN."))
    all_configs.update(flatten_class(TrainConfig, "Train."))
    all_configs.update(flatten_class(ModelConfig.Network, "Model.Net."))
    all_configs.update(flatten_class(StatsConfig, "Stats."))
    all_configs.update(flatten_class(TensorBoardConfig, "TB."))
    all_configs.update(flatten_class(DemoConfig, "Demo."))

    all_configs["General.DEVICE"] = str(DEVICE)
    all_configs["General.RANDOM_SEED"] = RANDOM_SEED
    all_configs["General.RUN_ID"] = RUN_ID

    all_configs = {
        k: v for k, v in all_configs.items() if not (k.endswith("_PATH") and v is None)
    }

    for key, value in all_configs.items():
        if isinstance(value, type) and issubclass(value, torch.nn.Module):
            all_configs[key] = value.__name__
        elif isinstance(value, (list, tuple)):
            all_configs[key] = str(value)
        if not isinstance(value, (int, float, str, bool)):
            all_configs[key] = str(value)

    return all_configs


File: config/general.py
# File: config/general.py
import torch
import os
import time
from utils.helpers import get_device

DEVICE = get_device()
RANDOM_SEED = 42
RUN_ID = f"run_{time.strftime('%Y%m%d_%H%M%S')}"
BASE_CHECKPOINT_DIR = "checkpoints"
BASE_LOG_DIR = "logs"

TOTAL_TRAINING_STEPS = 10_000_000

RUN_CHECKPOINT_DIR = os.path.join(BASE_CHECKPOINT_DIR, RUN_ID)
RUN_LOG_DIR = os.path.join(BASE_LOG_DIR, "tensorboard", RUN_ID)

MODEL_SAVE_PATH = os.path.join(
    RUN_CHECKPOINT_DIR, "ppo_agent_state.pth"
)  # Renamed file


File: config/validation.py
# File: config/validation.py
import os, torch
from .core import (
    EnvConfig,
    PPOConfig,
    RNNConfig,
    TrainConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
    VisConfig,
    DemoConfig,
)
from .general import (
    RUN_ID,
    DEVICE,
    MODEL_SAVE_PATH,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,
    TOTAL_TRAINING_STEPS,
)


def print_config_info_and_validate():
    env_config_instance = EnvConfig()
    ppo_config_instance = PPOConfig()
    rnn_config_instance = RNNConfig()

    print("-" * 70)
    print(f"RUN ID: {RUN_ID}")
    print(f"Log Directory: {RUN_LOG_DIR}")
    print(f"Checkpoint Directory: {RUN_CHECKPOINT_DIR}")
    print(f"Device: {DEVICE}")
    print(
        f"TB Logging: Histograms={'ON' if TensorBoardConfig.LOG_HISTOGRAMS else 'OFF'}, "
        f"Images={'ON' if TensorBoardConfig.LOG_IMAGES else 'OFF'}"
    )

    if TrainConfig.LOAD_CHECKPOINT_PATH:
        print(
            "*" * 70
            + f"\n*** Warning: LOAD CHECKPOINT from: {TrainConfig.LOAD_CHECKPOINT_PATH} ***\n"
            "*** Ensure ckpt matches current Model/PPO/RNN Config. ***\n" + "*" * 70
        )
    else:
        print("--- Starting training from scratch (no checkpoint specified). ---")

    print(f"--- Using PPO Algorithm ---")
    print(f"    Rollout Steps: {ppo_config_instance.NUM_STEPS_PER_ROLLOUT}")
    print(f"    PPO Epochs: {ppo_config_instance.PPO_EPOCHS}")
    print(
        f"    Minibatches: {ppo_config_instance.NUM_MINIBATCHES} (Size: {ppo_config_instance.MINIBATCH_SIZE})"
    )
    print(f"    Clip Param: {ppo_config_instance.CLIP_PARAM}")
    print(f"    GAE Lambda: {ppo_config_instance.GAE_LAMBDA}")
    print(
        f"    Value Coef: {ppo_config_instance.VALUE_LOSS_COEF}, Entropy Coef: {ppo_config_instance.ENTROPY_COEF}"
    )
    print(
        f"--- Using RNN: {rnn_config_instance.USE_RNN}"
        + (
            f" (LSTM Hidden: {rnn_config_instance.LSTM_HIDDEN_SIZE}, Layers: {rnn_config_instance.LSTM_NUM_LAYERS})"
            if rnn_config_instance.USE_RNN
            else ""
        )
        + " ---"
    )
    print(
        f"--- Using LR Scheduler: {ppo_config_instance.USE_LR_SCHEDULER}"
        + (
            f" (Linear Decay to {ppo_config_instance.LR_SCHEDULER_END_FRACTION * 100}%)"
            if ppo_config_instance.USE_LR_SCHEDULER
            else ""
        )
        + " ---"
    )

    print(
        f"Config: Env=(R={env_config_instance.ROWS}, C={env_config_instance.COLS}), "
        f"GridState={env_config_instance.GRID_STATE_SHAPE}, "
        f"ShapeState={env_config_instance.SHAPE_STATE_DIM}, "
        f"ActionDim={env_config_instance.ACTION_DIM}"
    )
    cnn_str = str(ModelConfig.Network.CONV_CHANNELS).replace(" ", "")
    mlp_str = str(ModelConfig.Network.COMBINED_FC_DIMS).replace(" ", "")
    shape_mlp_cfg_str = str(ModelConfig.Network.SHAPE_FEATURE_MLP_DIMS).replace(" ", "")
    print(f"Network: CNN={cnn_str}, ShapeMLP={shape_mlp_cfg_str}, Fusion={mlp_str}")

    print(
        f"Training: NUM_ENVS={env_config_instance.NUM_ENVS}, TOTAL_STEPS={TOTAL_TRAINING_STEPS/1e6:.1f}M"
    )
    print(
        f"Stats: AVG_WINDOWS={StatsConfig.STATS_AVG_WINDOW}, Console Log Freq={StatsConfig.CONSOLE_LOG_FREQ} (rollouts)"
    )

    if env_config_instance.NUM_ENVS >= 1024:
        print(
            "*" * 70
            + f"\n*** Warning: NUM_ENVS={env_config_instance.NUM_ENVS}. Monitor system resources. ***"
            + (
                "\n*** Using MPS device. Performance varies. Force CPU via env var if needed. ***"
                if DEVICE.type == "mps"
                else ""
            )
            + "\n"
            + "*" * 70
        )
    print(
        f"--- Rendering {VisConfig.NUM_ENVS_TO_RENDER if VisConfig.NUM_ENVS_TO_RENDER > 0 else 'ALL'} of {env_config_instance.NUM_ENVS} environments ---"
    )
    print("-" * 70)


File: training/rollout_collector.py
# File: training/rollout_collector.py
import time
import torch
import numpy as np
import random
import traceback
from typing import List, Dict, Any, Tuple, Optional

from config import (
    EnvConfig,
    RewardConfig,
    TensorBoardConfig,
    PPOConfig,
    RNNConfig,
    DEVICE,
)
from environment.game_state import GameState, StateType
from agent.ppo_agent import PPOAgent
from stats.stats_recorder import StatsRecorderBase
from utils.types import ActionType
from .rollout_storage import RolloutStorage


class RolloutCollector:
    """Handles interaction with parallel environments to collect rollouts for PPO."""

    def __init__(
        self,
        envs: List[GameState],
        agent: PPOAgent,
        stats_recorder: StatsRecorderBase,
        env_config: EnvConfig,
        ppo_config: PPOConfig,
        rnn_config: RNNConfig,
        reward_config: RewardConfig,
        tb_config: TensorBoardConfig,
    ):
        self.envs = envs
        self.agent = agent
        self.stats_recorder = stats_recorder
        self.num_envs = env_config.NUM_ENVS
        self.env_config = env_config
        self.ppo_config = ppo_config
        self.rnn_config = rnn_config
        self.reward_config = reward_config
        self.tb_config = tb_config
        self.device = DEVICE

        self.rollout_storage = RolloutStorage(
            ppo_config.NUM_STEPS_PER_ROLLOUT,
            self.num_envs,
            self.env_config,
            self.rnn_config,
            self.device,
        )

        self.current_obs_grid_cpu = np.zeros(
            (self.num_envs, *self.env_config.GRID_STATE_SHAPE), dtype=np.float32
        )
        self.current_obs_shapes_cpu = np.zeros(
            (self.num_envs, self.env_config.SHAPE_STATE_DIM), dtype=np.float32
        )
        self.current_dones_cpu = np.zeros(self.num_envs, dtype=bool)

        self.current_episode_scores = np.zeros(self.num_envs, dtype=np.float32)
        self.current_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)
        self.current_episode_game_scores = np.zeros(self.num_envs, dtype=np.int32)
        self.current_episode_lines_cleared = np.zeros(self.num_envs, dtype=np.int32)
        self.episode_count = 0

        self.current_hidden_states_device: Optional[
            Tuple[torch.Tensor, torch.Tensor]
        ] = None
        if self.rnn_config.USE_RNN:
            self.current_hidden_states_device = self.agent.get_initial_hidden_state(
                self.num_envs
            )

        self._reset_all_envs()

        initial_obs_grid_t = torch.from_numpy(self.current_obs_grid_cpu).to(
            self.rollout_storage.device
        )
        initial_obs_shapes_t = torch.from_numpy(self.current_obs_shapes_cpu).to(
            self.rollout_storage.device
        )
        initial_dones_t = (
            torch.from_numpy(self.current_dones_cpu)
            .float()
            .unsqueeze(1)
            .to(self.rollout_storage.device)
        )

        self.rollout_storage.obs_grid[0].copy_(initial_obs_grid_t)
        self.rollout_storage.obs_shapes[0].copy_(initial_obs_shapes_t)
        self.rollout_storage.dones[0].copy_(initial_dones_t)
        if self.rnn_config.USE_RNN and self.current_hidden_states_device is not None:
            self.rollout_storage.hidden_states[0].copy_(
                self.current_hidden_states_device[0]
            )

        print(f"[RolloutCollector] Initialized for {self.num_envs} environments.")

    def _reset_all_envs(self):
        """Resets all environments and updates initial CPU observations."""
        for i, env in enumerate(self.envs):
            state_dict = env.reset()
            self.current_obs_grid_cpu[i] = state_dict["grid"]
            self.current_obs_shapes_cpu[i] = state_dict["shapes"].flatten()
            self.current_dones_cpu[i] = False  # Ensure done is False after reset
            self.current_episode_scores[i] = 0.0
            self.current_episode_lengths[i] = 0
            self.current_episode_game_scores[i] = 0
            self.current_episode_lines_cleared[i] = 0
        if self.rnn_config.USE_RNN:
            self.current_hidden_states_device = self.agent.get_initial_hidden_state(
                self.num_envs
            )

    def _record_episode_stats(
        self,
        env_index: int,
        final_reward: float,
        step_count_offset: int,
        current_global_step: int,
    ):
        """Helper function to record stats for a finished episode."""
        self.episode_count += 1
        episode_score = self.current_episode_scores[env_index] + final_reward
        episode_length = self.current_episode_lengths[env_index] + step_count_offset
        game_score = self.current_episode_game_scores[env_index]
        lines_cleared = self.current_episode_lines_cleared[env_index]
        approx_global_step = current_global_step + env_index + 1

        # print(f"[RolloutCollector Debug] Recording Ep {self.episode_count} for Env {env_index}: "
        #       f"RLScore={episode_score:.2f}, Len={episode_length}, GameScore={game_score}, "
        #       f"Lines={lines_cleared} at Step ~{approx_global_step}")

        self.stats_recorder.record_episode(
            episode_score=episode_score,
            episode_length=episode_length,
            episode_num=self.episode_count,
            global_step=approx_global_step,
            game_score=game_score,
            lines_cleared=lines_cleared,
        )

    def collect_one_step(self, current_global_step: int) -> int:
        """Collects one step of experience from all environments using batching."""
        step_start_time = time.time()

        valid_actions_list: List[Optional[List[int]]] = [None] * self.num_envs
        envs_needing_action: List[int] = []
        initial_done_indices: List[int] = []  # Indices done *before* this step starts

        # Check initial done state and get valid actions
        for i in range(self.num_envs):
            if self.current_dones_cpu[i]:
                # print(f"[RolloutCollector Debug] Env {i} starting step as DONE.") # DEBUG
                initial_done_indices.append(i)
                continue  # Will be reset later

            valid_actions = self.envs[i].valid_actions()
            if not valid_actions:
                # print(f"[RolloutCollector Debug] Env {i} has NO valid actions, marking done.") # DEBUG
                # Mark as done *now* if no valid actions, handle reset later
                self.current_dones_cpu[i] = True  # Mark done for this step's logic
                initial_done_indices.append(
                    i
                )  # Treat as initially done for action selection skip
            else:
                valid_actions_list[i] = valid_actions
                envs_needing_action.append(i)

        # --- Batch Agent Action Selection ---
        actions_np = np.zeros(self.num_envs, dtype=np.int64)
        log_probs_np = np.zeros(self.num_envs, dtype=np.float32)
        values_np = np.zeros(self.num_envs, dtype=np.float32)
        next_hidden_states_device = None  # Will hold hidden state for the *next* step

        if envs_needing_action:  # Only run agent if some envs need actions
            active_indices = torch.tensor(envs_needing_action, dtype=torch.long)
            batch_obs_grid_cpu = self.current_obs_grid_cpu[active_indices]
            batch_obs_shapes_cpu = self.current_obs_shapes_cpu[active_indices]
            batch_valid_actions = [valid_actions_list[i] for i in envs_needing_action]

            batch_hidden_state = None
            if (
                self.rnn_config.USE_RNN
                and self.current_hidden_states_device is not None
            ):
                h_n, c_n = self.current_hidden_states_device
                batch_hidden_state = (
                    h_n[:, active_indices, :].contiguous(),
                    c_n[:, active_indices, :].contiguous(),
                )

            batch_obs_grid_t = torch.from_numpy(batch_obs_grid_cpu).to(
                self.agent.device
            )
            batch_obs_shapes_t = torch.from_numpy(batch_obs_shapes_cpu).to(
                self.agent.device
            )

            with torch.no_grad():
                (
                    batch_actions_t,
                    batch_log_probs_t,
                    batch_values_t,
                    batch_next_hidden,
                ) = self.agent.select_action_batch(
                    batch_obs_grid_t,
                    batch_obs_shapes_t,
                    batch_hidden_state,
                    batch_valid_actions,
                )

            actions_np[active_indices] = batch_actions_t.cpu().numpy()
            log_probs_np[active_indices] = batch_log_probs_t.cpu().numpy()
            values_np[active_indices] = batch_values_t.cpu().numpy()

            # Update the *full* hidden state tensor if using RNN
            if self.rnn_config.USE_RNN and batch_next_hidden is not None:
                if self.current_hidden_states_device is None:
                    self.current_hidden_states_device = (
                        self.agent.get_initial_hidden_state(self.num_envs)
                    )
                next_h = self.current_hidden_states_device[0].clone()
                next_c = self.current_hidden_states_device[1].clone()
                next_h[:, active_indices, :] = batch_next_hidden[0]
                next_c[:, active_indices, :] = batch_next_hidden[1]
                # Reset hidden state for initially done envs (will be overwritten if they reset again)
                if initial_done_indices:
                    reset_indices = torch.tensor(initial_done_indices, dtype=torch.long)
                    reset_h, reset_c = self.agent.get_initial_hidden_state(
                        len(initial_done_indices)
                    )
                    if reset_h is not None:
                        next_h[:, reset_indices, :] = reset_h
                        next_c[:, reset_indices, :] = reset_c
                next_hidden_states_device = (next_h, next_c)

        # --- Sequential Environment Stepping ---
        # step_dones_np reflects the done state *after* this step's action/reset
        step_dones_np = np.copy(self.current_dones_cpu)
        step_rewards_np = np.zeros(self.num_envs, dtype=np.float32)
        next_obs_grid_cpu = np.copy(self.current_obs_grid_cpu)
        next_obs_shapes_cpu = np.copy(self.current_obs_shapes_cpu)
        reset_indices_this_step: List[int] = (
            []
        )  # Track which envs reset *during* this step

        for i in range(self.num_envs):
            # If env was done at the start of the step (or became done due to no valid actions)
            if i in initial_done_indices:
                final_reward = 0.0
                step_offset = 0  # No step taken if already done at start
                # Check if it became done due to no valid actions *this* step
                if not self.current_dones_cpu[
                    i
                ]:  # Was not done before valid_actions check
                    final_reward = self.reward_config.PENALTY_GAME_OVER
                    log_probs_np[i] = -1e9
                    values_np[i] = 0.0
                    step_offset = 1  # Count this invalid state transition as one step

                self._record_episode_stats(
                    i, final_reward, step_offset, current_global_step
                )
                reset_indices_this_step.append(i)

                # Reset environment and get next state
                next_state_dict = self.envs[i].reset()
                next_obs_grid_cpu[i] = next_state_dict["grid"]
                next_obs_shapes_cpu[i] = next_state_dict["shapes"].flatten()
                step_dones_np[i] = False  # State after reset is not done

                # Reset episode trackers
                self.current_episode_scores[i] = 0.0
                self.current_episode_lengths[i] = 0
                self.current_episode_game_scores[i] = 0
                self.current_episode_lines_cleared[i] = 0
                # RNN state already handled during batch update or will be overwritten if it resets again
                continue  # Skip normal step logic

            # --- Normal environment step for envs that were active ---
            action_to_take = actions_np[i]
            try:
                reward, done = self.envs[i].step(action_to_take)
                step_rewards_np[i] = reward
                step_dones_np[i] = done  # Update done status based on step result

                self.current_episode_scores[i] += reward
                self.current_episode_lengths[i] += 1

                if done:
                    self._record_episode_stats(
                        i, 0.0, 0, current_global_step
                    )  # Record with reward accumulated so far
                    reset_indices_this_step.append(i)
                    next_state_dict = self.envs[i].reset()
                    step_dones_np[i] = False  # State after reset is not done

                    # Reset episode trackers
                    self.current_episode_scores[i] = 0.0
                    self.current_episode_lengths[i] = 0
                    self.current_episode_game_scores[i] = 0
                    self.current_episode_lines_cleared[i] = 0
                    # Reset RNN state for this specific env if needed
                    if (
                        self.rnn_config.USE_RNN
                        and next_hidden_states_device is not None
                    ):
                        reset_h, reset_c = self.agent.get_initial_hidden_state(1)
                        if reset_h is not None:
                            next_hidden_states_device[0][:, i : i + 1, :] = reset_h
                            next_hidden_states_device[1][:, i : i + 1, :] = reset_c
                else:  # Not done
                    next_state_dict = self.envs[i].get_state()
                    # Update current game stats if not done
                    self.current_episode_game_scores[i] = self.envs[i].game_score
                    self.current_episode_lines_cleared[i] = self.envs[
                        i
                    ].lines_cleared_this_episode

                # Store next state observation
                next_obs_grid_cpu[i] = next_state_dict["grid"]
                next_obs_shapes_cpu[i] = next_state_dict["shapes"].flatten()

            except Exception as e:
                print(f"ERROR: Env {i} step failed (Action: {action_to_take}): {e}")
                traceback.print_exc()
                step_rewards_np[i] = self.reward_config.PENALTY_GAME_OVER
                step_dones_np[i] = True  # Mark as done due to error
                reset_indices_this_step.append(i)  # Will reset next step

                # Attempt reset to get a valid next state, but keep done=True for this step
                try:
                    next_state_dict = self.envs[i].reset()
                    next_obs_grid_cpu[i] = next_state_dict["grid"]
                    next_obs_shapes_cpu[i] = next_state_dict["shapes"].flatten()
                    # Don't set step_dones_np[i] = False here, error occurred this step
                except Exception as reset_e:
                    print(f"FATAL: Env {i} failed reset after crash: {reset_e}")
                    next_obs_grid_cpu[i].fill(0)
                    next_obs_shapes_cpu[i].fill(0)

                self._record_episode_stats(
                    i, step_rewards_np[i], 1, current_global_step
                )  # Record error episode

                # Reset episode trackers
                self.current_episode_scores[i] = 0.0
                self.current_episode_lengths[i] = 0
                self.current_episode_game_scores[i] = 0
                self.current_episode_lines_cleared[i] = 0
                # Reset RNN state if needed
                if self.rnn_config.USE_RNN and next_hidden_states_device is not None:
                    reset_h, reset_c = self.agent.get_initial_hidden_state(1)
                    if reset_h is not None:
                        next_hidden_states_device[0][:, i : i + 1, :] = reset_h
                        next_hidden_states_device[1][:, i : i + 1, :] = reset_c

        # --- Store Data in RolloutStorage ---
        # Tensors for storage should reflect the state *before* the step was taken
        # and the results *of* that step.
        obs_grid_t = torch.from_numpy(self.current_obs_grid_cpu).to(
            self.rollout_storage.device
        )
        obs_shapes_t = torch.from_numpy(self.current_obs_shapes_cpu).to(
            self.rollout_storage.device
        )
        actions_t = (
            torch.from_numpy(actions_np)
            .long()
            .unsqueeze(1)
            .to(self.rollout_storage.device)
        )
        log_probs_t = (
            torch.from_numpy(log_probs_np)
            .float()
            .unsqueeze(1)
            .to(self.rollout_storage.device)
        )
        values_t = (
            torch.from_numpy(values_np)
            .float()
            .unsqueeze(1)
            .to(self.rollout_storage.device)
        )
        rewards_t = (
            torch.from_numpy(step_rewards_np)
            .float()
            .unsqueeze(1)
            .to(self.rollout_storage.device)
        )
        # Dones stored should be the result *after* the step (step_dones_np)
        # But wait, storage expects dones[t] to correspond to obs[t].
        # Let's store the dones *before* the step was taken (self.current_dones_cpu)
        # And the *next* observation goes into obs[t+1]
        dones_t_for_storage = (
            torch.from_numpy(self.current_dones_cpu)  # Done state *before* the action
            .float()
            .unsqueeze(1)
            .to(self.rollout_storage.device)
        )

        hidden_state_to_store = None
        if self.rnn_config.USE_RNN and self.current_hidden_states_device is not None:
            hidden_state_to_store = self.current_hidden_states_device[0].to(
                self.rollout_storage.device
            )

        # Insert uses self.step as the index, so obs[t], action[t], reward[t], done[t]
        self.rollout_storage.insert(
            obs_grid_t,
            obs_shapes_t,
            actions_t,
            log_probs_t,
            values_t,
            rewards_t,
            dones_t_for_storage,  # Store done state *before* action
            hidden_state_to_store,
        )

        # --- Update Collector's State for Next Iteration ---
        # The *next* observation is stored in the *next* slot in storage
        # We need to handle the step increment correctly in storage.insert
        # Assuming storage.insert places obs at index `step` and increments `step`.
        # We need to place the *next* observation into storage[step+1] (handled by after_update)

        # Update the current state for the *next* call to collect_one_step
        self.current_obs_grid_cpu = next_obs_grid_cpu
        self.current_obs_shapes_cpu = next_obs_shapes_cpu
        # ** THE FIX **: Use step_dones_np which correctly reflects the done state *after* potential resets
        self.current_dones_cpu = step_dones_np
        # print(f"[RolloutCollector Debug] End of step. Next current_dones_cpu: {self.current_dones_cpu.astype(int)}") # DEBUG

        if self.rnn_config.USE_RNN:
            self.current_hidden_states_device = next_hidden_states_device

        collection_time = time.time() - step_start_time
        sps = self.num_envs / max(1e-9, collection_time)
        self.stats_recorder.record_step(
            {
                "sps_collection": sps,
                "rollout_collection_time": collection_time,
            }
        )

        return self.num_envs

    def compute_advantages_for_storage(self):
        """Computes GAE advantages using the data in RolloutStorage."""
        with torch.no_grad():
            # Use the *current* observation (which is the state *after* the last collected step)
            # to estimate the value of the next state (V(s_T+1))
            final_obs_grid_t = torch.from_numpy(self.current_obs_grid_cpu).to(
                self.agent.device
            )
            final_obs_shapes_t = torch.from_numpy(self.current_obs_shapes_cpu).to(
                self.agent.device
            )
            final_hidden_state = None
            if (
                self.rnn_config.USE_RNN
                and self.current_hidden_states_device is not None
            ):
                final_hidden_state = (
                    self.current_hidden_states_device[0].to(self.agent.device),
                    self.current_hidden_states_device[1].to(self.agent.device),
                )

            if self.rnn_config.USE_RNN:
                final_obs_grid_t = final_obs_grid_t.unsqueeze(1)
                final_obs_shapes_t = final_obs_shapes_t.unsqueeze(1)

            _, next_value, _ = self.agent.network(
                final_obs_grid_t, final_obs_shapes_t, final_hidden_state
            )

            if self.rnn_config.USE_RNN:
                next_value = next_value.squeeze(1)

            if next_value.ndim == 1:
                next_value = next_value.unsqueeze(-1)

            # Get the dones corresponding to the *last* step taken (self.current_dones_cpu)
            # This determines if the final next_value should be masked.
            final_dones = (
                torch.from_numpy(self.current_dones_cpu)
                .float()
                .unsqueeze(1)
                .to(self.device)
            )

        # Pass the final dones to mask the next_value correctly during GAE calculation
        self.rollout_storage.compute_returns_and_advantages(
            next_value, final_dones, self.ppo_config.GAMMA, self.ppo_config.GAE_LAMBDA
        )

    def get_episode_count(self) -> int:
        return self.episode_count


File: training/__init__.py
# File: training/__init__.py
from .trainer import Trainer
from .rollout_collector import RolloutCollector
from .rollout_storage import RolloutStorage
from .checkpoint_manager import CheckpointManager

__all__ = [
    "Trainer",
    "RolloutCollector",
    "RolloutStorage",
    "CheckpointManager",
]


File: training/training_utils.py
# File: training/training_utils.py
import pygame
import numpy as np
from typing import Optional
from config import EnvConfig, VisConfig


def get_env_image_as_numpy(
    env, env_config: EnvConfig, vis_config: VisConfig
) -> Optional[np.ndarray]:
    """Renders a single environment state to a NumPy array for logging."""
    img_h = 300
    aspect_ratio = (env_config.COLS * 0.75 + 0.25) / max(1, env_config.ROWS)
    img_w = int(img_h * aspect_ratio)
    if img_w <= 0 or img_h <= 0:
        return None
    try:
        temp_surf = pygame.Surface((img_w, img_h))
        cell_w_px = img_w / (env_config.COLS * 0.75 + 0.25)
        cell_h_px = img_h / max(1, env_config.ROWS)
        temp_surf.fill(vis_config.BLACK)
        if hasattr(env, "grid") and hasattr(env.grid, "triangles"):
            for r in range(env.grid.rows):
                for c in range(env.grid.cols):
                    if r < len(env.grid.triangles) and c < len(env.grid.triangles[r]):
                        t = env.grid.triangles[r][c]
                        if t.is_death:
                            continue
                        pts = t.get_points(
                            ox=0, oy=0, cw=int(cell_w_px), ch=int(cell_h_px)
                        )
                        color = vis_config.GRAY
                        if t.is_occupied:
                            color = t.color if t.color else vis_config.RED
                        pygame.draw.polygon(temp_surf, color, pts)
        img_array = pygame.surfarray.array3d(temp_surf)
        return np.transpose(img_array, (1, 0, 2))
    except Exception as e:
        print(f"Error generating environment image for TB: {e}")
        return None


File: training/trainer.py
# File: training/trainer.py
import time
import torch
import numpy as np
import traceback
import random
from typing import List, Optional, Dict, Any, Union

from config import (
    EnvConfig,
    PPOConfig,
    RNNConfig,
    TrainConfig,
    ModelConfig,
    DEVICE,
    TensorBoardConfig,
    VisConfig,
    RewardConfig,
    TOTAL_TRAINING_STEPS,
)
from environment.game_state import GameState, StateType
from agent.ppo_agent import PPOAgent
from stats.stats_recorder import StatsRecorderBase
from utils.helpers import ensure_numpy
from .rollout_storage import RolloutStorage
from .rollout_collector import RolloutCollector
from .checkpoint_manager import CheckpointManager
from .training_utils import get_env_image_as_numpy


class Trainer:
    """Orchestrates the PPO training process."""

    def __init__(
        self,
        envs: List[GameState],
        agent: PPOAgent,
        stats_recorder: StatsRecorderBase,
        env_config: EnvConfig,
        ppo_config: PPOConfig,
        rnn_config: RNNConfig,
        train_config: TrainConfig,
        model_config: ModelConfig,
        model_save_path: str,
        load_checkpoint_path: Optional[str] = None,
    ):
        print("[Trainer-PPO] Initializing...")
        self.envs = envs
        self.agent = agent
        self.stats_recorder = stats_recorder
        self.num_envs = env_config.NUM_ENVS
        self.device = DEVICE
        self.env_config = env_config
        self.ppo_config = ppo_config
        self.rnn_config = rnn_config
        self.train_config = train_config
        self.model_config = model_config
        self.reward_config = RewardConfig()
        self.tb_config = TensorBoardConfig()
        self.vis_config = VisConfig()

        self.checkpoint_manager = CheckpointManager(
            agent=self.agent,
            model_save_path=model_save_path,
            load_checkpoint_path=load_checkpoint_path,
            device=self.device,
        )
        self.global_step, initial_episode_count = (
            self.checkpoint_manager.get_initial_state()
        )

        self.rollout_collector = RolloutCollector(
            envs=self.envs,
            agent=self.agent,
            stats_recorder=self.stats_recorder,
            env_config=self.env_config,
            ppo_config=self.ppo_config,
            rnn_config=self.rnn_config,
            reward_config=self.reward_config,
            tb_config=self.tb_config,
        )
        self.rollout_collector.episode_count = initial_episode_count

        self.last_image_log_step = (
            -self.tb_config.IMAGE_LOG_FREQ * self.ppo_config.NUM_STEPS_PER_ROLLOUT
        )
        self.last_checkpoint_step = 0

        # --- NEW: State for iterative training ---
        self.steps_collected_this_rollout = 0
        self.rollout_storage = (
            self.rollout_collector.rollout_storage
        )  # Use collector's storage
        # --- END NEW ---

        self._log_initial_state()
        print("[Trainer-PPO] Initialization complete.")

    def _log_initial_state(self):
        """Logs the state after initialization and potential loading."""
        initial_lr = self._get_current_lr()
        self.stats_recorder.record_step(
            {
                "lr": initial_lr,
                "global_step": self.global_step,
                "episode_count": self.rollout_collector.get_episode_count(),
            }
        )
        print(
            f"  -> Start Step={self.global_step}, Ep={self.rollout_collector.get_episode_count()}, LR={initial_lr:.1e}"
        )

    def _get_current_lr(self) -> float:
        """Retrieves the current learning rate from the optimizer."""
        return self.agent.optimizer.param_groups[0]["lr"]

    def _update_learning_rate(self):
        """Linearly decay learning rate if scheduler is enabled."""
        if not self.ppo_config.USE_LR_SCHEDULER:
            return

        frac = 1.0 - (self.global_step / TOTAL_TRAINING_STEPS)
        frac = max(self.ppo_config.LR_SCHEDULER_END_FRACTION, frac)
        new_lr = self.ppo_config.LEARNING_RATE * frac

        for param_group in self.agent.optimizer.param_groups:
            param_group["lr"] = new_lr

    # --- MODIFIED: Renamed and changed logic ---
    def perform_training_iteration(self):
        """Performs one step of environment interaction and potentially an agent update."""
        step_start_time = time.time()

        # Collect one step across all environments
        steps_collected_this_iter = self.rollout_collector.collect_one_step(
            self.global_step
        )
        self.global_step += steps_collected_this_iter
        self.steps_collected_this_rollout += 1  # Increment rollout step counter

        # Check if a full rollout is complete
        if self.steps_collected_this_rollout >= self.ppo_config.NUM_STEPS_PER_ROLLOUT:
            # --- Agent Update Phase ---
            update_start_time = time.time()

            # Compute advantages using the final value estimate
            self.rollout_collector.compute_advantages_for_storage()

            # Prepare data and update the agent
            self.rollout_storage.to(self.agent.device)
            update_data = self.rollout_storage.get_data_for_update()
            update_metrics = self.agent.update(update_data)

            # Reset storage and rollout counter
            self.rollout_storage.after_update()
            self.steps_collected_this_rollout = 0

            update_duration = time.time() - update_start_time

            # Update LR and log/save after the update
            self._update_learning_rate()
            self.maybe_save_checkpoint()
            self._maybe_log_image()

            # Record update-specific metrics
            self.stats_recorder.record_step(
                {
                    "update_time": update_duration,
                    "lr": self._get_current_lr(),
                    **update_metrics,
                    "global_step": self.global_step,  # Ensure global step is logged here too
                }
            )
            # --- End Agent Update Phase ---

        step_end_time = time.time()
        step_duration = step_end_time - step_start_time

        # Record step-timing metrics (happens every iteration)
        self.stats_recorder.record_step(
            {
                "step_time": step_duration,
                "num_steps_processed": steps_collected_this_iter,
                "global_step": self.global_step,
            }
        )

    # --- END MODIFIED ---

    def maybe_save_checkpoint(self, force_save=False):
        """Saves agent state based on frequency or if forced."""
        # --- MODIFIED: Checkpoint frequency based on agent updates (rollouts completed) ---
        # Calculate rollouts completed based on when steps_collected_this_rollout resets
        # This logic assumes maybe_save_checkpoint is called right after a potential update.
        save_freq_rollouts = self.train_config.CHECKPOINT_SAVE_FREQ
        steps_per_rollout = (
            self.ppo_config.NUM_STEPS_PER_ROLLOUT
        )  # Use steps per rollout directly

        # Check if an update just happened (steps_collected_this_rollout is 0)
        update_just_happened = self.steps_collected_this_rollout == 0

        if update_just_happened:
            # Calculate how many rollouts have passed since the last save
            rollouts_since_last_save = (
                self.global_step - self.last_checkpoint_step
            ) // (steps_per_rollout * self.num_envs)

            should_save_freq = (
                save_freq_rollouts > 0
                and rollouts_since_last_save >= save_freq_rollouts
            )

            if force_save or should_save_freq:
                self.checkpoint_manager.save_checkpoint(
                    self.global_step, self.rollout_collector.get_episode_count()
                )
                # Update last_checkpoint_step to the *start* of the rollout that just finished
                self.last_checkpoint_step = self.global_step - (
                    steps_per_rollout * self.num_envs
                )
        elif force_save:  # Allow forced save even if not exactly on rollout boundary
            self.checkpoint_manager.save_checkpoint(
                self.global_step, self.rollout_collector.get_episode_count()
            )
            self.last_checkpoint_step = self.global_step
        # --- END MODIFIED ---

    def _maybe_log_image(self):
        """Logs a sample environment state image to TensorBoard periodically."""
        if not self.tb_config.LOG_IMAGES or self.tb_config.IMAGE_LOG_FREQ <= 0:
            return

        # --- MODIFIED: Check image log frequency based on agent updates ---
        steps_per_rollout = self.ppo_config.NUM_STEPS_PER_ROLLOUT
        image_log_freq_steps = (
            self.tb_config.IMAGE_LOG_FREQ * steps_per_rollout * self.num_envs
        )

        # Check if an update just happened
        update_just_happened = self.steps_collected_this_rollout == 0

        if update_just_happened:
            steps_since_last = self.global_step - self.last_image_log_step
            if steps_since_last >= image_log_freq_steps:
                try:
                    env_idx = random.randrange(self.num_envs)
                    img_array = get_env_image_as_numpy(
                        self.envs[env_idx], self.env_config, self.vis_config
                    )
                    if img_array is not None:
                        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)
                        self.stats_recorder.record_image(
                            f"Environment/Sample State Env {env_idx}",
                            img_tensor,
                            self.global_step,
                        )
                        self.last_image_log_step = self.global_step
                except Exception as e:
                    print(f"Error logging environment image: {e}")
                    traceback.print_exc()
        # --- END MODIFIED ---

    def train_loop(self):
        """Main training loop until max steps."""
        print("[Trainer-PPO] Starting training loop...")
        try:
            while self.global_step < TOTAL_TRAINING_STEPS:
                # --- MODIFIED: Call iterative method ---
                self.perform_training_iteration()
                # --- END MODIFIED ---
        except KeyboardInterrupt:
            print("\n[Trainer-PPO] Training loop interrupted by user (Ctrl+C).")
        except Exception as e:
            print(f"\n[Trainer-PPO] CRITICAL ERROR in training loop: {e}")
            traceback.print_exc()
        finally:
            print("[Trainer-PPO] Training loop finished or terminated.")
            self.cleanup(save_final=True)

    def cleanup(self, save_final: bool = True):
        """Performs cleanup actions like saving final state and closing logger."""
        print("[Trainer-PPO] Cleaning up resources...")
        if save_final:
            print("[Trainer-PPO] Saving final checkpoint...")
            self.checkpoint_manager.save_checkpoint(
                self.global_step,
                self.rollout_collector.get_episode_count(),
                is_final=True,
            )
        else:
            print("[Trainer-PPO] Skipping final save as requested.")

        if hasattr(self.stats_recorder, "close"):
            try:
                self.stats_recorder.close()
            except Exception as e:
                print(f"Error closing stats recorder: {e}")

        print("[Trainer-PPO] Cleanup complete.")


File: training/rollout_storage.py
# File: training/rollout_storage.py
import torch
from typing import Optional, Tuple, Dict, List, Any
import numpy as np

from config import EnvConfig, PPOConfig, RNNConfig, DEVICE


class RolloutStorage:
    """Stores rollout data collected from parallel environments for PPO."""

    def __init__(
        self,
        num_steps: int,
        num_envs: int,
        env_config: EnvConfig,
        rnn_config: RNNConfig,
        device: torch.device,
    ):
        self.num_steps = num_steps
        self.num_envs = num_envs
        self.env_config = env_config
        self.rnn_config = rnn_config
        self.device = device

        grid_c, grid_h, grid_w = self.env_config.GRID_STATE_SHAPE
        shape_feat_dim = self.env_config.SHAPE_STATE_DIM

        # obs[t] corresponds to state s_t
        self.obs_grid = torch.zeros(
            num_steps + 1, num_envs, grid_c, grid_h, grid_w, device=self.device
        )
        self.obs_shapes = torch.zeros(
            num_steps + 1, num_envs, shape_feat_dim, device=self.device
        )
        # actions[t] is action a_t taken in state s_t
        self.actions = torch.zeros(num_steps, num_envs, 1, device=self.device).long()
        # log_probs[t] is log_prob of a_t
        self.log_probs = torch.zeros(num_steps, num_envs, 1, device=self.device)
        # rewards[t] is reward r_t received after taking a_t in s_t
        self.rewards = torch.zeros(num_steps, num_envs, 1, device=self.device)
        # dones[t] is True if s_t was a terminal state *before* action a_t was taken
        self.dones = torch.zeros(num_steps + 1, num_envs, 1, device=self.device)
        # values[t] is value V(s_t)
        self.values = torch.zeros(num_steps + 1, num_envs, 1, device=self.device)
        # returns[t] is the GAE return starting from step t
        self.returns = torch.zeros(
            num_steps, num_envs, 1, device=self.device
        )  # Only need T steps for returns/advantages

        self.hidden_states = None
        if self.rnn_config.USE_RNN:
            lstm_hidden_size = self.rnn_config.LSTM_HIDDEN_SIZE
            num_layers = self.rnn_config.LSTM_NUM_LAYERS
            # hidden_states[t] corresponds to state s_t
            self.hidden_states = torch.zeros(
                num_steps + 1,
                num_layers,
                num_envs,
                lstm_hidden_size,
                device=self.device,
            )

        self.step = 0

    def to(self, device: torch.device):
        """Move storage tensors to the specified device."""
        if self.device == device:
            return
        self.obs_grid = self.obs_grid.to(device)
        self.obs_shapes = self.obs_shapes.to(device)
        self.rewards = self.rewards.to(device)
        self.values = self.values.to(device)
        self.returns = self.returns.to(device)
        self.log_probs = self.log_probs.to(device)
        self.actions = self.actions.to(device)
        self.dones = self.dones.to(device)
        if self.hidden_states is not None:
            self.hidden_states = self.hidden_states.to(device)
        self.device = device
        print(f"[RolloutStorage] Moved tensors to {device}")

    def insert(
        self,
        obs_grid: torch.Tensor,  # s_t
        obs_shapes: torch.Tensor,  # s_t
        action: torch.Tensor,  # a_t
        log_prob: torch.Tensor,  # log_prob(a_t|s_t)
        value: torch.Tensor,  # V(s_t)
        reward: torch.Tensor,  # r_t (received after a_t)
        done: torch.Tensor,  # done state *before* a_t was taken
        hidden_state: Optional[torch.Tensor] = None,  # hidden state for s_t
    ):
        """Insert one step of data. Assumes input tensors are already on self.device."""
        if self.step >= self.num_steps:
            raise IndexError(
                f"RolloutStorage step index {self.step} out of bounds (max {self.num_steps-1})"
            )

        # Store state s_t and associated hidden state
        self.obs_grid[self.step].copy_(obs_grid)
        self.obs_shapes[self.step].copy_(obs_shapes)
        if self.hidden_states is not None and hidden_state is not None:
            self.hidden_states[self.step].copy_(hidden_state)

        # Store action a_t, its log_prob, and value V(s_t)
        self.actions[self.step].copy_(action)
        self.log_probs[self.step].copy_(log_prob)
        self.values[self.step].copy_(value)

        # Store reward r_t received after action a_t
        self.rewards[self.step].copy_(reward)

        # Store done state *before* action a_t was taken
        self.dones[self.step].copy_(done)

        # Increment step counter *after* storing data for index self.step
        self.step += 1

    def after_update(self):
        """Reset storage after PPO update, keeping the last observation (s_T+1)."""
        # s_T+1 becomes the new s_0
        self.obs_grid[0].copy_(self.obs_grid[self.num_steps])
        self.obs_shapes[0].copy_(self.obs_shapes[self.num_steps])
        # The done state corresponding to s_T+1 becomes the new dones[0]
        self.dones[0].copy_(self.dones[self.num_steps])
        if self.hidden_states is not None:
            # Hidden state for s_T+1 becomes the new hidden_states[0]
            self.hidden_states[0].copy_(self.hidden_states[self.num_steps])
        self.step = 0

    def compute_returns_and_advantages(
        self,
        next_value: torch.Tensor,  # V(s_T+1)
        final_dones: torch.Tensor,  # Done state corresponding to s_T+1
        gamma: float,
        gae_lambda: float,
    ):
        """Computes returns and GAE advantages. Assumes inputs are on self.device."""
        if self.step != self.num_steps:
            print(
                f"Warning: Computing returns before storage is full (step={self.step}, num_steps={self.num_steps})"
            )

        # Store V(s_T+1) and done_T+1 in the last slots
        self.values[self.num_steps] = next_value.to(self.device)
        self.dones[self.num_steps] = final_dones.to(self.device)

        gae = 0.0
        for step in reversed(range(self.num_steps)):
            # delta_t = r_t + gamma * V(s_t+1) * (1 - done_t+1) - V(s_t)
            # Note: dones[step+1] corresponds to the done state *after* step t, which is needed here.
            delta = (
                self.rewards[step]
                + gamma * self.values[step + 1] * (1.0 - self.dones[step + 1])
                - self.values[step]
            )
            # gae_t = delta_t + gamma * lambda * gae_{t+1} * (1 - done_t+1)
            gae = delta + gamma * gae_lambda * gae * (1.0 - self.dones[step + 1])
            # return_t = gae_t + V(s_t)
            self.returns[step] = gae + self.values[step]

    def get_data_for_update(self) -> Dict[str, Any]:
        """
        Returns collected data prepared for PPO update iterations.
        Data is returned as flattened tensors [N = T*B, ...].
        """
        # Advantages A_t = GAE_t = Return_t - V(s_t)
        advantages = self.returns[: self.num_steps] - self.values[: self.num_steps]
        num_samples = self.num_steps * self.num_envs

        def _flatten(tensor: torch.Tensor) -> torch.Tensor:
            # Reshape from [T, B, ...] to [T*B, ...]
            return tensor.reshape(num_samples, *tensor.shape[2:])

        data = {
            # s_t (observations for steps 0 to T-1)
            "obs_grid": _flatten(self.obs_grid[: self.num_steps]),
            "obs_shapes": _flatten(self.obs_shapes[: self.num_steps]),
            # a_t (actions for steps 0 to T-1)
            "actions": _flatten(self.actions).squeeze(-1),
            # log_prob(a_t|s_t)
            "log_probs": _flatten(self.log_probs).squeeze(-1),
            # V(s_t) (values for steps 0 to T-1)
            "values": _flatten(self.values[: self.num_steps]).squeeze(-1),
            # Return_t (returns for steps 0 to T-1)
            "returns": _flatten(self.returns[: self.num_steps]).squeeze(-1),
            # Advantage_t (advantages for steps 0 to T-1)
            "advantages": _flatten(advantages).squeeze(-1),
            # Initial hidden state for the sequence (hidden_state_0)
            "hidden_states": (
                self.hidden_states[0] if self.hidden_states is not None else None
            ),
            # Dones mask for RNN: dones[t] is True if s_t was terminal.
            # Reshape from [T+1, B, 1] to [B, T] for steps 0 to T-1
            "dones": self.dones[: self.num_steps].permute(1, 0, 2).squeeze(-1),
        }
        return data


File: training/checkpoint_manager.py
# File: training/checkpoint_manager.py
import os
import torch
import traceback
from typing import Optional, Dict, Any, Tuple

from agent.ppo_agent import PPOAgent


class CheckpointManager:
    """Handles loading and saving of PPO agent states."""

    def __init__(
        self,
        agent: PPOAgent,
        model_save_path: str,
        load_checkpoint_path: Optional[str],
        device: torch.device,
    ):
        self.agent = agent
        self.model_save_path = model_save_path
        self.device = device

        self.global_step = 0
        self.episode_count = 0

        if load_checkpoint_path:
            self.load_agent_checkpoint(load_checkpoint_path)
        else:
            print(
                "[CheckpointManager-PPO] No agent checkpoint specified, starting fresh."
            )

    def load_agent_checkpoint(self, path_to_load: str):
        if not os.path.isfile(path_to_load):
            print(
                f"[CheckpointManager-PPO] LOAD WARNING: Agent ckpt not found: {path_to_load}"
            )
            return
        print(f"[CheckpointManager-PPO] Loading agent checkpoint from: {path_to_load}")
        try:
            checkpoint = torch.load(path_to_load, map_location=self.device)
            self.agent.load_state_dict(checkpoint)

            self.global_step = checkpoint.get("global_step", 0)
            self.episode_count = checkpoint.get("episode_count", 0)
            print(
                f"  -> Resuming from Step: {self.global_step}, Ep: {self.episode_count}"
            )
        except KeyError as e:
            print(
                f"  -> ERROR loading agent checkpoint: Missing key '{e}'. Check compatibility."
            )
            self.global_step = 0
            self.episode_count = 0
        except Exception as e:
            print(f"  -> ERROR loading agent checkpoint ('{e}'). Check compatibility.")
            traceback.print_exc()
            self.global_step = 0
            self.episode_count = 0

    def save_checkpoint(
        self, global_step: int, episode_count: int, is_final: bool = False
    ):
        prefix = "FINAL" if is_final else f"step_{global_step}"
        save_dir = os.path.dirname(self.model_save_path)
        os.makedirs(save_dir, exist_ok=True)

        print(f"[CheckpointManager-PPO] Saving agent checkpoint ({prefix})...")
        try:
            agent_save_data = self.agent.get_state_dict()
            agent_save_data["global_step"] = global_step
            agent_save_data["episode_count"] = episode_count
            torch.save(agent_save_data, self.model_save_path)
            print(
                f"  -> Agent checkpoint saved: {os.path.basename(self.model_save_path)}"
            )
        except Exception as e:
            print(f"  -> ERROR saving agent checkpoint: {e}")
            traceback.print_exc()

    def get_initial_state(self) -> Tuple[int, int]:
        return self.global_step, self.episode_count


File: utils/__init__.py


File: utils/init_checks.py
# File: utils/init_checks.py
# --- Pre-Run Sanity Checks ---
import sys
import traceback
import numpy as np
from config import EnvConfig  # Import config class
from environment.game_state import GameState


def run_pre_checks() -> bool:
    """Performs basic checks on GameState and configuration compatibility."""
    print("--- Pre-Run Checks ---")
    try:
        print("Checking GameState and Configuration Compatibility...")
        env_config_instance = EnvConfig()  # Instantiate

        gs_test = GameState()
        gs_test.reset()
        s_test_dict = gs_test.get_state()

        if not isinstance(s_test_dict, dict):
            raise TypeError(
                f"GameState.get_state() should return a dict, but got {type(s_test_dict)}"
            )
        print("GameState state type check PASSED (returned dict).")

        # --- MODIFIED: Check grid shape (2 channels) ---
        if "grid" not in s_test_dict:
            raise KeyError("State dictionary missing 'grid' key.")
        grid_state = s_test_dict["grid"]
        expected_grid_shape = (
            env_config_instance.GRID_STATE_SHAPE
        )  # Uses property (2, H, W)
        if not isinstance(grid_state, np.ndarray):
            raise TypeError(
                f"State 'grid' component should be numpy array, but got {type(grid_state)}"
            )
        if grid_state.shape != expected_grid_shape:
            raise ValueError(
                f"State 'grid' shape mismatch! GameState:{grid_state.shape}, EnvConfig:{expected_grid_shape}"
            )
        print(f"GameState 'grid' state shape check PASSED (Shape: {grid_state.shape}).")
        # --- END MODIFIED ---

        # Check 'shapes' component (unchanged)
        if "shapes" not in s_test_dict:
            raise KeyError("State dictionary missing 'shapes' key.")
        shape_state = s_test_dict["shapes"]
        expected_shape_shape = (
            env_config_instance.NUM_SHAPE_SLOTS,
            env_config_instance.SHAPE_FEATURES_PER_SHAPE,
        )
        if not isinstance(shape_state, np.ndarray):
            raise TypeError(
                f"State 'shapes' component should be numpy array, but got {type(shape_state)}"
            )
        if shape_state.shape != expected_shape_shape:
            raise ValueError(
                f"State 'shapes' shape mismatch! GameState:{shape_state.shape}, EnvConfig:{expected_shape_shape}"
            )
        print(
            f"GameState 'shapes' state shape check PASSED (Shape: {shape_state.shape})."
        )

        _ = gs_test.valid_actions()
        print("GameState valid_actions check PASSED.")
        if not hasattr(gs_test, "game_score"):
            raise AttributeError("GameState missing 'game_score' attribute!")
        print("GameState 'game_score' attribute check PASSED.")
        if not hasattr(gs_test, "lines_cleared_this_episode"):
            raise AttributeError(
                "GameState missing 'lines_cleared_this_episode' attribute!"
            )
        print("GameState 'lines_cleared_this_episode' attribute check PASSED.")

        del gs_test
        print("--- Pre-Run Checks Complete ---")
        return True
    except (NameError, ImportError) as e:
        print(f"FATAL ERROR: Import/Name error: {e}")
    except (ValueError, AttributeError, TypeError, KeyError) as e:
        print(f"FATAL ERROR during pre-run checks: {e}")
    except Exception as e:
        print(f"FATAL ERROR during GameState pre-check: {e}")
        traceback.print_exc()
    sys.exit(1)


File: utils/types.py
# File: utils/types.py
from typing import NamedTuple, Union, Tuple, List, Dict, Any, Optional
import numpy as np
import torch

StateType = Dict[str, np.ndarray]
ActionType = int
AgentStateDict = Dict[str, Any]


File: utils/helpers.py
# File: utils/helpers.py
import torch
import numpy as np
import random
import os
import pickle
import cloudpickle
from typing import Union, Any


def get_device() -> torch.device:
    """Gets the appropriate torch device (MPS, CUDA, or CPU)."""
    force_cpu = os.environ.get("FORCE_CPU", "false").lower() == "true"
    if force_cpu:
        print("Forcing CPU device based on environment variable.")
        return torch.device("cpu")

    if torch.backends.mps.is_available():
        device_str = "mps"
    elif torch.cuda.is_available():
        device_str = "cuda"
    else:
        device_str = "cpu"

    print(f"Using device: {device_str.upper()}")
    if device_str == "cuda":
        print(f"CUDA Device Name: {torch.cuda.get_device_name(0)}")
    elif device_str == "mps":
        print("MPS device found on MacOS.")
    return torch.device(device_str)


def set_random_seeds(seed: int = 42):
    """Sets random seeds for Python, NumPy, and PyTorch."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # Note: Setting deterministic algorithms can impact performance
        # torch.backends.cudnn.deterministic = True
        # torch.backends.cudnn.benchmark = False
    print(f"Set random seeds to {seed}")


def ensure_numpy(data: Union[np.ndarray, list, tuple, torch.Tensor]) -> np.ndarray:
    """Ensures the input data is a numpy array with float32 type."""
    try:
        if isinstance(data, np.ndarray):
            if data.dtype != np.float32:
                return data.astype(np.float32)
            return data
        elif isinstance(data, torch.Tensor):
            return data.detach().cpu().numpy().astype(np.float32)
        elif isinstance(data, (list, tuple)):
            arr = np.array(data, dtype=np.float32)
            if arr.dtype == np.object_:  # Indicates ragged array
                raise ValueError(
                    "Cannot convert ragged list/tuple to float32 numpy array."
                )
            return arr
        else:
            # Attempt conversion for single numbers or other types
            return np.array([data], dtype=np.float32)
    except (ValueError, TypeError, RuntimeError) as e:
        print(
            f"CRITICAL ERROR in ensure_numpy conversion: {e}. Input type: {type(data)}. Data (partial): {str(data)[:100]}"
        )
        raise ValueError(f"ensure_numpy failed: {e}") from e


def save_object(obj: Any, filepath: str):
    """Saves an arbitrary Python object to a file using cloudpickle."""
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, "wb") as f:
            cloudpickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)
    except Exception as e:
        print(f"Error saving object to {filepath}: {e}")
        raise e  # Re-raise after logging


def load_object(filepath: str) -> Any:
    """Loads a Python object from a file using cloudpickle."""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found for loading: {filepath}")
    try:
        with open(filepath, "rb") as f:
            obj = cloudpickle.load(f)
        return obj
    except Exception as e:
        print(f"Error loading object from {filepath}: {e}")
        raise e  # Re-raise after logging


File: agent/__init__.py
# File: agent/__init__.py
from .ppo_agent import PPOAgent
from .model_factory import create_network
from agent.networks.agent_network import ActorCriticNetwork

__all__ = [
    "PPOAgent",
    "create_network",
    "ActorCriticNetwork",
]


File: agent/model_factory.py
# File: agent/model_factory.py
import torch.nn as nn
from config import ModelConfig, EnvConfig, PPOConfig, RNNConfig
from typing import Type

from agent.networks.agent_network import ActorCriticNetwork


def create_network(
    env_config: EnvConfig,
    action_dim: int,
    model_config: ModelConfig,
    rnn_config: RNNConfig,
) -> nn.Module:
    """Creates the ActorCriticNetwork based on configuration."""
    print(f"[ModelFactory] Creating ActorCriticNetwork (RNN: {rnn_config.USE_RNN})")
    return ActorCriticNetwork(
        env_config=env_config,
        action_dim=action_dim,
        model_config=model_config.Network,
        rnn_config=rnn_config,
    )


File: agent/ppo_agent.py
# File: agent/ppo_agent.py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import numpy as np
import traceback
from typing import Tuple, List, Dict, Any, Optional, Union

from config import (
    ModelConfig,
    EnvConfig,
    PPOConfig,
    RNNConfig,
    DEVICE,
    TensorBoardConfig,
    TOTAL_TRAINING_STEPS,
)
from environment.game_state import StateType
from utils.types import ActionType, AgentStateDict
from agent.model_factory import create_network
from agent.networks.agent_network import ActorCriticNetwork


class PPOAgent:
    """PPO Agent orchestrating network, action selection, and updates."""

    def __init__(
        self,
        model_config: ModelConfig,
        ppo_config: PPOConfig,
        rnn_config: RNNConfig,
        env_config: EnvConfig,
    ):
        print("[PPOAgent] Initializing...")
        self.device = DEVICE
        self.env_config = env_config
        self.ppo_config = ppo_config
        self.rnn_config = rnn_config
        self.tb_config = TensorBoardConfig()
        self.action_dim = env_config.ACTION_DIM

        self.network = create_network(
            env_config=self.env_config,
            action_dim=self.action_dim,
            model_config=model_config,
            rnn_config=self.rnn_config,
        ).to(self.device)

        self.optimizer = optim.AdamW(
            self.network.parameters(),
            lr=ppo_config.LEARNING_RATE,
            eps=ppo_config.ADAM_EPS,
        )

        self._print_init_info()

    def _print_init_info(self):
        print(f"[PPOAgent] Using Device: {self.device}")
        print(f"[PPOAgent] Network: {type(self.network).__name__}")
        print(f"[PPOAgent] Using RNN: {self.rnn_config.USE_RNN}")
        total_params = sum(
            p.numel() for p in self.network.parameters() if p.requires_grad
        )
        print(f"[PPOAgent] Trainable Parameters: {total_params / 1e6:.2f} M")

    @torch.no_grad()
    def select_action(
        self,
        state: StateType,
        hidden_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        deterministic: bool = False,
        valid_actions_indices: Optional[List[ActionType]] = None,
    ) -> Tuple[ActionType, float, float, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """
        Selects action for a SINGLE environment. Use select_action_batch for multiple.
        Returns action, log_prob, value, next_hidden.
        """
        self.network.eval()

        grid_np = state["grid"]
        shapes_np = state["shapes"]
        # Create batch of 1
        grid_t = torch.from_numpy(grid_np).float().unsqueeze(0).to(self.device)
        shapes_t = (
            torch.from_numpy(shapes_np.flatten()).float().unsqueeze(0).to(self.device)
        )

        # Add sequence dimension of 1 if using RNN
        if self.rnn_config.USE_RNN:
            grid_t = grid_t.unsqueeze(1)  # [1, 1, C, H, W]
            shapes_t = shapes_t.unsqueeze(1)  # [1, 1, F]
            if hidden_state:  # Ensure hidden state has batch dim 1
                hidden_state = (hidden_state[0][:, 0:1, :], hidden_state[1][:, 0:1, :])

        policy_logits, value, next_hidden_state = self.network(
            grid_t, shapes_t, hidden_state
        )

        # Remove sequence dimension if added
        if self.rnn_config.USE_RNN:
            policy_logits = policy_logits.squeeze(1)  # [1, T, A] -> [1, A]
            value = value.squeeze(1)  # [1, T, 1] -> [1, 1]

        policy_logits = torch.nan_to_num(
            policy_logits.squeeze(0), nan=-1e9
        )  # Squeeze batch dim -> [A]

        if valid_actions_indices is not None:
            mask = torch.full_like(policy_logits, -float("inf"))
            valid_indices_in_bounds = [
                idx
                for idx in valid_actions_indices
                if 0 <= idx < policy_logits.shape[0]
            ]
            if valid_indices_in_bounds:
                mask[valid_indices_in_bounds] = 0
                policy_logits += mask
            # Handle no valid actions case below

        if torch.all(policy_logits == -float("inf")):
            # print("Warning: No valid actions found in select_action. Returning default 0.")
            return 0, -1e9, value.squeeze().item(), next_hidden_state

        distribution = Categorical(logits=policy_logits)

        action_tensor = distribution.mode if deterministic else distribution.sample()
        action_log_prob = distribution.log_prob(action_tensor)
        action = action_tensor.item()

        # Fallback check (less likely now with batch method handling this)
        if valid_actions_indices is not None and action not in valid_actions_indices:
            # print(f"Warning: Sampled invalid action {action} in single select. Falling back.")
            if valid_indices_in_bounds:
                action = np.random.choice(valid_indices_in_bounds)
                action_log_prob = distribution.log_prob(
                    torch.tensor(action, device=self.device)
                )
            else:
                action = 0
                action_log_prob = torch.tensor(-1e9, device=self.device)

        return action, action_log_prob.item(), value.squeeze().item(), next_hidden_state

    @torch.no_grad()
    def select_action_batch(
        self,
        grid_batch: torch.Tensor,  # Shape [B, C, H, W]
        shape_batch: torch.Tensor,  # Shape [B, F]
        hidden_state_batch: Optional[Tuple[torch.Tensor, torch.Tensor]],
        valid_actions_lists: List[
            Optional[List[ActionType]]
        ],  # One list per env in batch
    ) -> Tuple[
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        Optional[Tuple[torch.Tensor, torch.Tensor]],
    ]:
        """
        Selects actions for a BATCH of environments.
        Returns batched actions, log_probs, values, next_hidden_states.
        """
        self.network.eval()
        batch_size = grid_batch.shape[0]

        # Ensure tensors are on the correct device
        grid_batch = grid_batch.to(self.device)
        shape_batch = shape_batch.to(self.device)
        if hidden_state_batch:
            hidden_state_batch = (
                hidden_state_batch[0].to(self.device),
                hidden_state_batch[1].to(self.device),
            )

        # Add sequence dimension of 1 if using RNN
        if self.rnn_config.USE_RNN:
            grid_batch = grid_batch.unsqueeze(1)  # [B, 1, C, H, W]
            shape_batch = shape_batch.unsqueeze(1)  # [B, 1, F]

        policy_logits, value, next_hidden_batch = self.network(
            grid_batch, shape_batch, hidden_state_batch
        )

        # Remove sequence dimension if added
        if self.rnn_config.USE_RNN:
            policy_logits = policy_logits.squeeze(1)  # [B, 1, A] -> [B, A]
            value = value.squeeze(1)  # [B, 1, 1] -> [B, 1]

        policy_logits = torch.nan_to_num(policy_logits, nan=-1e9)

        # Apply individual masks
        mask = torch.full_like(policy_logits, -float("inf"))
        any_valid = False
        for i in range(batch_size):
            valid_actions = valid_actions_lists[i]
            if valid_actions:
                valid_indices_in_bounds = [
                    idx for idx in valid_actions if 0 <= idx < self.action_dim
                ]
                if valid_indices_in_bounds:
                    mask[i, valid_indices_in_bounds] = 0
                    any_valid = True

        # Only apply mask if at least one environment had valid actions
        if any_valid:
            policy_logits += mask
        else:
            # If no env had valid actions, all logits might be -inf.
            # Categorical handles this, but sampling is ill-defined.
            pass

        # Handle cases where all actions for a specific env might be masked out
        # Set logits for such envs to a uniform distribution (0) to allow sampling a default
        all_masked_rows = torch.all(policy_logits == -float("inf"), dim=1)
        policy_logits[all_masked_rows] = 0.0

        distribution = Categorical(logits=policy_logits)
        actions_tensor = distribution.sample()
        action_log_probs = distribution.log_prob(actions_tensor)

        # Correct actions/log_probs for envs that had no valid actions initially
        # Assign action 0 and low log_prob to rows where all were masked
        actions_tensor[all_masked_rows] = 0
        action_log_probs[all_masked_rows] = -1e9

        return actions_tensor, action_log_probs, value.squeeze(-1), next_hidden_batch

    def evaluate_actions(
        self,
        grid_tensor: torch.Tensor,  # Shape [N, C, H, W] or [B, T, C, H, W]
        shape_tensor: torch.Tensor,  # Shape [N, F] or [B, T, F]
        actions: torch.Tensor,  # Shape [N] or [B, T]
        hidden_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        dones_tensor: Optional[torch.Tensor] = None,  # Shape [B, T] if RNN
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Evaluates actions given states, returns log_probs, values, entropy.
        Handles sequences for RNNs correctly. N = Batch*Time.
        Output shapes: log_probs [N], values [N], entropy [N]
        """
        self.network.train()
        is_sequence = self.rnn_config.USE_RNN and grid_tensor.ndim == 5

        if is_sequence:
            batch_size = grid_tensor.shape[0]
            seq_len = grid_tensor.shape[1]
            grid_tensor = grid_tensor.view(batch_size * seq_len, *grid_tensor.shape[2:])
            shape_tensor = shape_tensor.view(batch_size * seq_len, -1)
            actions = actions.view(-1)  # Flatten actions to [N]

            # Initial hidden state for the sequence evaluation
            if hidden_state is None:
                hidden_state = self.network.get_initial_hidden_state(batch_size)
            else:  # Ensure it's on the right device
                hidden_state = (
                    hidden_state[0].to(self.device),
                    hidden_state[1].to(self.device),
                )

            # Need to pass sequence information to the network if it handles resets internally
            # Assuming network handles sequence processing correctly now
            policy_logits, value, _ = self.network(
                grid_tensor.unsqueeze(
                    0
                ),  # Add batch dim back temporarily if needed by network? No, process flat.
                shape_tensor.unsqueeze(0),
                hidden_state=None,  # Pass flat tensor, RNN layer expects [Seq, Batch, Feat] or [Batch, Seq, Feat]
                # Let's assume network handles flat input if RNN enabled. THIS NEEDS CHECKING in Network.
            )
            # TODO: Fix RNN sequence handling in evaluate_actions.
            # The current network forward expects batched sequences [B, T, ...].
            # The rollout storage flattens to [N = B*T, ...].
            # We need to reshape back to [B, T, ...] before passing to network IF RNN is used.

            # --- Temporary Fix: Assuming stateless evaluation during update for now ---
            policy_logits, value, _ = self.network(
                grid_tensor, shape_tensor, hidden_state=None
            )

        else:  # Not using RNN or already flat batch
            policy_logits, value, _ = self.network(
                grid_tensor, shape_tensor, hidden_state=None
            )  # No hidden state needed

        policy_logits = torch.nan_to_num(policy_logits, nan=-1e9)
        distribution = Categorical(logits=policy_logits)

        # Ensure actions has the right shape for log_prob (expecting [N])
        action_log_probs = distribution.log_prob(actions)
        entropy = distribution.entropy()

        # Value should be squeezed to [N]
        value = value.squeeze(-1) if value.ndim > 1 and value.shape[-1] == 1 else value

        return action_log_probs, value, entropy  # Return flat tensors [N]

    def update(self, rollout_data: Dict[str, Any]) -> Dict[str, float]:
        """Performs PPO update using data from the rollout buffer."""
        self.network.train()

        # Data comes flattened from storage [N = B*T, ...]
        obs_grid = rollout_data["obs_grid"]
        obs_shapes = rollout_data["obs_shapes"]
        actions = rollout_data["actions"]  # Shape [N]
        old_log_probs = rollout_data["log_probs"]  # Shape [N]
        returns = rollout_data["returns"]  # Shape [N]
        advantages = rollout_data["advantages"]  # Shape [N]
        # RNN specific data (currently unused in evaluate_actions fix above)
        # hidden_states_initial = rollout_data.get("hidden_states", None) # Shape [L, B, H]
        # dones = rollout_data.get("dones", None) # Shape [B, T]

        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        total_policy_loss = 0.0
        total_value_loss = 0.0
        total_entropy = 0.0
        num_updates = 0

        num_samples = actions.shape[0]
        indices = np.arange(num_samples)

        for _ in range(self.ppo_config.PPO_EPOCHS):
            np.random.shuffle(indices)
            for start_idx in range(0, num_samples, self.ppo_config.MINIBATCH_SIZE):
                end_idx = start_idx + self.ppo_config.MINIBATCH_SIZE
                minibatch_indices = indices[start_idx:end_idx]

                mb_obs_grid = obs_grid[minibatch_indices]
                mb_obs_shapes = obs_shapes[minibatch_indices]
                mb_actions = actions[minibatch_indices]
                mb_old_log_probs = old_log_probs[minibatch_indices]
                mb_returns = returns[minibatch_indices]
                mb_advantages = advantages[minibatch_indices]

                # TODO: Pass RNN state and dones if evaluate_actions is fixed for sequences
                mb_hidden_state = None
                mb_dones = None

                new_log_probs, predicted_values, entropy = self.evaluate_actions(
                    mb_obs_grid, mb_obs_shapes, mb_actions, mb_hidden_state, mb_dones
                )

                # --- Loss Calculation (Shapes: [MB_SIZE]) ---
                ratio = torch.exp(new_log_probs - mb_old_log_probs)
                surr1 = ratio * mb_advantages
                surr2 = (
                    torch.clamp(
                        ratio,
                        1.0 - self.ppo_config.CLIP_PARAM,
                        1.0 + self.ppo_config.CLIP_PARAM,
                    )
                    * mb_advantages
                )
                policy_loss = -torch.min(surr1, surr2).mean()

                # Simple Value Loss (MSE)
                value_loss = F.mse_loss(predicted_values, mb_returns)

                entropy_loss = -entropy.mean()

                loss = (
                    policy_loss
                    + self.ppo_config.VALUE_LOSS_COEF * value_loss
                    + self.ppo_config.ENTROPY_COEF * entropy_loss
                )

                self.optimizer.zero_grad()
                loss.backward()
                if self.ppo_config.MAX_GRAD_NORM > 0:
                    nn.utils.clip_grad_norm_(
                        self.network.parameters(), self.ppo_config.MAX_GRAD_NORM
                    )
                self.optimizer.step()

                total_policy_loss += policy_loss.item()
                total_value_loss += value_loss.item()
                total_entropy += -entropy_loss.item()
                num_updates += 1

        avg_policy_loss = total_policy_loss / max(1, num_updates)
        avg_value_loss = total_value_loss / max(1, num_updates)
        avg_entropy = total_entropy / max(1, num_updates)

        return {
            "policy_loss": avg_policy_loss,
            "value_loss": avg_value_loss,
            "entropy": avg_entropy,
        }

    def get_state_dict(self) -> AgentStateDict:
        original_device = next(self.network.parameters()).device
        self.network.cpu()
        state = {
            "network_state_dict": self.network.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            # Add other agent state if necessary (e.g., exploration schedule state)
        }
        self.network.to(original_device)
        return state

    def load_state_dict(self, state_dict: AgentStateDict):
        print(f"[PPOAgent] Loading state dict. Target device: {self.device}")
        try:
            self.network.load_state_dict(state_dict["network_state_dict"])
            self.network.to(self.device)
            print("[PPOAgent] Network state loaded.")

            if "optimizer_state_dict" in state_dict:
                try:
                    # Need to re-initialize optimizer with current network params *before* loading state
                    self.optimizer = optim.AdamW(
                        self.network.parameters(),
                        lr=self.ppo_config.LEARNING_RATE,  # Use config LR, scheduler will adjust
                        eps=self.ppo_config.ADAM_EPS,
                    )
                    self.optimizer.load_state_dict(state_dict["optimizer_state_dict"])
                    # Move optimizer states to the correct device
                    for state in self.optimizer.state.values():
                        for k, v in state.items():
                            if isinstance(v, torch.Tensor):
                                state[k] = v.to(self.device)
                    print("[PPOAgent] Optimizer state loaded and moved to device.")
                except Exception as e:
                    print(
                        f"Warning: Could not load optimizer state ({e}). Re-initializing optimizer."
                    )
                    self.optimizer = optim.AdamW(
                        self.network.parameters(),
                        lr=self.ppo_config.LEARNING_RATE,
                        eps=self.ppo_config.ADAM_EPS,
                    )
            else:
                print(
                    "[PPOAgent] Optimizer state not found. Re-initializing optimizer."
                )
                self.optimizer = optim.AdamW(
                    self.network.parameters(),
                    lr=self.ppo_config.LEARNING_RATE,
                    eps=self.ppo_config.ADAM_EPS,
                )

            print("[PPOAgent] load_state_dict complete.")

        except Exception as e:
            print(f"CRITICAL ERROR during PPOAgent.load_state_dict: {e}")
            traceback.print_exc()

    def get_initial_hidden_state(
        self, num_envs: int
    ) -> Optional[Tuple[torch.Tensor, torch.Tensor]]:
        """Gets the initial hidden state for a batch of environments."""
        if not self.rnn_config.USE_RNN:
            return None
        return self.network.get_initial_hidden_state(
            num_envs
        )  # Network handles device placement


File: agent/networks/noisy_layer.py
# File: agent/networks/noisy_layer.py
# (No changes needed, already clean)
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional


class NoisyLinear(nn.Module):
    """
    Noisy Linear Layer for Noisy Network (Factorised Gaussian Noise).
    """

    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        # Learnable weights and biases (mean parameters)
        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_mu = nn.Parameter(torch.empty(out_features))

        # Learnable noise parameters (standard deviation parameters)
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))

        # Non-learnable noise buffers
        self.register_buffer("weight_epsilon", torch.empty(out_features, in_features))
        self.register_buffer("bias_epsilon", torch.empty(out_features))

        self.reset_parameters()
        self.reset_noise()  # Initial noise generation

    def reset_parameters(self):
        """Initialize mean and std parameters."""
        mu_range = 1.0 / math.sqrt(self.in_features)
        nn.init.uniform_(self.weight_mu, -mu_range, mu_range)
        nn.init.uniform_(self.bias_mu, -mu_range, mu_range)

        # Initialize sigma parameters (std dev)
        nn.init.constant_(
            self.weight_sigma, self.std_init / math.sqrt(self.in_features)
        )
        nn.init.constant_(self.bias_sigma, self.std_init / math.sqrt(self.out_features))

    def reset_noise(self):
        """Generate new noise samples using Factorised Gaussian noise."""
        epsilon_in = self._scale_noise(self.in_features)
        epsilon_out = self._scale_noise(self.out_features)

        # Outer product for weight noise, direct sample for bias noise
        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))
        self.bias_epsilon.copy_(epsilon_out)

    def _scale_noise(self, size: int) -> torch.Tensor:
        """Generate noise tensor with sign-sqrt transformation."""
        x = torch.randn(size, device=self.weight_mu.device)  # Noise on same device
        return x.sign().mul(x.abs().sqrt())

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass with noisy parameters if training, mean parameters otherwise."""
        if self.training:
            # Sample noise is implicitly used via weight_epsilon, bias_epsilon
            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon
            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon
            # Reset noise *after* use in forward pass for next iteration?
            # Or reset in train() method? Resetting in train() is common.
        else:
            # Use mean parameters during evaluation
            weight = self.weight_mu
            bias = self.bias_mu

        return F.linear(x, weight, bias)

    def train(self, mode: bool = True):
        """Override train mode to reset noise when entering training."""
        if self.training is False and mode is True:  # If switching from eval to train
            self.reset_noise()
        super().train(mode)


File: agent/networks/__init__.py


File: agent/networks/agent_network.py
# File: agent/networks/agent_network.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

from config import ModelConfig, EnvConfig, PPOConfig, RNNConfig, DEVICE
from typing import Tuple, List, Type, Optional


class ActorCriticNetwork(nn.Module):
    """
    Actor-Critic Network: CNN+MLP -> Fusion -> Optional LSTM -> Actor/Critic Heads.
    Handles both single step (eval) and sequence (RNN training) inputs.
    """

    def __init__(
        self,
        env_config: EnvConfig,
        action_dim: int,
        model_config: ModelConfig.Network,
        rnn_config: RNNConfig,
    ):
        super().__init__()
        self.action_dim = action_dim
        self.env_config = env_config
        self.config = model_config
        self.rnn_config = rnn_config
        self.device = DEVICE  # Store target device

        print(f"[ActorCriticNetwork] Target device set to: {self.device}")
        print(f"[ActorCriticNetwork] Using RNN: {self.rnn_config.USE_RNN}")

        self.grid_c, self.grid_h, self.grid_w = self.env_config.GRID_STATE_SHAPE
        self.shape_feat_dim = self.env_config.SHAPE_STATE_DIM
        self.num_shape_slots = self.env_config.NUM_SHAPE_SLOTS
        self.shape_feat_per_slot = self.env_config.SHAPE_FEATURES_PER_SHAPE

        print(f"[ActorCriticNetwork] Initializing:")
        print(f"  Input Grid Shape: [B, {self.grid_c}, {self.grid_h}, {self.grid_w}]")
        print(f"  Input Shape Features Dim: {self.shape_feat_dim}")

        # --- Build network components ---
        # Note: Layers are implicitly moved to self.device during initialization below
        self.conv_base, conv_out_h, conv_out_w, conv_out_c = self._build_cnn_branch()
        self.conv_out_size = self._get_conv_out_size(
            (self.grid_c, self.grid_h, self.grid_w)
        )
        print(
            f"  CNN Output Dim (HxWxC): ({conv_out_h}x{conv_out_w}x{conv_out_c}) -> Flat: {self.conv_out_size}"
        )

        self.shape_mlp, self.shape_mlp_out_dim = self._build_shape_mlp_branch()
        print(f"  Shape MLP Output Dim: {self.shape_mlp_out_dim}")

        combined_features_dim = self.conv_out_size + self.shape_mlp_out_dim
        print(f"  Combined Features Dim: {combined_features_dim}")

        self.fusion_mlp, self.fusion_output_dim = self._build_fusion_mlp_branch(
            combined_features_dim
        )
        print(f"  Fusion MLP Output Dim: {self.fusion_output_dim}")

        self.lstm_layer = None
        self.lstm_hidden_size = 0
        if self.rnn_config.USE_RNN:
            self.lstm_hidden_size = self.rnn_config.LSTM_HIDDEN_SIZE
            # LSTM layer will be explicitly moved to device
            self.lstm_layer = nn.LSTM(
                input_size=self.fusion_output_dim,
                hidden_size=self.lstm_hidden_size,
                num_layers=self.rnn_config.LSTM_NUM_LAYERS,
                batch_first=True,  # Expect [Batch, Seq, Feature]
            ).to(self.device)
            print(f"  LSTM Layer Added (Hidden Size: {self.lstm_hidden_size})")
            head_input_dim = self.lstm_hidden_size
        else:
            head_input_dim = self.fusion_output_dim

        # Heads will be explicitly moved to device
        self.actor_head = nn.Linear(head_input_dim, self.action_dim).to(self.device)
        self.critic_head = nn.Linear(head_input_dim, 1).to(self.device)
        print(f"  Actor Head Output Dim: {self.action_dim}")
        print(f"  Critic Head Output Dim: 1")

        self._init_head_weights()

    def _init_head_weights(self):
        """Initialize actor and critic head weights."""
        print("  Initializing Actor/Critic heads using Xavier Uniform.")
        # Orthogonal initialization is often preferred for policy/value heads
        # gain_actor = np.sqrt(2)
        # gain_critic = 1.0
        # nn.init.orthogonal_(self.actor_head.weight, gain=gain_actor)
        # nn.init.constant_(self.actor_head.bias, 0)
        # nn.init.orthogonal_(self.critic_head.weight, gain=gain_critic)
        # nn.init.constant_(self.critic_head.bias, 0)

        # Using Xavier for consistency with user's likely previous setup
        actor_gain = nn.init.calculate_gain("linear")
        critic_gain = nn.init.calculate_gain("linear")
        nn.init.xavier_uniform_(
            self.actor_head.weight, gain=0.01
        )  # Small gain for policy output
        nn.init.constant_(self.actor_head.bias, 0)
        nn.init.xavier_uniform_(self.critic_head.weight, gain=critic_gain)
        nn.init.constant_(self.critic_head.bias, 0)

    def _build_cnn_branch(self) -> Tuple[nn.Sequential, int, int, int]:
        conv_layers: List[nn.Module] = []
        current_channels = self.grid_c
        h, w = self.grid_h, self.grid_w
        cfg = self.config
        for i, out_channels in enumerate(cfg.CONV_CHANNELS):
            # Move layer to device upon creation
            conv_layer = nn.Conv2d(
                current_channels,
                out_channels,
                kernel_size=cfg.CONV_KERNEL_SIZE,
                stride=cfg.CONV_STRIDE,
                padding=cfg.CONV_PADDING,
                bias=not cfg.USE_BATCHNORM_CONV,
            ).to(self.device)
            conv_layers.append(conv_layer)
            if cfg.USE_BATCHNORM_CONV:
                conv_layers.append(nn.BatchNorm2d(out_channels).to(self.device))
            conv_layers.append(cfg.CONV_ACTIVATION())
            current_channels = out_channels
            h = (h + 2 * cfg.CONV_PADDING - cfg.CONV_KERNEL_SIZE) // cfg.CONV_STRIDE + 1
            w = (w + 2 * cfg.CONV_PADDING - cfg.CONV_KERNEL_SIZE) // cfg.CONV_STRIDE + 1
        return nn.Sequential(*conv_layers), h, w, current_channels

    def _get_conv_out_size(self, shape: Tuple[int, int, int]) -> int:
        # self.conv_base is already on self.device
        with torch.no_grad():
            dummy_input = torch.zeros(1, *shape, device=self.device)
            self.conv_base.eval()  # Set to eval mode for size calculation
            output = self.conv_base(dummy_input)
            self.conv_base.train()  # Set back to train mode
            return int(np.prod(output.size()[1:]))

    def _build_shape_mlp_branch(self) -> Tuple[nn.Sequential, int]:
        shape_mlp_layers: List[nn.Module] = []
        current_dim = self.env_config.SHAPE_STATE_DIM
        cfg = self.config
        for hidden_dim in cfg.SHAPE_FEATURE_MLP_DIMS:
            # Move layer to device upon creation
            lin_layer = nn.Linear(current_dim, hidden_dim).to(self.device)
            shape_mlp_layers.append(lin_layer)
            shape_mlp_layers.append(cfg.SHAPE_MLP_ACTIVATION())
            current_dim = hidden_dim
        return nn.Sequential(*shape_mlp_layers), current_dim

    def _build_fusion_mlp_branch(self, input_dim: int) -> Tuple[nn.Sequential, int]:
        fusion_layers: List[nn.Module] = []
        current_fusion_dim = input_dim
        cfg = self.config
        for i, hidden_dim in enumerate(cfg.COMBINED_FC_DIMS):
            # Move layer to device upon creation
            linear_layer = nn.Linear(
                current_fusion_dim, hidden_dim, bias=not cfg.USE_BATCHNORM_FC
            ).to(self.device)
            fusion_layers.append(linear_layer)
            if cfg.USE_BATCHNORM_FC:
                fusion_layers.append(nn.BatchNorm1d(hidden_dim).to(self.device))
            fusion_layers.append(cfg.COMBINED_ACTIVATION())
            if cfg.DROPOUT_FC > 0:
                fusion_layers.append(nn.Dropout(cfg.DROPOUT_FC).to(self.device))
            current_fusion_dim = hidden_dim
        return nn.Sequential(*fusion_layers), current_fusion_dim

    def forward(
        self,
        grid_tensor: torch.Tensor,  # Shape [N, C, H, W] or [B, T, C, H, W]
        shape_tensor: torch.Tensor,  # Shape [N, F] or [B, T, F]
        hidden_state: Optional[
            Tuple[torch.Tensor, torch.Tensor]
        ] = None,  # Shape [L, B, H]
    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:

        # Ensure input tensors are on the same device as the model
        model_device = next(self.parameters()).device
        grid_tensor = grid_tensor.to(model_device)
        shape_tensor = shape_tensor.to(model_device)
        if hidden_state:
            hidden_state = (
                hidden_state[0].to(model_device),
                hidden_state[1].to(model_device),
            )

        # Detect if input is sequence based on RNN config and dimensions
        is_sequence = self.rnn_config.USE_RNN and grid_tensor.ndim == 5
        initial_batch_size = grid_tensor.shape[0]
        seq_len = grid_tensor.shape[1] if is_sequence else 1

        # --- Reshape for Feature Extraction ---
        # Flatten batch and time dimensions: [B, T, ...] -> [N = B*T, ...]
        num_samples = initial_batch_size * seq_len
        # --- USE RESHAPE INSTEAD OF VIEW ---
        grid_input_flat = grid_tensor.reshape(
            num_samples, *self.env_config.GRID_STATE_SHAPE
        )
        shape_input_flat = shape_tensor.reshape(
            num_samples, self.env_config.SHAPE_STATE_DIM
        )
        # --- END MODIFICATION ---

        # --- Feature Extraction ---
        conv_output = self.conv_base(grid_input_flat)
        conv_output_flat = conv_output.view(
            num_samples, -1
        )  # Use view here is fine for flattening
        shape_output = self.shape_mlp(shape_input_flat)
        combined_features = torch.cat((conv_output_flat, shape_output), dim=1)
        fused_output = self.fusion_mlp(
            combined_features
        )  # Shape [N, fusion_output_dim]

        # --- Optional RNN ---
        next_hidden_state = hidden_state
        if self.rnn_config.USE_RNN and self.lstm_layer is not None:
            # Reshape for LSTM: [N, Feat] -> [B, T, Feat]
            lstm_input = fused_output.view(
                initial_batch_size, seq_len, self.fusion_output_dim
            )
            # LSTM forward pass
            lstm_output, next_hidden_state = self.lstm_layer(lstm_input, hidden_state)
            # Flatten LSTM output for heads: [B, T, lstm_hidden] -> [N, lstm_hidden]
            head_input = lstm_output.contiguous().view(num_samples, -1)
        else:
            # If no RNN, fusion output goes directly to heads
            head_input = fused_output

        # --- Actor and Critic Heads ---
        policy_logits = self.actor_head(head_input)  # Shape [N, action_dim]
        value = self.critic_head(head_input)  # Shape [N, 1]

        # --- Reshape Output if Input was Sequence ---
        if is_sequence:
            policy_logits = policy_logits.view(
                initial_batch_size, seq_len, -1
            )  # [B, T, A]
            value = value.view(initial_batch_size, seq_len, -1)  # [B, T, 1]

        return policy_logits, value, next_hidden_state

    def get_initial_hidden_state(
        self, batch_size: int
    ) -> Optional[Tuple[torch.Tensor, torch.Tensor]]:
        if not self.rnn_config.USE_RNN or self.lstm_layer is None:
            return None
        # Get the device from a layer parameter
        model_device = next(self.parameters()).device
        num_layers = self.rnn_config.LSTM_NUM_LAYERS
        hidden_size = self.rnn_config.LSTM_HIDDEN_SIZE
        # Create initial hidden states directly on the model's device
        h_0 = torch.zeros(num_layers, batch_size, hidden_size, device=model_device)
        c_0 = torch.zeros(num_layers, batch_size, hidden_size, device=model_device)
        return (h_0, c_0)


File: environment/game_state.py
# File: environment/game_state.py
import time
import numpy as np
from typing import List, Optional, Tuple, Dict, Union
from collections import deque
from typing import Deque

from .grid import Grid
from .shape import Shape
from config import EnvConfig, RewardConfig

StateType = Dict[str, np.ndarray]


class GameState:
    def __init__(self):
        self.env_config = EnvConfig()
        self.grid = Grid(self.env_config)
        self.shapes: List[Optional[Shape]] = [
            Shape() for _ in range(self.env_config.NUM_SHAPE_SLOTS)
        ]
        self.score = 0.0
        self.game_score = 0
        self.lines_cleared_this_episode = 0
        self.blink_time = 0.0
        self.last_time = time.time()
        self.freeze_time = 0.0
        self.line_clear_flash_time = 0.0
        self.line_clear_highlight_time: float = 0.0
        self.game_over_flash_time: float = 0.0
        self.cleared_triangles_coords: List[Tuple[int, int]] = []
        self.game_over = False
        self._last_action_valid = True
        self.rewards = RewardConfig()
        self.demo_selected_shape_idx: int = 0
        self.demo_target_row: int = self.env_config.ROWS // 2
        self.demo_target_col: int = self.env_config.COLS // 2

    def reset(self) -> StateType:
        self.grid = Grid(self.env_config)
        self.shapes = [Shape() for _ in range(self.env_config.NUM_SHAPE_SLOTS)]
        self.score = 0.0
        self.game_score = 0
        self.lines_cleared_this_episode = 0
        self.blink_time = 0.0
        self.freeze_time = 0.0
        self.line_clear_flash_time = 0.0
        self.line_clear_highlight_time = 0.0
        self.game_over_flash_time = 0.0
        self.cleared_triangles_coords = []
        self.game_over = False
        self._last_action_valid = True
        self.last_time = time.time()
        self.demo_selected_shape_idx = 0
        self.demo_target_row = self.env_config.ROWS // 2
        self.demo_target_col = self.env_config.COLS // 2
        return self.get_state()

    def valid_actions(self) -> List[int]:
        if self.game_over or self.freeze_time > 0:
            return []
        acts = []
        locations_per_shape = self.grid.rows * self.grid.cols
        for i, sh in enumerate(self.shapes):
            if not sh:
                continue
            for r in range(self.grid.rows):
                for c in range(self.grid.cols):
                    if self.grid.can_place(sh, r, c):
                        action_index = i * locations_per_shape + (
                            r * self.grid.cols + c
                        )
                        acts.append(action_index)
        return acts

    def _check_fundamental_game_over(self) -> bool:
        for sh in self.shapes:
            if not sh:
                continue
            for r in range(self.grid.rows):
                for c in range(self.grid.cols):
                    if self.grid.can_place(sh, r, c):
                        return False
        return True

    def is_over(self) -> bool:
        return self.game_over

    def is_frozen(self) -> bool:
        return self.freeze_time > 0

    def is_line_clearing(self) -> bool:
        return self.line_clear_flash_time > 0

    def is_highlighting_cleared(self) -> bool:
        return self.line_clear_highlight_time > 0

    def is_game_over_flashing(self) -> bool:
        return self.game_over_flash_time > 0

    def get_cleared_triangle_coords(self) -> List[Tuple[int, int]]:
        return self.cleared_triangles_coords

    def decode_act(self, a: int) -> Tuple[int, int, int]:
        locations_per_shape = self.grid.rows * self.grid.cols
        s_idx = a // locations_per_shape
        pos_idx = a % locations_per_shape
        rr = pos_idx // self.grid.cols
        cc = pos_idx % self.grid.cols
        return (s_idx, rr, cc)

    def _update_timers(self):
        now = time.time()
        dt = now - self.last_time
        self.last_time = now
        self.freeze_time = max(0, self.freeze_time - dt)
        self.blink_time = max(0, self.blink_time - dt)
        self.line_clear_flash_time = max(0, self.line_clear_flash_time - dt)
        self.line_clear_highlight_time = max(0, self.line_clear_highlight_time - dt)
        self.game_over_flash_time = max(0, self.game_over_flash_time - dt)
        if self.line_clear_highlight_time <= 0 and self.cleared_triangles_coords:
            self.cleared_triangles_coords = []

    def _handle_invalid_placement(self) -> float:
        self._last_action_valid = False
        reward = self.rewards.PENALTY_INVALID_MOVE
        return reward

    def _handle_game_over_state_change(self) -> float:
        if self.game_over:
            return 0.0
        self.game_over = True
        if self.freeze_time <= 0:
            self.freeze_time = 1.0
        # --- MODIFIED: Increase flash duration ---
        self.game_over_flash_time = 0.6
        # --- END MODIFIED ---
        return self.rewards.PENALTY_GAME_OVER

    def _handle_valid_placement(
        self, shp: Shape, s_idx: int, rr: int, cc: int
    ) -> float:
        self._last_action_valid = True
        reward = self.rewards.REWARD_ALIVE_STEP

        self.grid.place(shp, rr, cc)
        self.shapes[s_idx] = None
        self.game_score += len(shp.triangles)

        num_slots = self.env_config.NUM_SHAPE_SLOTS
        found_next = False
        if num_slots > 0:
            next_idx = (s_idx + 1) % num_slots
            for _ in range(num_slots):
                if (
                    0 <= next_idx < len(self.shapes)
                    and self.shapes[next_idx] is not None
                ):
                    self.demo_selected_shape_idx = next_idx
                    found_next = True
                    break
                next_idx = (next_idx + 1) % num_slots
            if not found_next:
                self.demo_selected_shape_idx = 0

        lines_cleared, triangles_cleared, cleared_coords = self.grid.clear_filled_rows()
        self.lines_cleared_this_episode += lines_cleared

        if lines_cleared == 1:
            reward += self.rewards.REWARD_CLEAR_1
        elif lines_cleared == 2:
            reward += self.rewards.REWARD_CLEAR_2
        elif lines_cleared >= 3:
            reward += self.rewards.REWARD_CLEAR_3PLUS

        if triangles_cleared > 0:
            self.game_score += triangles_cleared * 2
            self.blink_time = 0.5
            self.freeze_time = 0.5
            self.line_clear_flash_time = 0.3
            self.line_clear_highlight_time = 0.5
            self.cleared_triangles_coords = cleared_coords

        num_holes = self.grid.count_holes()
        reward += num_holes * self.rewards.PENALTY_HOLE_PER_HOLE

        if all(x is None for x in self.shapes):
            self.shapes = [Shape() for _ in range(self.env_config.NUM_SHAPE_SLOTS)]
            if not found_next:
                first_available = next(
                    (i for i, s in enumerate(self.shapes) if s is not None), 0
                )
                self.demo_selected_shape_idx = first_available

        if self._check_fundamental_game_over():
            reward += self._handle_game_over_state_change()

        return reward

    def step(self, a: int) -> Tuple[float, bool]:
        self._update_timers()

        if self.game_over:
            return (0.0, True)

        s_idx, rr, cc = self.decode_act(a)
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None

        is_valid_placement = shp is not None and self.grid.can_place(shp, rr, cc)

        if self.is_frozen():
            current_rl_reward = self._handle_invalid_placement()
        elif is_valid_placement:
            current_rl_reward = self._handle_valid_placement(shp, s_idx, rr, cc)
        else:
            current_rl_reward = self._handle_invalid_placement()
            if self._check_fundamental_game_over():
                current_rl_reward += self._handle_game_over_state_change()

        self.score += current_rl_reward
        return (current_rl_reward, self.game_over)

    def get_state(self) -> StateType:
        grid_state = self.grid.get_feature_matrix()
        shape_features_per = self.env_config.SHAPE_FEATURES_PER_SHAPE
        num_shapes_expected = self.env_config.NUM_SHAPE_SLOTS
        shape_rep = np.zeros(
            (num_shapes_expected, shape_features_per), dtype=np.float32
        )

        max_tris_norm = 6.0
        max_h_norm = float(self.grid.rows)
        max_w_norm = float(self.grid.cols)

        for i in range(num_shapes_expected):
            s = self.shapes[i] if i < len(self.shapes) else None
            if s:
                tri = s.triangles
                n = len(tri)
                ups = sum(1 for (_, _, u) in tri if u)
                dns = n - ups
                mnr, mnc, mxr, mxc = s.bbox()
                height = mxr - mnr + 1
                width = mxc - mnc + 1

                shape_rep[i, 0] = np.clip(float(n) / max_tris_norm, 0.0, 1.0)
                shape_rep[i, 1] = np.clip(float(ups) / max_tris_norm, 0.0, 1.0)
                shape_rep[i, 2] = np.clip(float(dns) / max_tris_norm, 0.0, 1.0)
                shape_rep[i, 3] = np.clip(float(height) / max_h_norm, 0.0, 1.0)
                shape_rep[i, 4] = np.clip(float(width) / max_w_norm, 0.0, 1.0)

        state_dict = {
            "grid": grid_state.astype(np.float32),
            "shapes": shape_rep.astype(np.float32),
        }
        return state_dict

    def is_blinking(self) -> bool:
        return self.blink_time > 0

    def get_shapes(self) -> List[Shape]:
        return [s for s in self.shapes if s is not None]

    # --- Demo Mode Methods ---
    def cycle_shape(self, direction: int):
        if self.game_over or self.freeze_time > 0:
            return
        num_slots = self.env_config.NUM_SHAPE_SLOTS
        if num_slots <= 0:
            return

        available_indices = [
            i for i, s in enumerate(self.shapes) if s is not None and 0 <= i < num_slots
        ]
        if not available_indices:
            return

        try:
            current_list_idx = available_indices.index(self.demo_selected_shape_idx)
        except ValueError:
            current_list_idx = 0

        new_list_idx = (current_list_idx + direction) % len(available_indices)
        self.demo_selected_shape_idx = available_indices[new_list_idx]

    def move_target(self, dr: int, dc: int):
        if self.game_over or self.freeze_time > 0:
            return
        self.demo_target_row = np.clip(self.demo_target_row + dr, 0, self.grid.rows - 1)
        self.demo_target_col = np.clip(self.demo_target_col + dc, 0, self.grid.cols - 1)

    def get_action_for_current_selection(self) -> Optional[int]:
        if self.game_over or self.freeze_time > 0:
            return None
        s_idx = self.demo_selected_shape_idx
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None
        if shp is None:
            return None

        rr, cc = self.demo_target_row, self.demo_target_col

        if self.grid.can_place(shp, rr, cc):
            locations_per_shape = self.grid.rows * self.grid.cols
            action_index = s_idx * locations_per_shape + (rr * self.grid.cols + cc)
            return action_index
        else:
            return None

    def get_current_selection_info(self) -> Tuple[Optional[Shape], int, int]:
        s_idx = self.demo_selected_shape_idx
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None
        return shp, self.demo_target_row, self.demo_target_col


File: environment/grid.py
# File: environment/grid.py
import numpy as np
from typing import List, Tuple, Optional
from .triangle import Triangle
from .shape import Shape
from config import EnvConfig


class Grid:
    """Represents the game board composed of Triangles."""

    def __init__(self, env_config: EnvConfig):
        self.rows = env_config.ROWS
        self.cols = env_config.COLS
        self.triangles: List[List[Triangle]] = []
        self._create()

    def _create(self) -> None:
        cols_per_row = [9, 11, 13, 15, 15, 13, 11, 9]

        if len(cols_per_row) != self.rows:
            raise ValueError(
                f"Grid._create error: Length of cols_per_row ({len(cols_per_row)}) must match EnvConfig.ROWS ({self.rows})"
            )
        if max(cols_per_row) > self.cols:
            raise ValueError(
                f"Grid._create error: Max playable columns ({max(cols_per_row)}) exceeds EnvConfig.COLS ({self.cols})"
            )

        self.triangles = []
        for r in range(self.rows):
            rowt = []
            base_playable_cols = cols_per_row[r]

            if base_playable_cols <= 0:
                initial_death_cols_left = self.cols
            elif base_playable_cols >= self.cols:
                initial_death_cols_left = 0
            else:
                initial_death_cols_left = (self.cols - base_playable_cols) // 2
            initial_first_death_col_right = initial_death_cols_left + base_playable_cols

            adjusted_death_cols_left = initial_death_cols_left + 1
            adjusted_first_death_col_right = initial_first_death_col_right - 1

            for c in range(self.cols):
                is_death_cell = (
                    (c < adjusted_death_cols_left)
                    or (
                        c >= adjusted_first_death_col_right
                        and adjusted_first_death_col_right > adjusted_death_cols_left
                    )
                    or (base_playable_cols <= 2)
                )

                is_up_cell = (r + c) % 2 == 0
                tri = Triangle(r, c, is_up=is_up_cell, is_death=is_death_cell)
                rowt.append(tri)
            self.triangles.append(rowt)

    def valid(self, r: int, c: int) -> bool:
        return 0 <= r < self.rows and 0 <= c < self.cols

    def can_place(self, shp: Shape, rr: int, cc: int) -> bool:
        for dr, dc, up in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if not self.valid(nr, nc):
                return False
            if not (
                0 <= nr < len(self.triangles) and 0 <= nc < len(self.triangles[nr])
            ):
                return False
            tri = self.triangles[nr][nc]
            if tri.is_death or tri.is_occupied or (tri.is_up != up):
                return False
        return True

    def place(self, shp: Shape, rr: int, cc: int) -> None:
        for dr, dc, _ in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if self.valid(nr, nc):
                if not (
                    0 <= nr < len(self.triangles) and 0 <= nc < len(self.triangles[nr])
                ):
                    continue
                tri = self.triangles[nr][nc]
                if not tri.is_death and not tri.is_occupied:
                    tri.is_occupied = True
                    tri.color = shp.color

    # --- MODIFIED: clear_filled_rows returns coords ---
    def clear_filled_rows(self) -> Tuple[int, int, List[Tuple[int, int]]]:
        """
        Clears fully occupied rows (ignoring death cells) by marking triangles as
        unoccupied. Does NOT apply gravity.
        Returns: (lines cleared, triangles cleared, list of (r, c) coords cleared).
        """
        lines_cleared = 0
        triangles_cleared = 0
        rows_to_clear_indices = []
        cleared_triangles_coords: List[Tuple[int, int]] = []  # Store coords here

        for r in range(self.rows):
            if not (0 <= r < len(self.triangles)):
                continue
            rowt = self.triangles[r]
            is_row_full = True
            num_placeable_triangles_in_row = 0
            for t in rowt:
                if not t.is_death:
                    num_placeable_triangles_in_row += 1
                    if not t.is_occupied:
                        is_row_full = False
                        break

            if is_row_full and num_placeable_triangles_in_row > 0:
                rows_to_clear_indices.append(r)
                lines_cleared += 1

        for r_idx in rows_to_clear_indices:
            if not (0 <= r_idx < len(self.triangles)):
                continue
            for t in self.triangles[r_idx]:
                if not t.is_death and t.is_occupied:
                    triangles_cleared += 1
                    t.is_occupied = False
                    t.color = None
                    cleared_triangles_coords.append((r_idx, t.col))  # Add coords

        return lines_cleared, triangles_cleared, cleared_triangles_coords

    # --- END MODIFIED ---

    def count_holes(self) -> int:
        holes = 0
        for c in range(self.cols):
            occupied_above = False
            for r in range(self.rows):
                if not (
                    0 <= r < len(self.triangles) and 0 <= c < len(self.triangles[r])
                ):
                    continue
                tri = self.triangles[r][c]
                if tri.is_death:
                    occupied_above = False
                    continue

                if tri.is_occupied:
                    occupied_above = True
                elif not tri.is_occupied and occupied_above:
                    holes += 1
        return holes

    def get_feature_matrix(self) -> np.ndarray:
        grid_state = np.zeros((2, self.rows, self.cols), dtype=np.float32)
        for r in range(self.rows):
            for c in range(self.cols):
                if not (
                    0 <= r < len(self.triangles) and 0 <= c < len(self.triangles[r])
                ):
                    continue
                t = self.triangles[r][c]
                if not t.is_death:
                    grid_state[0, r, c] = 1.0 if t.is_occupied else 0.0
                    grid_state[1, r, c] = 1.0 if t.is_up else 0.0
        return grid_state


File: environment/__init__.py


File: environment/triangle.py
# File: environment/triangle.py
# (No changes needed)
from typing import Tuple, Optional, List


class Triangle:
    """Represents a single triangular cell on the grid."""

    def __init__(self, row: int, col: int, is_up: bool, is_death: bool = False):
        self.row = row
        self.col = col
        self.is_up = is_up  # True if pointing up, False if pointing down
        self.is_death = is_death  # True if part of the unplayable border
        self.is_occupied = is_death  # Occupied if it's a death cell initially
        self.color: Optional[Tuple[int, int, int]] = (
            None  # Color if occupied by a shape
        )

    def get_points(
        self, ox: int, oy: int, cw: int, ch: int
    ) -> List[Tuple[float, float]]:
        """Calculates the vertex points for drawing the triangle."""
        x = ox + self.col * (
            cw * 0.75
        )  # Horizontal position based on column and overlap
        y = oy + self.row * ch  # Vertical position based on row
        if self.is_up:
            # Points for an upward-pointing triangle
            return [(x, y + ch), (x + cw, y + ch), (x + cw / 2, y)]
        else:
            # Points for a downward-pointing triangle
            return [(x, y), (x + cw, y), (x + cw / 2, y + ch)]


File: environment/shape.py
# File: environment/shape.py
# (No changes needed)
import random
from typing import List, Tuple
from config import EnvConfig, VisConfig  # Needs VisConfig only for colors

GOOGLE_COLORS = VisConfig.GOOGLE_COLORS  # Use colors from VisConfig


class Shape:
    """Represents a polyomino-like shape made of triangles."""

    def __init__(self) -> None:
        # List of (relative_row, relative_col, is_up) tuples defining the shape
        self.triangles: List[Tuple[int, int, bool]] = []
        self.color: Tuple[int, int, int] = random.choice(GOOGLE_COLORS)
        self._generate()  # Generate the shape structure

    def _generate(self) -> None:
        """Generates a random shape by adding adjacent triangles."""
        n = random.randint(1, 5)  # Number of triangles in the shape
        first_up = random.choice([True, False])  # Orientation of the root triangle
        self.triangles.append((0, 0, first_up))  # Add the root triangle at (0,0)

        # Add remaining triangles adjacent to existing ones
        for _ in range(n - 1):
            # Find valid neighbors of the *last added* triangle
            lr, lc, lu = self.triangles[-1]
            nbrs = self._find_valid_neighbors(lr, lc, lu)
            if nbrs:
                self.triangles.append(random.choice(nbrs))
            # else: Could break early if no valid neighbors found, shape < n

    def _find_valid_neighbors(
        self, r: int, c: int, up: bool
    ) -> List[Tuple[int, int, bool]]:
        """Finds potential neighbor triangles that are not already part of the shape."""
        if up:  # Neighbors of an UP triangle are DOWN triangles
            ns = [(r, c - 1, False), (r, c + 1, False), (r + 1, c, False)]
        else:  # Neighbors of a DOWN triangle are UP triangles
            ns = [(r, c - 1, True), (r, c + 1, True), (r - 1, c, True)]
        # Return only neighbors that are not already in self.triangles
        return [n for n in ns if n not in self.triangles]

    def bbox(self) -> Tuple[int, int, int, int]:
        """Calculates the bounding box (min_r, min_c, max_r, max_c) of the shape."""
        if not self.triangles:
            return (0, 0, 0, 0)
        rr = [t[0] for t in self.triangles]
        cc = [t[1] for t in self.triangles]
        return (min(rr), min(cc), max(rr), max(cc))


File: stats/aggregator.py
# File: stats/aggregator.py
import time
from collections import deque
from typing import Deque, Dict, Any, Optional, List
import numpy as np
from config import StatsConfig


class StatsAggregator:
    """
    Handles aggregation and storage of training statistics using deques.
    Calculates rolling averages and tracks best values. Does not perform logging.
    """

    def __init__(
        self,
        avg_windows: List[int] = StatsConfig.STATS_AVG_WINDOW,
        plot_window: int = StatsConfig.PLOT_DATA_WINDOW,
    ):
        if not avg_windows or not all(
            isinstance(w, int) and w > 0 for w in avg_windows
        ):
            print("Warning: Invalid avg_windows list. Using default [100].")
            self.avg_windows = [100]
        else:
            self.avg_windows = sorted(list(set(avg_windows)))

        if plot_window <= 0:
            plot_window = 10000
        self.plot_window = plot_window

        self.summary_avg_window = self.avg_windows[0]

        self.policy_losses: Deque[float] = deque(maxlen=plot_window)
        self.value_losses: Deque[float] = deque(maxlen=plot_window)
        self.entropies: Deque[float] = deque(maxlen=plot_window)
        self.grad_norms: Deque[float] = deque(maxlen=plot_window)
        self.avg_max_qs: Deque[float] = deque(maxlen=plot_window)
        self.episode_scores: Deque[float] = deque(maxlen=plot_window)
        self.episode_lengths: Deque[int] = deque(maxlen=plot_window)
        self.game_scores: Deque[int] = deque(maxlen=plot_window)
        self.episode_lines_cleared: Deque[int] = deque(maxlen=plot_window)
        self.sps_values: Deque[float] = deque(maxlen=plot_window)
        self.buffer_sizes: Deque[int] = deque(maxlen=plot_window)
        self.beta_values: Deque[float] = deque(maxlen=plot_window)
        self.best_rl_score_history: Deque[float] = deque(maxlen=plot_window)
        self.best_game_score_history: Deque[int] = deque(maxlen=plot_window)
        self.lr_values: Deque[float] = deque(maxlen=plot_window)
        self.epsilon_values: Deque[float] = deque(maxlen=plot_window)

        self.total_episodes = 0
        self.total_lines_cleared = 0
        self.current_epsilon: float = 0.0
        self.current_beta: float = 0.0
        self.current_buffer_size: int = 0
        self.current_global_step: int = 0
        self.current_sps: float = 0.0
        self.current_lr: float = 0.0

        self.best_score: float = -float("inf")
        self.previous_best_score: float = -float("inf")
        self.best_score_step: int = 0

        self.best_game_score: float = -float("inf")
        self.previous_best_game_score: float = -float("inf")
        self.best_game_score_step: int = 0

        self.best_value_loss: float = float("inf")
        self.previous_best_value_loss: float = float("inf")
        self.best_value_loss_step: int = 0

        print(
            f"[StatsAggregator] Initialized. Avg Windows: {self.avg_windows}, Plot Window: {self.plot_window}"
        )

    def record_episode(
        self,
        episode_score: float,
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,
    ) -> Dict[str, Any]:
        current_step = (
            global_step if global_step is not None else self.current_global_step
        )
        update_info = {"new_best_rl": False, "new_best_game": False}

        # --- DEBUG LOGGING ---
        # print(f"[StatsAggregator Debug] Appending Ep {episode_num}: RLScore={episode_score:.2f}, Len={episode_length}, GameScore={game_score}")
        # --- END DEBUG LOGGING ---

        self.episode_scores.append(episode_score)
        self.episode_lengths.append(episode_length)
        if game_score is not None:
            self.game_scores.append(game_score)
        if lines_cleared is not None:
            self.episode_lines_cleared.append(lines_cleared)
            self.total_lines_cleared += lines_cleared
        self.total_episodes = episode_num

        if episode_score > self.best_score:
            self.previous_best_score = self.best_score
            self.best_score = episode_score
            self.best_score_step = current_step
            update_info["new_best_rl"] = True

        if game_score is not None and game_score > self.best_game_score:
            self.previous_best_game_score = self.best_game_score
            self.best_game_score = float(game_score)
            self.best_game_score_step = current_step
            update_info["new_best_game"] = True

        self.best_rl_score_history.append(self.best_score)
        current_best_game = (
            int(self.best_game_score) if self.best_game_score > -float("inf") else 0
        )
        self.best_game_score_history.append(current_best_game)

        return update_info

    def record_step(self, step_data: Dict[str, Any]) -> Dict[str, Any]:
        g_step = step_data.get("global_step", self.current_global_step)
        if g_step > self.current_global_step:
            self.current_global_step = g_step

        update_info = {"new_best_loss": False}

        if "policy_loss" in step_data and step_data["policy_loss"] is not None:
            # --- DEBUG LOGGING ---
            # print(f"[StatsAggregator Debug] Appending Policy Loss: {step_data['policy_loss']:.4f} at Step {g_step}")
            # --- END DEBUG LOGGING ---
            self.policy_losses.append(step_data["policy_loss"])

        if "value_loss" in step_data and step_data["value_loss"] is not None:
            current_value_loss = step_data["value_loss"]
            # --- DEBUG LOGGING ---
            # print(f"[StatsAggregator Debug] Appending Value Loss: {current_value_loss:.4f} at Step {g_step}")
            # --- END DEBUG LOGGING ---
            self.value_losses.append(current_value_loss)
            if current_value_loss < self.best_value_loss and g_step > 0:
                self.previous_best_value_loss = self.best_value_loss
                self.best_value_loss = current_value_loss
                self.best_value_loss_step = g_step
                update_info["new_best_loss"] = True

        if "entropy" in step_data and step_data["entropy"] is not None:
            # --- DEBUG LOGGING ---
            # print(f"[StatsAggregator Debug] Appending Entropy: {step_data['entropy']:.4f} at Step {g_step}")
            # --- END DEBUG LOGGING ---
            self.entropies.append(step_data["entropy"])

        if "grad_norm" in step_data and step_data["grad_norm"] is not None:
            self.grad_norms.append(step_data["grad_norm"])
        if "avg_max_q" in step_data and step_data["avg_max_q"] is not None:
            self.avg_max_qs.append(step_data["avg_max_q"])
        if "beta" in step_data and step_data["beta"] is not None:
            self.current_beta = step_data["beta"]
            self.beta_values.append(self.current_beta)
        if "buffer_size" in step_data and step_data["buffer_size"] is not None:
            self.current_buffer_size = step_data["buffer_size"]
            self.buffer_sizes.append(self.current_buffer_size)
        if "lr" in step_data and step_data["lr"] is not None:
            self.current_lr = step_data["lr"]
            self.lr_values.append(self.current_lr)
        if "epsilon" in step_data and step_data["epsilon"] is not None:
            self.current_epsilon = step_data["epsilon"]
            self.epsilon_values.append(self.current_epsilon)

        if "step_time" in step_data and step_data["step_time"] > 1e-9:
            num_steps = step_data.get("num_steps_processed", 1)
            sps = num_steps / step_data["step_time"]
            self.sps_values.append(sps)
            self.current_sps = sps

        return update_info

    def get_summary(self, current_global_step: Optional[int] = None) -> Dict[str, Any]:
        if current_global_step is None:
            current_global_step = self.current_global_step

        summary_window = self.summary_avg_window

        def safe_mean(q: Deque, default=0.0) -> float:
            window_data = list(q)[-summary_window:]
            return float(np.mean(window_data)) if window_data else default

        summary = {
            "avg_score_window": safe_mean(self.episode_scores),
            "avg_length_window": safe_mean(self.episode_lengths),
            "policy_loss": safe_mean(self.policy_losses),
            "value_loss": safe_mean(self.value_losses),
            "entropy": safe_mean(self.entropies),
            "avg_max_q_window": safe_mean(self.avg_max_qs),
            "avg_game_score_window": safe_mean(self.game_scores),
            "avg_lines_cleared_window": safe_mean(self.episode_lines_cleared),
            "avg_sps_window": safe_mean(self.sps_values, default=self.current_sps),
            "avg_lr_window": safe_mean(self.lr_values, default=self.current_lr),
            "total_episodes": self.total_episodes,
            "beta": self.current_beta,
            "buffer_size": self.current_buffer_size,
            "steps_per_second": self.current_sps,
            "global_step": current_global_step,
            "current_lr": self.current_lr,
            "best_score": self.best_score,
            "previous_best_score": self.previous_best_score,
            "best_score_step": self.best_score_step,
            "best_game_score": self.best_game_score,
            "previous_best_game_score": self.previous_best_game_score,
            "best_game_score_step": self.best_game_score_step,
            "best_loss": self.best_value_loss,
            "previous_best_loss": self.previous_best_value_loss,
            "best_loss_step": self.best_value_loss_step,
            "num_ep_scores": len(self.episode_scores),
            "num_losses": len(self.value_losses),
            "summary_avg_window_size": summary_window,
        }
        return summary

    def get_plot_data(self) -> Dict[str, Deque]:
        # --- DEBUG LOGGING ---
        # print(f"[StatsAggregator Debug] get_plot_data lengths: "
        #       f"RLScore={len(self.episode_scores)}, GameScore={len(self.game_scores)}, EpLen={len(self.episode_lengths)}, "
        #       f"PLoss={len(self.policy_losses)}, VLoss={len(self.value_losses)}, Ent={len(self.entropies)}")
        # --- END DEBUG LOGGING ---
        return {
            "episode_scores": self.episode_scores.copy(),
            "episode_lengths": self.episode_lengths.copy(),
            "policy_loss": self.policy_losses.copy(),
            "value_loss": self.value_losses.copy(),
            "entropy": self.entropies.copy(),
            "avg_max_qs": self.avg_max_qs.copy(),
            "game_scores": self.game_scores.copy(),
            "episode_lines_cleared": self.episode_lines_cleared.copy(),
            "sps_values": self.sps_values.copy(),
            "buffer_sizes": self.buffer_sizes.copy(),
            "beta_values": self.beta_values.copy(),
            "best_rl_score_history": self.best_rl_score_history.copy(),
            "best_game_score_history": self.best_game_score_history.copy(),
            "lr_values": self.lr_values.copy(),
            "epsilon_values": self.epsilon_values.copy(),
        }


File: stats/__init__.py
# File: stats/__init__.py
from .stats_recorder import StatsRecorderBase
from .aggregator import StatsAggregator
from .simple_stats_recorder import SimpleStatsRecorder
from .tensorboard_logger import TensorBoardStatsRecorder

__all__ = [
    "StatsRecorderBase",
    "StatsAggregator",
    "SimpleStatsRecorder",
    "TensorBoardStatsRecorder",
]


File: stats/simple_stats_recorder.py
# File: stats/simple_stats_recorder.py
import time
from collections import deque
from typing import Deque, Dict, Any, Optional, Union, List
import numpy as np
import torch
from .stats_recorder import StatsRecorderBase
from .aggregator import StatsAggregator
from config import StatsConfig


class SimpleStatsRecorder(StatsRecorderBase):
    """
    Logs aggregated statistics to the console periodically.
    Delegates data storage and aggregation to a StatsAggregator instance.
    Provides no-op implementations for histogram, image, hparam, graph logging.
    """

    def __init__(
        self,
        aggregator: StatsAggregator,
        console_log_interval: int = StatsConfig.CONSOLE_LOG_FREQ,
    ):
        self.aggregator = aggregator
        self.console_log_interval = (
            max(1, console_log_interval) if console_log_interval > 0 else -1
        )
        self.last_log_time: float = time.time()
        self.last_log_step: int = 0
        self.start_time: float = time.time()
        self.summary_avg_window = self.aggregator.summary_avg_window
        print(
            f"[SimpleStatsRecorder] Initialized. Console Log Interval: {self.console_log_interval if self.console_log_interval > 0 else 'Disabled'}"
        )
        print(
            f"[SimpleStatsRecorder] Console logs will use Avg Window: {self.summary_avg_window}"
        )

    def record_episode(
        self,
        episode_score: float,
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,
    ):
        update_info = self.aggregator.record_episode(
            episode_score,
            episode_length,
            episode_num,
            global_step,
            game_score,
            lines_cleared,
        )
        current_step = (
            global_step
            if global_step is not None
            else self.aggregator.current_global_step
        )
        step_info = f"at Step ~{current_step/1e6:.1f}M"

        if update_info.get("new_best_rl"):
            prev_str = (
                f"{self.aggregator.previous_best_score:.2f}"
                if self.aggregator.previous_best_score > -float("inf")
                else "N/A"
            )
            print(
                f"\n---  New Best RL: {self.aggregator.best_score:.2f} {step_info} (Prev: {prev_str}) ---"
            )
        if update_info.get("new_best_game"):
            prev_str = (
                f"{self.aggregator.previous_best_game_score:.0f}"
                if self.aggregator.previous_best_game_score > -float("inf")
                else "N/A"
            )
            print(
                f"---  New Best Game: {self.aggregator.best_game_score:.0f} {step_info} (Prev: {prev_str}) ---"
            )
        # --- MODIFIED: Check for new best loss ---
        if update_info.get("new_best_loss"):
            prev_str = (
                f"{self.aggregator.previous_best_value_loss:.4f}"
                if self.aggregator.previous_best_value_loss < float("inf")
                else "N/A"
            )
            print(
                f"--- New Best Loss: {self.aggregator.best_value_loss:.4f} {step_info} (Prev: {prev_str}) ---"
            )
        # --- END MODIFIED ---

    def record_step(self, step_data: Dict[str, Any]):
        update_info = self.aggregator.record_step(step_data)
        g_step = step_data.get("global_step", self.aggregator.current_global_step)

        # --- MODIFIED: Check for new best loss after recording step data ---
        if update_info.get("new_best_loss"):
            prev_str = (
                f"{self.aggregator.previous_best_value_loss:.4f}"
                if self.aggregator.previous_best_value_loss < float("inf")
                else "N/A"
            )
            step_info = f"at Step ~{g_step/1e6:.1f}M"
            print(
                f"--- New Best Loss: {self.aggregator.best_value_loss:.4f} {step_info} (Prev: {prev_str}) ---"
            )
        # --- END MODIFIED ---

        self.log_summary(g_step)

    def get_summary(self, current_global_step: int) -> Dict[str, Any]:
        return self.aggregator.get_summary(current_global_step)

    def get_plot_data(self) -> Dict[str, Deque]:
        return self.aggregator.get_plot_data()

    def log_summary(self, global_step: int):
        if (
            self.console_log_interval <= 0
            or global_step < self.last_log_step + self.console_log_interval
        ):
            return

        summary = self.get_summary(global_step)
        elapsed_runtime = time.time() - self.start_time
        runtime_hrs = elapsed_runtime / 3600

        best_score_val = (
            f"{summary['best_score']:.2f}"
            if summary["best_score"] > -float("inf")
            else "N/A"
        )
        best_loss_val = (
            f"{summary['best_loss']:.4f}"
            if summary["best_loss"] < float("inf")
            else "N/A"
        )

        # --- MODIFIED: Use 'value_loss' key instead of 'avg_loss_window' ---
        avg_window_size = summary.get("summary_avg_window_size", "?")
        log_str = (
            f"[{runtime_hrs:.1f}h|Console] Step: {global_step/1e6:<6.2f}M | "
            f"Ep: {summary['total_episodes']:<7} | SPS: {summary['steps_per_second']:<5.0f} | "
            f"RLScore(Avg{avg_window_size}): {summary['avg_score_window']:<6.2f} (Best: {best_score_val}) | "
            f"Loss(Avg{avg_window_size}): {summary['value_loss']:.4f} (Best: {best_loss_val}) | "
            f"LR: {summary['current_lr']:.1e}"
        )
        # --- END MODIFIED ---

        # Conditionally add epsilon if present and non-default
        epsilon = summary.get("epsilon")
        if epsilon is not None and epsilon < 1.0:
            log_str += f" | Eps: {epsilon:.3f}"

        # Conditionally add beta if present and non-default
        beta = summary.get("beta")
        if beta is not None and beta > 0.0 and beta < 1.0:
            log_str += f" | Beta: {beta:.3f}"

        # Add Buffer Size if relevant (e.g., for experience replay methods)
        buffer_size = summary.get("buffer_size")
        if buffer_size is not None and buffer_size > 0:
            log_str += f" | Buf: {buffer_size/1e6:.2f}M"

        print(log_str)

        self.last_log_time = time.time()
        self.last_log_step = global_step

    def record_histogram(
        self,
        tag: str,
        values: Union[np.ndarray, torch.Tensor, List[float]],
        global_step: int,
    ):
        pass

    def record_image(
        self, tag: str, image: Union[np.ndarray, torch.Tensor], global_step: int
    ):
        pass

    def record_hparams(self, hparam_dict: Dict[str, Any], metric_dict: Dict[str, Any]):
        pass

    def record_graph(
        self, model: torch.nn.Module, input_to_model: Optional[Any] = None
    ):
        pass

    def close(self):
        print("[SimpleStatsRecorder] Closed.")


File: stats/stats_recorder.py
# File: stats/stats_recorder.py
import time
from abc import ABC, abstractmethod
from collections import deque
from typing import Deque, List, Dict, Any, Optional, Union
import numpy as np
import torch


class StatsRecorderBase(ABC):
    """Base class for recording training statistics."""

    @abstractmethod
    def record_episode(
        self,
        episode_score: float,
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,
    ):
        """Record stats for a completed episode."""
        pass

    @abstractmethod
    def record_step(self, step_data: Dict[str, Any]):
        """Record stats from a training or environment step."""
        pass

    @abstractmethod
    def record_histogram(
        self,
        tag: str,
        values: Union[np.ndarray, torch.Tensor, List[float]],
        global_step: int,
    ):
        """Record a histogram of values."""
        pass

    @abstractmethod
    def record_image(
        self, tag: str, image: Union[np.ndarray, torch.Tensor], global_step: int
    ):
        """Record an image."""
        pass

    @abstractmethod
    def record_hparams(self, hparam_dict: Dict[str, Any], metric_dict: Dict[str, Any]):
        """Record hyperparameters and final/key metrics."""
        pass

    @abstractmethod
    def record_graph(
        self, model: torch.nn.Module, input_to_model: Optional[Any] = None
    ):
        """Record the model graph."""
        pass

    @abstractmethod
    def get_summary(self, current_global_step: int) -> Dict[str, Any]:
        """Return a dictionary containing summary statistics."""
        pass

    @abstractmethod
    def get_plot_data(self) -> Dict[str, Deque]:
        """Return copies of data deques for plotting."""
        pass

    @abstractmethod
    def log_summary(self, global_step: int):
        """Trigger the logging action (e.g., print to console)."""
        pass

    @abstractmethod
    def close(self):
        """Perform any necessary cleanup."""
        pass


