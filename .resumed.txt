File: config.py
# File: config.py
import torch
import os
from utils.helpers import get_device
from collections import deque
from typing import Deque

# --- General ---
DEVICE = get_device()
RANDOM_SEED = 42
BUFFER_SAVE_PATH = os.path.join("checkpoints", "replay_buffer_state.pkl")


# --- WandB Logging --- <<< NEW >>>
class WandbConfig:
    USE_WANDB = True  # Set to False to disable wandb
    PROJECT_NAME = "TriCrack-DQN"
    RUN_NAME = None  # Set to None for auto-generated name, or provide a string
    ENTITY = None  # Your wandb username or team name (optional)
    LOG_INTERVAL_STEPS = 50_000  # How often to log aggregated stats to wandb


# --- Visualization (Pygame) ---
class VisConfig:
    SCREEN_WIDTH = 1600
    SCREEN_HEIGHT = 900
    VISUAL_STEP_DELAY = 0.001  # Slightly reduced for faster viz if needed
    LEFT_PANEL_WIDTH = 350  # Adjusted width for text stats
    ENV_SPACING = 1
    FPS = 60  # Increase FPS for smoother viz if desired
    WHITE = (255, 255, 255)
    BLACK = (0, 0, 0)
    LIGHTG = (140, 140, 140)
    GRAY = (50, 50, 50)
    RED = (255, 50, 50)
    BLUE = (50, 50, 255)
    YELLOW = (255, 255, 100)
    GOOGLE_COLORS = [(15, 157, 88), (244, 180, 0), (66, 133, 244), (219, 68, 55)]
    NUM_ENVS_TO_RENDER = 16  # Keep rendering a subset


# --- Environment ---
class EnvConfig:
    NUM_ENVS = 1024  # Keep parallel environments
    ROWS = 8
    COLS = 15
    GRID_FEATURES_PER_CELL = 3  # Occupied, Is_Up, Is_Death
    SHAPE_FEATURES_PER_SHAPE = 5  # N_Tris, Ups, Downs, Height, Width (Normalized)
    NUM_SHAPE_SLOTS = 3
    # Calculated state dim based on grid and shape features
    STATE_DIM = (ROWS * COLS * GRID_FEATURES_PER_CELL) + (
        NUM_SHAPE_SLOTS * SHAPE_FEATURES_PER_SHAPE
    )
    ACTION_DIM = NUM_SHAPE_SLOTS * (ROWS * COLS)


# --- Reward Shaping (RL Reward) ---
class RewardConfig:
    REWARD_PLACE_PER_TRI = 0.005
    REWARD_CLEAR_1 = 1.0
    REWARD_CLEAR_2 = 3.0
    REWARD_CLEAR_3PLUS = 6.0
    PENALTY_INVALID_MOVE = -0.5
    PENALTY_HOLE_PER_HOLE = -0.1  # Penalty for creating holes under blocks
    PENALTY_GAME_OVER = -10.0
    REWARD_ALIVE_STEP = 0.001  # Small reward for surviving


# --- DQN Algorithm ---
class DQNConfig:
    GAMMA = 0.99  # Discount factor
    TARGET_UPDATE_FREQ = 50_000  # Steps between target network updates
    LEARNING_RATE = 1e-5
    ADAM_EPS = 1e-4
    GRADIENT_CLIP_NORM = 10.0
    USE_NOISY_NETS = True  # <<< HARDCODED: Always use Noisy Nets for exploration
    USE_DOUBLE_DQN = True
    USE_DUELING = True


# --- Training Loop ---
class TrainConfig:
    BATCH_SIZE = 64
    LEARN_START_STEP = 200_000  # Steps before learning starts
    TOTAL_TRAINING_STEPS = 100_000_000
    LEARN_FREQ = 8  # Learn every LEARN_FREQ environment steps (per env)
    CHECKPOINT_SAVE_FREQ = 1_000_000  # Steps between saving checkpoints


# --- Replay Buffer ---
class BufferConfig:
    REPLAY_BUFFER_SIZE = 2_000_000
    USE_N_STEP = True  # Use N-Step Returns
    N_STEP = 15
    USE_PER = True  # Use Prioritized Experience Replay
    PER_ALPHA = 0.6  # PER exponent
    PER_BETA_START = 0.4  # Initial PER importance sampling exponent
    PER_BETA_FRAMES = 25_000_000  # Steps to anneal beta to 1.0
    PER_EPSILON = 1e-6  # Small value added to priorities
    LOAD_BUFFER = True  # Attempt to load buffer state from file


# --- Model Architecture ---
class ModelConfig:
    # Only one model type now
    SAVE_PATH = os.path.join("checkpoints", "dqn_agent_state.pth")
    LOAD_MODEL = True  # Attempt to load model state from file

    class Network:  # Renamed from 'Mixed'
        # CNN Branch (Grid)
        HEIGHT = EnvConfig.ROWS
        WIDTH = EnvConfig.COLS
        CONV_CHANNELS = [64, 128, 256]  # Output channels for Conv layers
        CONV_KERNEL_SIZE = 3
        CONV_STRIDE = 1
        CONV_PADDING = 1
        POOL_KERNEL_SIZE = 2
        POOL_STRIDE = 2
        CONV_ACTIVATION = torch.nn.ReLU
        USE_BATCHNORM_CONV = True

        # Shape MLP Branch
        SHAPE_MLP_HIDDEN_DIM = 128
        SHAPE_MLP_ACTIVATION = torch.nn.ReLU

        # Combined MLP Fusion Branch (Replaces Transformer)
        # Input will be flattened CNN output + Shape MLP output
        # Defines hidden layers of the MLP *before* splitting into Value/Advantage heads
        COMBINED_FC_DIMS = [1024, 512]  # Example: Two hidden layers
        COMBINED_ACTIVATION = torch.nn.ReLU
        USE_BATCHNORM_FC = True  # Apply BatchNorm in combined FC layers and heads
        DROPOUT_FC = 0.1  # Apply Dropout in combined FC layers and heads


# --- Statistics and Logging ---
class StatsConfig:
    # LOG_INTERVAL_STEPS now handled by WandbConfig
    STATS_AVG_WINDOW = 500  # Window for calculating rolling averages (in-memory)


# --- Config Consistency Checks ---
if EnvConfig.GRID_FEATURES_PER_CELL != 3:
    print(
        "Warning: Network assumes 3 features per cell (Occupied, Is_Up, Is_Death). Check EnvConfig/GameState."
    )
if ModelConfig.LOAD_MODEL:
    print("*" * 70)
    print(
        "*** Warning: LOAD_MODEL is True. Ensure saved checkpoint matches current ModelConfig. ***"
    )
    print(
        "*** Delete checkpoint file if starting with a new/modified architecture or config. ***"
    )
    print(f"*** Checkpoint path: {ModelConfig.SAVE_PATH} ***")
    print("*" * 70)
if BufferConfig.LOAD_BUFFER:
    print("*" * 70)
    print(
        "*** Warning: LOAD_BUFFER is True. Ensure saved buffer matches current config. ***"
    )
    print(
        "*** Delete buffer file if changing buffer type, N-step settings, or potentially state representation. ***"
    )
    print(f"*** Buffer path: {BUFFER_SAVE_PATH} ***")
    print("*" * 70)

# Noisy Nets are always used
print("--- Using Noisy Nets for exploration (Epsilon-greedy settings removed) ---")


print(
    f"Config Loaded: Env=(R={EnvConfig.ROWS}, C={EnvConfig.COLS}), StateDim={EnvConfig.STATE_DIM}, ActionDim={EnvConfig.ACTION_DIM}, Device={DEVICE}"
)
print(
    f"Network Config: CNN Channels={ModelConfig.Network.CONV_CHANNELS}, Shape MLP Dim={ModelConfig.Network.SHAPE_MLP_HIDDEN_DIM}, Fusion MLP Dims={ModelConfig.Network.COMBINED_FC_DIMS}, Dueling={DQNConfig.USE_DUELING}, Noisy={DQNConfig.USE_NOISY_NETS}"
)
print(
    f"Training Params: NUM_ENVS={EnvConfig.NUM_ENVS}, TOTAL_STEPS={TrainConfig.TOTAL_TRAINING_STEPS/1e6:.1f}M, BUFFER_SIZE={BufferConfig.REPLAY_BUFFER_SIZE/1e6:.1f}M, BATCH_SIZE={TrainConfig.BATCH_SIZE}, N_STEP={BufferConfig.N_STEP if BufferConfig.USE_N_STEP else 'N/A'}"
)
print(f"Buffer Params: PER={BufferConfig.USE_PER}, N-Step={BufferConfig.USE_N_STEP}")
print(
    f"Stats Params: AVG_WINDOW={StatsConfig.STATS_AVG_WINDOW}, WandB Logging={'Enabled' if WandbConfig.USE_WANDB else 'Disabled'}"
)

# Visualization Rendering Info
if (
    EnvConfig.NUM_ENVS > VisConfig.NUM_ENVS_TO_RENDER
    and VisConfig.NUM_ENVS_TO_RENDER > 0
):
    print(
        f"--- Rendering {VisConfig.NUM_ENVS_TO_RENDER} of {EnvConfig.NUM_ENVS} environments ---"
    )
elif VisConfig.NUM_ENVS_TO_RENDER <= 0:
    print(
        f"--- Rendering ALL {EnvConfig.NUM_ENVS} environments (may impact performance) ---"
    )
else:
    print(f"--- Rendering {EnvConfig.NUM_ENVS} environments ---")

if EnvConfig.NUM_ENVS >= 1024:
    print("*" * 70)
    print(
        f"*** Warning: NUM_ENVS={EnvConfig.NUM_ENVS}. Monitor system resources closely. Consider reducing. ***"
    )
    if DEVICE.type == "mps":
        print(
            "*** Using MPS device. Performance varies. Force CPU via env var if needed. ***"
        )
    print("*" * 70)


File: requirements.txt
# File: requirements.txt
pygame>=2.1.0
numpy>=1.20.0
torch>=1.10.0
wandb
cloudpickle

File: .resumed.txt


File: main_pygame.py
# File: main_pygame.py
import sys
import pygame
import numpy as np
import os
import time
import wandb  # <<< NEW >>>
from typing import List, Tuple, Optional, Dict, Any

# Import configurations
from config import (
    VisConfig,
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    WandbConfig,  # <<< Added WandbConfig
    DEVICE,
    RANDOM_SEED,
    BUFFER_SAVE_PATH,
)

# Import core components
try:
    from environment.game_state import GameState
except ImportError as e:
    print(f"Error importing environment: {e}")
    sys.exit(1)
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from agent.replay_buffer.buffer_utils import create_replay_buffer
from training.trainer import Trainer
from stats.stats_recorder import StatsRecorderBase, SimpleStatsRecorder
from stats.wandb_logger import WandbStatsRecorder  # <<< NEW >>>
from ui.renderer import UIRenderer  # <<< UPDATED UI Renderer
from utils.helpers import set_random_seeds, ensure_numpy


class MainApp:
    def __init__(self):
        print("Initializing Pygame Application...")
        set_random_seeds(RANDOM_SEED)
        pygame.init()
        pygame.font.init()

        # Store configs
        self.vis_config = VisConfig
        self.env_config = EnvConfig
        self.reward_config = RewardConfig
        self.dqn_config = DQNConfig
        self.train_config = TrainConfig
        self.buffer_config = BufferConfig
        # ExplorationConfig removed as Noisy Nets are default
        self.model_config = ModelConfig
        self.stats_config = StatsConfig
        self.wandb_config = WandbConfig  # <<< NEW >>>
        self.num_envs = self.env_config.NUM_ENVS

        # --- WandB Initialization --- <<< NEW >>>
        if self.wandb_config.USE_WANDB:
            try:
                # Combine all configs into one dictionary for wandb
                all_configs = {
                    **{
                        f"vis.{k}": v
                        for k, v in VisConfig.__dict__.items()
                        if not k.startswith("__")
                    },
                    **{
                        f"env.{k}": v
                        for k, v in EnvConfig.__dict__.items()
                        if not k.startswith("__")
                    },
                    **{
                        f"reward.{k}": v
                        for k, v in RewardConfig.__dict__.items()
                        if not k.startswith("__")
                    },
                    **{
                        f"dqn.{k}": v
                        for k, v in DQNConfig.__dict__.items()
                        if not k.startswith("__")
                    },
                    **{
                        f"train.{k}": v
                        for k, v in TrainConfig.__dict__.items()
                        if not k.startswith("__")
                    },
                    **{
                        f"buffer.{k}": v
                        for k, v in BufferConfig.__dict__.items()
                        if not k.startswith("__")
                    },
                    **{
                        f"model.{k}": v
                        for k, v in ModelConfig.__dict__.items()
                        if not k.startswith("__") and k != "Network"
                    },
                    # Flatten Network sub-config
                    **{
                        f"model.Network.{k}": v
                        for k, v in ModelConfig.Network.__dict__.items()
                        if not k.startswith("__")
                    },
                    **{
                        f"stats.{k}": v
                        for k, v in StatsConfig.__dict__.items()
                        if not k.startswith("__")
                    },
                    **{
                        f"wandb.{k}": v
                        for k, v in WandbConfig.__dict__.items()
                        if not k.startswith("__")
                    },
                    "DEVICE": str(DEVICE),  # Add device info
                    "RANDOM_SEED": RANDOM_SEED,
                }

                wandb.init(
                    project=self.wandb_config.PROJECT_NAME,
                    entity=self.wandb_config.ENTITY,
                    name=self.wandb_config.RUN_NAME,
                    config=all_configs,
                    sync_tensorboard=False,  # We are not using TensorBoard directly
                    monitor_gym=False,  # We are not using OpenAI Gym env directly
                    save_code=True,  # Save main script to wandb
                )
                print(
                    f"WandB initialized successfully. Run: {wandb.run.name} ({wandb.run.id})"
                )
            except Exception as e:
                print(f"Error initializing WandB: {e}. Disabling WandB for this run.")
                self.wandb_config.USE_WANDB = False
        else:
            print("WandB logging is disabled.")

        # Pygame setup
        self.screen = pygame.display.set_mode(
            (self.vis_config.SCREEN_WIDTH, self.vis_config.SCREEN_HEIGHT),
            pygame.RESIZABLE,
        )
        pygame.display.set_caption("TriCrack DQN - Refactored Training")
        self.clock = pygame.time.Clock()

        # App state
        self.is_training = False
        self.cleanup_confirmation_active = False
        self.last_cleanup_message_time = 0.0
        self.cleanup_message = ""
        self.status = "Paused"

        # Init RL components
        print("Initializing RL Components...")
        self._initialize_rl_components()

        # Init Renderer (Plotter removed)
        self.renderer = UIRenderer(self.screen, self.vis_config)  # No plotter needed

        print("Initialization Complete. Ready to start.")

    def _initialize_envs(self) -> List[GameState]:
        # (Same as before)
        print(f"Initializing {self.num_envs} game environments...")
        try:
            envs = [GameState() for _ in range(self.num_envs)]
            s_test = envs[0].reset()
            s_np = ensure_numpy(s_test)
            if s_np.shape[0] != self.env_config.STATE_DIM:
                raise ValueError(
                    f"FATAL: State dim mismatch! Env:{s_np.shape[0]}, Cfg:{self.env_config.STATE_DIM}"
                )
            _ = envs[0].valid_actions()
            _, _ = envs[0].step(0)
            print(f"Successfully initialized {self.num_envs} environments.")
            return envs
        except Exception as e:
            print(f"FATAL ERROR during env init: {e}")
            import traceback

            traceback.print_exc()
            pygame.quit()
            sys.exit(1)

    def _initialize_stats_recorder(self) -> StatsRecorderBase:
        """Creates the appropriate statistics recorder based on config."""
        # Close previous recorder if exists
        if hasattr(self, "stats_recorder") and self.stats_recorder:
            try:
                self.stats_recorder.close()
            except Exception as e:
                print(f"Warn: Error closing prev stats recorder: {e}")

        avg_window = self.stats_config.STATS_AVG_WINDOW
        log_interval = self.wandb_config.LOG_INTERVAL_STEPS

        if self.wandb_config.USE_WANDB:
            print(f"Using WandB Logger (Log Interval: {log_interval} steps)")
            try:
                # Pass wandb config if needed, interval handled internally now
                return WandbStatsRecorder(
                    console_log_interval=0,  # Disable its console logs, main loop handles it
                    avg_window=avg_window,
                    wandb_log_interval=log_interval,  # Pass interval to WandB logger
                )
            except Exception as e:
                print(f"Error initializing WandbStatsRecorder: {e}. Falling back.")
                # Fallback to simple recorder if WandB fails
                self.wandb_config.USE_WANDB = False  # Disable wandb flag
                return SimpleStatsRecorder(
                    console_log_interval=log_interval,  # Use interval for console log in fallback
                    avg_window=avg_window,
                )
        else:
            print("Using Simple In-Memory Stats Recorder (WandB Disabled).")
            # Simple recorder only logs to console if interval > 0
            return SimpleStatsRecorder(
                console_log_interval=log_interval, avg_window=avg_window
            )

    def _initialize_rl_components(self):
        # (Mostly same, ensure correct configs are passed)
        print("Initializing/Re-initializing RL components...")
        self.envs: List[GameState] = self._initialize_envs()
        self.agent: DQNAgent = DQNAgent(
            config=self.model_config,
            dqn_config=self.dqn_config,
            env_config=self.env_config,
        )
        self.buffer: ReplayBufferBase = create_replay_buffer(
            config=self.buffer_config,
            dqn_config=self.dqn_config,  # Pass dqn_config for NStep gamma
        )
        self.stats_recorder: StatsRecorderBase = (
            self._initialize_stats_recorder()
        )  # Creates WandB or Simple
        self.trainer: Trainer = Trainer(
            envs=self.envs,
            agent=self.agent,
            buffer=self.buffer,
            stats_recorder=self.stats_recorder,
            env_config=self.env_config,
            dqn_config=self.dqn_config,
            train_config=self.train_config,
            buffer_config=self.buffer_config,
            # ExplorationConfig removed
            model_config=self.model_config,
        )
        print("RL components initialization finished.")

    def _cleanup_data(self):
        """Stops training, deletes checkpoints, buffer, logs, and re-initializes."""
        print("\n--- CLEANUP DATA INITIATED ---")
        self.is_training = False
        self.status = "Cleaning"
        self.cleanup_confirmation_active = False
        messages = []

        # 1. Trainer Cleanup (saves final state, flushes buffer)
        if hasattr(self, "trainer") and self.trainer:
            print("Running trainer cleanup...")
            try:
                self.trainer.cleanup()
            except Exception as e:
                print(f"Error during trainer cleanup: {e}")
        else:
            print("Trainer object not found, skipping trainer cleanup.")

        # 2. Delete Agent Checkpoint
        ckpt_path = self.model_config.SAVE_PATH
        try:
            if os.path.isfile(ckpt_path):
                os.remove(ckpt_path)
                messages.append("Agent ckpt deleted.")
            else:
                messages.append("Agent ckpt not found.")
        except OSError as e:
            messages.append(f"Error deleting agent ckpt: {e}")

        # 3. Delete Buffer State
        buffer_path = BUFFER_SAVE_PATH
        try:
            if os.path.isfile(buffer_path):
                os.remove(buffer_path)
                messages.append("Buffer state deleted.")
            else:
                messages.append("Buffer state not found.")
        except OSError as e:
            messages.append(f"Error deleting buffer: {e}")

        # 4. Delete WandB local files (optional, but good for full cleanup)
        if self.wandb_config.USE_WANDB and wandb.run is not None:
            try:
                wandb_dir = wandb.run.dir
                # Be careful here - this might delete other runs if not specific enough
                # Usually wandb creates a 'wandb/run-<date>-<id>' directory
                # For simplicity, just closing wandb run might be enough if offline mode isn't used heavily
                # For a true 'delete local', one might need to find the specific run dir and remove it.
                # Let's just ensure the run is finished properly.
                wandb.finish(exit_code=0, quiet=True)  # Finish current run cleanly
                messages.append("WandB run finished (local files may remain).")
            except Exception as e:
                messages.append(f"Error finishing WandB run: {e}")

        # 5. Re-initialize RL components
        print("Re-initializing RL components after cleanup...")
        self._initialize_rl_components()  # This will re-init WandB if enabled

        # Re-initialize renderer
        self.renderer = UIRenderer(self.screen, self.vis_config)

        self.cleanup_message = "\n".join(messages)
        self.last_cleanup_message_time = time.time()
        self.status = "Paused"  # Set back to Paused
        print("--- CLEANUP DATA COMPLETE ---")

    def _handle_input(self) -> bool:
        # (Largely same, no plotter hover)
        mouse_pos = pygame.mouse.get_pos()
        sw, sh = self.screen.get_size()
        train_btn_rect = pygame.Rect(10, 10, 100, 40)
        cleanup_btn_rect = pygame.Rect(train_btn_rect.right + 10, 10, 120, 40)
        confirm_yes_rect = pygame.Rect(sw // 2 - 110, sh // 2 + 30, 100, 40)
        confirm_no_rect = pygame.Rect(sw // 2 + 10, sh // 2 + 30, 100, 40)
        self.renderer.check_hover(mouse_pos)  # Check for tooltip hover

        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return False
            if event.type == pygame.VIDEORESIZE:
                try:
                    self.screen = pygame.display.set_mode(
                        (event.w, event.h), pygame.RESIZABLE
                    )
                    self.renderer.screen = (
                        self.screen
                    )  # Update renderer's screen reference
                    print(f"Window resized: {event.w}x{event.h}")
                except pygame.error as e:
                    print(f"Error resizing window: {e}")

            if event.type == pygame.KEYDOWN:
                if event.key == pygame.K_ESCAPE:
                    if self.cleanup_confirmation_active:
                        self.cleanup_confirmation_active = False
                        self.cleanup_message = "Cleanup cancelled."
                        self.last_cleanup_message_time = time.time()
                    else:
                        return False  # Exit app on ESC if not confirming
                elif event.key == pygame.K_p and not self.cleanup_confirmation_active:
                    self.is_training = not self.is_training
                    print(
                        f"Training {'STARTED' if self.is_training else 'PAUSED'} (P key)"
                    )
                    self._try_save_checkpoint()  # Save on pause

            if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1:  # Left click
                if self.cleanup_confirmation_active:
                    if confirm_yes_rect.collidepoint(mouse_pos):
                        self._cleanup_data()
                    elif confirm_no_rect.collidepoint(mouse_pos):
                        self.cleanup_confirmation_active = False
                        self.cleanup_message = "Cleanup cancelled."
                        self.last_cleanup_message_time = time.time()
                else:
                    if train_btn_rect.collidepoint(mouse_pos):
                        self.is_training = not self.is_training
                        print(
                            f"Training {'STARTED' if self.is_training else 'PAUSED'} (Button)"
                        )
                        self._try_save_checkpoint()  # Save on pause
                    elif cleanup_btn_rect.collidepoint(mouse_pos):
                        self.is_training = (
                            False  # Pause training before showing confirmation
                        )
                        self.cleanup_confirmation_active = True
                        print("Cleanup requested.")
        return True

    def _try_save_checkpoint(self):
        """Saves checkpoint if trainer exists and has the method."""
        # Check if training is actually paused and the trainer/method exist
        if not self.is_training and hasattr(self.trainer, "_save_checkpoint"):
            print("Saving checkpoint on pause...")
            try:
                self.trainer._save_checkpoint(is_final=False)
            except Exception as e:
                print(f"Error saving checkpoint on pause: {e}")

    def _update(self):
        """Performs training step and updates status."""
        # Determine current status
        if not self.is_training:
            self.status = "Paused"
        elif self.trainer.global_step < self.train_config.LEARN_START_STEP:
            self.status = "Buffering"
        elif self.cleanup_confirmation_active:
            self.status = "Confirm Cleanup"
        else:
            self.status = "Training"

        if self.status == "Paused" or self.status == "Confirm Cleanup":
            return  # Don't step trainer

        try:
            step_start_time = time.time()
            self.trainer.step()  # Trainer calls stats_recorder internally
            step_duration = time.time() - step_start_time

            # Log summary (now handled by WandbStatsRecorder based on its interval)
            # We still might want a simple console log more frequently
            # Let's print basic info here regardless of wandb interval
            if (
                self.trainer.global_step % (self.num_envs * 100) == 0
            ):  # Log every 100 trainer steps
                summary = self.stats_recorder.get_summary(self.trainer.global_step)
                sps = summary.get("steps_per_second", 0.0)
                rl_score = summary.get("avg_score_100", 0.0)
                loss = summary.get("avg_loss_100", 0.0)
                # print(f"Step: {self.trainer.global_step/1e6:.2f}M | SPS: {sps:.0f} | RLScore: {rl_score:.2f} | Loss: {loss:.4f}")

            # Optional delay
            if self.vis_config.VISUAL_STEP_DELAY > 0:
                time.sleep(max(0, self.vis_config.VISUAL_STEP_DELAY - step_duration))

        except Exception as e:
            print(
                f"\n--- ERROR DURING TRAINING UPDATE (Step: {getattr(self.trainer, 'global_step', 'N/A')}) ---"
            )
            import traceback

            traceback.print_exc()
            print(f"--- Pausing training due to error. Check logs. ---")
            self.is_training = False
            self.status = "Error"  # Update status

    def _render(self):
        """Delegates rendering to the UIRenderer."""
        stats_summary = self.stats_recorder.get_summary(self.trainer.global_step)
        buffer_capacity = getattr(self.buffer, "capacity", 0)

        self.renderer.render_all(
            is_training=self.is_training,
            status=self.status,  # Pass current status
            stats_summary=stats_summary,
            buffer_capacity=buffer_capacity,
            envs=self.envs,
            num_envs=self.num_envs,
            env_config=self.env_config,
            cleanup_confirmation_active=self.cleanup_confirmation_active,
            cleanup_message=self.cleanup_message,
            last_cleanup_message_time=self.last_cleanup_message_time,
            wandb_run_url=(
                wandb.run.url if self.wandb_config.USE_WANDB and wandb.run else None
            ),  # Pass WandB URL
        )
        # Check if status message time expired
        if time.time() - self.last_cleanup_message_time >= 5.0:
            self.cleanup_message = ""

    def run(self):
        # (Same as before)
        print("Starting main application loop...")
        running = True
        while running:
            running = self._handle_input()
            if not running:
                break

            self._update()
            self._render()

            self.clock.tick(
                self.vis_config.FPS if self.vis_config.FPS > 0 else 0
            )  # Limit FPS

        print("Exiting application...")
        if hasattr(self, "trainer") and self.trainer:
            print("Performing final trainer cleanup...")
            self.trainer.cleanup()  # Saves checkpoint, flushes buffer
        if hasattr(self, "stats_recorder") and self.stats_recorder:
            self.stats_recorder.close()  # Closes WandB run if active

        pygame.quit()
        print("Application exited.")


def run_pre_checks():
    # (Same as before)
    print("--- Pre-Run Checks ---")
    try:
        print("Checking GameState and Configuration Compatibility...")
        gs_test = GameState()
        gs_test.reset()
        s_test = gs_test.get_state()
        if len(s_test) != EnvConfig.STATE_DIM:
            raise ValueError(
                f"State Dim Mismatch! GameState:{len(s_test)}, EnvConfig:{EnvConfig.STATE_DIM}"
            )
        print(f"GameState state dimension check PASSED (Length: {len(s_test)}).")
        _ = gs_test.valid_actions()
        print("GameState valid_actions check PASSED.")
        if not hasattr(gs_test, "game_score"):
            raise AttributeError("GameState missing 'game_score' attribute!")
        print("GameState 'game_score' attribute check PASSED.")
        if not hasattr(gs_test, "lines_cleared_this_episode"):
            raise AttributeError(
                "GameState missing 'lines_cleared_this_episode' attribute!"
            )
        print("GameState 'lines_cleared_this_episode' attribute check PASSED.")
        del gs_test
        print("--- Pre-Run Checks Complete ---")
        return True
    except (NameError, ImportError) as e:
        print(f"FATAL ERROR: Import/Name error: {e}")
    except (ValueError, AttributeError) as e:
        print(f"FATAL ERROR during pre-run checks: {e}")
    except Exception as e:
        print(f"FATAL ERROR during GameState pre-check: {e}")
        import traceback

        traceback.print_exc()
    sys.exit(1)


if __name__ == "__main__":
    # Ensure directories exist
    os.makedirs("checkpoints", exist_ok=True)
    os.makedirs("logs", exist_ok=True)  # Keep logs dir, wandb might use it too
    os.makedirs("ui", exist_ok=True)
    os.makedirs("agent/networks", exist_ok=True)  # Keep networks dir
    os.makedirs("stats", exist_ok=True)

    if run_pre_checks():
        app = MainApp()
        app.run()


File: visualization/__init__.py


File: ui/renderer.py
# File: ui/renderer.py
import pygame
import math
import time
from typing import List, Dict, Any, Optional, Tuple
from collections import deque

from config import (
    VisConfig,
    EnvConfig,
    ModelConfig,
    DQNConfig,
    DEVICE,
    BufferConfig,
    StatsConfig,
    TrainConfig,
    WandbConfig,
)
from environment.game_state import GameState
from environment.shape import Shape
from environment.triangle import Triangle

# Removed Plotter import

# Tooltip definitions (Updated, removed plot tooltips)
TOOLTIP_TEXTS = {
    "Status": "Current state: Paused, Buffering (collecting initial data), Training, Confirm Cleanup, or Error.",
    "Global Steps": "Total environment steps across all parallel environments.",
    "Total Episodes": "Total completed episodes across all environments.",
    "Steps/Sec": f"Average global steps processed per second (rolling average over ~{StatsConfig.STATS_AVG_WINDOW} logs).",
    "Avg RL Score": f"Average RL reward sum per episode (last {StatsConfig.STATS_AVG_WINDOW} eps).",
    "Best RL Score": "Highest RL reward sum in a single episode this run.",
    "Avg Game Score": f"Average game score per episode (last {StatsConfig.STATS_AVG_WINDOW} eps).",
    "Best Game Score": "Highest game score in a single episode this run.",
    "Avg Length": f"Average steps per episode (last {StatsConfig.STATS_AVG_WINDOW} eps).",
    "Avg Lines Clr": f"Average lines cleared per episode (last {StatsConfig.STATS_AVG_WINDOW} eps).",
    "Avg Loss": f"Average DQN loss (last {StatsConfig.STATS_AVG_WINDOW} training steps).",
    "Avg Max Q": f"Average max predicted Q-value (last {StatsConfig.STATS_AVG_WINDOW} training batches).",
    # "Epsilon": "Exploration rate (Always 0.0 with Noisy Nets).", # Removed Epsilon display
    "PER Beta": f"PER Importance Sampling exponent (anneals {BufferConfig.PER_BETA_START:.1f} -> 1.0). {BufferConfig.PER_BETA_FRAMES/1e6:.1f}M steps.",
    "Buffer": f"Replay buffer fill status ({BufferConfig.REPLAY_BUFFER_SIZE / 1e6:.1f}M capacity).",
    "Train Button": "Click to Start/Pause the training process (or press 'P').",
    "Cleanup Button": "Click to delete saved agent, buffer, and logs, then restart.",
    "Device": f"Computation device ({DEVICE.type.upper()}).",
    "Network": f"Agent Network Architecture (CNN+MLP Fusion). Noisy={DQNConfig.USE_NOISY_NETS}, Dueling={DQNConfig.USE_DUELING}",
    "WandB Status": "Status of Weights & Biases logging. Click URL if available.",
}


class UIRenderer:
    """Handles rendering the Pygame UI without the plotter."""

    def __init__(
        self, screen: pygame.Surface, vis_config: VisConfig
    ):  # No plotter needed
        self.screen = screen
        self.vis_config = vis_config
        # self.plotter = plotter # Removed

        # Fonts
        try:
            self.font_ui = pygame.font.SysFont(None, 24)
            self.font_env_score = pygame.font.SysFont(None, 18)
            self.font_env_overlay = pygame.font.SysFont(None, 36)
            self.font_tooltip = pygame.font.SysFont(None, 18)
            self.font_status = pygame.font.SysFont(None, 28)
            self.font_link = pygame.font.SysFont(None, 20)  # For WandB link
        except Exception as e:
            print(f"Warning: Error initializing SysFont: {e}. Using default font.")
            self.font_ui = pygame.font.Font(None, 24)
            self.font_env_score = pygame.font.Font(None, 18)
            self.font_env_overlay = pygame.font.Font(None, 36)
            self.font_tooltip = pygame.font.Font(None, 18)
            self.font_status = pygame.font.Font(None, 28)
            self.font_link = pygame.font.Font(None, 20)

        self.stat_rects: Dict[str, pygame.Rect] = {}
        self.hovered_stat_key: Optional[str] = None
        self.wandb_link_rect: Optional[pygame.Rect] = None

    def _render_left_panel(
        self,
        is_training: bool,
        status: str,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        wandb_run_url: Optional[str],  # <<< NEW >>> WandB URL
    ):
        """Renders the left information panel (text stats only)."""
        current_width, current_height = self.screen.get_size()
        lp_width = min(current_width, max(250, self.vis_config.LEFT_PANEL_WIDTH))
        lp_rect = pygame.Rect(0, 0, lp_width, current_height)

        status_color_map = {
            "Paused": (30, 30, 30),
            "Buffering": (30, 40, 30),
            "Training": (40, 30, 30),
            "Confirm Cleanup": (50, 20, 20),
            "Cleaning": (60, 30, 30),
            "Error": (60, 0, 0),
        }
        bg_color = status_color_map.get(status, (30, 30, 30))
        pygame.draw.rect(self.screen, bg_color, lp_rect)

        # Buttons
        train_btn_rect = pygame.Rect(10, 10, 100, 40)
        pygame.draw.rect(self.screen, (70, 70, 70), train_btn_rect, border_radius=5)
        btn_text = "Pause" if is_training else "Train"
        lbl_surf = self.font_ui.render(btn_text, True, VisConfig.WHITE)
        self.screen.blit(lbl_surf, lbl_surf.get_rect(center=train_btn_rect.center))
        self.stat_rects["Train Button"] = train_btn_rect  # Add tooltip

        cleanup_btn_rect = pygame.Rect(train_btn_rect.right + 10, 10, 120, 40)
        pygame.draw.rect(self.screen, (100, 40, 40), cleanup_btn_rect, border_radius=5)
        cleanup_lbl_surf = self.font_ui.render("Cleanup Data", True, VisConfig.WHITE)
        self.screen.blit(
            cleanup_lbl_surf, cleanup_lbl_surf.get_rect(center=cleanup_btn_rect.center)
        )
        self.stat_rects["Cleanup Button"] = cleanup_btn_rect  # Add tooltip

        # Status Text
        status_surf = self.font_status.render(
            f"Status: {status}", True, VisConfig.YELLOW
        )
        status_rect = status_surf.get_rect(topleft=(10, train_btn_rect.bottom + 10))
        self.screen.blit(status_surf, status_rect)
        self.stat_rects["Status"] = status_rect

        # Info Text & Tooltips
        self.stat_rects.clear()  # Clear old rects
        # Re-add button/status rects after clear
        self.stat_rects["Train Button"] = train_btn_rect
        self.stat_rects["Cleanup Button"] = cleanup_btn_rect
        self.stat_rects["Status"] = status_rect
        self.wandb_link_rect = None  # Reset link rect

        buffer_size = stats_summary.get("buffer_size", 0)
        buffer_perc = (
            (buffer_size / buffer_capacity * 100) if buffer_capacity > 0 else 0.0
        )

        info_lines_data = [
            (
                "Global Steps",
                f"{stats_summary.get('global_step', 0)/1e6:.2f}M / {TrainConfig.TOTAL_TRAINING_STEPS/1e6:.1f}M",
            ),
            ("Total Episodes", f"{stats_summary.get('total_episodes', 0)}"),
            ("Steps/Sec", f"{stats_summary.get('steps_per_second', 0.0):.1f}"),
            (
                "Avg RL Score",
                f"({stats_summary.get('num_ep_scores', 0)}): {stats_summary.get('avg_score_100', 0.0):.2f}",
            ),
            ("Best RL Score", f"{stats_summary.get('best_score', 0.0):.2f}"),
            (
                "Avg Game Score",
                f"({stats_summary.get('num_game_scores', 0)}): {stats_summary.get('avg_game_score_100', 0.0):.1f}",
            ),
            ("Best Game Score", f"{stats_summary.get('best_game_score', 0.0):.1f}"),
            (
                "Avg Length",
                f"({stats_summary.get('num_ep_lengths', 0)}): {stats_summary.get('avg_length_100', 0.0):.1f}",
            ),
            (
                "Avg Lines Clr",
                f"({stats_summary.get('num_lines_cleared', 0)}): {stats_summary.get('avg_lines_cleared_100', 0.0):.2f}",
            ),
            (
                "Avg Loss",
                f"({stats_summary.get('num_losses', 0)}): {stats_summary.get('avg_loss_100', 0.0):.4f}",
            ),
            (
                "Avg Max Q",
                f"({stats_summary.get('num_avg_max_qs', 0)}): {stats_summary.get('avg_max_q_100', 0.0):.3f}",
            ),
            # ("Epsilon", f"{stats_summary.get('epsilon', 0.0):.3f}"), # Removed Epsilon
            (
                "PER Beta",
                (
                    f"{stats_summary.get('beta', 0.0):.3f}"
                    if BufferConfig.USE_PER
                    else "N/A"
                ),
            ),
            (
                "Buffer",
                f"{buffer_size/1e6:.2f}M / {buffer_capacity/1e6:.1f}M ({buffer_perc:.1f}%)",
            ),
            ("Device", f"{DEVICE.type.upper()}"),
            ("Network", f"CNN+MLP Fusion"),  # Simplified network name display
        ]

        text_y_start = status_rect.bottom + 10  # Start below status text
        line_height = self.font_ui.get_linesize()

        for idx, (key, value_str) in enumerate(info_lines_data):
            line_text = f"{key}: {value_str}"
            line_surf = self.font_ui.render(line_text, True, VisConfig.WHITE)
            line_rect = line_surf.get_rect(
                topleft=(10, text_y_start + idx * line_height)
            )
            # Limit rect width to panel width for tooltip detection
            line_rect.width = min(line_rect.width, lp_width - 20)
            self.screen.blit(line_surf, line_rect)
            self.stat_rects[key] = line_rect

        # --- WandB Status --- <<< NEW >>>
        wandb_y_start = text_y_start + len(info_lines_data) * line_height + 10
        wandb_status_text = "WandB: "
        wandb_status_color = VisConfig.RED
        if WandbConfig.USE_WANDB:
            if wandb_run_url:
                wandb_status_text += "Running (Click URL)"
                wandb_status_color = VisConfig.GOOGLE_COLORS[0]  # Green
            else:
                wandb_status_text += "Initializing..."
                wandb_status_color = VisConfig.YELLOW
        else:
            wandb_status_text += "Disabled"
            wandb_status_color = VisConfig.LIGHTG  # Gray

        wandb_surf = self.font_ui.render(wandb_status_text, True, wandb_status_color)
        wandb_rect = wandb_surf.get_rect(topleft=(10, wandb_y_start))
        self.screen.blit(wandb_surf, wandb_rect)
        self.stat_rects["WandB Status"] = wandb_rect

        # Display URL if available and clickable
        if wandb_run_url:
            url_surf = self.font_link.render("View Run -->", True, VisConfig.BLUE)
            self.wandb_link_rect = url_surf.get_rect(
                topleft=(wandb_rect.right + 10, wandb_y_start + 2)
            )
            # Underline effect
            # pygame.draw.line(self.screen, VisConfig.BLUE, self.wandb_link_rect.bottomleft, self.wandb_link_rect.bottomright, 1)
            self.screen.blit(url_surf, self.wandb_link_rect)

        # --- Plot Area Removed ---

    # _render_shape_preview remains the same
    def _render_shape_preview(self, surf: pygame.Surface, shape: Shape, cell_size: int):
        if not shape or not shape.triangles:
            return
        min_r, min_c, max_r, max_c = shape.bbox()
        shape_h_cells = max_r - min_r + 1
        shape_w_cells = max_c - min_c + 1
        total_w_pixels = shape_w_cells * (cell_size * 0.75) + (cell_size * 0.25)
        total_h_pixels = shape_h_cells * cell_size
        offset_x = (surf.get_width() - total_w_pixels) / 2 - min_c * (cell_size * 0.75)
        offset_y = (surf.get_height() - total_h_pixels) / 2 - min_r * cell_size
        for dr, dc, up in shape.triangles:
            tri = Triangle(row=dr, col=dc, is_up=up)
            pts = tri.get_points(ox=offset_x, oy=offset_y, cw=cell_size, ch=cell_size)
            pygame.draw.polygon(surf, shape.color, pts)

    # _render_env remains the same
    def _render_env(
        self, surf: pygame.Surface, env: GameState, cell_w: int, cell_h: int
    ):
        try:
            bg_color = (
                VisConfig.YELLOW
                if env.is_blinking()
                else (
                    (30, 30, 100)
                    if env.is_frozen() and not env.is_over()
                    else (20, 20, 20)
                )
            )
            surf.fill(bg_color)

            # Render Grid Triangles
            if hasattr(env, "grid") and hasattr(env.grid, "triangles"):
                for r in range(env.grid.rows):
                    for c in range(env.grid.cols):
                        t = env.grid.triangles[r][c]
                        if not hasattr(t, "get_points"):
                            continue
                        try:
                            pts = t.get_points(ox=0, oy=0, cw=cell_w, ch=cell_h)
                            color = VisConfig.GRAY
                            if t.is_death:
                                color = VisConfig.BLACK
                            elif t.is_occupied:
                                color = t.color if t.color else VisConfig.RED
                            pygame.draw.polygon(surf, color, pts)
                        except Exception as e_render:
                            print(f"Error rendering tri ({r},{c}): {e_render}")
            else:  # Grid data invalid
                pygame.draw.rect(surf, (255, 0, 0), surf.get_rect(), 2)
                err_txt = self.font_env_overlay.render(
                    "Invalid Grid", True, VisConfig.RED
                )
                surf.blit(err_txt, err_txt.get_rect(center=surf.get_rect().center))

            # Render Scores
            rl_score_val = env.score
            game_score_val = env.game_score
            score_surf = self.font_env_score.render(
                f"GS: {game_score_val} | R: {rl_score_val:.1f}",
                True,
                VisConfig.WHITE,
                (0, 0, 0, 180),
            )
            surf.blit(score_surf, (4, 4))

            # Render Overlays
            if env.is_over():
                overlay = pygame.Surface(surf.get_size(), pygame.SRCALPHA)
                overlay.fill((100, 0, 0, 180))
                surf.blit(overlay, (0, 0))
                over_text = self.font_env_overlay.render(
                    "GAME OVER", True, VisConfig.WHITE
                )
                surf.blit(over_text, over_text.get_rect(center=surf.get_rect().center))
            elif (
                env.is_frozen() and not env.is_blinking()
            ):  # Don't show frozen text during blink
                freeze_text = self.font_env_overlay.render(
                    "Frozen", True, VisConfig.WHITE
                )
                surf.blit(
                    freeze_text,
                    freeze_text.get_rect(
                        center=(surf.get_width() // 2, surf.get_height() - 15)
                    ),
                )

        except AttributeError as e:
            pygame.draw.rect(surf, (255, 0, 0), surf.get_rect(), 2)
            err_txt = self.font_env_overlay.render(
                f"Attr Err: {e}", True, VisConfig.RED, VisConfig.BLACK
            )
            surf.blit(err_txt, err_txt.get_rect(center=surf.get_rect().center))
        except Exception as e:
            print(f"Unexpected Render Error in _render_env: {e}")
            pygame.draw.rect(surf, (255, 0, 0), surf.get_rect(), 2)
            import traceback

            traceback.print_exc()

    # _render_game_area remains the same
    def _render_game_area(
        self, envs: List[GameState], num_envs: int, env_config: EnvConfig
    ):
        current_width, current_height = self.screen.get_size()
        lp_width = min(current_width, max(250, self.vis_config.LEFT_PANEL_WIDTH))
        ga_rect = pygame.Rect(lp_width, 0, current_width - lp_width, current_height)

        if num_envs <= 0 or ga_rect.width <= 0 or ga_rect.height <= 0:
            return

        render_limit = self.vis_config.NUM_ENVS_TO_RENDER
        num_envs_to_render = (
            num_envs if render_limit <= 0 else min(num_envs, render_limit)
        )
        if num_envs_to_render <= 0:
            return

        aspect_ratio = ga_rect.width / ga_rect.height
        cols_env = max(1, int(math.sqrt(num_envs_to_render * aspect_ratio)))
        rows_env = math.ceil(num_envs_to_render / cols_env)

        total_spacing_w = (cols_env + 1) * self.vis_config.ENV_SPACING
        total_spacing_h = (rows_env + 1) * self.vis_config.ENV_SPACING
        cell_w = (ga_rect.width - total_spacing_w) // cols_env if cols_env > 0 else 0
        cell_h = (ga_rect.height - total_spacing_h) // rows_env if rows_env > 0 else 0

        if cell_w > 10 and cell_h > 10:
            env_idx = 0
            for r in range(rows_env):
                for c in range(cols_env):
                    if env_idx >= num_envs_to_render:
                        break
                    env_x = (
                        ga_rect.x + self.vis_config.ENV_SPACING * (c + 1) + c * cell_w
                    )
                    env_y = (
                        ga_rect.y + self.vis_config.ENV_SPACING * (r + 1) + r * cell_h
                    )
                    env_rect = pygame.Rect(env_x, env_y, cell_w, cell_h)
                    try:
                        sub_surf = self.screen.subsurface(env_rect)
                        tri_cell_w = cell_w / (env_config.COLS * 0.75 + 0.25)
                        tri_cell_h = cell_h / env_config.ROWS
                        self._render_env(
                            sub_surf, envs[env_idx], int(tri_cell_w), int(tri_cell_h)
                        )
                        # Shape Previews
                        available_shapes = envs[env_idx].get_shapes()
                        if available_shapes:
                            preview_dim = max(10, min(cell_w // 6, cell_h // 6, 25))
                            preview_spacing = 4
                            total_preview_width = (
                                len(available_shapes) * preview_dim
                                + max(0, len(available_shapes) - 1) * preview_spacing
                            )
                            start_x = (
                                sub_surf.get_width()
                                - total_preview_width
                                - preview_spacing
                            )
                            start_y = preview_spacing
                            for i, shape in enumerate(available_shapes):
                                preview_x = start_x + i * (
                                    preview_dim + preview_spacing
                                )
                                temp_shape_surf = pygame.Surface(
                                    (preview_dim, preview_dim), pygame.SRCALPHA
                                )
                                temp_shape_surf.fill((0, 0, 0, 0))
                                preview_cell_size = max(2, preview_dim // 4)
                                self._render_shape_preview(
                                    temp_shape_surf, shape, preview_cell_size
                                )
                                sub_surf.blit(temp_shape_surf, (preview_x, start_y))
                    except ValueError:
                        pygame.draw.rect(
                            self.screen, (0, 0, 50), env_rect, 1
                        )  # Subsurface error
                    except Exception as e_render_env:
                        print(f"Error rendering env {env_idx}: {e_render_env}")
                        pygame.draw.rect(self.screen, (50, 0, 50), env_rect, 1)
                    env_idx += 1
        else:
            err_surf = self.font_ui.render(
                f"Envs Too Small ({cell_w}x{cell_h})", True, VisConfig.GRAY
            )
            self.screen.blit(err_surf, err_surf.get_rect(center=ga_rect.center))

        # Display text if not all envs are rendered
        if num_envs_to_render < num_envs:
            info_surf = self.font_ui.render(
                f"Rendering {num_envs_to_render}/{num_envs} Envs",
                True,
                VisConfig.YELLOW,
                VisConfig.BLACK,
            )
            self.screen.blit(
                info_surf,
                info_surf.get_rect(bottomright=(ga_rect.right - 5, ga_rect.bottom - 5)),
            )

    # _render_cleanup_confirmation remains the same
    def _render_cleanup_confirmation(self):
        current_width, current_height = self.screen.get_size()
        overlay = pygame.Surface((current_width, current_height), pygame.SRCALPHA)
        overlay.fill((0, 0, 0, 200))
        self.screen.blit(overlay, (0, 0))
        center_x, center_y = current_width // 2, current_height // 2
        prompt_l1 = self.font_env_overlay.render(
            "DELETE ALL SAVED DATA?", True, VisConfig.RED
        )
        self.screen.blit(
            prompt_l1, prompt_l1.get_rect(center=(center_x, center_y - 60))
        )
        prompt_l2 = self.font_ui.render(
            "(Agent Checkpoint, Buffer State, Logs)", True, VisConfig.WHITE
        )  # Updated text
        self.screen.blit(
            prompt_l2, prompt_l2.get_rect(center=(center_x, center_y - 25))
        )
        prompt_l3 = self.font_ui.render(
            "This action cannot be undone!", True, VisConfig.YELLOW
        )
        self.screen.blit(prompt_l3, prompt_l3.get_rect(center=(center_x, center_y)))
        confirm_yes_rect = pygame.Rect(center_x - 110, center_y + 30, 100, 40)
        confirm_no_rect = pygame.Rect(center_x + 10, center_y + 30, 100, 40)
        pygame.draw.rect(self.screen, (0, 150, 0), confirm_yes_rect, border_radius=5)
        pygame.draw.rect(self.screen, (150, 0, 0), confirm_no_rect, border_radius=5)
        yes_text = self.font_ui.render("YES", True, VisConfig.WHITE)
        no_text = self.font_ui.render("NO", True, VisConfig.WHITE)
        self.screen.blit(yes_text, yes_text.get_rect(center=confirm_yes_rect.center))
        self.screen.blit(no_text, no_text.get_rect(center=confirm_no_rect.center))

    # _render_status_message remains the same
    def _render_status_message(self, message: str, last_message_time: float):
        if message and (time.time() - last_message_time < 5.0):
            current_width, current_height = self.screen.get_size()
            lines = message.split("\n")
            max_width = 0
            msg_surfs = []
            for line in lines:
                msg_surf = self.font_ui.render(
                    line, True, VisConfig.YELLOW, VisConfig.BLACK
                )
                msg_surfs.append(msg_surf)
                max_width = max(max_width, msg_surf.get_width())
            total_height = (
                sum(s.get_height() for s in msg_surfs) + max(0, len(lines) - 1) * 2
            )
            bg_rect = pygame.Rect(0, 0, max_width + 10, total_height + 10)
            bg_rect.midbottom = (current_width // 2, current_height - 10)
            pygame.draw.rect(self.screen, VisConfig.BLACK, bg_rect, border_radius=3)
            current_y = bg_rect.top + 5
            for msg_surf in msg_surfs:
                msg_rect = msg_surf.get_rect(midtop=(bg_rect.centerx, current_y))
                self.screen.blit(msg_surf, msg_rect)
                current_y += msg_surf.get_height() + 2
            return True
        return False

    # _render_tooltip remains the same
    def _render_tooltip(self):
        if self.hovered_stat_key and self.hovered_stat_key in TOOLTIP_TEXTS:
            tooltip_text = TOOLTIP_TEXTS[self.hovered_stat_key]
            mouse_pos = pygame.mouse.get_pos()
            # Simple multi-line split for long tooltips
            lines = []
            max_width = 300  # Approx max width before wrapping
            words = tooltip_text.split(" ")
            current_line = ""
            for word in words:
                test_line = current_line + " " + word if current_line else word
                test_surf = self.font_tooltip.render(test_line, True, VisConfig.BLACK)
                if test_surf.get_width() <= max_width:
                    current_line = test_line
                else:
                    lines.append(current_line)
                    current_line = word
            lines.append(current_line)

            line_surfs = [
                self.font_tooltip.render(line, True, VisConfig.BLACK) for line in lines
            ]
            total_height = sum(s.get_height() for s in line_surfs)
            max_line_width = max(s.get_width() for s in line_surfs)

            padding = 5
            tooltip_rect = pygame.Rect(
                mouse_pos[0] + 15,
                mouse_pos[1] + 10,
                max_line_width + padding * 2,
                total_height + padding * 2,
            )
            tooltip_rect.clamp_ip(self.screen.get_rect())  # Keep within screen bounds

            pygame.draw.rect(
                self.screen, VisConfig.YELLOW, tooltip_rect, border_radius=3
            )
            pygame.draw.rect(
                self.screen, VisConfig.BLACK, tooltip_rect, 1, border_radius=3
            )

            current_y = tooltip_rect.y + padding
            for surf in line_surfs:
                self.screen.blit(surf, (tooltip_rect.x + padding, current_y))
                current_y += surf.get_height()

    # check_hover remains the same (checks stat_rects)
    def check_hover(self, mouse_pos: Tuple[int, int]):
        self.hovered_stat_key = None
        for key, rect in self.stat_rects.items():
            if rect.collidepoint(mouse_pos):
                self.hovered_stat_key = key
                return  # Found one, stop checking

        # Check WandB link hover <<< NEW >>>
        if self.wandb_link_rect and self.wandb_link_rect.collidepoint(mouse_pos):
            self.hovered_stat_key = "WandB Status"  # Reuse status tooltip

    def render_all(
        self,
        is_training: bool,
        status: str,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        envs: List[GameState],
        num_envs: int,
        env_config: EnvConfig,
        cleanup_confirmation_active: bool,
        cleanup_message: str,
        last_cleanup_message_time: float,
        wandb_run_url: Optional[str],  # <<< NEW >>>
    ):
        """Renders all UI components."""
        try:
            self.screen.fill(VisConfig.BLACK)
            self._render_left_panel(
                is_training, status, stats_summary, buffer_capacity, wandb_run_url
            )
            self._render_game_area(envs, num_envs, env_config)

            if cleanup_confirmation_active:
                self._render_cleanup_confirmation()

            message_active = self._render_status_message(
                cleanup_message, last_cleanup_message_time
            )

            if not cleanup_confirmation_active and not message_active:
                self._render_tooltip()

            pygame.display.flip()
        except pygame.error as e:
            print(f"Pygame rendering error: {e}")
        except Exception as e:
            print(f"Unexpected critical rendering error: {e}")
            import traceback

            traceback.print_exc()


File: ui/__init__.py


File: training/__init__.py


File: training/trainer.py
# File: training/trainer.py
import time
import torch
import numpy as np
import os
import pickle
import random
from typing import List, Optional, Union, Tuple
from collections import deque
from typing import Deque

from config import (
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    DEVICE,
    BUFFER_SAVE_PATH,
    # ExplorationConfig removed
)
from environment.game_state import GameState
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from agent.replay_buffer.buffer_utils import create_replay_buffer
from stats.stats_recorder import StatsRecorderBase  # Base class
from utils.helpers import ensure_numpy
from utils.types import (
    ActionType,
    PrioritizedNumpyBatch,
    PrioritizedNumpyNStepBatch,
    NumpyBatch,
    NumpyNStepBatch,
)


class Trainer:
    """Orchestrates the DQN training process using Noisy Nets."""

    def __init__(
        self,
        envs: List[GameState],
        agent: DQNAgent,
        buffer: ReplayBufferBase,
        stats_recorder: StatsRecorderBase,
        env_config: EnvConfig,
        dqn_config: DQNConfig,
        train_config: TrainConfig,
        buffer_config: BufferConfig,
        # exploration_config removed
        model_config: ModelConfig,
    ):
        print("[Trainer] Initializing...")
        self.envs = envs
        self.agent = agent
        self.buffer = buffer
        self.stats_recorder = stats_recorder
        self.num_envs = env_config.NUM_ENVS
        self.device = DEVICE

        # Store configs
        self.env_config = env_config
        self.dqn_config = dqn_config
        self.train_config = train_config
        self.buffer_config = buffer_config
        self.model_config = model_config

        # Hardcoded as Noisy Nets are default now
        self.use_noisy_nets = True

        # State / Trackers
        self.global_step = 0
        self.episode_count = 0
        try:
            self.current_states = [ensure_numpy(env.reset()) for env in self.envs]
        except Exception as e:
            print(f"FATAL ERROR during initial reset: {e}")
            raise e
        self.current_episode_scores = np.zeros(self.num_envs, dtype=np.float32)
        self.current_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)
        self.current_episode_game_scores = np.zeros(self.num_envs, dtype=np.int32)
        self.current_episode_lines_cleared = np.zeros(self.num_envs, dtype=np.int32)

        # Load state
        self._load_checkpoint()
        self._load_buffer_state()

        # Initial PER Beta update & logging
        initial_beta = self._update_beta()
        # Log initial state to recorder
        self.stats_recorder.record_step(
            {
                "buffer_size": len(self.buffer),
                "epsilon": 0.0,  # Epsilon is 0 with Noisy Nets
                "beta": initial_beta,
            }
        )

        print(
            f"[Trainer] Init complete. Start Step={self.global_step}, Ep={self.episode_count}, Buf={len(self.buffer)}, Beta={initial_beta:.4f}, Noisy={self.use_noisy_nets}"
        )

    def _load_checkpoint(self):
        # (Same as before)
        if not self.model_config.LOAD_MODEL:
            print("[Trainer] LOAD_MODEL=False. Starting fresh.")
            return
        save_path = self.model_config.SAVE_PATH
        if os.path.isfile(save_path):
            print(f"[Trainer] Loading agent checkpoint: {save_path}")
            try:
                checkpoint = torch.load(save_path, map_location=self.device)
                self.agent.load_state_dict(checkpoint["agent_state_dict"])
                self.global_step = checkpoint.get("global_step", 0)
                self.episode_count = checkpoint.get("episode_count", 0)
                print(
                    f"[Trainer] Checkpoint loaded. Resuming from step {self.global_step}, ep {self.episode_count}"
                )
            except FileNotFoundError:
                print(
                    f"[Trainer] Checkpoint file disappeared? ({save_path}). Starting fresh."
                )
                self._reset_trainer_state()
            except KeyError as e:
                print(
                    f"[Trainer] Checkpoint missing key '{e}'. Incompatible? Starting fresh."
                )
                self._reset_trainer_state()
            except Exception as e:
                print(
                    f"[Trainer] CRITICAL ERROR loading checkpoint: {e}. Starting fresh."
                )
                import traceback

                traceback.print_exc()
                self._reset_trainer_state()
        else:
            print(f"[Trainer] No checkpoint found at {save_path}. Starting fresh.")
            self._reset_trainer_state()

    def _reset_trainer_state(self):
        self.global_step = 0
        self.episode_count = 0

    def _load_buffer_state(self):
        # (Same as before)
        if not self.buffer_config.LOAD_BUFFER:
            print("[Trainer] LOAD_BUFFER=False. Skipping buffer load.")
            return
        buffer_path = BUFFER_SAVE_PATH
        if os.path.isfile(buffer_path):
            print(f"[Trainer] Attempting to load buffer state: {buffer_path}")
            try:
                if hasattr(self.buffer, "load_state"):
                    self.buffer.load_state(buffer_path)
                    print(f"[Trainer] Buffer state loaded. Size: {len(self.buffer)}")
                else:
                    print("[Trainer] Warning: Buffer has no 'load_state' method.")
            except FileNotFoundError:
                print(
                    f"[Trainer] Buffer file disappeared? ({buffer_path}). Starting empty."
                )
            except (EOFError, pickle.UnpicklingError, ImportError, AttributeError) as e:
                print(
                    f"[Trainer] ERROR loading buffer (incompatible/corrupt?): {e}. Starting empty."
                )
                self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)
            except Exception as e:
                print(f"[Trainer] CRITICAL ERROR loading buffer: {e}. Starting empty.")
                import traceback

                traceback.print_exc()
                self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)
        else:
            print(
                f"[Trainer] No buffer state file found at {buffer_path}. Starting empty."
            )

    def _save_checkpoint(self, is_final=False):
        # (Same as before)
        prefix = "FINAL" if is_final else f"step_{self.global_step}"
        agent_save_path = self.model_config.SAVE_PATH
        print(f"[Trainer] Saving agent checkpoint ({prefix}) to: {agent_save_path}")
        try:
            os.makedirs(os.path.dirname(agent_save_path), exist_ok=True)
            save_data = {
                "global_step": self.global_step,
                "episode_count": self.episode_count,
                "agent_state_dict": self.agent.get_state_dict(),
            }
            torch.save(save_data, agent_save_path)
            print(f"[Trainer] Agent checkpoint ({prefix}) saved.")
        except Exception as e:
            print(f"[Trainer] ERROR saving agent checkpoint ({prefix}): {e}")

        buffer_save_path = BUFFER_SAVE_PATH
        print(
            f"[Trainer] Saving buffer state ({prefix}) to: {buffer_save_path} (Size: {len(self.buffer)})"
        )
        try:
            os.makedirs(os.path.dirname(buffer_save_path), exist_ok=True)
            if hasattr(self.buffer, "save_state"):
                self.buffer.save_state(buffer_save_path)
                print(f"[Trainer] Buffer state ({prefix}) saved.")
            else:
                print("[Trainer] Warning: Buffer does not support save_state.")
        except Exception as e:
            print(f"[Trainer] ERROR saving buffer state ({prefix}): {e}")
            import traceback

            traceback.print_exc()

    # _update_epsilon removed (always 0.0)

    def _update_beta(self) -> float:
        # (Same as before)
        if not self.buffer_config.USE_PER:
            beta = 1.0
        else:
            start = self.buffer_config.PER_BETA_START
            end = 1.0
            anneal_frames = self.buffer_config.PER_BETA_FRAMES
            if anneal_frames <= 0:
                beta = end
            else:
                fraction = min(1.0, float(self.global_step) / anneal_frames)
                beta = start + fraction * (end - start)
            # Update beta in the buffer object (necessary for PER)
            if hasattr(self.buffer, "set_beta"):
                self.buffer.set_beta(beta)
        # Record beta value for logging
        self.stats_recorder.record_step({"beta": beta})
        return beta

    def _collect_experience(self):
        """Performs one step in each parallel env, stores transition, handles resets."""
        # Epsilon is effectively 0, actions are selected greedily w.r.t noisy network
        actions: List[ActionType] = [-1] * self.num_envs

        # --- 1. Select Actions (using agent with Noisy Nets) ---
        for i in range(self.num_envs):
            # Reset env if done
            if self.envs[i].is_over():
                try:
                    self.current_states[i] = ensure_numpy(self.envs[i].reset())
                    # Reset episode trackers
                    self.current_episode_scores[i] = 0.0
                    self.current_episode_lengths[i] = 0
                    self.current_episode_game_scores[i] = 0
                    self.current_episode_lines_cleared[i] = 0
                except Exception as e:
                    print(f"ERROR: Env {i} failed reset: {e}")
                    # Handle error state? Mark env as unusable? For now, try to continue.
                    self.current_states[i] = np.zeros(
                        self.env_config.STATE_DIM, dtype=np.float32
                    )  # Dummy state

            # Get valid actions for the current state
            valid_actions = self.envs[i].valid_actions()

            # Choose action
            if not valid_actions:
                # If no valid actions (should only happen if env is over, but handle defensively)
                actions[i] = 0  # Choose a default action (e.g., index 0)
            else:
                try:
                    # Epsilon is ignored by agent.select_action when using Noisy Nets
                    actions[i] = self.agent.select_action(
                        self.current_states[i], 0.0, valid_actions
                    )
                except Exception as e:
                    print(f"ERROR: Agent select_action env {i}: {e}")
                    actions[i] = random.choice(
                        valid_actions
                    )  # Fallback to random valid action

        # --- 2. Step Environments & Store Transitions ---
        for i in range(self.num_envs):
            env = self.envs[i]
            current_state = self.current_states[i]
            action = actions[i]

            # Execute action in environment
            try:
                reward, done = env.step(action)
                next_state = ensure_numpy(env.get_state())
            except Exception as e:
                print(f"ERROR: Env {i} step failed (Action: {action}): {e}")
                reward = self.reward_config.PENALTY_GAME_OVER  # Use configured penalty
                done = True
                next_state = current_state  # Use current state as next state on error
                env.game_over = True  # Ensure env state reflects game over

            # Store transition in replay buffer
            try:
                # Buffer handles N-step logic internally if wrapped
                self.buffer.push(current_state, action, reward, next_state, done)
            except Exception as e:
                print(f"ERROR: Buffer push env {i}: {e}")

            # --- 3. Update Trackers ---
            self.current_episode_scores[i] += reward
            self.current_episode_lengths[i] += 1
            self.current_episode_game_scores[i] = (
                env.game_score
            )  # Update game score every step
            self.current_episode_lines_cleared[i] = env.lines_cleared_this_episode

            # Record step reward for stats
            self.stats_recorder.record_step({"step_reward": reward})

            # --- 4. Handle Episode End ---
            if done:
                self.episode_count += 1
                final_rl_score = self.current_episode_scores[i]
                final_length = self.current_episode_lengths[i]
                final_game_score = self.current_episode_game_scores[
                    i
                ]  # Use the value tracked
                final_lines_cleared = self.current_episode_lines_cleared[i]

                # Record episode summary stats
                self.stats_recorder.record_episode(
                    episode_score=final_rl_score,
                    episode_length=final_length,
                    episode_num=self.episode_count,
                    global_step=self.global_step
                    + self.num_envs,  # Estimate step at episode end
                    game_score=final_game_score,
                    lines_cleared=final_lines_cleared,
                )

                # Environment is reset at the beginning of the *next* action selection loop
                # Only reset trackers here (already done above when env.is_over() is true)
                # self.current_episode_scores[i] = 0.0 ... etc.
            else:
                # If not done, update the current state for the next iteration
                self.current_states[i] = next_state

        # Increment global step counter
        self.global_step += self.num_envs

        # Record current buffer size
        self.stats_recorder.record_step({"buffer_size": len(self.buffer)})

    def _train_batch(self):
        # (Same as before)
        if (
            len(self.buffer) < self.train_config.BATCH_SIZE
            or self.global_step < self.train_config.LEARN_START_STEP
        ):
            return

        beta = self._update_beta()  # Update and get current beta for PER
        is_n_step = self.buffer_config.USE_N_STEP and self.buffer_config.N_STEP > 1

        # Sample from buffer
        indices, is_weights_np, batch_np_tuple = None, None, None
        try:
            if self.buffer_config.USE_PER:
                sample_result = self.buffer.sample(self.train_config.BATCH_SIZE)
                # PER sample returns (batch_tuple, indices, is_weights)
                batch_np_tuple, indices, is_weights_np = (
                    sample_result if sample_result else (None, None, None)
                )
            else:
                # Uniform sample returns only batch_tuple
                batch_np_tuple = self.buffer.sample(self.train_config.BATCH_SIZE)

            if batch_np_tuple is None:
                print("Warn: Buffer sample returned None.")
                return
        except Exception as e:
            print(f"ERROR sampling buffer: {e}")
            import traceback

            traceback.print_exc()
            return

        # Compute loss
        try:
            loss, td_errors = self.agent.compute_loss(
                batch_np_tuple, is_n_step, is_weights_np
            )
        except Exception as e:
            print(f"ERROR computing loss: {e}")
            import traceback

            traceback.print_exc()
            # Potentially skip update if loss computation fails
            return

        # Update agent
        try:
            grad_norm = self.agent.update(loss)
        except Exception as e:
            print(f"ERROR updating agent: {e}")
            import traceback

            traceback.print_exc()
            # Potentially skip priority update if agent update fails
            return

        # Update priorities in PER buffer
        if self.buffer_config.USE_PER and indices is not None and td_errors is not None:
            try:
                # Ensure td_errors are detached and on CPU before converting to numpy
                td_errors_np = td_errors.squeeze().cpu().numpy()
                self.buffer.update_priorities(indices, td_errors_np)
            except Exception as e:
                print(f"ERROR updating priorities: {e}")

        # Record training step statistics
        self.stats_recorder.record_step(
            {
                "loss": loss.item(),
                "grad_norm": grad_norm,
                "avg_max_q": self.agent.get_last_avg_max_q(),
            }
        )

    def step(self):
        """Performs one iteration of experience collection and training."""
        step_start_time = time.time()

        # --- Experience Collection ---
        self._collect_experience()

        # --- Learning ---
        if (
            self.global_step >= self.train_config.LEARN_START_STEP
            and (self.global_step // self.num_envs) % self.train_config.LEARN_FREQ == 0
        ):
            if len(self.buffer) >= self.train_config.BATCH_SIZE:
                self._train_batch()

        # --- Target Network Update ---
        target_freq = self.dqn_config.TARGET_UPDATE_FREQ
        if target_freq > 0:
            # Check if the update frequency boundary was crossed in the last num_envs steps
            steps_before = self.global_step - self.num_envs
            if steps_before // target_freq < self.global_step // target_freq:
                if self.global_step > 0:  # Avoid update at step 0
                    print(
                        f"[Trainer] Updating target network at step {self.global_step}"
                    )
                    self.agent.update_target_network()

        # --- Checkpointing ---
        self.maybe_save_checkpoint()

        # Record step time (optional, could be done in stats recorder)
        step_duration = time.time() - step_start_time
        self.stats_recorder.record_step({"step_time": step_duration})

    def maybe_save_checkpoint(self):
        # (Same as before)
        save_freq = self.train_config.CHECKPOINT_SAVE_FREQ
        if save_freq <= 0:
            return

        # Check if the save frequency boundary was crossed
        steps_before = (
            self.global_step - self.num_envs
        )  # Approx steps before this multi-env step
        if steps_before // save_freq < self.global_step // save_freq:
            if self.global_step > 0:  # Avoid save at step 0
                self._save_checkpoint(is_final=False)

    def train(self):
        # (This is usually called externally now by main_pygame.py's update loop)
        # Kept here for potential standalone use.
        print("[Trainer] Starting training loop...")
        try:
            while self.global_step < self.train_config.TOTAL_TRAINING_STEPS:
                self.step()
                # Logging is now handled by the stats recorder (e.g., WandbStatsRecorder)
                # self.stats_recorder.log_summary(self.global_step) # Recorder logs based on its own interval
        except KeyboardInterrupt:
            print("\n[Trainer] Training loop interrupted.")
        except Exception as e:
            print(f"\n[Trainer] CRITICAL ERROR in training loop: {e}")
            import traceback

            traceback.print_exc()
        finally:
            print("[Trainer] Training loop finished/terminated.")
            self.cleanup()

    def cleanup(self):
        # (Same as before)
        print("[Trainer] Cleaning up resources...")

        # 1. Flush N-step buffer if used
        if hasattr(self.buffer, "flush_pending"):
            print("[Trainer] Flushing pending N-step transitions...")
            try:
                self.buffer.flush_pending()
            except Exception as e:
                print(f"ERROR during buffer flush: {e}")

        # 2. Save final agent and buffer state
        print("[Trainer] Saving final checkpoint...")
        try:
            self._save_checkpoint(is_final=True)
        except Exception as e:
            print(f"ERROR during final save: {e}")

        # 3. Close stats recorder (e.g., finish WandB run)
        if hasattr(self.stats_recorder, "close"):
            try:
                self.stats_recorder.close()
            except Exception as e:
                print(f"ERROR closing stats recorder: {e}")

        print("[Trainer] Cleanup complete.")


File: utils/__init__.py


File: utils/types.py
# File: utils/types.py
from typing import NamedTuple, Union, Tuple, List, Dict, Any, Optional
import numpy as np
import torch


class Transition(NamedTuple):
    state: np.ndarray
    action: int
    reward: float  # For N-step buffer, this holds the N-step RL reward
    next_state: np.ndarray  # For N-step buffer, this holds the N-step next state
    done: bool  # For N-step buffer, this holds the N-step done flag
    n_step_discount: Optional[float] = None  # gamma^k for N-step


# Type alias for state
StateType = np.ndarray
# Type alias for action
ActionType = int

# --- Batch Types (Numpy) ---
# Standard 1-step batch
NumpyBatch = Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]
# (states, actions, rewards, next_states, dones)

# N-step batch (includes discount factor gamma^k)
NumpyNStepBatch = Tuple[
    np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray
]
# (states, actions, n_step_rewards, n_step_next_states, n_step_dones, n_step_discounts)

# Prioritized 1-step batch
PrioritizedNumpyBatch = Tuple[NumpyBatch, np.ndarray, np.ndarray]
# ((s,a,r,ns,d), indices, weights)

# Prioritized N-step batch
PrioritizedNumpyNStepBatch = Tuple[NumpyNStepBatch, np.ndarray, np.ndarray]
# ((s,a,rn,nsn,dn,gamman), indices, weights)


# --- Batch Types (Tensor) ---
# Standard 1-step batch
TensorBatch = Tuple[
    torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor
]
# (states, actions, rewards, next_states, dones)

# N-step batch (includes discount factor gamma^k)
TensorNStepBatch = Tuple[
    torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor
]
# (states, actions, n_step_rewards, n_step_next_states, n_step_dones, n_step_discounts)


# --- Agent State ---
AgentStateDict = Dict[str, Any]


File: utils/helpers.py
# File: utils/helpers.py
import torch
import numpy as np
import random
import os
from typing import Union, Tuple, Optional, Any
import pickle
import cloudpickle


def get_device() -> torch.device:
    """Gets the appropriate torch device (MPS if available on Mac, CUDA, else CPU)."""
    force_cpu = os.environ.get("FORCE_CPU", "false").lower() == "true"
    if force_cpu:
        print("Forcing CPU device based on environment variable.")
        return torch.device("cpu")

    # <<< MODIFIED >>> Prioritize MPS on Mac
    if torch.backends.mps.is_available():
        device_str = "mps"
    elif torch.cuda.is_available():
        device_str = "cuda"
    else:
        device_str = "cpu"

    print(f"Using device: {device_str.upper()}")
    if device_str == "cuda":
        print(f"CUDA Device Name: {torch.cuda.get_device_name(0)}")
    elif device_str == "mps":
        print("MPS device found on MacOS.")
        # Note: MPS performance and compatibility can vary.
        # Some operations might be slower than CPU or unsupported.
        # If encountering issues (NaNs, errors, slow speed), consider forcing CPU.
    return torch.device(device_str)


def set_random_seeds(seed: int = 42):
    """Sets random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    # No specific seed setting for MPS needed as it uses CPU seeding IIRC
    print(f"Set random seeds to {seed}")


def ensure_numpy(
    data: Union[np.ndarray, list, tuple, torch.Tensor],
) -> np.ndarray:
    """
    Ensures the input data is a numpy array with float32 type.
    Handles numpy arrays, lists, tuples, and torch Tensors.
    """
    try:
        if isinstance(data, np.ndarray):
            # Ensure it's float32
            if data.dtype != np.float32:
                return data.astype(np.float32)
            return data
        elif isinstance(data, torch.Tensor):
            # Move to CPU before converting to numpy
            return data.detach().cpu().numpy().astype(np.float32)
        elif isinstance(data, (list, tuple)):
            # Attempt conversion, check for ragged arrays
            arr = np.array(data, dtype=np.float32)
            # Check if conversion resulted in dtype=object (often indicates ragged array)
            if arr.dtype == np.object_:
                raise ValueError(
                    "Cannot convert ragged list/tuple to float32 numpy array."
                )
            return arr
        else:
            # Handle other potential numeric types if necessary, otherwise raise error
            try:
                # Attempt conversion for single numbers
                return np.array([data], dtype=np.float32)
            except (TypeError, ValueError):
                raise TypeError(f"Unsupported type for ensure_numpy: {type(data)}")

    except (ValueError, TypeError, RuntimeError) as e:
        print(
            f"CRITICAL ERROR in ensure_numpy conversion: {e}. Input type: {type(data)}. Input data (partial): {str(data)[:100]}"
        )
        # Re-raise the error to halt execution, as this often indicates a fundamental problem
        raise ValueError(f"ensure_numpy failed: {e}") from e


def save_object(obj: Any, filepath: str):
    """Saves an arbitrary Python object to a file using cloudpickle."""
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, "wb") as f:
            cloudpickle.dump(
                obj, f, protocol=pickle.HIGHEST_PROTOCOL
            )  # Use highest protocol
        # print(f"Object saved to {filepath}") # Optional: uncomment for verbose saving
    except Exception as e:
        print(f"Error saving object to {filepath}: {e}")
        raise e


def load_object(filepath: str) -> Any:
    """Loads a Python object from a file using cloudpickle."""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found: {filepath}")
    try:
        with open(filepath, "rb") as f:
            obj = cloudpickle.load(f)
        # print(f"Object loaded from {filepath}") # Optional: uncomment for verbose loading
        return obj
    except Exception as e:
        print(f"Error loading object from {filepath}: {e}")
        raise e


File: agent/__init__.py


File: agent/dqn_agent.py
# File: agent/dqn_agent.py
# (Largely the same, but removed epsilon logic from select_action)
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import math
from typing import Tuple, List, Dict, Any, Optional, Union

from config import EnvConfig, ModelConfig, DQNConfig, DEVICE
from agent.model_factory import create_network
from utils.types import (
    StateType,
    ActionType,
    NumpyBatch,
    NumpyNStepBatch,
    AgentStateDict,
    TensorBatch,
    TensorNStepBatch,
)
from utils.helpers import ensure_numpy
from agent.networks.noisy_layer import (
    NoisyLinear,
)  # Needed for type check if resetting noise externally (though layer now handles it)


class DQNAgent:
    """DQN Agent using Noisy Nets for exploration."""

    def __init__(
        self, config: ModelConfig, dqn_config: DQNConfig, env_config: EnvConfig
    ):
        print("[DQNAgent] Initializing...")
        self.device = DEVICE
        self.action_dim = env_config.ACTION_DIM
        self.gamma = dqn_config.GAMMA
        self.use_double_dqn = dqn_config.USE_DOUBLE_DQN
        self.gradient_clip_norm = dqn_config.GRADIENT_CLIP_NORM
        self.use_noisy_nets = dqn_config.USE_NOISY_NETS  # Should be True from config
        self.use_dueling = dqn_config.USE_DUELING

        if not self.use_noisy_nets:
            print(
                "WARNING: DQNConfig.USE_NOISY_NETS is False, but this refactored agent expects it to be True. Exploration might be missing."
            )

        self.online_net = create_network(
            env_config.STATE_DIM,
            env_config.ACTION_DIM,
            config,
            dqn_config,  # Pass DQN config too
        ).to(self.device)
        self.target_net = create_network(
            env_config.STATE_DIM, env_config.ACTION_DIM, config, dqn_config
        ).to(self.device)

        self.target_net.load_state_dict(self.online_net.state_dict())
        self.target_net.eval()  # Target net always in eval mode

        self.optimizer = optim.AdamW(
            self.online_net.parameters(),
            lr=dqn_config.LEARNING_RATE,
            eps=dqn_config.ADAM_EPS,
            weight_decay=1e-5,  # Example weight decay
        )
        # SmoothL1Loss (Huber loss) is generally robust for Q-learning
        self.loss_fn = nn.SmoothL1Loss(
            reduction="none", beta=1.0
        )  # Use 'none' for PER weights

        self._last_avg_max_q: float = 0.0

        print(f"[DQNAgent] Online Network Type: {type(self.online_net).__name__}")
        print(f"[DQNAgent] Using Double DQN: {self.use_double_dqn}")
        print(f"[DQNAgent] Using Dueling: {self.use_dueling}")
        print(f"[DQNAgent] Using Noisy Nets: {self.use_noisy_nets}")
        print(
            f"[DQNAgent] Optimizer: AdamW (LR={dqn_config.LEARNING_RATE}, EPS={dqn_config.ADAM_EPS})"
        )
        total_params = sum(
            p.numel() for p in self.online_net.parameters() if p.requires_grad
        )
        print(f"[DQNAgent] Trainable Parameters: {total_params / 1e6:.2f} M")

    @torch.no_grad()
    def select_action(
        self, state: StateType, epsilon: float, valid_actions: List[ActionType]
    ) -> ActionType:
        """Selects action using the noisy online network. Epsilon is ignored."""
        if not valid_actions:
            # print("Warning: select_action called with no valid actions.")
            return 0  # Return a default action index

        # Ensure state is numpy float32
        state_np = ensure_numpy(state)
        state_t = torch.tensor(
            state_np, dtype=torch.float32, device=self.device
        ).unsqueeze(
            0
        )  # Add batch dim

        # Use online network in eval mode for action selection.
        # The NoisyLinear layers will *not* sample noise in eval mode, using only their learned means.
        # This selects the greedy action based on the current learned policy means.
        self.online_net.eval()
        q_values = self.online_net(state_t)[0]  # Get Q-values for the single state

        # --- Masking and Argmax ---
        # Create a tensor filled with negative infinity
        q_values_masked = torch.full_like(q_values, -float("inf"))

        # Convert valid actions to tensor indices
        valid_action_indices = torch.tensor(
            valid_actions, dtype=torch.long, device=self.device
        )

        # Use indexing to copy Q-values for valid actions
        q_values_masked[valid_action_indices] = q_values[valid_action_indices]

        # Find the action with the highest Q-value among valid actions
        best_action = torch.argmax(q_values_masked).item()

        # No need to switch back to train mode here, done in compute_loss/update

        return best_action

    def _np_batch_to_tensor(
        self, batch: Union[NumpyBatch, NumpyNStepBatch], is_n_step: bool
    ) -> Union[TensorBatch, TensorNStepBatch]:
        """Converts a numpy batch (potentially N-step) to tensors on the correct device."""
        if is_n_step:
            # N-step: (s, a, rn, nsn, dn, gamma_n)
            states, actions, rewards, next_states, dones, discounts = batch
            states_t = torch.tensor(states, dtype=torch.float32, device=self.device)
            actions_t = torch.tensor(
                actions, dtype=torch.long, device=self.device
            ).unsqueeze(
                1
            )  # Add dim for gather
            rewards_t = torch.tensor(
                rewards, dtype=torch.float32, device=self.device
            ).unsqueeze(1)
            next_states_t = torch.tensor(
                next_states, dtype=torch.float32, device=self.device
            )
            dones_t = torch.tensor(
                dones, dtype=torch.float32, device=self.device
            ).unsqueeze(1)
            discounts_t = torch.tensor(
                discounts, dtype=torch.float32, device=self.device
            ).unsqueeze(1)
            return states_t, actions_t, rewards_t, next_states_t, dones_t, discounts_t
        else:
            # 1-step: (s, a, r, ns, d)
            states, actions, rewards, next_states, dones = batch
            states_t = torch.tensor(states, dtype=torch.float32, device=self.device)
            actions_t = torch.tensor(
                actions, dtype=torch.long, device=self.device
            ).unsqueeze(1)
            rewards_t = torch.tensor(
                rewards, dtype=torch.float32, device=self.device
            ).unsqueeze(1)
            next_states_t = torch.tensor(
                next_states, dtype=torch.float32, device=self.device
            )
            dones_t = torch.tensor(
                dones, dtype=torch.float32, device=self.device
            ).unsqueeze(1)
            return states_t, actions_t, rewards_t, next_states_t, dones_t

    def compute_loss(
        self,
        batch: Union[NumpyBatch, NumpyNStepBatch],
        is_n_step: bool,
        is_weights: Optional[np.ndarray] = None,  # PER Importance Sampling weights
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Computes DQN loss (Huber Loss), handles N-step and PER weights."""

        # --- 1. Convert Batch to Tensors ---
        tensor_batch = self._np_batch_to_tensor(batch, is_n_step)
        if is_n_step:
            states, actions, rewards, next_states, dones, discounts = tensor_batch
        else:
            # For 1-step, create gamma tensor
            states, actions, rewards, next_states, dones = tensor_batch
            discounts = torch.full_like(
                rewards, self.gamma, device=self.device
            )  # gamma^1

        # Convert PER weights to tensor if provided
        is_weights_t = None
        if is_weights is not None:
            is_weights_t = torch.tensor(
                is_weights, dtype=torch.float32, device=self.device
            ).unsqueeze(1)

        # --- 2. Calculate Target Q-values (Q_target) ---
        # Target network calculations should be detached from the graph
        with torch.no_grad():
            # Target net is already in eval mode (noise off)
            # Select best actions for next states using the *online* network (Double DQN)
            # Online net needs to be in eval mode temporarily for consistent action selection
            self.online_net.eval()
            online_next_q = self.online_net(next_states)  # Q(s', a'; theta_online)
            best_next_actions = online_next_q.argmax(
                dim=1, keepdim=True
            )  # a' = argmax_a' Q(s', a'; theta_online)

            # Get Q-values for these best actions using the *target* network
            # Q_target(s', a') = Q(s', argmax_a' Q(s', a'; theta_online); theta_target)
            target_next_q_values = self.target_net(next_states).gather(
                1, best_next_actions
            )

            # Calculate the TD target: R + gamma^N * Q_target(s', a') * (1 - done)
            # Discounts here is gamma^N for N-step, or gamma^1 for 1-step
            target_q = rewards + discounts * target_next_q_values * (1.0 - dones)

        # --- 3. Calculate Current Q-values (Q_online) ---
        # Online network must be in train mode for gradients and for noise sampling
        self.online_net.train()
        # Q_online(s, a) = Q(s, a; theta_online)
        current_q = self.online_net(states).gather(1, actions)

        # --- 4. Calculate Loss ---
        # TD Error: target_q - current_q
        td_error = target_q - current_q
        # Huber Loss (element-wise)
        elementwise_loss = self.loss_fn(current_q, target_q)

        # Apply PER weights if available
        loss = (
            (is_weights_t * elementwise_loss).mean()
            if is_weights_t is not None
            else elementwise_loss.mean()
        )

        # --- 5. Update Stats (Optional but useful) ---
        # Calculate average max Q for logging (use eval mode for consistency)
        with torch.no_grad():
            self.online_net.eval()
            self._last_avg_max_q = self.online_net(states).max(dim=1)[0].mean().item()
            # No need to switch back to train here, update() will use the net in train mode

        # Return loss and absolute TD errors (for PER updates)
        return loss, td_error.abs().detach()

    def update(self, loss: torch.Tensor) -> float:
        """Performs one optimization step."""
        self.optimizer.zero_grad(set_to_none=True)  # More efficient zeroing
        loss.backward()

        # Gradient Clipping
        grad_norm = 0.0
        if self.gradient_clip_norm > 0:
            # Ensure online_net is in train mode before clipping/stepping
            self.online_net.train()
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.online_net.parameters(), max_norm=self.gradient_clip_norm
            ).item()  # .item() gets the scalar value

        self.optimizer.step()
        return grad_norm

    def get_last_avg_max_q(self) -> float:
        """Returns the average max Q value computed during the last loss calculation."""
        return self._last_avg_max_q

    def update_target_network(self):
        """Copies weights from the online network to the target network."""
        self.target_net.load_state_dict(self.online_net.state_dict())
        self.target_net.eval()  # Ensure target net stays in eval mode

    def get_state_dict(self) -> AgentStateDict:
        """Returns the agent's state for saving."""
        return {
            "online_net_state_dict": self.online_net.state_dict(),
            "target_net_state_dict": self.target_net.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
        }

    def load_state_dict(self, state_dict: AgentStateDict):
        """Loads the agent's state from a dictionary."""
        self.online_net.load_state_dict(state_dict["online_net_state_dict"])
        # Load target net separately, or copy if missing (for backward compatibility)
        if "target_net_state_dict" in state_dict:
            self.target_net.load_state_dict(state_dict["target_net_state_dict"])
        else:
            print(
                "Warning: Target network state not found in checkpoint, copying from online network."
            )
            self.target_net.load_state_dict(self.online_net.state_dict())

        # Load optimizer state - Use try-except as it might fail if model architecture changed
        try:
            self.optimizer.load_state_dict(state_dict["optimizer_state_dict"])
        except ValueError as e:
            print(
                f"Warning: Could not load optimizer state ({e}). Optimizer state reset. This is expected if the model architecture changed."
            )
        except Exception as e:
            print(
                f"Warning: Error loading optimizer state: {e}. Optimizer state reset."
            )

        # Ensure networks are in correct mode after loading
        self.online_net.train()
        self.target_net.eval()


File: agent/model_factory.py
# File: agent/model_factory.py
import torch.nn as nn
from config import ModelConfig, EnvConfig, DQNConfig  # Import relevant configs
from typing import Type

# Import the single network type we are using
from agent.networks.agent_network import AgentNetwork  # <<< RENAMED/REFINED Network


def create_network(
    state_dim: int, action_dim: int, model_config: ModelConfig, dqn_config: DQNConfig
) -> nn.Module:
    """Creates the single, defined AgentNetwork."""

    dueling = dqn_config.USE_DUELING
    use_noisy = dqn_config.USE_NOISY_NETS  # Should be True

    print(
        f"[ModelFactory] Creating AgentNetwork - Dueling: {dueling}, NoisyNets: {use_noisy}"
    )

    # Instantiate the AgentNetwork directly
    # Pass the specific sub-config ModelConfig.Network
    return AgentNetwork(
        state_dim=state_dim,
        action_dim=action_dim,
        config=model_config.Network,  # Pass the Network sub-config
        env_config=EnvConfig,  # AgentNetwork needs EnvConfig
        dueling=dueling,
        use_noisy=use_noisy,
    )


File: agent/replay_buffer/uniform_buffer.py
# File: agent/replay_buffer/uniform_buffer.py
import random
import numpy as np
from collections import deque
from typing import Deque, Tuple, Optional, Any, Dict, Union
from .base_buffer import ReplayBufferBase
from utils.types import (
    Transition,
    StateType,
    ActionType,
    NumpyBatch,
    NumpyNStepBatch,
)
from utils.helpers import save_object, load_object


class UniformReplayBuffer(ReplayBufferBase):
    """Standard uniform experience replay buffer."""

    def __init__(self, capacity: int):
        super().__init__(capacity)
        self.buffer: Deque[Transition] = deque(maxlen=capacity)

    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
        **kwargs,  # Accept potential n_step_discount from NStepWrapper
    ):
        n_step_discount = kwargs.get("n_step_discount")
        transition = Transition(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done,
            n_step_discount=n_step_discount,
        )
        self.buffer.append(transition)

    def sample(self, batch_size: int) -> Optional[Union[NumpyBatch, NumpyNStepBatch]]:
        if len(self.buffer) < batch_size:
            return None

        batch_transitions = random.sample(self.buffer, batch_size)

        # Check if the first sampled transition has n_step_discount
        is_n_step = batch_transitions[0].n_step_discount is not None

        if is_n_step:
            # Unpack N-step fields: s, a, rn, nsn, dn, gamma_n
            s, a, rn, nsn, dn, gamma_n = zip(
                *[
                    (
                        t.state,
                        t.action,
                        t.reward,
                        t.next_state,
                        t.done,
                        t.n_step_discount,
                    )
                    for t in batch_transitions
                ]
            )
            # Convert to numpy arrays
            states_np = np.array(s, dtype=np.float32)
            actions_np = np.array(a, dtype=np.int64)
            rewards_np = np.array(rn, dtype=np.float32)
            next_states_np = np.array(nsn, dtype=np.float32)
            dones_np = np.array(
                dn, dtype=np.float32
            )  # Use float32 for dones (often multiplied)
            discounts_np = np.array(gamma_n, dtype=np.float32)
            return (
                states_np,
                actions_np,
                rewards_np,
                next_states_np,
                dones_np,
                discounts_np,
            )
        else:
            # Unpack 1-step fields: s, a, r, ns, d
            s, a, r, ns, d = zip(
                *[
                    (t.state, t.action, t.reward, t.next_state, t.done)
                    for t in batch_transitions
                ]
            )
            # Convert to numpy arrays
            states_np = np.array(s, dtype=np.float32)
            actions_np = np.array(a, dtype=np.int64)
            rewards_np = np.array(r, dtype=np.float32)
            next_states_np = np.array(ns, dtype=np.float32)
            dones_np = np.array(d, dtype=np.float32)  # Use float32 for dones
            return (states_np, actions_np, rewards_np, next_states_np, dones_np)

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        pass  # No-op

    def set_beta(self, beta: float):
        pass  # No-op

    def flush_pending(self):
        pass  # No-op

    def __len__(self) -> int:
        return len(self.buffer)

    def get_state(self) -> Dict[str, Any]:
        """Return state for pickling."""
        # Convert deque to list for robust pickling/saving
        return {"buffer": list(self.buffer)}

    def load_state_from_data(self, state: Dict[str, Any]):
        """Load state from dictionary."""
        saved_buffer_list = state.get("buffer", [])
        # Recreate deque with current capacity, filling from saved list
        self.buffer = deque(saved_buffer_list, maxlen=self.capacity)
        print(f"[UniformReplayBuffer] Loaded {len(self.buffer)} transitions.")

    def save_state(self, filepath: str):
        """Save buffer state to file."""
        state = self.get_state()
        save_object(state, filepath)

    def load_state(self, filepath: str):
        """Load buffer state from file."""
        state = load_object(filepath)
        self.load_state_from_data(state)


File: agent/replay_buffer/buffer_utils.py
from config import BufferConfig, DQNConfig
from .base_buffer import ReplayBufferBase
from .uniform_buffer import UniformReplayBuffer
from .prioritized_buffer import PrioritizedReplayBuffer
from .nstep_buffer import NStepBufferWrapper


def create_replay_buffer(
    config: BufferConfig, dqn_config: DQNConfig
) -> ReplayBufferBase:
    """Factory function to create the appropriate replay buffer based on config."""

    print("[BufferFactory] Creating replay buffer...")
    print(f"  Capacity: {config.REPLAY_BUFFER_SIZE}")
    print(f"  Use PER: {config.USE_PER}")
    print(f"  Use N-Step: {config.USE_N_STEP} (N={config.N_STEP})")

    if config.USE_PER:
        core_buffer = PrioritizedReplayBuffer(
            capacity=config.REPLAY_BUFFER_SIZE,
            alpha=config.PER_ALPHA,
            epsilon=config.PER_EPSILON,
        )
        print(
            f"  Type: Prioritized (alpha={config.PER_ALPHA}, eps={config.PER_EPSILON})"
        )
    else:
        core_buffer = UniformReplayBuffer(capacity=config.REPLAY_BUFFER_SIZE)
        print("  Type: Uniform")

    if config.USE_N_STEP and config.N_STEP > 1:
        final_buffer = NStepBufferWrapper(
            wrapped_buffer=core_buffer,
            n_step=config.N_STEP,
            gamma=dqn_config.GAMMA,
        )
        print(
            f"  Wrapped with: NStepBufferWrapper (N={config.N_STEP}, gamma={dqn_config.GAMMA})"
        )
    else:
        final_buffer = core_buffer

    print(f"[BufferFactory] Final buffer type: {type(final_buffer).__name__}")
    return final_buffer


File: agent/replay_buffer/__init__.py


File: agent/replay_buffer/prioritized_buffer.py
# File: agent/replay_buffer/prioritized_buffer.py
import random
import numpy as np
from typing import (
    Optional,
    Tuple,
    Any,
    Dict,
    Union,
    List,
)
from .base_buffer import ReplayBufferBase
from .sum_tree import SumTree
from utils.types import (
    Transition,
    StateType,
    ActionType,
    NumpyBatch,
    PrioritizedNumpyBatch,
    NumpyNStepBatch,
    PrioritizedNumpyNStepBatch,
)
from utils.helpers import save_object, load_object


class PrioritizedReplayBuffer(ReplayBufferBase):
    """Prioritized Experience Replay (PER) buffer using a SumTree."""

    def __init__(self, capacity: int, alpha: float, epsilon: float):
        super().__init__(capacity)
        self.tree = SumTree(capacity)
        self.alpha = alpha
        self.epsilon = epsilon
        self.beta = 0.0  # Set externally by Trainer
        self.max_priority = 1.0

    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
        **kwargs,  # Accept potential n_step_discount
    ):
        """Adds new experience with maximum priority."""
        n_step_discount = kwargs.get("n_step_discount")
        transition = Transition(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done,
            n_step_discount=n_step_discount,
        )
        # Add with max priority initially
        self.tree.add(self.max_priority, transition)

    def sample(
        self, batch_size: int
    ) -> Optional[Union[PrioritizedNumpyBatch, PrioritizedNumpyNStepBatch]]:
        """Samples batch using priorities, calculates IS weights."""
        if len(self) < batch_size:
            return None

        batch_data: List[Transition] = []
        indices = np.empty(batch_size, dtype=np.int64)  # Tree indices
        priorities = np.empty(batch_size, dtype=np.float64)
        segment = self.tree.total() / batch_size

        for i in range(batch_size):
            a = segment * i
            b = segment * (i + 1)
            s = random.uniform(a, b)
            s = max(1e-9, s)  # Avoid s=0 issues

            idx, p, data = self.tree.get(s)

            # Retry sampling if data is invalid (e.g., None during buffer fill)
            retries = 0
            max_retries = 10  # Increase max retries slightly
            while not isinstance(data, Transition) and retries < max_retries:
                # Resample from the entire range if the segment failed
                s = random.uniform(1e-9, self.tree.total())
                idx, p, data = self.tree.get(s)
                retries += 1

            if not isinstance(data, Transition):
                print(
                    f"Error: PER sample failed to get valid data after {max_retries} retries. Skipping batch."
                )
                # Return None if any sample fails, as batch size would be wrong
                return None

            priorities[i] = p
            batch_data.append(data)
            indices[i] = idx

        sampling_probabilities = priorities / self.tree.total()
        # Add small epsilon to prevent zero probability, important for IS weights
        sampling_probabilities = np.maximum(sampling_probabilities, 1e-9)

        # Calculate Importance Sampling (IS) weights
        is_weights = np.power(len(self) * sampling_probabilities, -self.beta)
        # Normalize weights by the maximum weight for stability
        is_weights /= is_weights.max() + 1e-9

        # Check if N-step based on first item
        is_n_step = batch_data[0].n_step_discount is not None

        if is_n_step:
            s, a, rn, nsn, dn, gamma_n = zip(
                *[
                    (
                        t.state,
                        t.action,
                        t.reward,
                        t.next_state,
                        t.done,
                        t.n_step_discount,
                    )
                    for t in batch_data
                ]
            )
            states_np = np.array(s, dtype=np.float32)
            actions_np = np.array(a, dtype=np.int64)
            rewards_np = np.array(rn, dtype=np.float32)
            next_states_np = np.array(nsn, dtype=np.float32)
            dones_np = np.array(dn, dtype=np.float32)
            discounts_np = np.array(gamma_n, dtype=np.float32)
            batch_tuple = (
                states_np,
                actions_np,
                rewards_np,
                next_states_np,
                dones_np,
                discounts_np,
            )
            return batch_tuple, indices, is_weights.astype(np.float32)
        else:
            s, a, r, ns, d = zip(
                *[
                    (t.state, t.action, t.reward, t.next_state, t.done)
                    for t in batch_data
                ]
            )
            states_np = np.array(s, dtype=np.float32)
            actions_np = np.array(a, dtype=np.int64)
            rewards_np = np.array(r, dtype=np.float32)
            next_states_np = np.array(ns, dtype=np.float32)
            dones_np = np.array(d, dtype=np.float32)
            batch_tuple = (states_np, actions_np, rewards_np, next_states_np, dones_np)
            return batch_tuple, indices, is_weights.astype(np.float32)

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """Updates priorities of experiences at given tree indices."""
        if len(indices) != len(priorities):
            print(
                f"Error: Mismatch indices ({len(indices)}) vs priorities ({len(priorities)}) in PER update"
            )
            return

        # Use TD error magnitude for priority, add epsilon, raise to alpha
        priorities = np.abs(priorities) + self.epsilon
        priorities = np.power(priorities, self.alpha)
        # Clip priorities to avoid extreme values? Optional.
        # priorities = np.clip(priorities, 1e-6, self.max_priority * 10) # Example clipping

        for idx, priority in zip(indices, priorities):
            # Check index validity (should be leaf node index)
            if not (self.tree.capacity - 1 <= idx < 2 * self.tree.capacity - 1):
                # This might happen if buffer wraps around between sampling and updating.
                # print(f"Warning: Attempting update on invalid tree index {idx}. Skipping.")
                continue
            self.tree.update(idx, priority)
            self.max_priority = max(self.max_priority, priority)

    def set_beta(self, beta: float):
        self.beta = beta

    def flush_pending(self):
        pass  # No-op

    def __len__(self) -> int:
        return self.tree.n_entries

    def get_state(self) -> Dict[str, Any]:
        """Return state for saving."""
        return {
            "tree_nodes": self.tree.tree.copy(),
            "tree_data": self.tree.data.copy(),  # Save actual transition data
            "tree_write_ptr": self.tree.write_ptr,
            "tree_n_entries": self.tree.n_entries,
            "max_priority": self.max_priority,
            "alpha": self.alpha,
            "epsilon": self.epsilon,
            # Beta is transient, set by trainer based on global step
        }

    def load_state_from_data(self, state: Dict[str, Any]):
        """Load state from dictionary."""
        if "tree_nodes" not in state or "tree_data" not in state:
            print("Error: Invalid PER state format during load. Skipping.")
            return

        loaded_capacity = len(state["tree_data"])
        if loaded_capacity != self.capacity:
            print(
                f"Warning: Loaded PER capacity ({loaded_capacity}) != current buffer capacity ({self.capacity}). Recreating tree with current capacity."
            )
            # Recreate tree with current capacity and load data partially if needed
            self.tree = SumTree(self.capacity)
            # Load data up to current capacity
            num_to_load = min(loaded_capacity, self.capacity)
            # This simple load might lose priority structure if capacity changed.
            # A more complex load would rebuild the tree from loaded data/priorities.
            # For simplicity, we just load the data array part. Priorities will reset.
            self.tree.data[:num_to_load] = state["tree_data"][:num_to_load]
            self.tree.write_ptr = (
                state.get("tree_write_ptr", 0) % self.capacity
            )  # Ensure ptr is valid
            self.tree.n_entries = min(state.get("tree_n_entries", 0), self.capacity)
            # Reset priorities as tree structure might be invalid
            self.tree.tree.fill(0)  # Clear old priorities
            self.max_priority = 1.0  # Reset max priority
            print(
                f"[PrioritizedReplayBuffer] Loaded {self.tree.n_entries} transitions (priorities reset due to capacity mismatch)."
            )

        else:
            # Capacities match, load everything
            self.tree.tree = state["tree_nodes"]
            self.tree.data = state["tree_data"]
            self.tree.write_ptr = state.get("tree_write_ptr", 0)
            self.tree.n_entries = state.get("tree_n_entries", 0)
            self.max_priority = state.get("max_priority", 1.0)
            print(
                f"[PrioritizedReplayBuffer] Loaded {self.tree.n_entries} transitions."
            )

        # Load config params if they exist in save, otherwise keep current config
        self.alpha = state.get("alpha", self.alpha)
        self.epsilon = state.get("epsilon", self.epsilon)

    def save_state(self, filepath: str):
        """Save buffer state to file."""
        state = self.get_state()
        save_object(state, filepath)

    def load_state(self, filepath: str):
        """Load buffer state from file."""
        state = load_object(filepath)
        self.load_state_from_data(state)


File: agent/replay_buffer/base_buffer.py
from abc import ABC, abstractmethod
from typing import Any, Optional, Tuple, Dict
import numpy as np
from utils.types import (
    Transition,
    StateType,
    ActionType,
    NumpyBatch,
    NumpyNStepBatch,
)


class ReplayBufferBase(ABC):
    """Abstract base class for all replay buffers."""

    def __init__(self, capacity: int):
        self.capacity = capacity

    @abstractmethod
    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
        **kwargs  # Allow passing extra info like n_step_discount
    ):
        """Add a new experience to the buffer."""
        pass

    @abstractmethod
    def sample(
        self, batch_size: int
    ) -> Optional[Any]:  # Return type depends on PER/NStep
        """Sample a batch of experiences from the buffer."""
        pass

    @abstractmethod
    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """Update priorities for PER (no-op for uniform buffer)."""
        pass

    @abstractmethod
    def __len__(self) -> int:
        """Return the current size of the buffer."""
        pass

    @abstractmethod
    def set_beta(self, beta: float):
        """Set the beta value for PER IS weights (no-op for uniform buffer)."""
        pass

    @abstractmethod
    def flush_pending(self):
        """Process any pending transitions (e.g., for N-step)."""
        pass

    @abstractmethod
    def get_state(self) -> Dict[str, Any]:
        """Return the buffer's state as a dictionary suitable for saving."""
        pass

    @abstractmethod
    def load_state_from_data(self, state: Dict[str, Any]):
        """Load the buffer's state from a dictionary."""
        pass

    @abstractmethod
    def save_state(self, filepath: str):
        """Save the buffer's state to a file."""
        pass

    @abstractmethod
    def load_state(self, filepath: str):
        """Load the buffer's state from a file."""
        pass


File: agent/replay_buffer/nstep_buffer.py
# File: agent/replay_buffer/nstep_buffer.py
from collections import deque
import numpy as np
from typing import Deque, Tuple, Optional, Any, Dict, List
from .base_buffer import ReplayBufferBase
from utils.types import Transition, StateType, ActionType
from utils.helpers import save_object, load_object


class NStepBufferWrapper(ReplayBufferBase):
    """
    Wraps another buffer to implement N-step returns.
    Calculates N-step transitions (s, a, R_n, s_n, done_n, gamma^n)
    and pushes them to the wrapped buffer. Handles episode termination correctly.
    """

    def __init__(self, wrapped_buffer: ReplayBufferBase, n_step: int, gamma: float):
        super().__init__(
            wrapped_buffer.capacity
        )  # Capacity is managed by wrapped buffer
        if n_step <= 0:
            raise ValueError("N-step must be positive")
        self.wrapped_buffer = wrapped_buffer
        self.n_step = n_step
        self.gamma = gamma
        # This deque stores raw (s, a, r, ns, d) tuples temporarily
        self.n_step_deque: Deque[
            Tuple[StateType, ActionType, float, StateType, bool]
        ] = deque(maxlen=n_step)

    def _calculate_n_step_transition(
        self, current_deque_list: List[Tuple]
    ) -> Optional[Transition]:
        """
        Calculates the N-step return from a list copy of the deque.
        Returns: A Transition object with N-step values, or None if input is empty.
        """
        if not current_deque_list:
            return None

        n_step_reward = 0.0
        discount_accum = 1.0
        effective_n = len(current_deque_list)

        state_0, action_0 = current_deque_list[0][0], current_deque_list[0][1]

        for i in range(effective_n):
            s, a, r, ns, d = current_deque_list[i]
            n_step_reward += discount_accum * r

            if d:  # Episode terminated within these N steps
                n_step_next_state = ns  # Use the actual terminal state
                n_step_done = True
                # Discount factor for the terminal state (used in Q-learning target)
                n_step_discount = self.gamma ** (i + 1)

                # Create the N-step transition ending early
                return Transition(
                    state=state_0,
                    action=action_0,
                    reward=n_step_reward,
                    next_state=n_step_next_state,
                    done=n_step_done,
                    n_step_discount=n_step_discount,
                )

            discount_accum *= self.gamma

        # If loop completes without finding a terminal state:
        # The N-step transition uses the state after N steps
        n_step_next_state = current_deque_list[-1][
            3
        ]  # next_state from the Nth transition
        n_step_done = current_deque_list[-1][4]  # done flag from the Nth transition
        n_step_discount = (
            self.gamma**effective_n
        )  # Discount factor for Q(s_N) is gamma^N

        return Transition(
            state=state_0,
            action=action_0,
            reward=n_step_reward,
            next_state=n_step_next_state,
            done=n_step_done,
            n_step_discount=n_step_discount,
        )

    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
    ):
        """Adds raw transition, processes N-step if possible, pushes to wrapped buffer."""
        # Store the raw transition (s, a, r, ns, d)
        current_transition = (state, action, reward, next_state, done)
        self.n_step_deque.append(current_transition)

        # If the deque doesn't yet have N items, we can't form a full N-step transition yet
        if len(self.n_step_deque) < self.n_step:
            # If the episode ended before N steps, process the partial transitions now
            if done:
                self._flush_on_done()
            return  # Otherwise, wait for more steps

        # --- Deque has N items ---
        # Calculate the N-step transition starting from the oldest element
        # Use a list copy to avoid modifying the deque during calculation
        n_step_transition = self._calculate_n_step_transition(list(self.n_step_deque))

        if n_step_transition:
            self.wrapped_buffer.push(
                state=n_step_transition.state,
                action=n_step_transition.action,
                reward=n_step_transition.reward,
                next_state=n_step_transition.next_state,
                done=n_step_transition.done,
                n_step_discount=n_step_transition.n_step_discount,  # Pass the calculated discount
            )
        # Oldest element is automatically removed due to maxlen=n_step when appending

        # If the *newly added* transition was terminal (done=True), we need to flush remaining starts
        if done:
            self._flush_on_done()

    def _flush_on_done(self):
        """Processes remaining partial transitions when an episode ends."""
        # The deque contains transitions leading up to and including the 'done' state.
        # The full N-step (or less if episode < N) ending at 'done' was processed by push().
        # We now need to process transitions that start *after* the initial state
        # of the previously processed transition.
        temp_deque = list(self.n_step_deque)
        # Process starting from the second element onwards
        while len(temp_deque) > 1:  # Stop when only the 'done' transition remains
            temp_deque.pop(0)  # Remove the already processed starting state
            n_step_transition = self._calculate_n_step_transition(temp_deque)
            if n_step_transition:
                self.wrapped_buffer.push(
                    state=n_step_transition.state,
                    action=n_step_transition.action,
                    reward=n_step_transition.reward,
                    next_state=n_step_transition.next_state,
                    done=n_step_transition.done,
                    n_step_discount=n_step_transition.n_step_discount,
                )
        # Clear the deque after flushing all relevant starting points
        self.n_step_deque.clear()

    def sample(self, batch_size: int) -> Any:
        """Samples directly from the wrapped buffer."""
        return self.wrapped_buffer.sample(batch_size)

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        self.wrapped_buffer.update_priorities(indices, priorities)

    def set_beta(self, beta: float):
        if hasattr(self.wrapped_buffer, "set_beta"):
            self.wrapped_buffer.set_beta(beta)

    def flush_pending(self):
        """Processes and pushes any remaining transitions before exit/save."""
        print(
            f"[NStepWrapper] Flushing {len(self.n_step_deque)} pending transitions on cleanup."
        )
        # Process all remaining partial transitions in the deque, similar to flush_on_done
        temp_deque = list(self.n_step_deque)
        while len(temp_deque) > 0:
            n_step_transition = self._calculate_n_step_transition(temp_deque)
            if n_step_transition:
                self.wrapped_buffer.push(
                    state=n_step_transition.state,
                    action=n_step_transition.action,
                    reward=n_step_transition.reward,
                    next_state=n_step_transition.next_state,
                    done=n_step_transition.done,
                    n_step_discount=n_step_transition.n_step_discount,
                )
            temp_deque.pop(0)  # Remove the processed starting state

        self.n_step_deque.clear()

        # Also flush the underlying buffer if it has its own mechanism (it shouldn't for PER/Uniform)
        if hasattr(self.wrapped_buffer, "flush_pending"):
            self.wrapped_buffer.flush_pending()

    def __len__(self) -> int:
        # Length is the number of processed N-step transitions in the wrapped buffer
        return len(self.wrapped_buffer)

    def get_state(self) -> Dict[str, Any]:
        """Return state for saving."""
        return {
            "n_step_deque": list(self.n_step_deque),  # Save pending raw transitions
            "wrapped_buffer_state": self.wrapped_buffer.get_state(),
        }

    def load_state_from_data(self, state: Dict[str, Any]):
        """Load state from dictionary."""
        # Load pending transitions
        saved_deque_list = state.get("n_step_deque", [])
        self.n_step_deque = deque(saved_deque_list, maxlen=self.n_step)
        print(f"[NStepWrapper] Loaded {len(self.n_step_deque)} pending transitions.")

        # Load wrapped buffer state
        wrapped_state = state.get("wrapped_buffer_state")
        if wrapped_state is not None:
            self.wrapped_buffer.load_state_from_data(wrapped_state)
        else:
            print(
                "[NStepWrapper] Warning: No wrapped buffer state found in saved data."
            )

    def save_state(self, filepath: str):
        """Save buffer state to file."""
        state = self.get_state()
        save_object(state, filepath)

    def load_state(self, filepath: str):
        """Load buffer state from file."""
        try:
            state = load_object(filepath)
            self.load_state_from_data(state)
        except FileNotFoundError:
            print(
                f"[NStepWrapper] Load failed: File not found at {filepath}. Starting empty."
            )
        except Exception as e:
            print(f"[NStepWrapper] Load failed: {e}. Starting empty.")


File: agent/replay_buffer/sum_tree.py
import numpy as np


class SumTree:
    """
    Simple SumTree implementation using numpy arrays.
    Tree structure: [root] [layer 1] [layer 2] ... [leaves]
    Size = 2 * capacity - 1. Leaves start at index capacity - 1.
    Data array holds corresponding experiences.
    """

    def __init__(self, capacity: int):
        if capacity <= 0 or not isinstance(capacity, int):
            raise ValueError("SumTree capacity must be a positive integer")
        # Ensure capacity is power of 2 for simpler indexing? Not strictly required.
        self.capacity = capacity
        self.tree = np.zeros(
            2 * capacity - 1, dtype=np.float64
        )  # Use float64 for precision
        # dtype=object allows storing arbitrary Transition objects
        self.data = np.zeros(capacity, dtype=object)
        self.write_ptr = 0
        self.n_entries = 0

    def _propagate(self, idx: int, change: float):
        """Propagate priority change up the tree."""
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)

    def _retrieve(self, idx: int, s: float) -> int:
        """Find leaf index for a given cumulative priority value s."""
        left = 2 * idx + 1
        right = left + 1

        if left >= len(self.tree):  # Leaf node found
            return idx

        # Handle edge case where tree[left] might be slightly larger due to fp error
        if s <= self.tree[left] + 1e-8:  # Added tolerance
            return self._retrieve(left, s)
        else:
            # Ensure s subtraction doesn't go negative due to fp errors
            s_new = s - self.tree[left]
            # if s_new < 0: s_new = 0 # Clamp if necessary (shouldn't happen often)
            return self._retrieve(right, s_new)

    def total(self) -> float:
        """Total priority sum."""
        return self.tree[0]

    def add(self, priority: float, data: object):
        """Add new experience, overwriting oldest if full."""
        if priority < 0:
            priority = abs(priority) + 1e-6  # Ensure positive

        tree_idx = self.write_ptr + self.capacity - 1
        self.data[self.write_ptr] = data
        self.update(tree_idx, priority)  # Update priority in tree

        self.write_ptr += 1
        if self.write_ptr >= self.capacity:
            self.write_ptr = 0  # Wrap around

        # Only increment n_entries up to capacity
        if self.n_entries < self.capacity:
            self.n_entries += 1

    def update(self, tree_idx: int, priority: float):
        """Update priority of an experience at a given tree index."""
        if priority < 0:
            priority = abs(priority) + 1e-6  # Ensure positive
        if not (self.capacity - 1 <= tree_idx < 2 * self.capacity - 1):
            # print(f"Warning: Invalid tree index {tree_idx} for update. Capacity {self.capacity}. Skipping.")
            return  # Silently skip invalid index updates

        change = priority - self.tree[tree_idx]
        self.tree[tree_idx] = priority
        # Propagate change only if it's significant enough to avoid fp noise
        # Or always propagate? Always propagate seems safer.
        if abs(change) > 1e-9 and tree_idx > 0:
            self._propagate(tree_idx, change)

    def get(self, s: float) -> tuple[int, float, object]:
        """Sample an experience (returns tree_idx, priority, data)."""
        if self.total() <= 0 or self.n_entries == 0:
            # print("Warning: Sampling from empty or zero-priority SumTree.")
            # Need to return valid types even if empty
            return 0, 0.0, None

        # Clip s to valid range [epsilon, total] to avoid issues at boundaries
        s = np.clip(s, 1e-9, self.total())

        idx = self._retrieve(0, s)  # Get leaf node index in the tree array
        data_idx = idx - self.capacity + 1  # Corresponding index in data array

        # Validate data_idx before access
        if not (0 <= data_idx < self.n_entries):
            # This can happen if sampling races with adding near capacity
            # Fallback: sample again or return last valid entry? Return last valid entry.
            # print(f"Warning: SumTree get resulted in invalid data index {data_idx} (n_entries={self.n_entries}). Returning last valid entry.")
            if self.n_entries > 0:
                last_valid_data_idx = (
                    self.write_ptr - 1 + self.capacity
                ) % self.capacity
                last_valid_tree_idx = last_valid_data_idx + self.capacity - 1
                # Ensure last valid priority is read correctly
                priority = (
                    self.tree[last_valid_tree_idx]
                    if self.capacity - 1 <= last_valid_tree_idx < 2 * self.capacity - 1
                    else 0.0
                )
                return (last_valid_tree_idx, priority, self.data[last_valid_data_idx])
            else:  # Truly empty
                return 0, 0.0, None

        # Return tree_idx, priority from tree, data from data array
        return (idx, self.tree[idx], self.data[data_idx])

    def __len__(self) -> int:
        return self.n_entries


File: agent/networks/noisy_layer.py
# File: agent/networks/noisy_layer.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional


class NoisyLinear(nn.Module):
    """
    Noisy Linear Layer for Noisy Network (Factorised Gaussian Noise).
    Adapted from implementation details in the Noisy Nets paper and common libraries.
    """

    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        # Learnable weights and biases (mean parameters)
        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_mu = nn.Parameter(torch.empty(out_features))

        # Learnable noise parameters (standard deviation parameters)
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))

        # Non-learnable noise buffers (registered buffers are part of state_dict)
        self.register_buffer("weight_epsilon", torch.empty(out_features, in_features))
        self.register_buffer("bias_epsilon", torch.empty(out_features))

        self.reset_parameters()
        self.reset_noise()

    def reset_parameters(self):
        """Initialize mean and std parameters."""
        mu_range = 1.0 / math.sqrt(self.in_features)
        nn.init.uniform_(self.weight_mu, -mu_range, mu_range)
        nn.init.uniform_(self.bias_mu, -mu_range, mu_range)

        # Initialize sigma parameters with constant value from paper
        nn.init.constant_(
            self.weight_sigma, self.std_init / math.sqrt(self.in_features)
        )
        nn.init.constant_(
            self.bias_sigma, self.std_init / math.sqrt(self.out_features)
        )  # Note: out_features here

    def reset_noise(self):
        """Generate new noise samples."""
        # Factorised Gaussian noise
        epsilon_in = self._scale_noise(self.in_features)
        epsilon_out = self._scale_noise(self.out_features)

        # Outer product to generate weight noise
        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))
        # Bias noise is directly sampled
        self.bias_epsilon.copy_(epsilon_out)

    def _scale_noise(self, size: int) -> torch.Tensor:
        """Generate scaled noise tensor."""
        x = torch.randn(
            size, device=self.weight_mu.device
        )  # Ensure noise is on same device
        # Apply sign-sqrt transformation
        return x.sign().mul(x.abs().sqrt())

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass with noisy weights and biases."""
        if self.training:
            # Calculate noisy weights and biases
            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon
            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon
            # Reset noise for the next forward pass during training
            # Note: Some implementations reset noise *before* the forward pass.
            # Resetting after seems common too. Let's reset after.
            # self.reset_noise() # Resetting here might be slightly less efficient if layer used multiple times in one forward pass
        else:
            # In evaluation mode, use only the mean parameters
            weight = self.weight_mu
            bias = self.bias_mu

        return F.linear(x, weight, bias)

    def train(self, mode: bool = True):
        """Override train mode to potentially reset noise when switching."""
        super().train(mode)
        if mode:
            self.reset_noise()  # Reset noise when entering training mode


File: agent/networks/__init__.py


File: agent/networks/agent_network.py
# File: agent/networks/agent_network.py
# (Refined version of MixedNet, removing Transformer, using NoisyLinear in heads)
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from config import ModelConfig, EnvConfig  # Needs both
from typing import Tuple, List, Type

from .noisy_layer import NoisyLinear  # Import NoisyLinear


class AgentNetwork(nn.Module):
    """
    Main Agent Network: CNN (Grid) + MLP (Shape) -> Fused MLP -> Dueling Heads (Noisy optional).
    """

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        config: ModelConfig.Network,  # Use the specific sub-config
        env_config: EnvConfig,  # Need env_config for dimensions
        dueling: bool,
        use_noisy: bool,  # Flag to use NoisyLinear in heads
    ):
        super().__init__()
        self.dueling = dueling
        self.action_dim = action_dim
        self.env_config = env_config
        self.config = config
        self.use_noisy = use_noisy  # Store noisy flag

        # --- Calculate expected feature dimensions ---
        self.grid_h = env_config.ROWS
        self.grid_w = env_config.COLS
        self.grid_feat_per_cell = env_config.GRID_FEATURES_PER_CELL
        self.expected_grid_flat_dim = (
            self.grid_h * self.grid_w * self.grid_feat_per_cell
        )

        self.num_shape_slots = env_config.NUM_SHAPE_SLOTS
        self.shape_feat_per_shape = env_config.SHAPE_FEATURES_PER_SHAPE
        self.expected_shape_flat_dim = self.num_shape_slots * self.shape_feat_per_shape

        self.expected_total_dim = (
            self.expected_grid_flat_dim + self.expected_shape_flat_dim
        )
        if state_dim != self.expected_total_dim:
            raise ValueError(
                f"AgentNetwork init: Mismatch! state_dim ({state_dim}) != calculated expected_total_dim ({self.expected_total_dim}). Grid={self.expected_grid_flat_dim}, Shape={self.expected_shape_flat_dim}"
            )

        print(f"[AgentNetwork] Initializing (NoisyNets Heads: {self.use_noisy}):")
        print(
            f"  - Input Dim: {state_dim} (Grid: {self.expected_grid_flat_dim}, Shape: {self.expected_shape_flat_dim})"
        )

        # --- 1. Convolutional Branch (Grid Features) ---
        conv_layers = []
        current_channels = self.grid_feat_per_cell
        h, w = self.grid_h, self.grid_w
        for i, out_channels in enumerate(config.CONV_CHANNELS):
            conv_layers.append(
                nn.Conv2d(
                    current_channels,
                    out_channels,
                    kernel_size=config.CONV_KERNEL_SIZE,
                    stride=config.CONV_STRIDE,
                    padding=config.CONV_PADDING,
                    bias=not config.USE_BATCHNORM_CONV,  # Bias false if using BN
                )
            )
            if config.USE_BATCHNORM_CONV:
                conv_layers.append(nn.BatchNorm2d(out_channels))  # BN before activation
            conv_layers.append(config.CONV_ACTIVATION())
            conv_layers.append(
                nn.MaxPool2d(
                    kernel_size=config.POOL_KERNEL_SIZE, stride=config.POOL_STRIDE
                )
            )
            current_channels = out_channels
            # Calculate output size after conv and pool
            h = (
                h + 2 * config.CONV_PADDING - config.CONV_KERNEL_SIZE
            ) // config.CONV_STRIDE + 1
            w = (
                w + 2 * config.CONV_PADDING - config.CONV_KERNEL_SIZE
            ) // config.CONV_STRIDE + 1
            h = (h - config.POOL_KERNEL_SIZE) // config.POOL_STRIDE + 1
            w = (w - config.POOL_KERNEL_SIZE) // config.POOL_STRIDE + 1

        self.conv_base = nn.Sequential(*conv_layers)
        # Calculate final flattened size
        self.conv_out_size = self._get_conv_out_size(
            (self.grid_feat_per_cell, self.grid_h, self.grid_w)
        )
        print(
            f"  - Conv Branch Output Dim (HxWxC): ({h}x{w}x{current_channels}) -> Flattened: {self.conv_out_size}"
        )

        # --- 2. Shape Feature Branch (MLP) ---
        # Simple MLP for shape features (using standard Linear layers)
        shape_mlp_layers = []
        shape_mlp_layers.append(
            nn.Linear(self.expected_shape_flat_dim, config.SHAPE_MLP_HIDDEN_DIM)
        )
        # Optional: Add BatchNorm/Activation/Dropout for shape MLP if needed
        # if config.USE_BATCHNORM_FC: shape_mlp_layers.append(nn.BatchNorm1d(config.SHAPE_MLP_HIDDEN_DIM))
        shape_mlp_layers.append(config.SHAPE_MLP_ACTIVATION())
        # if config.DROPOUT_FC > 0: shape_mlp_layers.append(nn.Dropout(config.DROPOUT_FC))
        self.shape_mlp = nn.Sequential(*shape_mlp_layers)
        shape_mlp_out_dim = config.SHAPE_MLP_HIDDEN_DIM
        print(f"  - Shape MLP Output Dim: {shape_mlp_out_dim}")

        # --- 3. Combined Feature Fusion (MLP) ---
        combined_features_dim = self.conv_out_size + shape_mlp_out_dim
        print(
            f"  - Combined Features Dim (CNN_flat + Shape_MLP): {combined_features_dim}"
        )

        # Build the fusion MLP layers (these feed into the final heads)
        # Use standard Linear layers here unless specifically want noise earlier
        fusion_layers: List[nn.Module] = []
        current_fusion_dim = combined_features_dim
        # Linear Layer class choice for fusion part (usually standard Linear)
        fusion_linear_layer_class = nn.Linear
        for i, hidden_dim in enumerate(config.COMBINED_FC_DIMS):
            fusion_layers.append(
                fusion_linear_layer_class(current_fusion_dim, hidden_dim)
            )
            if config.USE_BATCHNORM_FC:
                fusion_layers.append(nn.BatchNorm1d(hidden_dim))  # BN before activation
            fusion_layers.append(config.COMBINED_ACTIVATION())
            if config.DROPOUT_FC > 0:
                fusion_layers.append(nn.Dropout(config.DROPOUT_FC))
            current_fusion_dim = hidden_dim

        self.fusion_mlp = nn.Sequential(*fusion_layers)
        head_input_dim = current_fusion_dim  # Input dim for Value/Advantage heads
        print(f"  - Fusion MLP Output Dim (Input to Heads): {head_input_dim}")

        # --- 4. Final Output Head(s) ---
        # Use NoisyLinear if self.use_noisy is True
        head_linear_layer_class = NoisyLinear if self.use_noisy else nn.Linear

        if self.dueling:
            # Value Head: Fusion Output -> 1
            self.value_head = nn.Sequential(
                head_linear_layer_class(head_input_dim, 1)
                # No activation/BN usually needed on final value output
            )
            # Advantage Head: Fusion Output -> Action Dim
            self.advantage_head = nn.Sequential(
                head_linear_layer_class(head_input_dim, action_dim)
                # No activation/BN usually needed on final advantage output
            )
            print(f"  - Using Dueling Heads ({'Noisy' if use_noisy else 'Linear'})")
        else:
            # Single Output Head: Fusion Output -> Action Dim
            self.output_head = nn.Sequential(
                head_linear_layer_class(head_input_dim, action_dim)
            )
            print(
                f"  - Using Single Output Head ({'Noisy' if use_noisy else 'Linear'})"
            )

    def _get_conv_out_size(self, shape: Tuple[int, int, int]) -> int:
        """Helper to calculate the flattened output size of the conv base."""
        with torch.no_grad():
            # Create a dummy input with batch size 1
            dummy_input = torch.zeros(1, *shape)
            output = self.conv_base(dummy_input)
            # Calculate the product of dimensions after channel dim (C, H, W) -> H*W*C
            return int(np.prod(output.size()[1:]))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # --- Input Splitting and Processing ---
        if x.dim() != 2 or x.size(1) != self.expected_total_dim:
            raise ValueError(
                f"AgentNetwork forward: Invalid input shape {x.shape}. Expected [B={x.shape[0]}, Features={self.expected_total_dim}]."
            )
        batch_size = x.size(0)

        # Split input into grid and shape features
        grid_features_flat = x[:, : self.expected_grid_flat_dim]
        shape_features_flat = x[:, self.expected_grid_flat_dim :]

        # Reshape grid features for CNN: [B, Flat] -> [B, C, H, W]
        grid_features_reshaped = grid_features_flat.view(
            batch_size, self.grid_feat_per_cell, self.grid_h, self.grid_w
        )

        # Process through CNN branch
        conv_output = self.conv_base(grid_features_reshaped)
        conv_output_flat = conv_output.view(batch_size, -1)  # Flatten CNN output

        # Process through Shape MLP branch
        shape_output = self.shape_mlp(shape_features_flat)

        # --- Feature Fusion ---
        # Concatenate flattened CNN output and Shape MLP output
        combined_features = torch.cat((conv_output_flat, shape_output), dim=1)

        # Process through fusion MLP
        fused_output = self.fusion_mlp(combined_features)

        # --- Output Heads ---
        if self.dueling:
            value = self.value_head(fused_output)  # [B, 1]
            advantage = self.advantage_head(fused_output)  # [B, ActionDim]
            # Combine value and advantage: Q = V + (A - mean(A))
            q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))
        else:
            q_values = self.output_head(fused_output)  # [B, ActionDim]

        return q_values

    def reset_noise(self):
        """Resets noise in all NoisyLinear layers within the network."""
        if self.use_noisy:
            for module in self.modules():
                if isinstance(module, NoisyLinear):
                    module.reset_noise()


File: environment/game_state.py
# File: environment/game_state.py
import time
import numpy as np
from typing import List, Optional, Tuple
from collections import deque  # Import deque
from typing import Deque  # Import Deque

from .grid import Grid
from .shape import Shape
from config import EnvConfig, RewardConfig


class GameState:
    def __init__(self):
        self.grid = Grid()
        self.shapes: List[Optional[Shape]] = [
            Shape() for _ in range(EnvConfig.NUM_SHAPE_SLOTS)
        ]
        self.score = 0.0  # Cumulative RL reward
        self.game_score = 0  # Game-specific score
        self.lines_cleared_this_episode = 0  # <<< NEW >>> Track lines cleared
        self.blink_time = 0.0
        self.last_time = time.time()
        self.freeze_time = 0.0
        self.game_over = False
        self._last_action_valid = True
        self.rewards = RewardConfig

    def reset(self) -> np.ndarray:
        self.grid = Grid()
        self.shapes = [Shape() for _ in range(EnvConfig.NUM_SHAPE_SLOTS)]
        self.score = 0.0
        self.game_score = 0
        self.lines_cleared_this_episode = 0  # <<< NEW >>> Reset lines cleared
        self.blink_time = 0.0
        self.freeze_time = 0.0
        self.game_over = False
        self._last_action_valid = True
        self.last_time = time.time()
        return self.get_state()

    # valid_actions, is_over, is_frozen, decode_act remain the same
    def valid_actions(self) -> List[int]:
        if self.game_over or self.freeze_time > 0:
            return []
        acts = []
        locations_per_shape = self.grid.rows * self.grid.cols
        for i, sh in enumerate(self.shapes):
            if not sh:
                continue
            for r in range(self.grid.rows):
                for c in range(self.grid.cols):
                    if self.grid.can_place(sh, r, c):
                        action_index = i * locations_per_shape + (
                            r * self.grid.cols + c
                        )
                        acts.append(action_index)
        return acts

    def is_over(self) -> bool:
        return self.game_over

    def is_frozen(self) -> bool:
        return self.freeze_time > 0

    def decode_act(self, a: int) -> Tuple[int, int, int]:
        locations_per_shape = self.grid.rows * self.grid.cols
        s_idx = a // locations_per_shape
        pos_idx = a % locations_per_shape
        rr = pos_idx // self.grid.cols
        cc = pos_idx % self.grid.cols
        return (s_idx, rr, cc)

    def _update_timers(self):
        now = time.time()
        dt = now - self.last_time
        self.last_time = now
        self.freeze_time = max(0, self.freeze_time - dt)
        self.blink_time = max(0, self.blink_time - dt)

    def _handle_invalid_placement(self) -> float:
        self._last_action_valid = False
        reward = self.rewards.PENALTY_INVALID_MOVE
        if not self.valid_actions():
            self.game_over = True
            self.freeze_time = 1.0
            reward += self.rewards.PENALTY_GAME_OVER
        return reward

    def _handle_valid_placement(
        self, shp: Shape, s_idx: int, rr: int, cc: int
    ) -> float:
        self._last_action_valid = True
        reward = 0.0
        reward += self.rewards.REWARD_PLACE_PER_TRI * len(shp.triangles)
        self.game_score += len(shp.triangles)
        self.grid.place(shp, rr, cc)
        self.shapes[s_idx] = None

        lines_cleared, triangles_cleared = self.grid.clear_filled_rows()
        self.lines_cleared_this_episode += lines_cleared  # <<< NEW >>> Accumulate lines

        if lines_cleared == 1:
            reward += self.rewards.REWARD_CLEAR_1
        elif lines_cleared == 2:
            reward += self.rewards.REWARD_CLEAR_2
        elif lines_cleared >= 3:
            reward += self.rewards.REWARD_CLEAR_3PLUS
        if triangles_cleared > 0:
            self.game_score += triangles_cleared * 2
            self.blink_time = 0.5
            self.freeze_time = 0.5

        num_holes = self.grid.count_holes()
        reward += num_holes * self.rewards.PENALTY_HOLE_PER_HOLE

        if all(x is None for x in self.shapes):
            self.shapes = [Shape() for _ in range(EnvConfig.NUM_SHAPE_SLOTS)]

        if not self.valid_actions():
            self.game_over = True
            self.freeze_time = 1.0
            reward += self.rewards.PENALTY_GAME_OVER
        return reward

    def step(self, a: int) -> Tuple[float, bool]:
        self._update_timers()
        if self.game_over:
            return (0.0, True)
        if self.freeze_time > 0:
            return (0.0, False)

        s_idx, rr, cc = self.decode_act(a)
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None
        is_valid_placement = shp is not None and self.grid.can_place(shp, rr, cc)

        current_rl_reward = (
            self._handle_valid_placement(shp, s_idx, rr, cc)
            if is_valid_placement
            else self._handle_invalid_placement()
        )

        if not self.game_over:
            current_rl_reward += self.rewards.REWARD_ALIVE_STEP

        self.score += current_rl_reward
        return (current_rl_reward, self.game_over)

    # get_state remains the same
    def get_state(self) -> np.ndarray:
        board_state = self.grid.get_feature_matrix().flatten()
        shape_features_per = EnvConfig.SHAPE_FEATURES_PER_SHAPE
        num_shapes_expected = EnvConfig.NUM_SHAPE_SLOTS
        shape_features_total = num_shapes_expected * shape_features_per
        shape_rep = np.zeros(shape_features_total, dtype=np.float32)
        idx = 0
        max_tris_norm = 6.0
        max_h_norm = float(self.grid.rows)
        max_w_norm = float(self.grid.cols)
        for i in range(num_shapes_expected):
            s = self.shapes[i] if i < len(self.shapes) else None
            if s:
                tri = s.triangles
                n = len(tri)
                ups = sum(1 for (_, _, u) in tri if u)
                dns = n - ups
                mnr, mnc, mxr, mxc = s.bbox()
                height = mxr - mnr + 1
                width = mxc - mnc + 1
                shape_rep[idx] = np.clip(float(n) / max_tris_norm, 0.0, 1.0)
                shape_rep[idx + 1] = np.clip(float(ups) / max_tris_norm, 0.0, 1.0)
                shape_rep[idx + 2] = np.clip(float(dns) / max_tris_norm, 0.0, 1.0)
                shape_rep[idx + 3] = np.clip(float(height) / max_h_norm, 0.0, 1.0)
                shape_rep[idx + 4] = np.clip(float(width) / max_w_norm, 0.0, 1.0)
            idx += shape_features_per
        state_array = np.concatenate((board_state, shape_rep))
        if len(state_array) != EnvConfig.STATE_DIM:
            raise ValueError(
                f"State length mismatch: {len(state_array)} vs {EnvConfig.STATE_DIM}"
            )
        return state_array

    def is_blinking(self) -> bool:
        return self.blink_time > 0

    def get_shapes(self) -> List[Shape]:
        return [s for s in self.shapes if s is not None]


File: environment/grid.py
# File: environment/grid.py
# File: environment/grid.py
"""
grid.py - The board of triangles.
"""
import numpy as np
from typing import List, Tuple  # <<< Added Tuple
from .triangle import Triangle
from .shape import Shape
from config import EnvConfig


class Grid:
    def __init__(self):
        self.rows = EnvConfig.ROWS
        self.cols = EnvConfig.COLS
        # Basic padding extension if rows > 8
        self.pad = [4, 3, 2, 1, 1, 2, 3, 4] + [0] * max(0, self.rows - 8)
        self.triangles: List[List[Triangle]] = []
        self._create()

    def _create(self) -> None:
        self.triangles = []
        for r in range(self.rows):
            rowt = []
            p = self.pad[r % len(self.pad)]  # Use modulo for robustness
            for c in range(self.cols):
                d = c < p or c >= self.cols - p
                tri = Triangle(r, c, (r + c) % 2 == 0, d)
                rowt.append(tri)
            self.triangles.append(rowt)

    def valid(self, r: int, c: int) -> bool:
        return 0 <= r < self.rows and 0 <= c < self.cols

    def can_place(self, shp: Shape, rr: int, cc: int) -> bool:
        for dr, dc, up in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if not self.valid(nr, nc):
                return False
            tri = self.triangles[nr][nc]
            if tri.is_death or tri.is_occupied or (tri.is_up != up):
                return False
        return True

    def place(self, shp: Shape, rr: int, cc: int) -> None:
        for dr, dc, _ in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if self.valid(nr, nc):  # Safety check
                tri = self.triangles[nr][nc]
                if (
                    not tri.is_death and not tri.is_occupied
                ):  # Ensure not placing on death/occupied
                    tri.is_occupied = True
                    tri.color = shp.color
                # else: # Optional: Log warning if trying to place on invalid cell after can_place check
                #     print(f"Warning: Place called on invalid cell ({nr},{nc}) despite can_place check.")
            else:
                print(
                    f"Warning: Attempted to place triangle out of bounds at ({nr}, {nc}) during place."
                )

    # <<< MODIFIED >>> Now returns (lines_cleared, triangles_cleared)
    def clear_filled_rows(self) -> Tuple[int, int]:
        lines_cleared = 0
        triangles_cleared = 0
        rows_to_clear_indices = []

        for r in range(self.rows):
            rowt = self.triangles[r]
            is_row_full = True
            num_triangles_in_row = 0  # Count non-death triangles in this row
            for t in rowt:
                if not t.is_death:
                    num_triangles_in_row += 1
                    if not t.is_occupied:
                        is_row_full = False
                        # break # Optimization: can stop checking this row early if one is empty
            # Only count as full if the row actually has placeable triangles
            if is_row_full and num_triangles_in_row > 0:
                rows_to_clear_indices.append(r)
                lines_cleared += 1

        # Clear the identified rows and count cleared triangles
        for r_idx in rows_to_clear_indices:
            for t in self.triangles[r_idx]:
                if (
                    not t.is_death and t.is_occupied
                ):  # Only count/clear occupied non-death tris
                    triangles_cleared += 1
                    t.is_occupied = False
                    t.color = None

        # Optional: Implement gravity (complex for hex grid)

        return lines_cleared, triangles_cleared

    # <<< NEW: Helper to count triangles in specific rows (used by GameState for score) >>>
    # This might not be strictly needed if clear_filled_rows returns the count, but added for potential flexibility
    def count_triangles_in_rows(self, row_indices: List[int]) -> int:
        count = 0
        for r_idx in row_indices:
            if 0 <= r_idx < self.rows:
                for t in self.triangles[r_idx]:
                    if not t.is_death:
                        count += 1
        return count

    def count_holes(self) -> int:
        """Counts empty, non-death cells with an occupied cell above them in the same column."""
        holes = 0
        for r in range(1, self.rows):
            for c in range(self.cols):
                current_tri = self.triangles[r][c]
                if not current_tri.is_occupied and not current_tri.is_death:
                    # Check if any cell directly above in the same column is occupied
                    is_covered = False
                    for r_above in range(r):
                        if (
                            self.valid(r_above, c)
                            and self.triangles[r_above][c].is_occupied
                        ):
                            is_covered = True
                            break
                    if is_covered:
                        holes += 1
        return holes

    def get_feature_matrix(self) -> np.ndarray:
        """Returns grid state as [Channel, Height, Width] numpy array."""
        # Channels: 0=Occupied, 1=Is_Up, 2=Is_Death
        grid_state = np.zeros((3, self.rows, self.cols), dtype=np.float32)
        for r in range(self.rows):
            for c in range(self.cols):
                t = self.triangles[r][c]
                grid_state[0, r, c] = 1.0 if t.is_occupied else 0.0
                grid_state[1, r, c] = 1.0 if t.is_up else 0.0
                grid_state[2, r, c] = 1.0 if t.is_death else 0.0
        return grid_state


File: environment/__init__.py


File: environment/triangle.py
"""
triangle.py - Single cell with occupant color
"""

from typing import Tuple, Optional, List

class Triangle:
    def __init__(self, row: int, col: int, is_up: bool, is_death: bool=False):
        self.row = row
        self.col = col
        self.is_up = is_up
        self.is_death = is_death
        self.is_occupied = is_death
        self.color: Optional[Tuple[int,int,int]] = None

    def get_points(self, ox:int, oy:int, cw:int, ch:int)->List[Tuple[float,float]]:
        x=ox+self.col*(cw*0.75)
        y=oy+self.row*ch
        if self.is_up:
            return [(x,y+ch),(x+cw,y+ch),(x+cw/2,y)]
        else:
            return [(x,y),(x+cw,y),(x+cw/2,y+ch)]

File: environment/shape.py
"""
shape.py - A multi-triangle shape picked from Google color palette
"""

import random
from typing import List, Tuple
from config import EnvConfig, VisConfig

GOOGLE_COLORS = VisConfig().GOOGLE_COLORS

class Shape:
    def __init__(self)->None:
        self.triangles:List[Tuple[int,int,bool]]=[]
        self.color:Tuple[int,int,int]=random.choice(GOOGLE_COLORS)
        self._generate()

    def _generate(self)->None:
        n = random.randint(1,5)
        first_up = random.choice([True,False])
        self.triangles.append((0,0,first_up))
        for _ in range(n-1):
            lr,lc,lu=self.triangles[-1]
            nbrs=self._neighbors(lr,lc,lu)
            if nbrs:self.triangles.append(random.choice(nbrs))

    def _neighbors(self,r:int,c:int,up:bool):
        if up:
            ns=[(r,c-1,False),(r,c+1,False),(r+1,c,False)]
        else:
            ns=[(r,c-1,True),(r,c+1,True),(r-1,c,True)]
        return [x for x in ns if x not in self.triangles]

    def bbox(self)->Tuple[int,int,int,int]:
        rr=[t[0] for t in self.triangles]
        cc=[t[1] for t in self.triangles]
        return (min(rr),min(cc),max(rr),max(cc))


File: stats/__init__.py


File: stats/stats_recorder.py
# File: stats/stats_recorder.py
import time
from abc import ABC, abstractmethod
from collections import deque
from typing import Deque, List, Dict, Any, Optional
import numpy as np


class StatsRecorderBase(ABC):
    """Base class for recording training statistics."""

    @abstractmethod
    def record_episode(
        self,
        episode_score: float,  # RL Score
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,  # <<< NEW >>> Lines Cleared
    ):
        """Record stats for a completed episode."""
        pass

    @abstractmethod
    def record_step(self, step_data: Dict[str, Any]):
        """Record stats from a training or environment step."""
        pass

    @abstractmethod
    def get_summary(self, current_global_step: int) -> Dict[str, Any]:
        """Return a dictionary containing summary statistics."""
        pass

    @abstractmethod
    def log_summary(self, global_step: int):
        """Log the summary statistics (e.g., print to console)."""
        pass

    @abstractmethod
    def close(self):
        """Perform any necessary cleanup (e.g., close files/connections)."""
        pass


class SimpleStatsRecorder(StatsRecorderBase):
    """Records stats in memory using deques and prints summaries."""

    def __init__(self, console_log_interval: int = 1000, avg_window: int = 100):
        if avg_window <= 0:
            avg_window = 100
        self.console_log_interval = max(1, console_log_interval)
        self.avg_window = avg_window

        # Deques for averaging recent data
        step_reward_window = max(avg_window * 10, 1000)
        self.step_rewards: Deque[float] = deque(maxlen=step_reward_window)
        self.losses: Deque[float] = deque(maxlen=avg_window)
        self.grad_norms: Deque[float] = deque(maxlen=avg_window)
        self.avg_max_qs: Deque[float] = deque(maxlen=avg_window)
        self.episode_scores: Deque[float] = deque(maxlen=avg_window)  # RL Scores
        self.episode_lengths: Deque[int] = deque(maxlen=avg_window)
        self.game_scores: Deque[int] = deque(maxlen=avg_window)
        self.episode_lines_cleared: Deque[int] = deque(maxlen=avg_window)  # <<< NEW >>>

        # Aggregate stats
        self.total_episodes = 0
        self.best_score = -float("inf")
        self.best_game_score = -float("inf")
        self.total_lines_cleared = 0  # <<< NEW >>>

        # Current values
        self.current_epsilon: float = 1.0
        self.current_beta: float = 0.0
        self.current_buffer_size: int = 0

        # Timing
        self.last_log_time: float = time.time()
        self.last_log_step: int = 0

        print(
            f"[SimpleStatsRecorder] Initialized. Log Interval: {self.console_log_interval} steps, Avg Window: {self.avg_window}"
        )

    def record_episode(
        self,
        episode_score: float,  # RL Score
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,  # <<< NEW >>>
    ):
        """Records stats for one completed episode."""
        self.episode_scores.append(episode_score)
        self.episode_lengths.append(episode_length)
        if game_score is not None:
            self.game_scores.append(game_score)
        if lines_cleared is not None:  # <<< NEW >>>
            self.episode_lines_cleared.append(lines_cleared)
            self.total_lines_cleared += lines_cleared

        self.total_episodes = episode_num
        step_info = (
            f"at Step ~{global_step/1e6:.1f}M" if global_step is not None else ""
        )

        if episode_score > self.best_score:
            self.best_score = episode_score
            print(
                f"\n--- New Best RL Score: {self.best_score:.2f} (Ep {episode_num} {step_info}) ---"
            )
        if game_score is not None and game_score > self.best_game_score:
            self.best_game_score = game_score
            print(
                f"--- New Best Game Score: {self.best_game_score} (Ep {episode_num} {step_info}) ---"
            )

    # record_step remains the same
    def record_step(self, step_data: Dict[str, Any]):
        if "loss" in step_data:
            self.losses.append(step_data["loss"])
        if "grad_norm" in step_data:
            self.grad_norms.append(step_data["grad_norm"])
        if "step_reward" in step_data:
            self.step_rewards.append(step_data["step_reward"])
        if "epsilon" in step_data:
            self.current_epsilon = step_data["epsilon"]
        if "beta" in step_data:
            self.current_beta = step_data["beta"]
        if "buffer_size" in step_data:
            self.current_buffer_size = step_data["buffer_size"]
        if "avg_max_q" in step_data:
            self.avg_max_qs.append(step_data["avg_max_q"])

    def get_summary(self, current_global_step: int) -> Dict[str, Any]:
        """Calculates and returns a dictionary of summary statistics."""
        current_time = time.time()
        elapsed_time = max(1e-6, current_time - self.last_log_time)
        steps_since_last = max(0, current_global_step - self.last_log_step)
        steps_per_sec = steps_since_last / elapsed_time if elapsed_time > 0 else 0.0

        avg_score = np.mean(self.episode_scores) if self.episode_scores else 0.0
        avg_length = np.mean(self.episode_lengths) if self.episode_lengths else 0.0
        avg_loss = np.mean(self.losses) if self.losses else 0.0
        avg_grad = np.mean(self.grad_norms) if self.grad_norms else 0.0
        avg_step_reward = np.mean(self.step_rewards) if self.step_rewards else 0.0
        avg_max_q = np.mean(self.avg_max_qs) if self.avg_max_qs else 0.0
        avg_game_score = np.mean(self.game_scores) if self.game_scores else 0.0
        avg_lines_cleared = (
            np.mean(self.episode_lines_cleared) if self.episode_lines_cleared else 0.0
        )  # <<< NEW >>>

        summary = {
            "avg_score_100": avg_score,
            "avg_length_100": avg_length,
            "avg_loss_100": avg_loss,
            "avg_grad_100": avg_grad,
            "avg_max_q_100": avg_max_q,
            "avg_game_score_100": avg_game_score,
            "avg_lines_cleared_100": avg_lines_cleared,  # <<< NEW >>>
            "avg_step_reward_1k": avg_step_reward,
            "total_episodes": self.total_episodes,
            "best_score": self.best_score if self.best_score > -float("inf") else 0.0,
            "best_game_score": (
                self.best_game_score if self.best_game_score > -float("inf") else 0.0
            ),
            "total_lines_cleared": self.total_lines_cleared,  # <<< NEW >>>
            "epsilon": self.current_epsilon,
            "beta": self.current_beta,
            "buffer_size": self.current_buffer_size,
            "steps_per_second": steps_per_sec,
            "global_step": current_global_step,
            "num_ep_scores": len(self.episode_scores),
            "num_ep_lengths": len(self.episode_lengths),
            "num_losses": len(self.losses),
            "num_avg_max_qs": len(self.avg_max_qs),
            "num_game_scores": len(self.game_scores),
            "num_lines_cleared": len(self.episode_lines_cleared),  # <<< NEW >>>
        }
        return summary

    def log_summary(self, global_step: int):
        """Logs summary to console if interval has passed."""
        if (
            global_step == 0
            or global_step < self.last_log_step + self.console_log_interval
        ):
            return

        summary = self.get_summary(global_step)

        log_str = (
            f"[Stats] Step: {global_step/1e6:<7.2f}M | "
            f"Ep: {summary['total_episodes']:<8} | "
            f"SPS: {summary['steps_per_second']:<6.0f} | "
            f"RLScore({summary['num_ep_scores']}): {summary['avg_score_100']:<6.2f} | "
            f"GameScore({summary['num_game_scores']}): {summary['avg_game_score_100']:<6.1f} | "
            f"Lines({summary['num_lines_cleared']}): {summary['avg_lines_cleared_100']:<5.2f} | "  # <<< NEW >>>
            # f"BestRL: {summary['best_score']:.2f} | " # Optional: Shorten log line
            # f"BestGame: {summary['best_game_score']:.1f} | " # Optional: Shorten log line
            f"Len({summary['num_ep_lengths']}): {summary['avg_length_100']:.1f} | "
            f"Loss({summary['num_losses']}): {summary['avg_loss_100']:.4f} | "
            f"AvgMaxQ({summary['num_avg_max_qs']}): {summary['avg_max_q_100']:.3f} | "
            f"Eps: {summary['epsilon']:.3f} | "  # Still log epsilon even if noisy=0
            f"Beta: {summary['beta']:.3f} | "
            f"Buf: {summary['buffer_size']/1e6:.2f}M"
        )
        print(log_str)

        self.last_log_time = time.time()
        self.last_log_step = global_step

    def close(self):
        print("[SimpleStatsRecorder] Closed.")


