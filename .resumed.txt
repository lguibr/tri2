File: requirements.txt
pygame>=2.1.0
numpy>=1.20.0
torch>=1.10.0
tensorboard
cloudpickle
torchvision
matplotlib

File: .resumed.txt


File: main_pygame.py
# File: main_pygame.py
import sys
import pygame
import numpy as np
import os
import time
import traceback
import torch
from typing import List, Tuple, Optional, Dict, Any, Deque

# --- Project Imports ---
# Setup & Logging
from logger import TeeLogger
from app_setup import (
    initialize_pygame,
    initialize_directories,
    load_and_validate_configs,
)

# Configs (Import necessary instances/constants)
from config import (
    VisConfig,
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    TensorBoardConfig,
    DemoConfig,
    DEVICE,
    RANDOM_SEED,
    BUFFER_SAVE_PATH,
    MODEL_SAVE_PATH,
    BASE_CHECKPOINT_DIR,
    BASE_LOG_DIR,
    RUN_LOG_DIR,
)

# Core Components & Helpers
from environment.game_state import GameState, StateType
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from training.trainer import Trainer
from stats.stats_recorder import StatsRecorderBase
from ui.renderer import UIRenderer
from ui.input_handler import InputHandler
from utils.helpers import set_random_seeds
from utils.init_checks import run_pre_checks
from init.rl_components import (
    initialize_envs,
    initialize_agent_buffer,
    initialize_stats_recorder,
    initialize_trainer,
)


class MainApp:
    """Main application class orchestrating the Pygame UI and RL training."""

    def __init__(self):
        print("Initializing Application...")
        set_random_seeds(RANDOM_SEED)

        # --- Configuration Loading ---
        # Instantiate config classes needed by the app directly
        self.vis_config = VisConfig()
        self.env_config = EnvConfig()
        self.dqn_config = DQNConfig()
        self.train_config = TrainConfig()
        self.buffer_config = BufferConfig()
        self.model_config = ModelConfig()
        self.stats_config = StatsConfig()
        self.tensorboard_config = TensorBoardConfig()
        self.demo_config = DemoConfig()
        self.reward_config = RewardConfig()  # Although not directly used, good practice

        # Load combined config dict and validate
        self.config_dict = load_and_validate_configs()
        self.num_envs = self.env_config.NUM_ENVS

        # --- Setup Pygame and Directories ---
        initialize_directories()
        self.screen, self.clock = initialize_pygame(self.vis_config)

        # --- App State ---
        self.app_state = "Initializing"
        self.is_training = False
        self.cleanup_confirmation_active = False
        self.last_cleanup_message_time = 0.0
        self.cleanup_message = ""
        self.status = "Initializing Components"

        # --- Initialize Components ---
        self.renderer: Optional[UIRenderer] = None
        self.input_handler: Optional[InputHandler] = None
        self.envs: List[GameState] = []
        self.agent: Optional[DQNAgent] = None
        self.buffer: Optional[ReplayBufferBase] = None
        self.stats_recorder: Optional[StatsRecorderBase] = None
        self.trainer: Optional[Trainer] = None
        self.demo_env: Optional[GameState] = None

        self._initialize_core_components()

        # Transition to Main Menu after successful initialization
        self.app_state = "MainMenu"
        self.status = "Paused"
        print("Initialization Complete. Ready.")
        print(f"--- tensorboard --logdir {os.path.abspath(BASE_LOG_DIR)} ---")

    def _initialize_core_components(self):
        """Initializes Renderer, RL components, Demo Env, and Input Handler."""
        try:
            # Init Renderer FIRST for immediate feedback
            self.renderer = UIRenderer(self.screen, self.vis_config)
            self.renderer.render_all(  # Render initializing screen
                app_state=self.app_state,
                is_training=False,
                status=self.status,
                stats_summary={},
                buffer_capacity=0,
                envs=[],
                num_envs=0,
                env_config=self.env_config,
                cleanup_confirmation_active=False,
                cleanup_message="",
                last_cleanup_message_time=0,
                tensorboard_log_dir=None,
                plot_data={},
                demo_env=None,
            )
            pygame.time.delay(100)  # Brief pause to show initializing screen

            # Init RL components
            self._initialize_rl_components()

            # Init Demo Env
            self._initialize_demo_env()

            # Init Input Handler (pass self methods as callbacks)
            self.input_handler = InputHandler(
                self.screen,
                self.renderer,
                self._toggle_training,
                self._request_cleanup,
                self._cancel_cleanup,
                self._confirm_cleanup,
                self._exit_app,
                self._start_demo_mode,
                self._exit_demo_mode,
                self._handle_demo_input,
            )
        except Exception as init_err:
            print(f"FATAL ERROR during component initialization: {init_err}")
            traceback.print_exc()
            # Attempt to show error on screen if renderer exists
            if self.renderer:
                try:
                    self.app_state = "Error"
                    self.status = "Initialization Failed"
                    self.renderer._render_error_screen(self.status)
                    pygame.display.flip()
                    time.sleep(5)  # Show error for a bit
                except Exception:
                    pass  # Ignore errors during error rendering
            pygame.quit()
            sys.exit(1)

    def _initialize_rl_components(self):
        """Initializes RL components using helper functions."""
        print("Initializing RL components...")
        start_time = time.time()
        try:
            self.envs = initialize_envs(self.num_envs, self.env_config)
            self.agent, self.buffer = initialize_agent_buffer(
                self.model_config, self.dqn_config, self.env_config, self.buffer_config
            )
            if self.buffer is None:
                raise RuntimeError("Buffer initialization failed unexpectedly.")

            self.stats_recorder = initialize_stats_recorder(
                self.stats_config,
                self.tensorboard_config,
                self.config_dict,
                self.agent,
                self.env_config,
            )
            if self.stats_recorder is None:
                raise RuntimeError("Stats Recorder initialization failed unexpectedly.")

            self.trainer = initialize_trainer(
                self.envs,
                self.agent,
                self.buffer,
                self.stats_recorder,
                self.env_config,
                self.dqn_config,
                self.train_config,
                self.buffer_config,
                self.model_config,
            )
            print(f"RL components initialized in {time.time() - start_time:.2f}s")
        except Exception as e:
            # Let the caller handle the fatal error exit
            print(f"Error during RL component initialization: {e}")
            raise e  # Re-raise to be caught by _initialize_core_components

    def _initialize_demo_env(self):
        """Initializes the separate environment for demo mode."""
        print("Initializing Demo Environment...")
        try:
            self.demo_env = GameState()
            self.demo_env.reset()
            print("Demo environment initialized.")
        except Exception as e:
            print(f"ERROR initializing demo environment: {e}")
            traceback.print_exc()
            self.demo_env = None  # Continue without demo mode
            print("Warning: Demo mode may be unavailable.")

    # --- Input Handler Callbacks (Remain in MainApp) ---
    def _toggle_training(self):
        if self.app_state != "MainMenu":
            return
        self.is_training = not self.is_training
        print(f"Training {'STARTED' if self.is_training else 'PAUSED'}")
        if not self.is_training:
            self._try_save_checkpoint()

    def _request_cleanup(self):
        if self.app_state != "MainMenu":
            return
        was_training = self.is_training
        self.is_training = False
        if was_training:
            self._try_save_checkpoint()
        self.cleanup_confirmation_active = True
        print("Cleanup requested. Confirm action.")

    def _cancel_cleanup(self):
        self.cleanup_confirmation_active = False
        self.cleanup_message = "Cleanup cancelled."
        self.last_cleanup_message_time = time.time()
        print("Cleanup cancelled by user.")

    def _confirm_cleanup(self):
        print("Cleanup confirmed by user. Starting process...")
        try:
            self._cleanup_data()
        except Exception as e:
            print(f"FATAL ERROR during cleanup: {e}")
            traceback.print_exc()
            self.status = "Error: Cleanup Failed Critically"
            self.app_state = "Error"
        finally:
            self.cleanup_confirmation_active = False  # Ensure flag is reset
            print(
                f"Cleanup process finished. State: {self.app_state}, Status: {self.status}"
            )

    def _exit_app(self) -> bool:
        print("Exit requested.")
        return False  # Signal exit to main loop

    def _start_demo_mode(self):
        if self.demo_env is None:
            print("Cannot start demo mode: Demo environment failed to initialize.")
            return
        if self.app_state == "MainMenu":
            print("Entering Demo Mode...")
            self.is_training = False
            self._try_save_checkpoint()
            self.app_state = "Playing"
            self.status = "Playing Demo"
            self.demo_env.reset()

    def _exit_demo_mode(self):
        if self.app_state == "Playing":
            print("Exiting Demo Mode...")
            self.app_state = "MainMenu"
            self.status = "Paused"

    def _handle_demo_input(self, event: pygame.event.Event):
        """Handles keyboard input during demo mode."""
        if self.app_state != "Playing" or self.demo_env is None:
            return
        if self.demo_env.is_frozen() or self.demo_env.is_over():
            return

        if event.type == pygame.KEYDOWN:
            action_taken = False
            if event.key == pygame.K_LEFT:
                self.demo_env.move_target(0, -1)
                action_taken = True
            elif event.key == pygame.K_RIGHT:
                self.demo_env.move_target(0, 1)
                action_taken = True
            elif event.key == pygame.K_UP:
                self.demo_env.move_target(-1, 0)
                action_taken = True
            elif event.key == pygame.K_DOWN:
                self.demo_env.move_target(1, 0)
                action_taken = True
            elif event.key == pygame.K_q:
                self.demo_env.cycle_shape(-1)
                action_taken = True
            elif event.key == pygame.K_e:
                self.demo_env.cycle_shape(1)
                action_taken = True
            elif event.key == pygame.K_SPACE:
                action_index = self.demo_env.get_action_for_current_selection()
                if action_index is not None:
                    state_before = self.demo_env.get_state()
                    reward, done = self.demo_env.step(action_index)
                    next_state_after = self.demo_env.get_state()
                    if self.buffer:
                        try:
                            self.buffer.push(
                                state_before,
                                action_index,
                                reward,
                                next_state_after,
                                done,
                            )
                            if (
                                len(self.buffer) % 50 == 0
                                and len(self.buffer)
                                <= self.train_config.LEARN_START_STEP
                            ):
                                print(
                                    f"[Demo] Added experience. Buffer: {len(self.buffer)}/{self.buffer.capacity}"
                                )
                        except Exception as buf_e:
                            print(f"Error pushing demo experience to buffer: {buf_e}")
                    action_taken = True
                else:
                    action_taken = (
                        True  # Still counts as an action (attempted placement)
                    )

            if self.demo_env.is_over():
                print("[Demo] Game Over! Press ESC to exit.")

    # --- Core Logic Methods (Remain in MainApp) ---
    def _cleanup_data(self):
        """Deletes current run's checkpoint/buffer and re-initializes."""
        print("\n--- CLEANUP DATA INITIATED (Current Run Only) ---")
        self.app_state = "Initializing"
        self.is_training = False
        self.status = "Cleaning"
        messages = []

        # Render initializing screen during cleanup
        if self.renderer:
            try:
                self.renderer.render_all(
                    app_state=self.app_state,
                    is_training=False,
                    status=self.status,
                    stats_summary={},
                    buffer_capacity=0,
                    envs=[],
                    num_envs=0,
                    env_config=self.env_config,
                    cleanup_confirmation_active=False,
                    cleanup_message="",
                    last_cleanup_message_time=0,
                    tensorboard_log_dir=None,
                    plot_data={},
                    demo_env=self.demo_env,
                )
                pygame.display.flip()
                pygame.time.delay(100)
            except Exception as render_err:
                print(f"Warning: Error rendering during cleanup start: {render_err}")

        # Close trainer/stats FIRST
        if self.trainer:
            print("[Cleanup] Running trainer cleanup...")
            try:
                self.trainer.cleanup(save_final=False)
            except Exception as e:
                print(f"Error during trainer cleanup: {e}")
        if self.stats_recorder:
            print("[Cleanup] Closing stats recorder...")
            try:
                self.stats_recorder.close()
            except Exception as e:
                print(f"Error closing stats recorder: {e}")

        # Delete files
        print("[Cleanup] Deleting files...")
        for path, desc in [
            (MODEL_SAVE_PATH, "Agent ckpt"),
            (BUFFER_SAVE_PATH, "Buffer state"),
        ]:
            try:
                if os.path.isfile(path):
                    os.remove(path)
                    msg = f"{desc} deleted: {os.path.basename(path)}"
                else:
                    msg = f"{desc} not found (current run)."
                print(f"  - {msg}")
                messages.append(msg)
            except OSError as e:
                msg = f"Error deleting {desc}: {e}"
                print(f"  - {msg}")
                messages.append(msg)

        time.sleep(0.1)

        # Re-initialize RL components
        print("[Cleanup] Re-initializing RL components...")
        try:
            self._initialize_rl_components()
            if self.demo_env:
                self.demo_env.reset()
            print("[Cleanup] RL components re-initialized successfully.")
            messages.append("RL components re-initialized.")
            self.status = "Paused"
            self.app_state = "MainMenu"
        except Exception as e:
            print(f"FATAL ERROR during RL re-initialization after cleanup: {e}")
            traceback.print_exc()
            self.status = "Error: Re-init Failed"
            self.app_state = "Error"
            messages.append("ERROR RE-INITIALIZING RL COMPONENTS!")
            if self.renderer:
                try:
                    self.renderer._render_error_screen(self.status)
                except Exception as render_err_final:
                    print(f"Warning: Failed to render error screen: {render_err_final}")

        self.cleanup_message = "\n".join(messages)
        self.last_cleanup_message_time = time.time()
        print(
            f"--- CLEANUP DATA COMPLETE (Final State: {self.app_state}, Status: {self.status}) ---"
        )

    def _try_save_checkpoint(self):
        """Saves checkpoint if not training and trainer exists."""
        if self.app_state == "MainMenu" and not self.is_training and self.trainer:
            print("Saving checkpoint on pause...")
            try:
                self.trainer.maybe_save_checkpoint(force_save=True)
            except Exception as e:
                print(f"Error saving checkpoint on pause: {e}")

    def _update(self):
        """Updates the application state and performs training steps."""
        should_step_trainer = False

        if self.app_state == "MainMenu":
            if self.cleanup_confirmation_active:
                self.status = "Confirm Cleanup"
            elif not self.is_training and self.status != "Error":
                self.status = "Paused"
            elif not self.trainer:
                if self.status != "Error":
                    self.status = "Error: Trainer Missing"
            elif self.is_training:
                current_fill = len(self.buffer) if self.buffer else 0
                current_step = self.trainer.global_step if self.trainer else 0
                needed = self.train_config.LEARN_START_STEP
                is_buffer_ready = current_fill >= needed and current_step >= needed

                self.status = (
                    f"Buffering ({current_fill / max(1, needed) * 100:.0f}%)"
                    if not is_buffer_ready
                    else "Training"
                )
                should_step_trainer = True
            else:  # Not training
                if not self.status.startswith("Buffering") and self.status != "Error":
                    self.status = "Paused"

            if should_step_trainer:
                if not self.trainer:
                    print("Error: Trainer became unavailable during _update.")
                    self.status = "Error: Trainer Lost"
                    self.is_training = False
                else:
                    try:
                        step_start_time = time.time()
                        self.trainer.step()
                        step_duration = time.time() - step_start_time
                        if self.vis_config.VISUAL_STEP_DELAY > 0:
                            time.sleep(
                                max(
                                    0, self.vis_config.VISUAL_STEP_DELAY - step_duration
                                )
                            )
                    except Exception as e:
                        print(
                            f"\n--- ERROR DURING TRAINING UPDATE (Step: {getattr(self.trainer, 'global_step', 'N/A')}) ---"
                        )
                        traceback.print_exc()
                        print("--- Pausing training due to error. ---")
                        self.is_training = False
                        self.status = "Error: Training Step Failed"
                        self.app_state = "Error"

        elif self.app_state == "Playing":
            if self.demo_env and hasattr(self.demo_env, "_update_timers"):
                self.demo_env._update_timers()
            self.status = "Playing Demo"
        # Other states (Initializing, Error) have status set elsewhere

    def _render(self):
        """Renders the UI based on the current application state."""
        stats_summary = {}
        plot_data: Dict[str, Deque] = {}
        buffer_capacity = 0

        if self.stats_recorder:
            current_step = getattr(self.trainer, "global_step", 0)
            try:
                stats_summary = self.stats_recorder.get_summary(current_step)
            except Exception as e:
                print(f"Error getting stats summary: {e}")
                stats_summary = {"global_step": current_step}
            try:
                plot_data = self.stats_recorder.get_plot_data()
            except Exception as e:
                print(f"Error getting plot data: {e}")
                plot_data = {}
        elif self.app_state == "Error":
            stats_summary = {"global_step": getattr(self.trainer, "global_step", 0)}

        if self.buffer:
            try:
                buffer_capacity = getattr(self.buffer, "capacity", 0)
            except Exception:
                buffer_capacity = 0

        if not self.renderer:
            print("Error: Renderer not initialized in _render.")
            try:  # Basic error render
                self.screen.fill((0, 0, 0))
                font = pygame.font.SysFont(None, 50)
                surf = font.render("Renderer Error!", True, (255, 0, 0))
                self.screen.blit(
                    surf, surf.get_rect(center=self.screen.get_rect().center)
                )
                pygame.display.flip()
            except Exception:
                pass
            return

        try:
            self.renderer.render_all(
                app_state=self.app_state,
                is_training=self.is_training,
                status=self.status,
                stats_summary=stats_summary,
                buffer_capacity=buffer_capacity,
                envs=(self.envs if hasattr(self, "envs") else []),
                num_envs=self.num_envs,
                env_config=self.env_config,
                cleanup_confirmation_active=self.cleanup_confirmation_active,
                cleanup_message=self.cleanup_message,
                last_cleanup_message_time=self.last_cleanup_message_time,
                tensorboard_log_dir=(
                    self.tensorboard_config.LOG_DIR
                    if self.tensorboard_config.LOG_DIR
                    else None
                ),
                plot_data=plot_data,
                demo_env=self.demo_env,
            )
        except Exception as render_all_err:
            print(f"CRITICAL ERROR in renderer.render_all: {render_all_err}")
            traceback.print_exc()
            try:
                self.app_state = "Error"
                self.status = "Render Error"
                self.renderer._render_error_screen(self.status)
            except Exception as e:
                print(f"Error rendering error screen: {e}")

        # Clear transient message
        if time.time() - self.last_cleanup_message_time >= 5.0:
            self.cleanup_message = ""

    def _perform_cleanup(self):
        """Handles final cleanup of resources."""
        print("Exiting application...")
        if self.trainer:
            print("Performing final trainer cleanup...")
            try:
                save_on_exit = self.status != "Cleaning" and self.app_state != "Error"
                self.trainer.cleanup(save_final=save_on_exit)
            except Exception as final_cleanup_err:
                print(f"Error during final trainer cleanup: {final_cleanup_err}")
        elif self.stats_recorder:  # Close stats recorder even if trainer failed/missing
            print("Closing stats recorder...")
            try:
                self.stats_recorder.close()
            except Exception as log_e:
                print(f"Error closing stats recorder on exit: {log_e}")

        pygame.quit()
        print("Application exited.")

    def run(self):
        """Main application loop."""
        print("Starting main application loop...")
        running = True
        try:
            while running:
                start_frame_time = time.perf_counter()

                # Handle Input
                if self.input_handler:
                    try:
                        running = self.input_handler.handle_input(
                            self.app_state, self.cleanup_confirmation_active
                        )
                    except Exception as input_err:
                        print(
                            f"\n--- UNHANDLED ERROR IN INPUT LOOP ({self.app_state}) ---"
                        )
                        traceback.print_exc()
                        running = False  # Exit on unhandled input error
                else:  # Basic exit if handler failed
                    for event in pygame.event.get():
                        if event.type == pygame.QUIT:
                            running = False
                        if (
                            event.type == pygame.KEYDOWN
                            and event.key == pygame.K_ESCAPE
                        ):
                            if self.app_state == "Playing":
                                self._exit_demo_mode()
                            elif not self.cleanup_confirmation_active:
                                running = False
                    if not running:
                        break

                if not running:
                    break

                # Update State
                try:
                    self._update()
                except Exception as update_err:
                    print(
                        f"\n--- UNHANDLED ERROR IN UPDATE LOOP ({self.app_state}) ---"
                    )
                    traceback.print_exc()
                    self.status = "Error: Update Loop Failed"
                    self.app_state = "Error"
                    self.is_training = False

                # Render Frame
                try:
                    self._render()
                except Exception as render_err:
                    print(
                        f"\n--- UNHANDLED ERROR IN RENDER LOOP ({self.app_state}) ---"
                    )
                    traceback.print_exc()
                    self.status = "Error: Render Loop Failed"
                    self.app_state = "Error"

                # Frame Rate Limiting
                frame_time = time.perf_counter() - start_frame_time
                target_frame_time = (
                    1.0 / self.vis_config.FPS if self.vis_config.FPS > 0 else 0
                )
                sleep_time = max(0, target_frame_time - frame_time)
                if sleep_time > 0.001:
                    time.sleep(sleep_time)

        except KeyboardInterrupt:
            print("\nCtrl+C detected. Exiting gracefully...")
        except Exception as e:
            print(f"\n--- UNHANDLED EXCEPTION IN MAIN LOOP ({self.app_state}) ---")
            traceback.print_exc()
            print("--- EXITING ---")
        finally:
            self._perform_cleanup()


# --- Main Execution Block ---
if __name__ == "__main__":
    # Ensure base directories exist before logger setup
    os.makedirs(BASE_CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(BASE_LOG_DIR, exist_ok=True)
    os.makedirs(RUN_LOG_DIR, exist_ok=True)  # Ensure run-specific log dir exists

    log_filepath = os.path.join(RUN_LOG_DIR, "console_output.log")

    # Setup logging
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    logger = TeeLogger(log_filepath, original_stdout)
    sys.stdout = logger
    sys.stderr = logger

    app_instance = None
    exit_code = 0

    try:
        if run_pre_checks():
            app_instance = MainApp()
            app_instance.run()  # run() now handles cleanup internally
    except SystemExit as exit_err:
        print(f"Exiting due to SystemExit (Code: {getattr(exit_err, 'code', 'N/A')}).")
        exit_code = (
            getattr(exit_err, "code", 1)
            if isinstance(getattr(exit_err, "code", 1), int)
            else 1
        )
    except Exception as main_err:
        print("\n--- UNHANDLED EXCEPTION DURING APP INITIALIZATION OR RUN ---")
        traceback.print_exc()
        print("--- EXITING DUE TO ERROR ---")
        exit_code = 1
        # Ensure cleanup is attempted even if run() didn't finish
        if app_instance and hasattr(app_instance, "_perform_cleanup"):
            print("Attempting cleanup after main exception...")
            try:
                app_instance._perform_cleanup()
            except Exception as cleanup_err:
                print(f"Error during cleanup after main exception: {cleanup_err}")
    finally:
        # Restore logging
        if logger:
            final_app_state = getattr(app_instance, "app_state", "UNKNOWN")
            print(
                f"Restoring console output (Final App State: {final_app_state}). Full log saved to: {log_filepath}"
            )
            logger.close()
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        print(f"Console logging restored. Full log should be in: {log_filepath}")
        sys.exit(exit_code)  # Exit with appropriate code


File: app_setup.py
# File: app_setup.py
import os
import pygame
from typing import Tuple, Dict, Any

from config import (
    VisConfig,
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    TensorBoardConfig,
    DemoConfig,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,
    get_config_dict,
    print_config_info_and_validate,
)


def initialize_pygame(
    vis_config: VisConfig,
) -> Tuple[pygame.Surface, pygame.time.Clock]:
    """Initializes Pygame, sets up the screen and clock."""
    print("Initializing Pygame...")
    pygame.init()
    pygame.font.init()
    screen = pygame.display.set_mode(
        (vis_config.SCREEN_WIDTH, vis_config.SCREEN_HEIGHT), pygame.RESIZABLE
    )
    pygame.display.set_caption("TriCrack DQN")
    clock = pygame.time.Clock()
    print("Pygame initialized.")
    return screen, clock


def initialize_directories():
    """Creates necessary runtime directories."""
    os.makedirs(RUN_CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(RUN_LOG_DIR, exist_ok=True)
    print(f"Ensured directories exist: {RUN_CHECKPOINT_DIR}, {RUN_LOG_DIR}")


def load_and_validate_configs() -> Dict[str, Any]:
    """Loads all config classes and returns the combined config dictionary."""
    # Instantiating config classes implicitly loads defaults
    # The get_config_dict function retrieves their values
    config_dict = get_config_dict()
    print_config_info_and_validate()
    return config_dict


File: visualization/__init__.py


File: init/rl_components.py
import sys
import traceback
import numpy as np
import torch
from typing import List, Tuple, Optional, Dict, Any, Callable

# Import configurations
from config import (
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    TensorBoardConfig,
    DEVICE,
    BUFFER_SAVE_PATH,
    MODEL_SAVE_PATH,
    get_config_dict,
)

# Import core components
try:
    from environment.game_state import GameState, StateType
except ImportError as e:
    print(f"Error importing environment: {e}")
    sys.exit(1)
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from agent.replay_buffer.buffer_utils import create_replay_buffer
from training.trainer import Trainer
from stats.stats_recorder import StatsRecorderBase
from stats.tensorboard_logger import TensorBoardStatsRecorder


def initialize_envs(num_envs: int, env_config: EnvConfig) -> List[GameState]:
    print(f"Initializing {num_envs} game environments...")
    try:
        envs = [
            GameState() for _ in range(num_envs)
        ]  # GameState uses its internal EnvConfig instance now
        s_test_dict = envs[0].reset()  # Returns dict

        if not isinstance(s_test_dict, dict):
            raise TypeError("Env reset did not return a dictionary state.")

        # --- MODIFIED: Check grid component shape (2 channels) ---
        if "grid" not in s_test_dict:
            raise KeyError("State dict missing 'grid'")
        grid_state = s_test_dict["grid"]
        expected_grid_shape = (
            env_config.GRID_STATE_SHAPE
        )  # Get expected shape (2, H, W) from instance
        if (
            not isinstance(grid_state, np.ndarray)
            or grid_state.shape != expected_grid_shape
        ):
            raise ValueError(
                f"Initial grid state shape mismatch! Env:{grid_state.shape}, Cfg:{expected_grid_shape}"
            )
        print(f"Initial grid state shape check PASSED: {grid_state.shape}")
        # --- END MODIFIED ---

        # Check shapes component (unchanged)
        if "shapes" not in s_test_dict:
            raise KeyError("State dict missing 'shapes'")
        shape_state = s_test_dict["shapes"]
        expected_shape_shape = (
            env_config.NUM_SHAPE_SLOTS,
            env_config.SHAPE_FEATURES_PER_SHAPE,
        )
        if (
            not isinstance(shape_state, np.ndarray)
            or shape_state.shape != expected_shape_shape
        ):
            raise ValueError(
                f"Initial shape state shape mismatch! Env:{shape_state.shape}, Cfg:{expected_shape_shape}"
            )
        print(f"Initial shape state shape check PASSED: {shape_state.shape}")

        valid_acts_init = envs[0].valid_actions()
        if valid_acts_init:
            _, _ = envs[0].step(valid_acts_init[0])
        else:
            print(
                "Warning: No valid actions available after initial reset for testing step()."
            )

        print(f"Successfully initialized {num_envs} environments.")
        return envs
    except Exception as e:
        print(f"FATAL ERROR during env init: {e}")
        traceback.print_exc()
        raise e


def initialize_agent_buffer(
    model_config: ModelConfig,
    dqn_config: DQNConfig,
    env_config: EnvConfig,
    buffer_config: BufferConfig,
) -> Tuple[DQNAgent, ReplayBufferBase]:
    print("Initializing Agent and Buffer...")
    # Pass EnvConfig instance to DQNAgent
    agent = DQNAgent(config=model_config, dqn_config=dqn_config, env_config=env_config)
    buffer = create_replay_buffer(config=buffer_config, dqn_config=dqn_config)
    print("Agent and Buffer initialized.")
    return agent, buffer


def initialize_stats_recorder(
    stats_config: StatsConfig,
    tb_config: TensorBoardConfig,
    config_dict: Dict[str, Any],
    agent: Optional[DQNAgent],
    env_config: EnvConfig,  # Expecting instance
) -> StatsRecorderBase:
    """Creates the TensorBoard recorder, logs graph (on CPU) and hparams."""
    print(f"Initializing Stats Recorder (TensorBoard)...")
    avg_window = stats_config.STATS_AVG_WINDOW
    console_log_freq = stats_config.CONSOLE_LOG_FREQ

    dummy_grid_cpu = None
    dummy_shapes_cpu = None
    model_for_graph_cpu = None

    if agent and agent.online_net:
        try:
            # --- MODIFIED: Create dummy grid with 2 channels ---
            expected_grid_shape = env_config.GRID_STATE_SHAPE  # Get shape (2, H, W)
            dummy_grid_np = np.zeros(expected_grid_shape, dtype=np.float32)
            # --- END MODIFIED ---
            dummy_shapes_np = np.zeros(
                (env_config.NUM_SHAPE_SLOTS, env_config.SHAPE_FEATURES_PER_SHAPE),
                dtype=np.float32,
            )
            dummy_grid_cpu = torch.tensor(dummy_grid_np, device="cpu").unsqueeze(0)
            dummy_shapes_cpu = torch.tensor(dummy_shapes_np, device="cpu").unsqueeze(0)

            if not hasattr(agent, "dqn_config"):
                raise AttributeError("DQNAgent missing 'dqn_config'")
            if not hasattr(agent.online_net, "config"):
                raise AttributeError("Agent's online_net missing 'config'")

            # Recreate model on CPU for graph logging
            model_for_graph_cpu = type(agent.online_net)(
                env_config=env_config,  # Pass instance
                action_dim=env_config.ACTION_DIM,  # Access property value
                model_config=agent.online_net.config,
                dqn_config=agent.dqn_config,
                dueling=agent.online_net.dueling,
                use_noisy=agent.online_net.use_noisy,
            ).to("cpu")

            model_for_graph_cpu.load_state_dict(agent.online_net.state_dict())
            model_for_graph_cpu.eval()
            print(
                "[Stats Init] Prepared model copy and dummy input on CPU for graph logging."
            )
        except AttributeError as ae:
            print(
                f"Warning: Attribute error preparing model/input for graph logging: {ae}. Check AgentNetwork init/attributes."
            )
            dummy_grid_cpu = None
            dummy_shapes_cpu = None
            model_for_graph_cpu = None
        except Exception as e:
            print(f"Warning: Failed to prepare model/input for graph logging: {e}")
            traceback.print_exc()
            dummy_grid_cpu = None
            dummy_shapes_cpu = None
            model_for_graph_cpu = None

    print(f"Using TensorBoard Logger (Log Dir: {tb_config.LOG_DIR})")
    try:
        dummy_input_tuple = (
            (dummy_grid_cpu, dummy_shapes_cpu)
            if dummy_grid_cpu is not None and dummy_shapes_cpu is not None
            else None
        )
        stats_recorder = TensorBoardStatsRecorder(
            log_dir=tb_config.LOG_DIR,
            hparam_dict=config_dict,
            model_for_graph=model_for_graph_cpu,
            dummy_input_for_graph=dummy_input_tuple,  # Pass tuple
            console_log_interval=console_log_freq,
            avg_window=avg_window,
            histogram_log_interval=tb_config.HISTOGRAM_LOG_FREQ,
            image_log_interval=tb_config.IMAGE_LOG_FREQ if tb_config.LOG_IMAGES else -1,
            shape_q_log_interval=(
                tb_config.SHAPE_Q_LOG_FREQ
                if tb_config.LOG_SHAPE_PLACEMENT_Q_VALUES
                else -1
            ),  # Pass new freq
        )
        print("Stats Recorder initialized.")
        return stats_recorder
    except Exception as e:
        print(f"FATAL: Error initializing TensorBoardStatsRecorder: {e}. Exiting.")
        traceback.print_exc()
        raise e


def initialize_trainer(
    envs: List[GameState],
    agent: DQNAgent,
    buffer: ReplayBufferBase,
    stats_recorder: StatsRecorderBase,
    env_config: EnvConfig,  # Expecting instance
    dqn_config: DQNConfig,
    train_config: TrainConfig,
    buffer_config: BufferConfig,
    model_config: ModelConfig,
) -> Trainer:
    print("Initializing Trainer...")
    trainer = Trainer(
        envs=envs,
        agent=agent,
        buffer=buffer,
        stats_recorder=stats_recorder,
        env_config=env_config,  # Pass instance
        dqn_config=dqn_config,
        train_config=train_config,
        buffer_config=buffer_config,
        model_config=model_config,
        model_save_path=MODEL_SAVE_PATH,
        buffer_save_path=BUFFER_SAVE_PATH,
        load_checkpoint_path=train_config.LOAD_CHECKPOINT_PATH,
        load_buffer_path=train_config.LOAD_BUFFER_PATH,
    )
    print("Trainer initialization finished.")
    return trainer


File: init/__init__.py
# File: init/__init__.py
from .rl_components import (
    initialize_envs,
    initialize_agent_buffer,
    initialize_stats_recorder,
    initialize_trainer,
)

__all__ = [
    "initialize_envs",
    "initialize_agent_buffer",
    "initialize_stats_recorder",
    "initialize_trainer",
]


File: ui/tooltips.py
# File: ui/tooltips.py
# (No significant changes needed, this file was already focused)
import pygame
from typing import Tuple, Dict, Optional
from config import VisConfig


class TooltipRenderer:
    """Handles rendering of tooltips when hovering over specific UI elements."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.font_tooltip = self._init_font()
        self.hovered_stat_key: Optional[str] = None
        self.stat_rects: Dict[str, pygame.Rect] = {}  # Rects to check for hover
        self.tooltip_texts: Dict[str, str] = {}  # Text corresponding to each rect key

    def _init_font(self):
        """Initializes the font used for tooltips."""
        try:
            # Smaller font for tooltips
            return pygame.font.SysFont(None, 18)
        except Exception as e:
            print(f"Warning: SysFont error for tooltip font: {e}. Using default.")
            return pygame.font.Font(None, 18)

    def check_hover(self, mouse_pos: Tuple[int, int]):
        """Checks if the mouse is hovering over any registered stat rect."""
        self.hovered_stat_key = None
        # Iterate in reverse order of drawing to prioritize top elements
        for key, rect in reversed(self.stat_rects.items()):
            # Ensure rect is valid before checking collision
            if (
                rect
                and rect.width > 0
                and rect.height > 0
                and rect.collidepoint(mouse_pos)
            ):
                self.hovered_stat_key = key
                return  # Found one, stop checking

    def render_tooltip(self):
        """Renders the tooltip if a stat element is being hovered over. Does not flip display."""
        if not self.hovered_stat_key or self.hovered_stat_key not in self.tooltip_texts:
            return  # No active hover or no text defined for this key

        tooltip_text = self.tooltip_texts[self.hovered_stat_key]
        mouse_pos = pygame.mouse.get_pos()

        # --- Text Wrapping Logic ---
        lines = []
        max_width = 300  # Max tooltip width in pixels
        words = tooltip_text.split(" ")
        current_line = ""
        for word in words:
            test_line = f"{current_line} {word}" if current_line else word
            try:
                test_surf = self.font_tooltip.render(test_line, True, VisConfig.BLACK)
                if test_surf.get_width() <= max_width:
                    current_line = test_line
                else:
                    lines.append(current_line)
                    current_line = word
            except Exception as e:
                print(f"Warning: Font render error during tooltip wrap: {e}")
                lines.append(current_line)  # Add what we had
                current_line = word  # Start new line
        lines.append(current_line)  # Add the last line

        # --- Render Wrapped Text ---
        line_surfs = []
        total_height = 0
        max_line_width = 0
        try:
            for line in lines:
                if not line:
                    continue  # Skip empty lines
                surf = self.font_tooltip.render(line, True, VisConfig.BLACK)
                line_surfs.append(surf)
                total_height += surf.get_height()
                max_line_width = max(max_line_width, surf.get_width())
        except Exception as e:
            print(f"Warning: Font render error creating tooltip surfaces: {e}")
            return  # Cannot render tooltip

        if not line_surfs:
            return  # No valid lines to render

        # --- Calculate Tooltip Rect and Draw ---
        padding = 5
        tooltip_rect = pygame.Rect(
            mouse_pos[0] + 15,  # Offset from cursor x
            mouse_pos[1] + 10,  # Offset from cursor y
            max_line_width + padding * 2,
            total_height + padding * 2,
        )

        # Clamp tooltip rect to stay within screen bounds
        tooltip_rect.clamp_ip(self.screen.get_rect())

        try:
            # Draw background and border
            pygame.draw.rect(
                self.screen, VisConfig.YELLOW, tooltip_rect, border_radius=3
            )
            pygame.draw.rect(
                self.screen, VisConfig.BLACK, tooltip_rect, 1, border_radius=3
            )

            # Draw text lines onto the screen
            current_y = tooltip_rect.y + padding
            for surf in line_surfs:
                self.screen.blit(surf, (tooltip_rect.x + padding, current_y))
                current_y += surf.get_height()
        except Exception as e:
            print(f"Warning: Error drawing tooltip background/text: {e}")

    def update_rects_and_texts(
        self, rects: Dict[str, pygame.Rect], texts: Dict[str, str]
    ):
        """Updates the dictionaries used for hover detection and text lookup. Called by UIRenderer."""
        self.stat_rects = rects
        self.tooltip_texts = texts


File: ui/demo_renderer.py
# File: ui/demo_renderer.py
import pygame
import math
import traceback
from typing import Optional, Tuple

from config import VisConfig, EnvConfig, DemoConfig
from environment.game_state import GameState
from .panels.game_area import GameAreaRenderer  # Need this for grid rendering


class DemoRenderer:
    """Handles rendering specifically for the interactive Demo Mode."""

    def __init__(
        self,
        screen: pygame.Surface,
        vis_config: VisConfig,
        demo_config: DemoConfig,
        game_area_renderer: GameAreaRenderer,
    ):
        self.screen = screen
        self.vis_config = vis_config
        self.demo_config = demo_config
        self.game_area_renderer = (
            game_area_renderer  # Use existing grid/shape render logic
        )
        self._init_demo_fonts()

    def _init_demo_fonts(self):
        """Initialize fonts specific to demo mode."""
        try:
            self.demo_hud_font = pygame.font.SysFont(
                None, self.demo_config.HUD_FONT_SIZE
            )
            self.demo_help_font = pygame.font.SysFont(
                None, self.demo_config.HELP_FONT_SIZE
            )
            # Ensure the game_area_renderer fonts are also loaded if needed here
            if not hasattr(
                self.game_area_renderer, "fonts"
            ) or not self.game_area_renderer.fonts.get("ui"):
                self.game_area_renderer._init_fonts()  # Ensure base fonts are loaded
        except Exception as e:
            print(f"Warning: SysFont error for demo fonts: {e}. Using default.")
            self.demo_hud_font = pygame.font.Font(None, self.demo_config.HUD_FONT_SIZE)
            self.demo_help_font = pygame.font.Font(
                None, self.demo_config.HELP_FONT_SIZE
            )

    def render(self, demo_env: GameState, env_config: EnvConfig):
        """Renders the single-player interactive demo mode. Does NOT flip display."""
        if not demo_env:
            print("Error: DemoRenderer called with demo_env=None")
            # Optionally render an error message here
            return

        self.screen.fill(self.demo_config.BACKGROUND_COLOR)
        sw, sh = self.screen.get_size()
        padding = 30
        hud_height = 60
        help_height = 30
        max_game_h = sh - 2 * padding - hud_height - help_height
        max_game_w = sw - 2 * padding

        if max_game_h <= 0 or max_game_w <= 0:
            self._render_too_small_message(
                "Demo Area Too Small", self.screen.get_rect()
            )
            return

        aspect_ratio = (env_config.COLS * 0.75 + 0.25) / max(1, env_config.ROWS)
        game_w = max_game_w
        game_h = game_w / aspect_ratio if aspect_ratio > 0 else max_game_h
        if game_h > max_game_h:
            game_h = max_game_h
            game_w = game_h * aspect_ratio
        game_w = math.floor(min(game_w, max_game_w))
        game_h = math.floor(min(game_h, max_game_h))
        game_x = (sw - game_w) // 2
        game_y = padding
        game_rect = pygame.Rect(game_x, game_y, game_w, game_h)
        clipped_game_rect = game_rect.clip(self.screen.get_rect())

        # 1. Render Game Area (Grid + Preview)
        if clipped_game_rect.width > 10 and clipped_game_rect.height > 10:
            try:
                game_surf = self.screen.subsurface(clipped_game_rect)
                # Render the main grid using GameAreaRenderer's method
                self.game_area_renderer._render_single_env_grid(
                    game_surf, demo_env, env_config
                )

                # Render Placement Preview on top of the grid
                preview_tri_cell_w, preview_tri_cell_h = (
                    self._calculate_demo_triangle_size(
                        clipped_game_rect.width, clipped_game_rect.height, env_config
                    )
                )
                if preview_tri_cell_w > 0 and preview_tri_cell_h > 0:
                    # Calculate offset needed for _render_placement_preview based on scaling
                    padding_grid = self.vis_config.ENV_GRID_PADDING
                    drawable_w = max(0, clipped_game_rect.width - 2 * padding_grid)
                    drawable_h = max(0, clipped_game_rect.height - 2 * padding_grid)
                    grid_cols_eff_width = env_config.COLS * 0.75 + 0.25
                    scale_w = drawable_w / max(1, grid_cols_eff_width)
                    scale_h = drawable_h / max(1, env_config.ROWS)
                    final_scale = min(scale_w, scale_h)
                    final_grid_pixel_w = max(1, grid_cols_eff_width * final_scale)
                    final_grid_pixel_h = max(1, env_config.ROWS * final_scale)
                    grid_ox = padding_grid + (drawable_w - final_grid_pixel_w) / 2
                    grid_oy = padding_grid + (drawable_h - final_grid_pixel_h) / 2

                    self._render_placement_preview(
                        game_surf,  # Draw on the game surface
                        demo_env,
                        preview_tri_cell_w,
                        preview_tri_cell_h,
                        grid_ox,
                        grid_oy,
                    )

            except ValueError as e:
                print(f"Error subsurface demo game ({clipped_game_rect}): {e}")
                pygame.draw.rect(self.screen, VisConfig.RED, clipped_game_rect, 1)
            except Exception as render_e:
                print(f"Error rendering demo game area: {render_e}")
                traceback.print_exc()
                pygame.draw.rect(self.screen, VisConfig.RED, clipped_game_rect, 1)
        else:
            self._render_too_small_message("Demo Area Too Small", clipped_game_rect)

        # 2. Render Shape Previews (Right Side)
        preview_area_w = min(150, sw - clipped_game_rect.right - padding // 2)
        if preview_area_w > 20:
            preview_area_rect = pygame.Rect(
                clipped_game_rect.right + padding // 2,
                clipped_game_rect.top,
                preview_area_w,
                clipped_game_rect.height,
            )
            clipped_preview_area_rect = preview_area_rect.clip(self.screen.get_rect())
            if (
                clipped_preview_area_rect.width > 0
                and clipped_preview_area_rect.height > 0
            ):
                try:
                    preview_area_surf = self.screen.subsurface(
                        clipped_preview_area_rect
                    )
                    self._render_demo_shape_previews(preview_area_surf, demo_env)
                except ValueError as e:
                    print(f"Error subsurface demo shape preview area: {e}")
                    pygame.draw.rect(
                        self.screen, VisConfig.RED, clipped_preview_area_rect, 1
                    )
                except Exception as e:
                    print(f"Error rendering demo shape previews: {e}")
                    traceback.print_exc()

        # 3. Render HUD (Below Game Area)
        hud_y = game_rect.bottom + 10
        score_text = f"Score: {demo_env.game_score} | Lines: {demo_env.lines_cleared_this_episode}"
        try:
            score_surf = self.demo_hud_font.render(score_text, True, VisConfig.WHITE)
            score_rect = score_surf.get_rect(midtop=(sw // 2, hud_y))
            self.screen.blit(score_surf, score_rect)
        except Exception as e:
            print(f"HUD render error: {e}")

        # 4. Render Help Text (Bottom)
        try:
            help_surf = self.demo_help_font.render(
                self.demo_config.HELP_TEXT, True, VisConfig.LIGHTG
            )
            help_rect = help_surf.get_rect(centerx=sw // 2, bottom=sh - 10)
            self.screen.blit(help_surf, help_rect)
        except Exception as e:
            print(f"Help render error: {e}")

    def _calculate_demo_triangle_size(
        self, surf_w, surf_h, env_config: EnvConfig
    ) -> Tuple[int, int]:
        """Calculates the cell width/height for triangles within a given surface."""
        padding = self.vis_config.ENV_GRID_PADDING
        drawable_w = max(1, surf_w - 2 * padding)
        drawable_h = max(1, surf_h - 2 * padding)
        grid_rows = env_config.ROWS
        grid_cols_eff_width = env_config.COLS * 0.75 + 0.25
        if grid_rows <= 0 or grid_cols_eff_width <= 0:
            return 0, 0
        scale_w = drawable_w / grid_cols_eff_width
        scale_h = drawable_h / grid_rows
        final_scale = min(scale_w, scale_h)
        if final_scale <= 0:
            return 0, 0
        # Use integer scaling for simplicity in drawing
        tri_cell_size = max(1, int(final_scale))
        return tri_cell_size, tri_cell_size

    def _render_placement_preview(
        self,
        surf: pygame.Surface,
        env: GameState,
        cell_w: int,
        cell_h: int,
        offset_x: float,
        offset_y: float,
    ):
        """Renders the preview of where the selected shape will be placed."""
        if cell_w <= 0 or cell_h <= 0:
            return
        shp, rr, cc = env.get_current_selection_info()
        if shp is None:
            return

        is_valid = env.grid.can_place(shp, rr, cc)
        preview_color = (
            self.demo_config.PREVIEW_COLOR
            if is_valid
            else self.demo_config.INVALID_PREVIEW_COLOR
        )

        # Create a temporary surface with alpha for drawing the preview
        temp_surface = pygame.Surface(surf.get_size(), pygame.SRCALPHA)
        temp_surface.fill((0, 0, 0, 0))  # Transparent

        for dr, dc, up in shp.triangles:
            nr, nc = rr + dr, cc + dc
            # Check if the target triangle exists and is not a death cell
            if (
                env.grid.valid(nr, nc)
                and 0 <= nr < len(env.grid.triangles)
                and 0 <= nc < len(env.grid.triangles[nr])
                and not env.grid.triangles[nr][nc].is_death
            ):
                # Use the actual triangle from the grid to get correct orientation for points
                temp_tri = env.grid.triangles[nr][nc]
                try:
                    pts = temp_tri.get_points(
                        ox=offset_x, oy=offset_y, cw=cell_w, ch=cell_h
                    )
                    pygame.draw.polygon(temp_surface, preview_color, pts)
                except Exception as e:
                    # Reduce log spam for minor render errors during preview
                    # print(f"Error rendering preview tri ({nr},{nc}): {e}")
                    pass

        # Blit the transparent preview surface onto the main game surface
        surf.blit(temp_surface, (0, 0))

    def _render_demo_shape_previews(self, surf: pygame.Surface, env: GameState):
        """Renders the small previews of available shapes in the side area."""
        surf.fill((25, 25, 25))  # Background for shape preview area
        all_slots = env.shapes
        selected_shape_obj = (
            all_slots[env.demo_selected_shape_idx]
            if 0 <= env.demo_selected_shape_idx < len(all_slots)
            else None
        )
        num_slots = env.env_config.NUM_SHAPE_SLOTS
        surf_w, surf_h = surf.get_size()
        preview_padding = 5

        if num_slots <= 0:
            return

        # Calculate layout: Vertical list of previews
        preview_h = max(20, (surf_h - (num_slots + 1) * preview_padding) / num_slots)
        preview_w = max(20, surf_w - 2 * preview_padding)
        current_preview_y = preview_padding

        for i in range(num_slots):
            shp = all_slots[i] if i < len(all_slots) else None
            preview_rect = pygame.Rect(
                preview_padding, current_preview_y, preview_w, preview_h
            )
            clipped_preview_rect = preview_rect.clip(surf.get_rect())

            if clipped_preview_rect.width <= 0 or clipped_preview_rect.height <= 0:
                current_preview_y += preview_h + preview_padding
                continue

            # Determine background and border based on selection
            bg_color = (40, 40, 40)
            border_color = VisConfig.GRAY
            border_width = 1
            if shp is not None and shp == selected_shape_obj:
                border_color = self.demo_config.SELECTED_SHAPE_HIGHLIGHT_COLOR
                border_width = 2

            pygame.draw.rect(surf, bg_color, clipped_preview_rect, border_radius=3)
            pygame.draw.rect(
                surf, border_color, clipped_preview_rect, border_width, border_radius=3
            )

            # Render the shape itself inside the preview box
            if shp is not None:
                try:
                    # Create a subsurface for the shape rendering area inside the box
                    inner_padding = 2
                    shape_render_area_rect = pygame.Rect(
                        inner_padding,
                        inner_padding,
                        clipped_preview_rect.width - 2 * inner_padding,
                        clipped_preview_rect.height - 2 * inner_padding,
                    )
                    if (
                        shape_render_area_rect.width > 0
                        and shape_render_area_rect.height > 0
                    ):
                        # Use subsurface from the main surf, not clipped_preview_rect surface
                        shape_sub_surf = surf.subsurface(
                            preview_rect.left + shape_render_area_rect.left,
                            preview_rect.top + shape_render_area_rect.top,
                            shape_render_area_rect.width,
                            shape_render_area_rect.height,
                        )
                        # Calculate cell size to fit the shape
                        min_r, min_c, max_r, max_c = shp.bbox()
                        shape_h = max(1, max_r - min_r + 1)
                        shape_w_eff = max(1, (max_c - min_c + 1) * 0.75 + 0.25)
                        scale_h = shape_render_surf.get_height() / shape_h
                        scale_w = shape_render_surf.get_width() / shape_w_eff
                        cell_size = max(1, min(scale_h, scale_w))
                        # Render the shape using game_area_renderer's helper
                        self.game_area_renderer._render_single_shape(
                            shape_render_surf, shp, int(cell_size)
                        )
                except ValueError as sub_err:
                    print(f"Error subsurface shape preview {i}: {sub_err}")
                    pygame.draw.rect(surf, VisConfig.RED, clipped_preview_rect, 1)
                except Exception as e:
                    print(f"Error rendering demo shape preview {i}: {e}")
                    pygame.draw.rect(surf, VisConfig.RED, clipped_preview_rect, 1)

            current_preview_y += preview_h + preview_padding

    def _render_too_small_message(self, text: str, area_rect: pygame.Rect):
        """Helper to render 'Too Small' messages."""
        try:
            # Use a font known to exist in game_area_renderer or init one here
            font = self.game_area_renderer.fonts.get("ui") or pygame.font.SysFont(
                None, 24
            )
            err_surf = font.render(text, True, VisConfig.GRAY)
            # Center the message within the specified area_rect relative to the screen
            target_rect = err_surf.get_rect(center=area_rect.center)
            self.screen.blit(err_surf, target_rect)
        except Exception as e:
            print(f"Error rendering 'too small' message: {e}")


File: ui/renderer.py
# File: ui/renderer.py
import pygame
import time
import traceback
from typing import List, Dict, Any, Optional, Tuple, Deque

from config import VisConfig, EnvConfig, TensorBoardConfig, DemoConfig
from environment.game_state import GameState
from .panels import LeftPanelRenderer, GameAreaRenderer
from .overlays import OverlayRenderer
from .tooltips import TooltipRenderer
from .plotter import Plotter
from .demo_renderer import DemoRenderer  # Import the new demo renderer


class UIRenderer:
    """Orchestrates rendering of all UI components."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.plotter = Plotter()
        self.left_panel = LeftPanelRenderer(screen, vis_config, self.plotter)
        self.game_area = GameAreaRenderer(screen, vis_config)
        self.overlays = OverlayRenderer(screen, vis_config)
        self.tooltips = TooltipRenderer(screen, vis_config)
        self.demo_config = DemoConfig()  # Need demo config for demo renderer
        # --- NEW: Instantiate DemoRenderer ---
        self.demo_renderer = DemoRenderer(
            screen, vis_config, self.demo_config, self.game_area
        )
        # --- END NEW ---
        self.last_plot_update_time = 0

    def check_hover(self, mouse_pos: Tuple[int, int], app_state: str):
        """Passes hover check to the tooltip renderer."""
        if app_state == "MainMenu":
            self.tooltips.update_rects_and_texts(
                self.left_panel.get_stat_rects(), self.left_panel.get_tooltip_texts()
            )
            self.tooltips.check_hover(mouse_pos)
        else:
            # Disable tooltips in other states for now
            self.tooltips.hovered_stat_key = None
            self.tooltips.stat_rects.clear()

    def force_redraw(self):
        """Forces components like the plotter to redraw on the next frame."""
        self.plotter.last_plot_update_time = 0

    def render_all(
        self,
        app_state: str,
        is_training: bool,
        status: str,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        envs: List[GameState],
        num_envs: int,
        env_config: EnvConfig,
        cleanup_confirmation_active: bool,
        cleanup_message: str,
        last_cleanup_message_time: float,
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
        demo_env: Optional[GameState] = None,
    ):
        """Renders UI based on the application state."""
        try:
            # 1. Render main content based on state (WITHOUT flipping)
            if app_state == "MainMenu":
                self._render_main_menu(
                    is_training,
                    status,
                    stats_summary,
                    buffer_capacity,
                    envs,
                    num_envs,
                    env_config,
                    cleanup_message,
                    last_cleanup_message_time,
                    tensorboard_log_dir,
                    plot_data,
                )
            elif app_state == "Playing":
                # --- MODIFIED: Call DemoRenderer ---
                if demo_env:
                    self.demo_renderer.render(demo_env, env_config)
                else:
                    # Render error if demo_env is missing
                    print("Error: Attempting to render demo mode without demo_env.")
                    self._render_simple_message("Demo Env Error!", VisConfig.RED)
                # --- END MODIFIED ---
            elif app_state == "Initializing":
                self._render_initializing_screen(status)
            elif app_state == "Error":
                self._render_error_screen(status)

            # 2. Render Overlays ON TOP if needed
            # Cleanup confirmation overlay takes precedence
            if cleanup_confirmation_active and app_state != "Error":
                self.overlays.render_cleanup_confirmation()
            # Render transient status message if cleanup overlay is NOT active
            elif not cleanup_confirmation_active:
                self.overlays.render_status_message(
                    cleanup_message, last_cleanup_message_time
                )

            # 3. Render Tooltip (only in MainMenu and if cleanup not active)
            if app_state == "MainMenu" and not cleanup_confirmation_active:
                # Tooltip rects/texts are updated during check_hover or implicitly by left_panel.render
                # We just need to call render_tooltip here.
                self.tooltips.render_tooltip()

            # 4. Final Flip
            pygame.display.flip()

        except pygame.error as e:
            # Handle specific pygame errors if needed
            print(f"Pygame rendering error in render_all: {e}")
            traceback.print_exc()
        except Exception as e:
            print(f"Unexpected critical rendering error in render_all: {e}")
            traceback.print_exc()
            # Attempt to render a basic error screen on critical failure
            try:
                self._render_simple_message("Critical Render Error!", VisConfig.RED)
                pygame.display.flip()
            except Exception:
                pass  # Ignore errors during error rendering

    def _render_main_menu(
        self,
        is_training: bool,
        status: str,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        envs: List[GameState],
        num_envs: int,
        env_config: EnvConfig,
        cleanup_message: str,  # No longer need cleanup_confirmation_active here
        last_cleanup_message_time: float,
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
    ):
        """Renders the main training dashboard view. Does NOT flip display."""
        self.screen.fill(VisConfig.BLACK)  # Clear screen

        # Render Main Panels (Left Panel updates tooltip rects internally now)
        self.left_panel.render(
            is_training,
            status,
            stats_summary,
            buffer_capacity,
            tensorboard_log_dir,
            plot_data,
            app_state="MainMenu",  # Pass state for conditional rendering
        )
        self.game_area.render(envs, num_envs, env_config)

        # Status message and tooltips are handled by render_all after this

    def _render_initializing_screen(
        self, status_message: str = "Initializing RL Components..."
    ):
        """Renders the initializing screen with a status message."""
        self._render_simple_message(status_message, VisConfig.WHITE)

    def _render_error_screen(self, status_message: str):
        """Renders the error screen. Does NOT flip display."""
        try:
            self.screen.fill((40, 0, 0))  # Dark red background
            font_title = pygame.font.SysFont(None, 70)
            font_msg = pygame.font.SysFont(None, 30)

            title_surf = font_title.render("APPLICATION ERROR", True, VisConfig.RED)
            title_rect = title_surf.get_rect(
                center=(self.screen.get_width() // 2, self.screen.get_height() // 3)
            )

            msg_surf = font_msg.render(
                f"Status: {status_message}", True, VisConfig.YELLOW
            )
            msg_rect = msg_surf.get_rect(
                center=(self.screen.get_width() // 2, title_rect.bottom + 30)
            )

            exit_surf = font_msg.render(
                "Press ESC or close window to exit.", True, VisConfig.WHITE
            )
            exit_rect = exit_surf.get_rect(
                center=(self.screen.get_width() // 2, self.screen.get_height() * 0.8)
            )

            self.screen.blit(title_surf, title_rect)
            self.screen.blit(msg_surf, msg_rect)
            self.screen.blit(exit_surf, exit_rect)

        except Exception as e:
            print(f"Error rendering error screen: {e}")
            # Fallback to simple message
            self._render_simple_message(f"Error State: {status_message}", VisConfig.RED)

    def _render_simple_message(self, message: str, color: Tuple[int, int, int]):
        """Renders a simple centered text message."""
        try:
            self.screen.fill(VisConfig.BLACK)
            font = pygame.font.SysFont(None, 50)
            text_surf = font.render(message, True, color)
            text_rect = text_surf.get_rect(center=self.screen.get_rect().center)
            self.screen.blit(text_surf, text_rect)
        except Exception as e:
            print(f"Error rendering simple message '{message}': {e}")


File: ui/__init__.py
from .renderer import UIRenderer
from .input_handler import InputHandler

__all__ = ["UIRenderer", "InputHandler"]


File: ui/plotter.py
# File: ui/plotter.py
# (No significant changes needed, this file was already focused)
import pygame
import numpy as np
from typing import Dict, Optional, Deque, List, Union, Tuple
from collections import deque
import matplotlib
import time
import warnings
from io import BytesIO

# Ensure Matplotlib uses Agg backend for non-interactive plotting
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from config import VisConfig, BufferConfig, StatsConfig, DQNConfig

# --- Matplotlib Style Configuration ---
try:
    plt.style.use("dark_background")
    plt.rcParams.update(
        {
            "font.size": 9,
            "axes.labelsize": 9,
            "axes.titlesize": 10,
            "xtick.labelsize": 8,
            "ytick.labelsize": 8,
            "legend.fontsize": 7,
            "figure.facecolor": "#262626",
            "axes.facecolor": "#303030",
            "axes.edgecolor": "#707070",
            "axes.labelcolor": "#D0D0D0",
            "xtick.color": "#C0C0C0",
            "ytick.color": "#C0C0C0",
            "grid.color": "#505050",
            "grid.linestyle": "--",
            "grid.alpha": 0.5,
        }
    )
except Exception as e:
    print(f"Warning: Failed to set Matplotlib style: {e}")
# --- End Style Configuration ---


def normalize_color_for_matplotlib(
    color_tuple_0_255: Tuple[int, int, int],
) -> Tuple[float, float, float]:
    """Converts a 0-255 RGB tuple to a 0.0-1.0 RGB tuple for Matplotlib."""
    if isinstance(color_tuple_0_255, tuple) and len(color_tuple_0_255) == 3:
        return tuple(c / 255.0 for c in color_tuple_0_255)
    else:
        print(f"Warning: Invalid color tuple {color_tuple_0_255}, returning black.")
        return (0.0, 0.0, 0.0)  # Return black on error


class Plotter:
    """Handles creating Pygame surfaces from Matplotlib plots of training data."""

    def __init__(self):
        self.plot_surface: Optional[pygame.Surface] = None  # Cached plot surface
        self.last_plot_update_time: float = 0.0
        self.plot_update_interval: float = 1.0  # Seconds between plot redraws
        self.rolling_window_size = (
            StatsConfig.STATS_AVG_WINDOW
        )  # Window for rolling avg line
        self.default_line_width = 1.5
        self.avg_line_width = 2.0
        self.avg_line_alpha = 0.7

    def create_plot_surface(
        self, plot_data: Dict[str, Deque], target_width: int, target_height: int
    ) -> Optional[pygame.Surface]:
        """Creates a Pygame surface containing Matplotlib plots (3x3 layout)."""
        if target_width <= 10 or target_height <= 10 or not plot_data:
            return None  # Cannot render if area is too small or no data

        # Extract data lists from the dictionary, handle missing keys gracefully
        data_lists = {
            key: list(plot_data.get(key, deque()))
            for key in [
                "episode_scores",
                "game_scores",
                "losses",
                "episode_lengths",
                "sps_values",
                "best_game_score_history",
                "lr_values",
                "buffer_sizes",
                "beta_values",
            ]
        }

        # Check if there's *any* data to plot
        if not any(data_lists.values()):
            return None

        fig = None
        try:
            # Ignore UserWarnings from Matplotlib (e.g., about tight_layout)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", category=UserWarning)

                dpi = 85  # Adjust DPI based on desired resolution vs performance
                fig_width_in = target_width / dpi
                fig_height_in = target_height / dpi

                fig, axes = plt.subplots(
                    3, 3, figsize=(fig_width_in, fig_height_in), dpi=dpi, sharex=False
                )
                # Adjust spacing to prevent labels/titles overlapping
                fig.subplots_adjust(
                    hspace=0.65,
                    wspace=0.4,
                    left=0.12,
                    right=0.97,
                    bottom=0.15,
                    top=0.92,
                )
                axes_flat = axes.flatten()

                # Define colors using VisConfig and normalize for Matplotlib
                colors = {
                    "rl_score": normalize_color_for_matplotlib(
                        VisConfig.GOOGLE_COLORS[0]
                    ),
                    "game_score": normalize_color_for_matplotlib(
                        VisConfig.GOOGLE_COLORS[1]
                    ),
                    "loss": normalize_color_for_matplotlib(VisConfig.GOOGLE_COLORS[3]),
                    "len": normalize_color_for_matplotlib(VisConfig.BLUE),
                    "sps": normalize_color_for_matplotlib(VisConfig.LIGHTG),
                    "best_game": normalize_color_for_matplotlib(
                        (255, 165, 0)
                    ),  # Orange
                    "lr": normalize_color_for_matplotlib((255, 0, 255)),  # Magenta
                    "buffer": normalize_color_for_matplotlib(VisConfig.RED),
                    "beta": normalize_color_for_matplotlib(
                        (100, 100, 255)
                    ),  # Light Blue
                    "avg": normalize_color_for_matplotlib(VisConfig.YELLOW),
                    "placeholder": normalize_color_for_matplotlib(VisConfig.GRAY),
                }

                # X-axis label reflects the visible window size
                plot_window_label = f"Plot Window (~{self.rolling_window_size} points)"

                # --- Plot each metric ---
                self._plot_data_list(
                    axes_flat[0],
                    data_lists["episode_scores"],
                    "RL Score",
                    colors["rl_score"],
                    colors["avg"],
                    xlabel=plot_window_label,
                )
                self._plot_data_list(
                    axes_flat[1],
                    data_lists["game_scores"],
                    "Game Score",
                    colors["game_score"],
                    colors["avg"],
                    xlabel=plot_window_label,
                )
                self._plot_data_list(
                    axes_flat[2],
                    data_lists["losses"],
                    "Loss",
                    colors["loss"],
                    colors["avg"],
                    xlabel=plot_window_label,
                    show_placeholder=True,
                    placeholder_text="Loss data after Learn Start",
                )
                self._plot_data_list(
                    axes_flat[3],
                    data_lists["episode_lengths"],
                    "Ep Length",
                    colors["len"],
                    colors["avg"],
                    xlabel=plot_window_label,
                )
                self._plot_data_list(
                    axes_flat[4],
                    data_lists["best_game_score_history"],
                    "Best Game Score",
                    colors["best_game"],
                    None,
                    xlabel=plot_window_label,
                )
                self._plot_data_list(
                    axes_flat[5],
                    data_lists["sps_values"],
                    "Steps/Sec",
                    colors["sps"],
                    colors["avg"],
                    xlabel=plot_window_label,
                )
                self._plot_data_list(
                    axes_flat[6],
                    data_lists["lr_values"],
                    "Learning Rate",
                    colors["lr"],
                    None,
                    xlabel=plot_window_label,
                    y_log_scale=True,
                )

                # Convert buffer size to fill percentage
                buffer_fill_percent = [
                    (s / max(1, BufferConfig.REPLAY_BUFFER_SIZE) * 100)
                    for s in data_lists["buffer_sizes"]
                ]
                self._plot_data_list(
                    axes_flat[7],
                    buffer_fill_percent,
                    "Buffer Fill %",
                    colors["buffer"],
                    colors["avg"],
                    xlabel=plot_window_label,
                )

                # Plot PER Beta only if PER is enabled
                if BufferConfig.USE_PER:
                    self._plot_data_list(
                        axes_flat[8],
                        data_lists["beta_values"],
                        "PER Beta",
                        colors["beta"],
                        colors["avg"],
                        xlabel=plot_window_label,
                    )
                else:
                    # Show placeholder if PER is disabled
                    axes_flat[8].text(
                        0.5,
                        0.5,
                        "PER Disabled",
                        ha="center",
                        va="center",
                        transform=axes_flat[8].transAxes,
                        fontsize=8,
                        color=colors["placeholder"],
                    )
                    axes_flat[8].set_yticks([])
                    axes_flat[8].set_xticks([])

                # General plot styling
                for ax in axes_flat:
                    ax.tick_params(
                        axis="x", rotation=0
                    )  # Ensure x-axis labels are horizontal

                # --- Convert Matplotlib plot to Pygame surface ---
                buf = BytesIO()
                fig.savefig(
                    buf,
                    format="png",
                    transparent=False,
                    facecolor=plt.rcParams["figure.facecolor"],
                )
                buf.seek(0)
                plot_img_surface = pygame.image.load(buf).convert()
                buf.close()

                # Scale surface smoothly if generated size doesn't match target (can happen with DPI/figsize)
                if (
                    plot_img_surface.get_size() != (target_width, target_height)
                    and target_width > 0
                    and target_height > 0
                ):
                    plot_img_surface = pygame.transform.smoothscale(
                        plot_img_surface, (target_width, target_height)
                    )

                return plot_img_surface

        except Exception as e:
            print(f"Error creating plot surface: {e}")
            import traceback

            traceback.print_exc()
            return None
        finally:
            # Ensure Matplotlib figure is closed to free memory
            if fig is not None:
                plt.close(fig)

    def _plot_data_list(
        self,
        ax,
        data: List[Union[float, int]],
        label: str,
        color,
        avg_color: Optional[Tuple[float, float, float]],
        xlabel: Optional[str] = None,
        show_placeholder: bool = True,
        placeholder_text: Optional[str] = None,
        y_log_scale: bool = False,
    ):
        """Helper function to plot a single list of data onto a Matplotlib axis."""
        n_points = len(data)
        latest_val_str = ""
        window_size = self.rolling_window_size  # Window for rolling average

        # Calculate latest value/average to display in title
        if data:
            current_val = data[-1]
            if n_points >= window_size and avg_color is not None:
                try:  # Handle potential errors if data contains non-numerics briefly
                    latest_avg = np.mean(data[-window_size:])
                    latest_val_str = f" (Now: {current_val:.3g}, Avg: {latest_avg:.3g})"
                except Exception:
                    latest_val_str = f" (Now: {current_val:.3g})"
            else:
                latest_val_str = f" (Now: {current_val:.3g})"
        ax.set_title(
            f"{label}{latest_val_str}", fontsize=plt.rcParams["axes.titlesize"]
        )

        # Handle empty data case
        placeholder_text_color = normalize_color_for_matplotlib(VisConfig.GRAY)
        if n_points == 0:
            if show_placeholder:
                p_text = (
                    placeholder_text if placeholder_text else f"{label}\n(No data yet)"
                )
                ax.text(
                    0.5,
                    0.5,
                    p_text,
                    ha="center",
                    va="center",
                    transform=ax.transAxes,
                    fontsize=8,
                    color=placeholder_text_color,
                )
            ax.set_yticks([])
            ax.set_xticks([])
            return

        try:
            # Plot raw data points
            x_coords = np.arange(n_points)
            ax.plot(
                x_coords,
                data,
                color=color,
                linewidth=self.default_line_width,
                label=f"{label} (Raw)",
            )

            # Plot rolling average if applicable
            if avg_color is not None and n_points >= window_size:
                # Use convolution for efficient rolling average calculation
                weights = np.ones(window_size) / window_size
                rolling_avg = np.convolve(data, weights, mode="valid")
                # Adjust x-coordinates for the rolling average plot
                avg_x_coords = np.arange(window_size - 1, n_points)
                ax.plot(
                    avg_x_coords,
                    rolling_avg,
                    color=avg_color,
                    linewidth=self.avg_line_width,
                    alpha=self.avg_line_alpha,
                    label=f"{label} (Avg {window_size})",
                )

            # --- Styling ---
            ax.tick_params(axis="both", which="major")
            if xlabel:
                ax.set_xlabel(xlabel)
            ax.grid(
                True,
                linestyle=plt.rcParams["grid.linestyle"],
                alpha=plt.rcParams["grid.alpha"],
            )

            # Set Y limits dynamically based on data range
            min_val = np.min(data)
            max_val = np.max(data)
            # Add padding to Y limits, ensuring some padding even if min=max
            padding = (
                (max_val - min_val) * 0.1
                if max_val > min_val
                else max(abs(max_val * 0.1), 1.0)
            )
            padding = max(padding, 1e-6)  # Ensure padding is non-zero
            ax.set_ylim(min_val - padding, max_val + padding)

            # Apply log scale if requested and data is positive
            if y_log_scale and min_val > 1e-9:  # Use small epsilon for > 0 check
                ax.set_yscale("log")
                # Adjust lower y-limit for log scale to avoid issues near zero
                ax.set_ylim(bottom=max(min_val * 0.9, 1e-9))
            else:
                ax.set_yscale("linear")  # Ensure linear scale otherwise

            # Set X limits dynamically
            if n_points > 1:
                ax.set_xlim(-0.02 * n_points, n_points - 1 + 0.02 * n_points)
            elif n_points == 1:
                ax.set_xlim(-0.5, 0.5)  # Handle single point case

            # Adjust number of x-axis ticks for readability
            if n_points > 10:
                ax.xaxis.set_major_locator(
                    plt.MaxNLocator(integer=True, nbins=4)
                )  # Limit tick count

        except Exception as plot_err:
            print(f"ERROR during _plot_data_list for '{label}': {plot_err}")
            error_text_color = normalize_color_for_matplotlib(VisConfig.RED)
            ax.text(
                0.5,
                0.5,
                f"Plotting Error\n({label})",
                ha="center",
                va="center",
                transform=ax.transAxes,
                fontsize=8,
                color=error_text_color,
            )
            ax.set_yticks([])
            ax.set_xticks([])

    def get_cached_or_updated_plot(
        self, plot_data: Dict[str, Deque], target_width: int, target_height: int
    ) -> Optional[pygame.Surface]:
        """Returns the cached plot surface or generates a new one if the interval has passed,
        size changed, or data just arrived."""
        current_time = time.time()
        has_data = any(plot_data.values())  # Check if any deque has data

        # Conditions for updating the plot
        needs_update_time = (
            current_time - self.last_plot_update_time > self.plot_update_interval
        )
        size_changed = self.plot_surface and self.plot_surface.get_size() != (
            target_width,
            target_height,
        )
        first_data_received = (
            has_data and self.plot_surface is None
        )  # Update when first data comes in
        can_create_plot = target_width > 50 and target_height > 50  # Basic size check

        if can_create_plot and (
            needs_update_time or size_changed or first_data_received
        ):
            new_plot_surface = self.create_plot_surface(
                plot_data, target_width, target_height
            )
            # Only update cache if plot generation was successful
            if new_plot_surface:
                self.plot_surface = new_plot_surface
                self.last_plot_update_time = current_time

        return self.plot_surface


File: ui/overlays.py
# File: ui/overlays.py
# (No significant changes needed, this file was already focused)
import pygame
import time
import traceback
from typing import Tuple
from config import VisConfig


class OverlayRenderer:
    """Renders overlay elements like confirmation dialogs and status messages."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.fonts = self._init_fonts()

    def _init_fonts(self):
        """Initializes fonts used for overlays."""
        fonts = {}
        try:
            fonts["overlay_title"] = pygame.font.SysFont(None, 36)
            fonts["overlay_text"] = pygame.font.SysFont(None, 24)
        except Exception as e:
            print(f"Warning: SysFont error for overlay fonts: {e}. Using default.")
            fonts["overlay_title"] = pygame.font.Font(None, 36)
            fonts["overlay_text"] = pygame.font.Font(None, 24)
        return fonts

    def render_cleanup_confirmation(self):
        """Renders the confirmation dialog for cleanup. Does not flip display."""
        try:
            current_width, current_height = self.screen.get_size()

            # Semi-transparent background overlay
            overlay_surface = pygame.Surface(
                (current_width, current_height), pygame.SRCALPHA
            )
            overlay_surface.fill((0, 0, 0, 200))  # Black with alpha
            self.screen.blit(overlay_surface, (0, 0))

            center_x, center_y = current_width // 2, current_height // 2

            # --- Render Text Lines ---
            if "overlay_title" not in self.fonts or "overlay_text" not in self.fonts:
                print("ERROR: Overlay fonts not loaded!")
                # Draw basic fallback text
                fallback_font = pygame.font.Font(None, 30)
                err_surf = fallback_font.render("CONFIRM CLEANUP?", True, VisConfig.RED)
                self.screen.blit(
                    err_surf, err_surf.get_rect(center=(center_x, center_y - 30))
                )
                yes_surf = fallback_font.render("YES", True, VisConfig.WHITE)
                no_surf = fallback_font.render("NO", True, VisConfig.WHITE)
                self.screen.blit(
                    yes_surf, yes_surf.get_rect(center=(center_x - 60, center_y + 50))
                )
                self.screen.blit(
                    no_surf, no_surf.get_rect(center=(center_x + 60, center_y + 50))
                )
                return  # Stop here if fonts failed

            # Use loaded fonts
            prompt_l1 = self.fonts["overlay_title"].render(
                "DELETE CURRENT RUN DATA?", True, VisConfig.RED
            )
            prompt_l2 = self.fonts["overlay_text"].render(
                "(Agent Checkpoint & Buffer State)", True, VisConfig.WHITE
            )
            prompt_l3 = self.fonts["overlay_text"].render(
                "This action cannot be undone!", True, VisConfig.YELLOW
            )

            # Position and blit text
            self.screen.blit(
                prompt_l1, prompt_l1.get_rect(center=(center_x, center_y - 60))
            )
            self.screen.blit(
                prompt_l2, prompt_l2.get_rect(center=(center_x, center_y - 25))
            )
            self.screen.blit(prompt_l3, prompt_l3.get_rect(center=(center_x, center_y)))

            # --- Render Buttons ---
            # Recalculate rects based on current screen size for responsiveness
            confirm_yes_rect = pygame.Rect(center_x - 110, center_y + 30, 100, 40)
            confirm_no_rect = pygame.Rect(center_x + 10, center_y + 30, 100, 40)

            pygame.draw.rect(
                self.screen, (0, 150, 0), confirm_yes_rect, border_radius=5
            )  # Green YES
            pygame.draw.rect(
                self.screen, (150, 0, 0), confirm_no_rect, border_radius=5
            )  # Red NO

            yes_text = self.fonts["overlay_text"].render("YES", True, VisConfig.WHITE)
            no_text = self.fonts["overlay_text"].render("NO", True, VisConfig.WHITE)

            self.screen.blit(
                yes_text, yes_text.get_rect(center=confirm_yes_rect.center)
            )
            self.screen.blit(no_text, no_text.get_rect(center=confirm_no_rect.center))

        except pygame.error as pg_err:
            print(f"Pygame Error in render_cleanup_confirmation: {pg_err}")
            traceback.print_exc()
        except Exception as e:
            print(f"Error in render_cleanup_confirmation: {e}")
            traceback.print_exc()

    def render_status_message(self, message: str, last_message_time: float) -> bool:
        """
        Renders a status message (e.g., after cleanup) temporarily at the bottom center.
        Does not flip display. Returns True if a message was rendered.
        """
        # Check if message exists and hasn't timed out
        if not message or (time.time() - last_message_time >= 5.0):
            return False

        try:
            if "overlay_text" not in self.fonts:  # Check if font loaded
                print(
                    "Warning: Cannot render status message, overlay_text font missing."
                )
                return False

            current_width, current_height = self.screen.get_size()
            lines = message.split("\n")
            max_width = 0
            msg_surfs = []

            # Render each line and find max width
            for line in lines:
                msg_surf = self.fonts["overlay_text"].render(
                    line,
                    True,
                    VisConfig.YELLOW,
                    VisConfig.BLACK,  # Yellow text on black bg
                )
                msg_surfs.append(msg_surf)
                max_width = max(max_width, msg_surf.get_width())

            if not msg_surfs:
                return False  # No lines to render

            # Calculate background size and position
            total_height = (
                sum(s.get_height() for s in msg_surfs) + max(0, len(lines) - 1) * 2
            )
            padding = 5
            bg_rect = pygame.Rect(
                0, 0, max_width + padding * 2, total_height + padding * 2
            )
            bg_rect.midbottom = (
                current_width // 2,
                current_height - 10,
            )  # Position at bottom center

            # Draw background and border
            pygame.draw.rect(self.screen, VisConfig.BLACK, bg_rect, border_radius=3)
            pygame.draw.rect(self.screen, VisConfig.YELLOW, bg_rect, 1, border_radius=3)

            # Draw text lines centered within the background
            current_y = bg_rect.top + padding
            for msg_surf in msg_surfs:
                msg_rect = msg_surf.get_rect(midtop=(bg_rect.centerx, current_y))
                self.screen.blit(msg_surf, msg_rect)
                current_y += msg_surf.get_height() + 2  # Move Y for next line

            return True  # Message was rendered
        except Exception as e:
            print(f"Error rendering status message: {e}")
            traceback.print_exc()
            return False  # Message render failed


File: ui/input_handler.py
# File: ui/input_handler.py
# (No significant changes needed, this file was already reasonably focused)
import pygame
from typing import Tuple, Callable, Optional

# --- Define Callback Types (for clarity) ---
HandleDemoInputCallback = Callable[[pygame.event.Event], None]
ToggleTrainingCallback = Callable[[], None]
RequestCleanupCallback = Callable[[], None]
CancelCleanupCallback = Callable[[], None]
ConfirmCleanupCallback = Callable[[], None]
ExitAppCallback = Callable[[], bool]  # Returns False to signal exit
StartDemoModeCallback = Callable[[], None]
ExitDemoModeCallback = Callable[[], None]
# --- End Callback Types ---

# Forward declaration for type hinting UIRenderer
if False:  # Prevent circular import at runtime
    from .renderer import UIRenderer


class InputHandler:
    """Handles Pygame events and triggers callbacks based on application state."""

    def __init__(
        self,
        screen: pygame.Surface,
        renderer: "UIRenderer",  # Use forward declaration
        toggle_training_cb: ToggleTrainingCallback,
        request_cleanup_cb: RequestCleanupCallback,
        cancel_cleanup_cb: CancelCleanupCallback,
        confirm_cleanup_cb: ConfirmCleanupCallback,
        exit_app_cb: ExitAppCallback,
        start_demo_mode_cb: StartDemoModeCallback,
        exit_demo_mode_cb: ExitDemoModeCallback,
        handle_demo_input_cb: HandleDemoInputCallback,
    ):
        self.screen = screen  # Keep reference to update size on resize
        self.renderer = renderer
        # Store callbacks provided by MainApp
        self.toggle_training_cb = toggle_training_cb
        self.request_cleanup_cb = request_cleanup_cb
        self.cancel_cleanup_cb = cancel_cleanup_cb
        self.confirm_cleanup_cb = confirm_cleanup_cb
        self.exit_app_cb = exit_app_cb
        self.start_demo_mode_cb = start_demo_mode_cb
        self.exit_demo_mode_cb = exit_demo_mode_cb
        self.handle_demo_input_cb = handle_demo_input_cb

        # Store button rect definitions locally for collision detection
        # Note: These are visually defined in LeftPanelRenderer, coupling exists.
        self._update_button_rects()  # Initialize rects

    def _update_button_rects(self):
        """Calculates button rects based on initial layout assumptions."""
        # These might need adjustment if the layout in LeftPanelRenderer changes significantly.
        self.train_btn_rect = pygame.Rect(10, 10, 100, 40)
        self.cleanup_btn_rect = pygame.Rect(self.train_btn_rect.right + 10, 10, 160, 40)
        self.demo_btn_rect = pygame.Rect(self.cleanup_btn_rect.right + 10, 10, 120, 40)
        # Confirmation buttons are relative to screen center, calculated dynamically
        sw, sh = self.screen.get_size()
        self.confirm_yes_rect = pygame.Rect(sw // 2 - 110, sh // 2 + 30, 100, 40)
        self.confirm_no_rect = pygame.Rect(sw // 2 + 10, sh // 2 + 30, 100, 40)

    def handle_input(self, app_state: str, cleanup_confirmation_active: bool) -> bool:
        """
        Processes Pygame events based on the current app_state and cleanup overlay status.
        Returns True to continue running, False to exit.
        """
        try:
            mouse_pos = pygame.mouse.get_pos()
        except pygame.error:
            mouse_pos = (0, 0)  # Fallback if display not initialized/error

        # Update confirmation button rects in case of resize
        sw, sh = self.screen.get_size()
        self.confirm_yes_rect.center = (sw // 2 - 60, sh // 2 + 50)
        self.confirm_no_rect.center = (sw // 2 + 60, sh // 2 + 50)

        # Check tooltip hover (only if overlay is not active and in MainMenu)
        # Renderer handles tooltip display, input handler just triggers the check.
        if app_state == "MainMenu" and not cleanup_confirmation_active:
            if hasattr(self.renderer, "check_hover"):
                self.renderer.check_hover(mouse_pos, app_state)

        # --- Event Loop ---
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return self.exit_app_cb()  # Signal exit immediately

            # Handle Window Resizing
            if event.type == pygame.VIDEORESIZE:
                try:
                    # Ensure minimum size
                    new_w, new_h = max(320, event.w), max(240, event.h)
                    self.screen = pygame.display.set_mode(
                        (new_w, new_h), pygame.RESIZABLE
                    )
                    # Update screen references in all UI components that hold one
                    self._update_ui_screen_references(self.screen)
                    # Update local button rects that depend on screen size
                    self._update_button_rects()
                    # Force plotter redraw as its size changed
                    if hasattr(self.renderer, "force_redraw"):
                        self.renderer.force_redraw()
                    print(f"Window resized: {new_w}x{new_h}")
                except pygame.error as e:
                    print(f"Error resizing window: {e}")
                continue  # Skip other event processing for this frame after resize

            # --- State-Specific Input Handling ---

            # 1. Cleanup Confirmation Overlay (Highest Priority)
            if cleanup_confirmation_active:
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                    self.cancel_cleanup_cb()
                elif (
                    event.type == pygame.MOUSEBUTTONDOWN and event.button == 1
                ):  # Left click
                    if self.confirm_yes_rect.collidepoint(mouse_pos):
                        self.confirm_cleanup_cb()
                    elif self.confirm_no_rect.collidepoint(mouse_pos):
                        self.cancel_cleanup_cb()
                # Consume event if overlay is active, preventing other actions
                continue

            # 2. Playing State (Demo Mode)
            elif app_state == "Playing":
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        self.exit_demo_mode_cb()
                    else:
                        # Delegate game control input to specific handler
                        self.handle_demo_input_cb(event)
                # No other input handled in this state currently

            # 3. Main Menu State
            elif app_state == "MainMenu":
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_ESCAPE:
                        return self.exit_app_cb()  # Exit App
                    elif event.key == pygame.K_p:
                        self.toggle_training_cb()  # Toggle Training

                if (
                    event.type == pygame.MOUSEBUTTONDOWN and event.button == 1
                ):  # Left click
                    if self.train_btn_rect.collidepoint(mouse_pos):
                        self.toggle_training_cb()
                    elif self.cleanup_btn_rect.collidepoint(mouse_pos):
                        self.request_cleanup_cb()  # Show confirmation
                    elif self.demo_btn_rect.collidepoint(mouse_pos):
                        self.start_demo_mode_cb()
                    # No action for clicking elsewhere in main menu

            # 4. Error State
            elif app_state == "Error":
                if event.type == pygame.KEYDOWN and event.key == pygame.K_ESCAPE:
                    return self.exit_app_cb()  # Allow exit from error state
                # No other input handled in error state

            # 5. Other States (e.g., Initializing, Cleaning) - Ignore input for now
            # else: pass

        return True  # Continue running

    def _update_ui_screen_references(self, new_screen: pygame.Surface):
        """Recursively updates the screen reference in the renderer and its sub-components."""
        # List of components known to hold a screen reference
        components_to_update = [
            self.renderer,
            getattr(self.renderer, "left_panel", None),
            getattr(self.renderer, "game_area", None),
            getattr(self.renderer, "overlays", None),
            getattr(self.renderer, "tooltips", None),
            getattr(
                self.renderer, "demo_renderer", None
            ),  # Update new demo renderer too
        ]
        for component in components_to_update:
            if component and hasattr(component, "screen"):
                component.screen = new_screen


File: ui/panels/__init__.py
from .left_panel import LeftPanelRenderer
from .game_area import GameAreaRenderer

__all__ = ["LeftPanelRenderer", "GameAreaRenderer"]


File: ui/panels/game_area.py
# File: ui/panels/game_area.py
# --- Game Area Rendering Logic ---
import pygame
import math
import traceback
from typing import List, Tuple
from config import VisConfig, EnvConfig
from environment.game_state import GameState
from environment.shape import Shape
from environment.triangle import Triangle
import numpy as np  # Import numpy


class GameAreaRenderer:
    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.fonts = self._init_fonts()

    def _init_fonts(self):
        fonts = {}
        try:
            fonts["env_score"] = pygame.font.SysFont(None, 18)
            fonts["env_overlay"] = pygame.font.SysFont(None, 36)
            fonts["ui"] = pygame.font.SysFont(None, 24)
        except Exception as e:
            print(f"Warning: SysFont error: {e}. Using default.")
            fonts["env_score"] = pygame.font.Font(None, 18)
            fonts["env_overlay"] = pygame.font.Font(None, 36)
            fonts["ui"] = pygame.font.Font(None, 24)
        return fonts

    def render(self, envs: List[GameState], num_envs: int, env_config: EnvConfig):
        # ... (render method remains mostly the same, calls _render_env_grid)
        current_width, current_height = self.screen.get_size()
        lp_width = min(current_width, max(300, self.vis_config.LEFT_PANEL_WIDTH))
        ga_rect = pygame.Rect(lp_width, 0, current_width - lp_width, current_height)

        if num_envs <= 0 or ga_rect.width <= 0 or ga_rect.height <= 0:
            return

        render_limit = self.vis_config.NUM_ENVS_TO_RENDER
        num_to_render = num_envs if render_limit <= 0 else min(num_envs, render_limit)
        if num_to_render <= 0:
            return

        cols_env, rows_env, cell_w, cell_h = self._calculate_grid_layout(
            ga_rect, num_to_render
        )

        # Check minimum size *before* trying to render grid or message
        min_cell_dim = 30  # Adjust as needed
        if cell_w > min_cell_dim and cell_h > min_cell_dim:
            self._render_env_grid(
                envs,
                num_to_render,
                env_config,
                ga_rect,
                cols_env,
                rows_env,
                cell_w,
                cell_h,
            )
        else:
            self._render_too_small_message(ga_rect, cell_w, cell_h)

        if num_to_render < num_envs:
            self._render_render_limit_text(ga_rect, num_to_render, num_envs)

    def _calculate_grid_layout(
        self, ga_rect: pygame.Rect, num_to_render: int
    ) -> Tuple[int, int, int, int]:
        # ... (this method remains the same)
        aspect_ratio = ga_rect.width / max(1, ga_rect.height)
        cols_env = max(1, int(math.sqrt(num_to_render * aspect_ratio)))
        rows_env = max(1, math.ceil(num_to_render / cols_env))
        total_spacing_w = (cols_env + 1) * self.vis_config.ENV_SPACING
        total_spacing_h = (rows_env + 1) * self.vis_config.ENV_SPACING
        cell_w = max(1, (ga_rect.width - total_spacing_w) // cols_env)
        cell_h = max(1, (ga_rect.height - total_spacing_h) // rows_env)
        return cols_env, rows_env, cell_w, cell_h

    def _render_env_grid(
        self, envs, num_to_render, env_config, ga_rect, cols, rows, cell_w, cell_h
    ):
        """Iterates through the grid positions and renders each environment."""
        env_idx = 0
        for r in range(rows):
            for c in range(cols):
                if env_idx >= num_to_render:
                    break
                env_x = ga_rect.x + self.vis_config.ENV_SPACING * (c + 1) + c * cell_w
                env_y = ga_rect.y + self.vis_config.ENV_SPACING * (r + 1) + r * cell_h
                env_rect = pygame.Rect(env_x, env_y, cell_w, cell_h)

                # Clip the rect to the screen to prevent subsurface errors
                clipped_env_rect = env_rect.clip(self.screen.get_rect())
                if clipped_env_rect.width <= 0 or clipped_env_rect.height <= 0:
                    env_idx += 1
                    continue  # Skip rendering if clipped rect is invalid

                try:
                    # Use the clipped rect for the subsurface
                    sub_surf = self.screen.subsurface(clipped_env_rect)
                    # Adjust rendering position if clipped (sub_surf topleft is relative)
                    # We'll handle offsets inside _render_single_env now
                    self._render_single_env(sub_surf, envs[env_idx], env_config)

                except ValueError as subsurface_error:
                    print(
                        f"Warning: Subsurface error env {env_idx} ({clipped_env_rect}): {subsurface_error}"
                    )
                    pygame.draw.rect(self.screen, (0, 0, 50), clipped_env_rect, 1)
                except Exception as e_render_env:
                    print(f"Error rendering env {env_idx}: {e_render_env}")
                    traceback.print_exc()
                    pygame.draw.rect(self.screen, (50, 0, 50), clipped_env_rect, 1)

                env_idx += 1

    # --- REPLACED: _render_single_env ---
    def _render_single_env(
        self, surf: pygame.Surface, env: GameState, env_config: EnvConfig
    ):
        """Renders the grid, shapes, scores, and overlays for a single environment cell."""
        cell_w = surf.get_width()
        cell_h = surf.get_height()
        if cell_w <= 0 or cell_h <= 0:
            return

        # --- MODIFIED: Background Color Logic ---
        # Determine background color based on env state, checking flash first
        bg_color = VisConfig.GRAY  # Default background
        if env.is_line_clearing():  # Check flash first
            bg_color = VisConfig.LINE_CLEAR_FLASH_COLOR
        elif env.is_blinking():
            bg_color = VisConfig.YELLOW
        elif env.is_frozen() and not env.is_over():
            bg_color = (30, 30, 100)  # Frozen blue
        elif env.is_over():
            bg_color = (40, 20, 20)  # Game over dark red
        # else: bg_color remains VisConfig.GRAY
        surf.fill(bg_color)
        # --- END MODIFIED ---

        # --- Define Areas ---
        shape_area_height_ratio = 0.20  # Use 20% for shapes
        grid_area_height = math.floor(cell_h * (1.0 - shape_area_height_ratio))
        shape_area_height = cell_h - grid_area_height
        shape_area_y = grid_area_height

        # Create subsurfaces for grid and shapes if dimensions are valid
        grid_surf = None
        shape_surf = None
        if grid_area_height > 0 and cell_w > 0:
            try:
                grid_rect = pygame.Rect(0, 0, cell_w, grid_area_height)
                grid_surf = surf.subsurface(grid_rect)
            except ValueError as e:
                print(f"Warning: Grid subsurface error ({grid_rect}): {e}")
                pygame.draw.rect(surf, VisConfig.RED, grid_rect, 1)

        if shape_area_height > 0 and cell_w > 0:
            try:
                shape_rect = pygame.Rect(0, shape_area_y, cell_w, shape_area_height)
                shape_surf = surf.subsurface(shape_rect)
                shape_surf.fill((35, 35, 35))  # Darker background for shapes
            except ValueError as e:
                print(f"Warning: Shape subsurface error ({shape_rect}): {e}")
                pygame.draw.rect(surf, VisConfig.RED, shape_rect, 1)

        # --- Render Grid ---
        if grid_surf:
            self._render_single_env_grid(grid_surf, env, env_config)

        # --- Render Shapes ---
        if shape_surf:
            self._render_shape_previews(shape_surf, env)

        # --- Render Scores (on top of everything in the cell) ---
        try:
            score_surf = self.fonts["env_score"].render(
                f"GS: {env.game_score} R: {env.score:.1f}",
                True,
                VisConfig.WHITE,
                (0, 0, 0, 180),  # Semi-transparent black background
            )
            # Position score at top-left of the main surface `surf`
            surf.blit(score_surf, (2, 2))
        except Exception as e:
            print(f"Error rendering score: {e}")

        # --- Render Overlays (Game Over/Frozen) ---
        if env.is_over():
            self._render_overlay_text(surf, "GAME OVER", VisConfig.RED)
        elif (
            env.is_frozen() and not env.is_blinking() and not env.is_line_clearing()
        ):  # Dont show frozen if flashing
            # Only show "Frozen" if not game over
            self._render_overlay_text(surf, "Frozen", VisConfig.BLUE)

    def _render_overlay_text(
        self, surf: pygame.Surface, text: str, color: Tuple[int, int, int]
    ):
        """Helper to render centered overlay text with a background."""
        try:
            overlay_font = self.fonts["env_overlay"]
            text_surf = overlay_font.render(
                text,
                True,
                VisConfig.WHITE,
                (color[0] // 2, color[1] // 2, color[2] // 2, 200),
            )
            text_rect = text_surf.get_rect(center=surf.get_rect().center)
            surf.blit(text_surf, text_rect)
        except Exception as e:
            print(f"Error rendering overlay text '{text}': {e}")

    # --- USE SIMPLIFIED SCALING ---
    def _render_single_env_grid(
        self, surf: pygame.Surface, env: GameState, env_config: EnvConfig
    ):
        """Renders the playable grid, scaled and centered within the surface."""
        try:
            padding = self.vis_config.ENV_GRID_PADDING
            drawable_w = max(
                1, surf.get_width() - 2 * padding
            )  # Ensure at least 1 pixel
            drawable_h = max(
                1, surf.get_height() - 2 * padding
            )  # Ensure at least 1 pixel

            # Calculate the required dimensions in "triangle units"
            grid_rows = env_config.ROWS
            grid_cols_effective_width = env_config.COLS * 0.75 + 0.25

            if grid_rows <= 0 or grid_cols_effective_width <= 0:
                return

            # Calculate scale factors based on available drawable space
            scale_w_based = drawable_w / grid_cols_effective_width
            scale_h_based = drawable_h / grid_rows

            # Choose the MINIMUM scale factor to ensure it fits in both dimensions
            final_scale = min(scale_w_based, scale_h_based)
            if final_scale <= 0:
                return  # Cannot render with zero or negative scale

            # Calculate final pixel dimensions of the grid using the chosen scale
            final_grid_pixel_w = grid_cols_effective_width * final_scale
            final_grid_pixel_h = grid_rows * final_scale

            # Calculate triangle cell dimensions based on the final scale
            # Assuming base unit height = 1, base unit width = 1 before scaling
            tri_cell_h = max(1, final_scale)
            tri_cell_w = max(
                1, final_scale
            )  # Simplification: Scale width same as height

            # Calculate centering offsets within the padded drawable area
            grid_ox = padding + (drawable_w - final_grid_pixel_w) / 2
            grid_oy = padding + (drawable_h - final_grid_pixel_h) / 2

            # Render Grid Triangles (only non-death)
            if hasattr(env, "grid") and hasattr(env.grid, "triangles"):
                for r in range(env.grid.rows):
                    for c in range(env.grid.cols):
                        # Check bounds carefully
                        if not (
                            0 <= r < len(env.grid.triangles)
                            and 0 <= c < len(env.grid.triangles[r])
                        ):
                            continue  # Skip if out of bounds
                        t = env.grid.triangles[r][c]
                        # --- Only render non-death cells ---
                        if not t.is_death:
                            if not hasattr(t, "get_points"):
                                continue
                            try:
                                pts = t.get_points(
                                    ox=grid_ox,
                                    oy=grid_oy,
                                    cw=int(tri_cell_w),
                                    ch=int(tri_cell_h),
                                )
                                color = VisConfig.LIGHTG  # Default empty color
                                if t.is_occupied:
                                    color = t.color if t.color else VisConfig.RED
                                pygame.draw.polygon(surf, color, pts)
                                # Optional: Draw border for empty cells
                                pygame.draw.polygon(
                                    surf, VisConfig.GRAY, pts, 1
                                )  # Add border for clarity
                            except Exception as e_render:
                                # Reduce log spam for minor render errors
                                # print(f"Error rendering tri ({r},{c}): {e_render}")
                                pass  # Silently skip triangle if points error
            else:
                # Draw error indication if grid is missing
                pygame.draw.rect(surf, VisConfig.RED, surf.get_rect(), 2)
                err_txt = self.fonts["ui"].render(
                    "Invalid Grid Data", True, VisConfig.RED
                )
                surf.blit(err_txt, err_txt.get_rect(center=surf.get_rect().center))

        except Exception as e:
            print(f"Unexpected Render Error in _render_single_env_grid: {e}")
            traceback.print_exc()
            pygame.draw.rect(surf, VisConfig.RED, surf.get_rect(), 2)

    # --- MODIFIED: Renders shapes in their allocated area ---
    def _render_shape_previews(self, surf: pygame.Surface, env: GameState):
        """Renders small previews of available shapes horizontally in the given surface."""
        available_shapes = env.get_shapes()
        if not available_shapes:
            return

        surf_w = surf.get_width()
        surf_h = surf.get_height()
        if surf_w <= 0 or surf_h <= 0:
            return

        num_shapes = len(available_shapes)
        padding = 4  # Padding around shapes
        total_padding_needed = (num_shapes + 1) * padding
        available_width_for_shapes = surf_w - total_padding_needed

        if available_width_for_shapes <= 0:
            return  # Not enough space

        # Calculate width per shape, limited by available height
        width_per_shape = available_width_for_shapes / num_shapes
        height_limit = surf_h - 2 * padding
        preview_dim = max(
            5, min(width_per_shape, height_limit)
        )  # Square-ish preview size

        start_x = (
            padding
            + (surf_w - (num_shapes * preview_dim + (num_shapes - 1) * padding)) / 2
        )
        start_y = padding + (surf_h - preview_dim) / 2

        current_x = start_x
        for shape in available_shapes:
            preview_rect = pygame.Rect(current_x, start_y, preview_dim, preview_dim)
            if (
                preview_rect.right > surf_w - padding
            ):  # Prevent overflow if calculation is slightly off
                break

            try:
                # Create a temporary surface for the shape preview
                temp_shape_surf = pygame.Surface(
                    (preview_dim, preview_dim), pygame.SRCALPHA
                )
                temp_shape_surf.fill((0, 0, 0, 0))  # Transparent background

                # Calculate cell size to fit the shape bbox within preview_dim
                min_r, min_c, max_r, max_c = shape.bbox()
                shape_h_cells = max(1, max_r - min_r + 1)
                shape_w_cells_eff = max(1, (max_c - min_c + 1) * 0.75 + 0.25)

                scale_h = preview_dim / shape_h_cells
                scale_w = preview_dim / shape_w_cells_eff
                cell_size = max(
                    1, min(scale_h, scale_w)
                )  # Base cell size on limiting dimension

                # Render the shape centered on the temp surface
                self._render_single_shape(temp_shape_surf, shape, cell_size)

                # Blit the temp surface onto the main shape preview surface
                surf.blit(temp_shape_surf, preview_rect.topleft)
                current_x += preview_dim + padding

            except Exception as e:
                print(f"Error rendering shape preview: {e}")
                pygame.draw.rect(surf, VisConfig.RED, preview_rect, 1)  # Draw error box
                current_x += preview_dim + padding  # Still advance

    def _render_single_shape(self, surf: pygame.Surface, shape: Shape, cell_size: int):
        # ... (this method remains the same - renders a shape centered on the given surface)
        if not shape or not shape.triangles or cell_size <= 0:
            return
        min_r, min_c, max_r, max_c = shape.bbox()
        shape_h_cells = max_r - min_r + 1
        shape_w_cells_eff = (max_c - min_c + 1) * 0.75 + 0.25
        if shape_w_cells_eff <= 0 or shape_h_cells <= 0:
            return

        # Calculate pixel dimensions required by the shape at this cell_size
        total_w_pixels = shape_w_cells_eff * cell_size
        total_h_pixels = shape_h_cells * cell_size

        # Calculate offset to center the shape on the surface
        # Offset relative to the shape's min_r, min_c
        offset_x = (surf.get_width() - total_w_pixels) / 2 - min_c * (cell_size * 0.75)
        offset_y = (surf.get_height() - total_h_pixels) / 2 - min_r * cell_size

        for dr, dc, up in shape.triangles:
            tri = Triangle(row=dr, col=dc, is_up=up)  # Temp instance for points
            try:
                pts = tri.get_points(
                    ox=offset_x, oy=offset_y, cw=cell_size, ch=cell_size
                )
                pygame.draw.polygon(surf, shape.color, pts)
            except Exception as e:
                print(f"Warning: Error rendering shape preview tri ({dr},{dc}): {e}")

    def _render_too_small_message(self, ga_rect: pygame.Rect, cell_w: int, cell_h: int):
        # ... (this method remains the same)
        try:
            err_surf = self.fonts["ui"].render(
                f"Envs Too Small ({cell_w}x{cell_h})", True, VisConfig.GRAY
            )
            self.screen.blit(err_surf, err_surf.get_rect(center=ga_rect.center))
        except Exception as e:
            print(f"Error rendering 'too small' message: {e}")

    def _render_render_limit_text(
        self, ga_rect: pygame.Rect, num_rendered: int, num_total: int
    ):
        # ... (this method remains the same)
        try:
            info_surf = self.fonts["ui"].render(
                f"Rendering {num_rendered}/{num_total} Envs",
                True,
                VisConfig.YELLOW,
                VisConfig.BLACK,
            )
            self.screen.blit(
                info_surf,
                info_surf.get_rect(bottomright=(ga_rect.right - 5, ga_rect.bottom - 5)),
            )
        except Exception as e:
            print(f"Error rendering limit text: {e}")


File: ui/panels/left_panel.py
# File: ui/panels/left_panel.py
import pygame
import os
import time
from typing import Dict, Any, Optional, Deque, Tuple

from config import (
    VisConfig,
    BufferConfig,
    StatsConfig,
    DQNConfig,
    DEVICE,
    TensorBoardConfig,
)
from config.general import TOTAL_TRAINING_STEPS
from ui.plotter import Plotter

# Tooltips specific to this panel (Keep definition here for locality)
TOOLTIP_TEXTS = {
    "Status": "Current state: Paused, Buffering, Training, Confirm Cleanup, Cleaning, or Error.",
    "Global Steps": "Total environment steps taken / Total planned steps.",
    "Total Episodes": "Total completed episodes across all environments.",
    "Steps/Sec (Current)": f"Current avg Steps/Sec (~{StatsConfig.STATS_AVG_WINDOW} intervals). See plot for history.",
    "Buffer Fill": f"Current replay buffer fill % ({BufferConfig.REPLAY_BUFFER_SIZE / 1e6:.1f}M cap). See plot.",
    "PER Beta": f"Current PER IS exponent ({BufferConfig.PER_BETA_START:.1f}->1.0). See plot.",
    "Learning Rate": "Current learning rate. See plot for history/schedule.",
    "Train Button": "Click to Start/Pause training (or press 'P').",
    "Cleanup Button": "Click to DELETE agent ckpt & buffer for CURRENT run ONLY, then re-init.",
    "Device": f"Computation device detected ({DEVICE.type.upper()}).",
    "Network": f"CNN+MLP Fusion. Dueling={DQNConfig.USE_DUELING}, Noisy={DQNConfig.USE_NOISY_NETS}, C51={DQNConfig.USE_DISTRIBUTIONAL}",
    "TensorBoard Status": "Indicates TB logging status and log directory.",
    "Notification Area": "Displays the latest best achievements (RL Score, Game Score, Loss).",
    "Best RL Score Info": "Best RL Score achieved: Current Value (Previous Value) - Steps Ago",
    "Best Game Score Info": "Best Game Score achieved: Current Value (Previous Value) - Steps Ago",
    "Best Loss Info": "Best (Lowest) Loss achieved: Current Value (Previous Value) - Steps Ago",
    "Play Demo Button": "Click to enter interactive play mode. Gameplay fills the replay buffer.",
}


class LeftPanelRenderer:
    def __init__(self, screen: pygame.Surface, vis_config: VisConfig, plotter: Plotter):
        self.screen = screen
        self.vis_config = vis_config
        self.plotter = plotter
        self.fonts = self._init_fonts()
        self.stat_rects: Dict[str, pygame.Rect] = {}  # Rects for tooltip hit detection

    def _init_fonts(self):
        """Loads necessary fonts using system defaults."""
        fonts = {}
        default_font_size = 24
        status_font_size = 28
        logdir_font_size = 16
        plot_placeholder_size = 20
        notification_size = 19
        notification_label_size = 16

        # Use SysFont for all, with a final fallback to pygame default font
        font_configs = {
            "ui": default_font_size,
            "status": status_font_size,
            "logdir": logdir_font_size,
            "plot_placeholder": plot_placeholder_size,
            "notification": notification_size,
            "notification_label": notification_label_size,
        }

        for key, size in font_configs.items():
            try:
                fonts[key] = pygame.font.SysFont(None, size)
            except Exception as e_sysfont:
                print(
                    f"Warning: SysFont error for '{key}': {e_sysfont}. Using default."
                )
                try:
                    fonts[key] = pygame.font.Font(None, size)
                except Exception as e_default:
                    print(
                        f"CRITICAL WARNING: Default font failed for '{key}': {e_default}. UI may be broken."
                    )
                    # As a last resort, create a dummy font object? Or let it crash?
                    # For now, let's assign None and handle potential errors later.
                    fonts[key] = None

        # Check for None fonts (critical failure)
        for key, font_obj in fonts.items():
            if font_obj is None:
                print(f"ERROR: Font '{key}' failed to load entirely.")

        return fonts

    def render(
        self,
        is_training: bool,
        status: str,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
        app_state: str,
    ):
        """Renders the entire left panel by calling sub-render methods."""
        current_width, current_height = self.screen.get_size()
        lp_width = min(current_width, max(300, self.vis_config.LEFT_PANEL_WIDTH))
        lp_rect = pygame.Rect(0, 0, lp_width, current_height)

        # Background based on status
        status_color_map = {
            "Paused": (30, 30, 30),
            "Buffering": (30, 40, 30),
            "Training": (40, 30, 30),
            "Confirm Cleanup": (50, 20, 20),
            "Cleaning": (60, 30, 30),
            "Error": (60, 0, 0),
            "Playing Demo": (30, 30, 40),
            "Initializing": (40, 40, 40),
        }
        bg_color = status_color_map.get(status, (30, 30, 30))
        pygame.draw.rect(self.screen, bg_color, lp_rect)

        self.stat_rects.clear()  # Reset rects for tooltips each frame

        # --- Render sections sequentially ---
        current_y = 10  # Starting Y position

        # 1. Buttons and Status (Layout depends on app_state)
        notification_area_rect, current_y = self._render_buttons_and_status(
            current_y, lp_width, app_state, is_training, status
        )

        # 2. Notification Area (if applicable)
        if notification_area_rect:
            self._render_notification_area(notification_area_rect, stats_summary)
            # Adjust current_y to be below the notifications if they were rendered
            current_y = max(current_y, notification_area_rect.bottom + 10)

        # 3. Info Text Block
        current_y = self._render_info_text(
            current_y, stats_summary, buffer_capacity, lp_width
        )

        # 4. TensorBoard Status
        current_y = self._render_tb_status(
            current_y + 10, tensorboard_log_dir, lp_width
        )

        # 5. Plot Area
        self._render_plot_area(
            current_y + 15, lp_width, current_height, plot_data, status
        )

    def _render_buttons_and_status(
        self,
        y_start: int,
        panel_width: int,
        app_state: str,
        is_training: bool,
        status: str,
    ) -> Tuple[Optional[pygame.Rect], int]:
        """Renders the top buttons (if in MainMenu) and the status line.
        Returns the Rect for the notification area (or None) and the next Y coordinate.
        """
        notification_rect = None
        next_y = y_start

        if app_state == "MainMenu":
            # Define button rects
            train_btn_rect = pygame.Rect(10, y_start, 100, 40)
            cleanup_btn_rect = pygame.Rect(train_btn_rect.right + 10, y_start, 160, 40)
            demo_btn_rect = pygame.Rect(cleanup_btn_rect.right + 10, y_start, 120, 40)

            # Draw buttons
            self._draw_button(
                train_btn_rect,
                "Pause" if is_training and status == "Training" else "Train",
                (70, 70, 70),
            )
            self._draw_button(cleanup_btn_rect, "Cleanup This Run", (100, 40, 40))
            self._draw_button(demo_btn_rect, "Play Demo", (40, 100, 40))

            # Register button rects for tooltips
            self.stat_rects["Train Button"] = train_btn_rect
            self.stat_rects["Cleanup Button"] = cleanup_btn_rect
            self.stat_rects["Play Demo Button"] = demo_btn_rect

            # Calculate notification area position (to the right of buttons)
            notification_x = demo_btn_rect.right + 15
            notification_w = panel_width - notification_x - 10
            if notification_w > 50 and self.fonts.get("notification"):
                line_h = self.fonts["notification"].get_linesize()
                notification_h = line_h * 3 + 12  # Height for 3 lines of notifications
                notification_rect = pygame.Rect(
                    notification_x, y_start, notification_w, notification_h
                )

            next_y = train_btn_rect.bottom + 10  # Next element starts below buttons

        # Render Status Text (Always)
        status_text = f"Status: {status}"
        if app_state == "Playing":
            status_text = "Status: Playing Demo"
        elif app_state != "MainMenu":
            status_text = f"Status: {app_state}"  # e.g., Initializing

        status_font = self.fonts.get("status")
        if status_font:
            status_surf = status_font.render(status_text, True, VisConfig.YELLOW)
            # Position status text below buttons if they exist, otherwise at the top
            status_rect_top = next_y if app_state == "MainMenu" else y_start
            status_rect = status_surf.get_rect(topleft=(10, status_rect_top))
            self.screen.blit(status_surf, status_rect)
            if app_state == "MainMenu":  # Only add tooltip for status in main menu
                self.stat_rects["Status"] = status_rect
            next_y = status_rect.bottom + 5  # Update next_y based on status text bottom
        else:
            next_y += 20  # Estimate space if font failed

        # Determine the starting Y for the *next* block
        final_next_y = next_y

        return notification_rect, final_next_y

    def _draw_button(self, rect: pygame.Rect, text: str, color: Tuple[int, int, int]):
        """Helper to draw a single button."""
        pygame.draw.rect(self.screen, color, rect, border_radius=5)
        ui_font = self.fonts.get("ui")
        if ui_font:
            lbl_surf = ui_font.render(text, True, VisConfig.WHITE)
            self.screen.blit(lbl_surf, lbl_surf.get_rect(center=rect.center))
        else:
            # Draw placeholder if font failed
            pygame.draw.line(
                self.screen, VisConfig.RED, rect.topleft, rect.bottomright, 2
            )
            pygame.draw.line(
                self.screen, VisConfig.RED, rect.topright, rect.bottomleft, 2
            )

    def _format_steps_ago(self, current_step: int, best_step: int) -> str:
        """Formats the difference in steps into a readable string (k steps, M steps)."""
        if best_step <= 0 or current_step <= best_step:
            return "Now"
        diff = current_step - best_step
        if diff < 1000:
            return f"{diff} steps ago"
        elif diff < 1_000_000:
            return f"{diff / 1000:.1f}k steps ago"
        else:
            return f"{diff / 1_000_000:.1f}M steps ago"

    def _render_notification_area(self, area_rect: pygame.Rect, stats: Dict[str, Any]):
        """Renders the latest best score/loss messages with details."""
        pygame.draw.rect(self.screen, (45, 45, 45), area_rect, border_radius=3)
        pygame.draw.rect(self.screen, VisConfig.LIGHTG, area_rect, 1, border_radius=3)
        self.stat_rects["Notification Area"] = area_rect  # Tooltip for the whole area

        padding = 5
        label_font = self.fonts.get("notification_label")
        value_font = self.fonts.get("notification")

        # Check if fonts are loaded
        if not label_font or not value_font:
            # Optionally draw an error message inside the area
            err_font = self.fonts.get("ui")  # Try using ui font for error
            if err_font:
                err_surf = err_font.render("Font Error", True, VisConfig.RED)
                self.screen.blit(err_surf, err_surf.get_rect(center=area_rect.center))
            return  # Cannot render notifications without fonts

        line_height = value_font.get_linesize()
        label_color = VisConfig.LIGHTG
        value_color = VisConfig.WHITE
        prev_color = VisConfig.GRAY
        time_color = (180, 180, 100)  # Yellowish for time

        current_step = stats.get("global_step", 0)

        # --- Helper function to render a single notification line ---
        def render_line(
            y_pos, label, current_val, prev_val, best_step, val_format, tooltip_key
        ):
            # Label
            label_surf = label_font.render(label, True, label_color)
            label_rect = label_surf.get_rect(topleft=(area_rect.left + padding, y_pos))
            self.screen.blit(label_surf, label_rect)
            current_x = label_rect.right + 4

            # Current Value (handle inf/-inf)
            current_val_str = "N/A"
            if isinstance(current_val, (int, float)) and abs(current_val) != float(
                "inf"
            ):
                current_val_str = val_format.format(current_val)
            val_surf = value_font.render(current_val_str, True, value_color)
            val_rect = val_surf.get_rect(topleft=(current_x, y_pos))
            self.screen.blit(val_surf, val_rect)
            current_x = val_rect.right + 4

            # Previous Value (handle inf/-inf)
            prev_val_str = "(N/A)"
            if isinstance(prev_val, (int, float)) and abs(prev_val) != float("inf"):
                prev_val_str = f"({val_format.format(prev_val)})"
            prev_surf = label_font.render(prev_val_str, True, prev_color)
            prev_rect = prev_surf.get_rect(
                topleft=(current_x, y_pos + 1)
            )  # Slightly offset
            self.screen.blit(prev_surf, prev_rect)
            current_x = prev_rect.right + 6

            # Steps Ago
            steps_ago_str = self._format_steps_ago(current_step, best_step)
            time_surf = label_font.render(steps_ago_str, True, time_color)
            time_rect = time_surf.get_rect(
                topleft=(current_x, y_pos + 1)
            )  # Slightly offset

            # Clip rendering to the notification area width
            available_width = area_rect.right - time_rect.left - padding
            clip_rect = pygame.Rect(0, 0, max(0, available_width), time_rect.height)
            if time_rect.width > available_width > 0:
                self.screen.blit(time_surf, time_rect, area=clip_rect)
            elif available_width > 0:  # Only blit if space available
                self.screen.blit(time_surf, time_rect)

            # Create combined rect for tooltip hover detection (clipped to area)
            combined_rect = label_rect.union(val_rect).union(prev_rect).union(time_rect)
            self.stat_rects[tooltip_key] = combined_rect.clip(area_rect)

        # --- End render_line helper ---

        y = area_rect.top + padding
        render_line(
            y,
            "RL Score:",
            stats.get("best_score", -float("inf")),
            stats.get("previous_best_score", -float("inf")),
            stats.get("best_score_step", 0),
            "{:.2f}",
            "Best RL Score Info",
        )
        y += line_height
        render_line(
            y,
            "Game Score:",
            stats.get("best_game_score", -float("inf")),
            stats.get("previous_best_game_score", -float("inf")),
            stats.get("best_game_score_step", 0),
            "{:.0f}",
            "Best Game Score Info",
        )
        y += line_height
        render_line(
            y,
            "Loss:",
            stats.get("best_loss", float("inf")),
            stats.get("previous_best_loss", float("inf")),
            stats.get("best_loss_step", 0),
            "{:.4f}",
            "Best Loss Info",
        )

    def _render_info_text(
        self, y_start: int, stats: Dict[str, Any], capacity: int, panel_width: int
    ) -> int:
        """Renders the main block of statistics text."""
        ui_font = self.fonts.get("ui")
        if not ui_font:
            return y_start + 100  # Estimate space if font failed

        line_height = ui_font.get_linesize()
        buffer_size = stats.get("buffer_size", 0)
        buffer_perc = (buffer_size / max(1, capacity) * 100) if capacity > 0 else 0.0
        global_step = stats.get("global_step", 0)

        # Define lines to render
        info_lines = [
            (
                "Global Steps",
                f"{global_step/1e6:.2f}M / {TOTAL_TRAINING_STEPS/1e6:.1f}M",
            ),
            ("Total Episodes", f"{stats.get('total_episodes', 0)}"),
            ("Steps/Sec (Current)", f"{stats.get('steps_per_second', 0.0):.1f}"),
            ("Buffer Fill", f"{buffer_perc:.1f}% ({buffer_size/1e6:.2f}M)"),
            (
                "PER Beta",
                f"{stats.get('beta', 0.0):.3f}" if BufferConfig.USE_PER else "N/A",
            ),
            ("Learning Rate", f"{stats.get('current_lr', 0.0):.1e}"),
            ("Device", f"{DEVICE.type.upper()}"),
            (
                "Network",
                f"Duel={DQNConfig.USE_DUELING}, Noisy={DQNConfig.USE_NOISY_NETS}, C51={DQNConfig.USE_DISTRIBUTIONAL}",
            ),
        ]

        last_y = y_start
        x_pos_key = 10
        x_pos_val_offset = 5  # Space between key and value

        for idx, (key, value_str) in enumerate(info_lines):
            current_y = y_start + idx * line_height
            try:
                # Render Key
                key_surf = ui_font.render(f"{key}:", True, VisConfig.LIGHTG)
                key_rect = key_surf.get_rect(topleft=(x_pos_key, current_y))
                self.screen.blit(key_surf, key_rect)

                # Render Value
                value_surf = ui_font.render(f"{value_str}", True, VisConfig.WHITE)
                value_rect = value_surf.get_rect(
                    topleft=(key_rect.right + x_pos_val_offset, current_y)
                )

                # Clip value rendering if it exceeds panel width
                clip_width = max(
                    0, panel_width - value_rect.left - 10
                )  # 10px right margin
                if value_rect.width > clip_width:
                    self.screen.blit(
                        value_surf,
                        value_rect,
                        area=pygame.Rect(0, 0, clip_width, value_rect.height),
                    )
                else:
                    self.screen.blit(value_surf, value_rect)

                # Register combined rect for tooltip
                combined_rect = key_rect.union(value_rect)
                combined_rect.width = min(
                    combined_rect.width, panel_width - x_pos_key - 10
                )  # Clip tooltip rect too
                self.stat_rects[key] = combined_rect
                last_y = combined_rect.bottom

            except Exception as e:
                print(f"Error rendering stat line '{key}': {e}")
                last_y = current_y + line_height  # Advance Y even on error

        return last_y  # Return Y position below the last rendered item

    def _render_tb_status(
        self, y_start: int, log_dir: Optional[str], panel_width: int
    ) -> int:
        """Renders the TensorBoard status line and log directory."""
        ui_font = self.fonts.get("ui")
        logdir_font = self.fonts.get("logdir")
        if not ui_font or not logdir_font:
            return y_start + 30  # Estimate space if fonts failed

        tb_active = (
            TensorBoardConfig.LOG_HISTOGRAMS
            or TensorBoardConfig.LOG_IMAGES
            or TensorBoardConfig.LOG_SHAPE_PLACEMENT_Q_VALUES
        )
        tb_color = VisConfig.GOOGLE_COLORS[0] if tb_active else VisConfig.GRAY
        tb_text = f"TensorBoard: {'Logging Active' if tb_active else 'Logging Minimal'}"

        tb_surf = ui_font.render(tb_text, True, tb_color)
        tb_rect = tb_surf.get_rect(topleft=(10, y_start))
        self.screen.blit(tb_surf, tb_rect)
        self.stat_rects["TensorBoard Status"] = tb_rect  # Initial rect for tooltip

        last_y = tb_rect.bottom

        if log_dir:
            try:
                # Attempt to shorten log directory path for display
                panel_char_width = max(10, panel_width // logdir_font.size("A")[0])
                try:
                    rel_log_dir = os.path.relpath(log_dir)
                except ValueError:  # Handle different drives on Windows
                    rel_log_dir = log_dir

                if len(rel_log_dir) > panel_char_width:
                    parts = log_dir.replace("\\", "/").split("/")
                    if len(parts) >= 2:
                        rel_log_dir = os.path.join("...", *parts[-2:])
                        if len(rel_log_dir) > panel_char_width:  # If still too long
                            rel_log_dir = (
                                "..."
                                + os.path.basename(log_dir)[-(panel_char_width - 3) :]
                            )
                    else:  # Just the run ID folder
                        rel_log_dir = (
                            "..." + os.path.basename(log_dir)[-(panel_char_width - 3) :]
                        )

            except Exception:  # Fallback on any path manipulation error
                rel_log_dir = os.path.basename(log_dir)

            dir_surf = logdir_font.render(
                f"Log Dir: {rel_log_dir}", True, VisConfig.LIGHTG
            )
            dir_rect = dir_surf.get_rect(topleft=(10, tb_rect.bottom + 2))

            # Clip rendering if needed
            clip_width = max(0, panel_width - dir_rect.left - 10)
            if dir_rect.width > clip_width:
                self.screen.blit(
                    dir_surf,
                    dir_rect,
                    area=pygame.Rect(0, 0, clip_width, dir_rect.height),
                )
            else:
                self.screen.blit(dir_surf, dir_rect)

            # Update tooltip rect to include the log dir line
            combined_tb_rect = tb_rect.union(dir_rect)
            combined_tb_rect.width = min(
                combined_tb_rect.width, panel_width - 10 - combined_tb_rect.left
            )
            self.stat_rects["TensorBoard Status"] = combined_tb_rect
            last_y = dir_rect.bottom

        return last_y

    def _render_plot_area(
        self,
        y_start: int,
        panel_width: int,
        screen_height: int,
        plot_data: Dict[str, Deque],
        status: str,
    ):
        """Renders the Matplotlib plot area using the Plotter."""
        plot_area_height = screen_height - y_start - 10  # Bottom margin
        plot_area_width = panel_width - 20  # Left/right margins

        if plot_area_width <= 50 or plot_area_height <= 50:
            # Area too small to render plot, maybe show a placeholder
            return

        # Get the plot surface (cached or newly generated)
        plot_surface = self.plotter.get_cached_or_updated_plot(
            plot_data, plot_area_width, plot_area_height
        )
        plot_area_rect = pygame.Rect(10, y_start, plot_area_width, plot_area_height)

        if plot_surface:
            self.screen.blit(plot_surface, plot_area_rect.topleft)
        else:
            # Render placeholder if plot couldn't be generated
            pygame.draw.rect(self.screen, (40, 40, 40), plot_area_rect, 1)
            placeholder_text = "Waiting for data..."
            if status == "Buffering":
                placeholder_text = "Buffering... Waiting for plot data..."
            elif status == "Error":
                placeholder_text = "Plotting disabled due to error."
            elif not plot_data or not any(plot_data.values()):
                placeholder_text = "No plot data yet..."

            placeholder_font = self.fonts.get("plot_placeholder")
            if placeholder_font:
                placeholder_surf = placeholder_font.render(
                    placeholder_text, True, VisConfig.GRAY
                )
                placeholder_rect = placeholder_surf.get_rect(
                    center=plot_area_rect.center
                )

                # Clip placeholder text rendering to fit within the plot area
                blit_pos = (
                    max(plot_area_rect.left, placeholder_rect.left),
                    max(plot_area_rect.top, placeholder_rect.top),
                )
                clip_area_rect = plot_area_rect.clip(placeholder_rect)
                blit_area = clip_area_rect.move(
                    -placeholder_rect.left, -placeholder_rect.top
                )

                if blit_area.width > 0 and blit_area.height > 0:
                    self.screen.blit(placeholder_surf, blit_pos, area=blit_area)
            else:
                # Draw simple cross if font failed
                pygame.draw.line(
                    self.screen,
                    VisConfig.GRAY,
                    plot_area_rect.topleft,
                    plot_area_rect.bottomright,
                )
                pygame.draw.line(
                    self.screen,
                    VisConfig.GRAY,
                    plot_area_rect.topright,
                    plot_area_rect.bottomleft,
                )

    def get_stat_rects(self) -> Dict[str, pygame.Rect]:
        """Returns the dictionary of rects for tooltip handling."""
        return self.stat_rects.copy()

    def get_tooltip_texts(self) -> Dict[str, str]:
        """Returns the dictionary of tooltip texts."""
        return TOOLTIP_TEXTS


File: config/__init__.py
# Expose core config classes and general constants/paths
from .core import (
    VisConfig,
    EnvConfig,
    RewardConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
    DemoConfig,  # Add this if you created the class
)
from .general import (
    DEVICE,
    RANDOM_SEED,
    RUN_ID,
    BASE_CHECKPOINT_DIR,
    BASE_LOG_DIR,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,  # Need this path constant
    BUFFER_SAVE_PATH,
    MODEL_SAVE_PATH,
    TOTAL_TRAINING_STEPS,  # Need this constant
)
from .utils import get_config_dict
from .validation import print_config_info_and_validate

# --- MODIFIED: Assign LOG_DIR after imports ---
# This ensures the correct run-specific directory is set in the config class
# before it's potentially used elsewhere (like in the stats recorder init).
TensorBoardConfig.LOG_DIR = RUN_LOG_DIR
# --- END MODIFIED ---

# You can choose to expose everything or just specific items
__all__ = [
    # Core Classes
    "VisConfig",
    "EnvConfig",
    "RewardConfig",
    "DQNConfig",
    "TrainConfig",
    "BufferConfig",
    "ModelConfig",
    "StatsConfig",
    "TensorBoardConfig",
    "DemoConfig",  # Add this
    # General Constants/Paths
    "DEVICE",
    "RANDOM_SEED",
    "RUN_ID",
    "BASE_CHECKPOINT_DIR",
    "BASE_LOG_DIR",
    "RUN_CHECKPOINT_DIR",
    "RUN_LOG_DIR",
    "BUFFER_SAVE_PATH",
    "MODEL_SAVE_PATH",
    "TOTAL_TRAINING_STEPS",
    # Utils/Validation
    "get_config_dict",
    "print_config_info_and_validate",
]


File: config/core.py
# --- Core Configuration Classes ---
import torch
from typing import Deque, Dict, Any, List, Type, Tuple, Optional

from .general import TOTAL_TRAINING_STEPS


# --- Visualization (Pygame) ---
class VisConfig:
    SCREEN_WIDTH = 1600
    SCREEN_HEIGHT = 900
    VISUAL_STEP_DELAY = 0
    LEFT_PANEL_WIDTH = SCREEN_WIDTH // 2
    ENV_SPACING = 1
    ENV_GRID_PADDING = 1  # Padding inside each environment cell
    FPS = 0
    WHITE = (255, 255, 255)
    BLACK = (0, 0, 0)
    LIGHTG = (140, 140, 140)
    GRAY = (50, 50, 50)
    RED = (255, 50, 50)
    BLUE = (50, 50, 255)
    YELLOW = (255, 255, 100)
    GOOGLE_COLORS = [(15, 157, 88), (244, 180, 0), (66, 133, 244), (219, 68, 55)]
    NUM_ENVS_TO_RENDER = 48
    # --- NEW: Added flash color ---
    LINE_CLEAR_FLASH_COLOR = (180, 180, 220)  # Light purplish-white flash
    # --- END NEW ---


# --- Environment ---
class EnvConfig:
    NUM_ENVS = 256
    # --- MODIFIED DIMENSIONS ---
    ROWS = 8
    COLS = 15
    # --- END MODIFIED ---

    # Features per grid cell: Occupied (1/0), Is_Up (1/0)
    GRID_FEATURES_PER_CELL = 2
    # Features per shape: N_Triangles, N_Up, N_Down, Height, Width (all normalized)
    SHAPE_FEATURES_PER_SHAPE = 5
    NUM_SHAPE_SLOTS = 3  # How many shapes are available to choose from

    @property
    def GRID_STATE_SHAPE(self) -> Tuple[int, int, int]:
        # Shape of the grid input to the CNN: [Channels, Height, Width]
        return (self.GRID_FEATURES_PER_CELL, self.ROWS, self.COLS)

    @property
    def SHAPE_STATE_DIM(self) -> int:
        # Total dimension of the flattened shape features input to the MLP
        return self.NUM_SHAPE_SLOTS * self.SHAPE_FEATURES_PER_SHAPE

    @property
    def ACTION_DIM(self) -> int:
        # Total number of possible discrete actions (Shape_Slot * Grid_Position)
        # The network outputs Q-values for all these potential actions.
        # Invalid actions (based on current state) are masked out during selection.
        return self.NUM_SHAPE_SLOTS * (self.ROWS * self.COLS)


# --- Reward Shaping (RL Reward) ---
class RewardConfig:
    REWARD_PLACE_PER_TRI = 0.0  # Small reward for placing triangles (can be non-zero)
    REWARD_CLEAR_1 = 1.0
    REWARD_CLEAR_2 = 3.0
    REWARD_CLEAR_3PLUS = 6.0
    PENALTY_INVALID_MOVE = -0.1  # Penalty for attempting an impossible placement
    PENALTY_HOLE_PER_HOLE = -0.05  # Penalty per empty cell below an occupied cell
    PENALTY_GAME_OVER = -1.0  # Large penalty for losing the game
    REWARD_ALIVE_STEP = 0.0  # Small reward for surviving each step (can be non-zero)


# --- DQN Algorithm ---
class DQNConfig:
    GAMMA = 0.99  # Discount factor for future rewards
    TARGET_UPDATE_FREQ = (
        5_000  # How often to copy online network weights to target network (steps)
    )
    LEARNING_RATE = 5e-5  # AdamW optimizer learning rate
    ADAM_EPS = 1e-4  # AdamW epsilon for numerical stability
    GRADIENT_CLIP_NORM = 10.0  # Max norm for gradient clipping (0 to disable)
    USE_DOUBLE_DQN = True  # Use Double DQN target calculation
    USE_DUELING = True  # Use Dueling Network Architecture
    USE_NOISY_NETS = (
        True  # Use Noisy Linear layers for exploration (instead of epsilon-greedy)
    )
    # --- Distributional (C51) DQN Settings ---
    USE_DISTRIBUTIONAL = (
        False  # Set to True to use C51 (experimental, requires more tuning)
    )
    V_MIN = -10.0  # Minimum value for distributional support
    V_MAX = 10.0  # Maximum value for distributional support
    NUM_ATOMS = 51  # Number of atoms in the distribution
    # --- Learning Rate Scheduler ---
    USE_LR_SCHEDULER = True  # Use Cosine Annealing LR scheduler
    LR_SCHEDULER_T_MAX: int = TOTAL_TRAINING_STEPS  # Scheduler period (total steps)
    LR_SCHEDULER_ETA_MIN: float = 1e-7  # Minimum learning rate for scheduler


# --- Training Loop ---
class TrainConfig:
    BATCH_SIZE = 64  # Number of experiences sampled per training step
    LEARN_START_STEP = 5_000  # Global steps before learning starts (buffer warmup)
    LEARN_FREQ = 4  # Perform a learning update every N global steps
    CHECKPOINT_SAVE_FREQ = (
        20_000  # Save model and buffer state every N global steps (0 to disable)
    )
    LOAD_CHECKPOINT_PATH: str | None = (
        None  # Path to agent .pth file to load (optional)
    )
    LOAD_BUFFER_PATH: str | None = None  # Path to buffer .pkl file to load (optional)


# --- Replay Buffer ---
class BufferConfig:
    REPLAY_BUFFER_SIZE = 200_000  # Maximum number of transitions in the buffer
    USE_N_STEP = True  # Use N-Step returns
    N_STEP = 3  # Number of steps for N-Step returns
    USE_PER = True  # Use Prioritized Experience Replay
    PER_ALPHA = 0.6  # PER exponent α (controls prioritization intensity)
    PER_BETA_START = 0.4  # Initial PER importance sampling exponent β
    PER_BETA_FRAMES = TOTAL_TRAINING_STEPS // 2  # Steps over which β anneals to 1.0
    PER_EPSILON = 1e-6  # Small value added to priorities to ensure non-zero probability


# --- Model Architecture ---
class ModelConfig:
    class Network:  # Nested class for network-specific parameters
        # Get grid dimensions dynamically from EnvConfig to avoid hardcoding
        _env_cfg_instance = EnvConfig()
        HEIGHT = _env_cfg_instance.ROWS
        WIDTH = _env_cfg_instance.COLS
        del _env_cfg_instance  # Avoid keeping instance here
        # --- CNN Branch ---
        CONV_CHANNELS = [32, 64, 64]  # Output channels for each conv layer
        CONV_KERNEL_SIZE = 3
        CONV_STRIDE = 1
        CONV_PADDING = 1  # Use 'same' padding equivalent if stride=1, kernel=3
        CONV_ACTIVATION = torch.nn.ReLU
        USE_BATCHNORM_CONV = True  # Use BatchNorm after conv layers (before activation)
        # --- Shape MLP Branch ---
        # Note: Input dim is calculated automatically (NUM_SHAPE_SLOTS * SHAPE_FEATURES_PER_SHAPE)
        SHAPE_FEATURE_MLP_DIMS = [64]  # Hidden layer dimensions for shape feature MLP
        SHAPE_MLP_ACTIVATION = torch.nn.ReLU
        # --- Fusion MLP Branch ---
        # Note: Input dim is calculated automatically (CNN_out_flat + ShapeMLP_out)
        COMBINED_FC_DIMS = [256, 128]  # Hidden layer dimensions after fusion
        COMBINED_ACTIVATION = torch.nn.ReLU
        USE_BATCHNORM_FC = True  # Use BatchNorm in fusion MLP layers
        DROPOUT_FC = 0.0  # Dropout probability in fusion MLP layers (0 to disable)


# --- Statistics and Logging ---
class StatsConfig:
    STATS_AVG_WINDOW = (
        100  # Window size for calculating rolling averages (episodes/steps)
    )
    CONSOLE_LOG_FREQ = (
        5_000  # Log summary to console every N global steps (0 to disable)
    )


# --- TensorBoard Logging ---
class TensorBoardConfig:
    LOG_HISTOGRAMS = True  # Log histograms of weights, biases, Q-values, etc.
    HISTOGRAM_LOG_FREQ = 10_000  # Log histograms every N global steps
    LOG_IMAGES = True  # Log sample environment states as images
    IMAGE_LOG_FREQ = 50_000  # Log images every N global steps
    LOG_DIR: Optional[str] = None  # Set automatically in config/__init__.py
    # --- Specific Histogram Logging ---
    LOG_SHAPE_PLACEMENT_Q_VALUES = (
        True  # Log Q-value distributions for shapes/placements
    )
    SHAPE_Q_LOG_FREQ = 5_000  # How often to log these specific Q-value histograms


# --- NEW: Demo Mode Visuals (Optional) ---
class DemoConfig:
    BACKGROUND_COLOR = (10, 10, 20)  # Slightly different background
    PREVIEW_COLOR = (200, 200, 200, 100)  # Semi-transparent white for placement preview
    INVALID_PREVIEW_COLOR = (255, 0, 0, 100)  # Red preview if invalid
    SELECTED_SHAPE_HIGHLIGHT_COLOR = VisConfig.YELLOW
    HUD_FONT_SIZE = 24
    HELP_FONT_SIZE = 18
    HELP_TEXT = "[Arrows]=Move | [Q/E]=Cycle Shape | [Space]=Place | [ESC]=Exit"


File: config/utils.py
# File: config/utils.py
# --- Configuration Utilities ---
import torch
from typing import Dict, Any
from .core import (
    VisConfig,
    EnvConfig,
    RewardConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
)
from .general import DEVICE, RANDOM_SEED, RUN_ID


def get_config_dict() -> Dict[str, Any]:
    """Returns a flat dictionary of all relevant config values for logging."""
    all_configs = {}

    # Helper to flatten classes (excluding methods, dunders, ClassVars, nested Classes)
    def flatten_class(cls, prefix=""):
        d = {}
        for k, v in cls.__dict__.items():
            # Basic check: not dunder, not callable, not a type (nested class)
            if not k.startswith("__") and not callable(v) and not isinstance(v, type):
                # More robust checking might be needed for complex configs
                d[f"{prefix}{k}"] = v
        return d

    all_configs.update(flatten_class(VisConfig, "Vis."))
    all_configs.update(flatten_class(EnvConfig, "Env."))
    all_configs.update(flatten_class(RewardConfig, "Reward."))
    all_configs.update(flatten_class(DQNConfig, "DQN."))
    all_configs.update(flatten_class(TrainConfig, "Train."))
    all_configs.update(flatten_class(BufferConfig, "Buffer."))
    all_configs.update(
        flatten_class(ModelConfig.Network, "Model.Net.")
    )  # Flatten sub-class explicitly
    all_configs.update(flatten_class(StatsConfig, "Stats."))
    all_configs.update(flatten_class(TensorBoardConfig, "TB."))

    # Add general constants
    all_configs["General.DEVICE"] = str(DEVICE)
    all_configs["General.RANDOM_SEED"] = RANDOM_SEED
    all_configs["General.RUN_ID"] = RUN_ID

    # Filter out None values specifically for paths that might not be set
    # This prevents logging None for load paths if they aren't specified
    all_configs = {
        k: v for k, v in all_configs.items() if not (k.endswith("_PATH") and v is None)
    }

    # Convert torch.nn activation functions to string representation for logging
    for key, value in all_configs.items():
        if isinstance(value, type) and issubclass(value, torch.nn.Module):
            all_configs[key] = value.__name__
        # Handle potential list values (e.g., CONV_CHANNELS) - convert to string?
        # Tensorboard hparams prefers simple types (int, float, bool, str)
        if isinstance(value, list):
            all_configs[key] = str(value)

    return all_configs


File: config/general.py
# File: config/general.py
# --- General Constants and Paths ---
import torch
import os
import time
from utils.helpers import get_device

# --- General ---
DEVICE = get_device()
RANDOM_SEED = 42
RUN_ID = f"run_{time.strftime('%Y%m%d_%H%M%S')}"
BASE_CHECKPOINT_DIR = "checkpoints"
BASE_LOG_DIR = "logs"

# --- Define Total Steps Here ---
# This makes it available for default LR scheduler T_max
TOTAL_TRAINING_STEPS = 10_000_000

# --- Derived Paths (using RUN_ID) ---
RUN_CHECKPOINT_DIR = os.path.join(BASE_CHECKPOINT_DIR, RUN_ID)
RUN_LOG_DIR = os.path.join(BASE_LOG_DIR, "tensorboard", RUN_ID)

BUFFER_SAVE_PATH = os.path.join(RUN_CHECKPOINT_DIR, "replay_buffer_state.pkl")
MODEL_SAVE_PATH = os.path.join(RUN_CHECKPOINT_DIR, "dqn_agent_state.pth")

# --- REMOVED Assign derived paths to Config classes ---
# from .core import TensorBoardConfig # <<< REMOVE THIS IMPORT
# TensorBoardConfig.LOG_DIR = RUN_LOG_DIR # <<< REMOVE THIS ASSIGNMENT
# --- END REMOVED ---


File: config/validation.py
# File: config/validation.py
import os, torch
from .core import (
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
    VisConfig,
    DemoConfig,  # Added DemoConfig just in case for future validation
)
from .general import (
    RUN_ID,
    DEVICE,
    MODEL_SAVE_PATH,
    BUFFER_SAVE_PATH,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,
    TOTAL_TRAINING_STEPS,
)


def print_config_info_and_validate():
    # --- MODIFIED: Instantiate EnvConfig ---
    env_config_instance = EnvConfig()
    # --- END MODIFIED ---

    print("-" * 70)
    print(f"RUN ID: {RUN_ID}")
    print(f"Log Directory: {RUN_LOG_DIR}")
    print(f"Checkpoint Directory: {RUN_CHECKPOINT_DIR}")
    print(f"Device: {DEVICE}")
    print(
        f"TB Logging: Histograms={'ON' if TensorBoardConfig.LOG_HISTOGRAMS else 'OFF'}, "
        f"Images={'ON' if TensorBoardConfig.LOG_IMAGES else 'OFF'}, "
        f"ShapeQs={'ON' if TensorBoardConfig.LOG_SHAPE_PLACEMENT_Q_VALUES else 'OFF'}"  # Added ShapeQ log status
    )
    # --- MODIFIED: Check GRID_FEATURES_PER_CELL on instance ---
    if env_config_instance.GRID_FEATURES_PER_CELL != 2:  # Check against 2 now
        # --- END MODIFIED ---
        print(
            f"Warning: Network expects {EnvConfig.GRID_FEATURES_PER_CELL} features per cell (Occupied, Is_Up)."
            f" Config has: {env_config_instance.GRID_FEATURES_PER_CELL}"
        )
    if TrainConfig.LOAD_CHECKPOINT_PATH:
        print(
            "*" * 70
            + f"\n*** Warning: LOAD CHECKPOINT from: {TrainConfig.LOAD_CHECKPOINT_PATH} ***\n*** Ensure ckpt matches current Model/DQN Config (distributional, noisy, dueling, scheduler). ***\n"
            + "*" * 70
        )
    else:
        print("--- Starting training from scratch (no checkpoint specified). ---")
    if TrainConfig.LOAD_BUFFER_PATH:
        print(
            "*" * 70
            + f"\n*** Warning: LOAD BUFFER from: {TrainConfig.LOAD_BUFFER_PATH} ***\n*** Ensure buffer matches current Buffer Config (PER, N-Step). ***\n"
            + "*" * 70
        )
    else:
        print("--- Starting with an empty replay buffer (no buffer specified). ---")
    print(f"--- Using Noisy Nets: {DQNConfig.USE_NOISY_NETS} ---")
    print(
        f"--- Using Distributional (C51): {DQNConfig.USE_DISTRIBUTIONAL} (Atoms: {DQNConfig.NUM_ATOMS}, Vmin: {DQNConfig.V_MIN}, Vmax: {DQNConfig.V_MAX}) ---"
    )
    print(
        f"--- Using LR Scheduler: {DQNConfig.USE_LR_SCHEDULER}"
        + (
            f" (CosineAnnealingLR, T_max={DQNConfig.LR_SCHEDULER_T_MAX/1e6:.1f}M steps, eta_min={DQNConfig.LR_SCHEDULER_ETA_MIN:.1e})"
            if DQNConfig.USE_LR_SCHEDULER
            else ""
        )
        + " ---"
    )
    # --- MODIFIED: Print state shapes and action dim from instance ---
    print(
        f"Config: Env=(R={env_config_instance.ROWS}, C={env_config_instance.COLS}), "
        f"GridState={env_config_instance.GRID_STATE_SHAPE}, "
        f"ShapeState={env_config_instance.SHAPE_STATE_DIM}, "
        f"ActionDim={env_config_instance.ACTION_DIM}"
    )
    # --- END MODIFIED ---
    cnn_str = str(ModelConfig.Network.CONV_CHANNELS).replace(" ", "")
    mlp_str = str(ModelConfig.Network.COMBINED_FC_DIMS).replace(" ", "")
    # --- MODIFIED: Print shape MLP dims from config ---
    shape_mlp_cfg_str = str(ModelConfig.Network.SHAPE_FEATURE_MLP_DIMS).replace(" ", "")
    # --- MODIFIED: Removed ShapeEmb ---
    print(
        f"Network: CNN={cnn_str}, ShapeMLP={shape_mlp_cfg_str}, Fusion={mlp_str}, Dueling={DQNConfig.USE_DUELING}"
    )
    # --- END MODIFIED ---

    # --- MODIFIED: Access NUM_ENVS from instance ---
    print(
        f"Training: NUM_ENVS={env_config_instance.NUM_ENVS}, TOTAL_STEPS={TOTAL_TRAINING_STEPS/1e6:.1f}M, BUFFER={BufferConfig.REPLAY_BUFFER_SIZE/1e6:.1f}M, BATCH={TrainConfig.BATCH_SIZE}, LEARN_START={TrainConfig.LEARN_START_STEP/1e3:.1f}k"
    )
    # --- END MODIFIED ---
    print(
        f"Buffer: PER={BufferConfig.USE_PER}, N-Step={BufferConfig.N_STEP if BufferConfig.USE_N_STEP else 'N/A'}"
    )
    print(
        f"Stats: AVG_WINDOW={StatsConfig.STATS_AVG_WINDOW}, Console Log Freq={StatsConfig.CONSOLE_LOG_FREQ}"
    )
    # --- MODIFIED: Access NUM_ENVS from instance ---
    if env_config_instance.NUM_ENVS >= 1024:
        # --- END MODIFIED ---
        print(
            "*" * 70
            + f"\n*** Warning: NUM_ENVS={env_config_instance.NUM_ENVS}. Monitor system resources. ***"
            + (
                "\n*** Using MPS device. Performance varies. Force CPU via env var if needed. ***"
                if DEVICE.type == "mps"
                else ""
            )
            + "\n"
            + "*" * 70
        )
    # --- MODIFIED: Access NUM_ENVS from instance ---
    print(
        f"--- Rendering {VisConfig.NUM_ENVS_TO_RENDER if VisConfig.NUM_ENVS_TO_RENDER > 0 else 'ALL'} of {env_config_instance.NUM_ENVS} environments ---"
    )
    # --- END MODIFIED ---
    print("-" * 70)


File: training/__init__.py


File: training/trainer.py
# File: training/trainer.py
import time
import torch
import torch.nn.functional as F
import numpy as np
import os
import pickle
import random
import traceback
import pygame
from typing import List, Optional, Union, Tuple, Callable, Dict
from collections import deque

from config import (
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    DEVICE,
    TensorBoardConfig,
    VisConfig,
    RewardConfig,
    TOTAL_TRAINING_STEPS,
)
from environment.game_state import GameState, StateType  # Use the Dict type from env
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from agent.replay_buffer.buffer_utils import create_replay_buffer
from stats.stats_recorder import StatsRecorderBase
from utils.helpers import ensure_numpy, load_object, save_object
from utils.types import ActionType


class Trainer:
    """Orchestrates the DQN training process."""

    def __init__(
        self,
        envs: List[GameState],
        agent: DQNAgent,
        buffer: ReplayBufferBase,
        stats_recorder: StatsRecorderBase,
        env_config: EnvConfig,  # Keep passing instance
        dqn_config: DQNConfig,
        train_config: TrainConfig,
        buffer_config: BufferConfig,
        model_config: ModelConfig,
        model_save_path: str,
        buffer_save_path: str,
        load_checkpoint_path: Optional[str] = None,
        load_buffer_path: Optional[str] = None,
    ):

        print("[Trainer] Initializing...")
        self.envs = envs
        self.agent = agent
        self.buffer = buffer
        self.stats_recorder = stats_recorder
        self.num_envs = env_config.NUM_ENVS
        self.device = DEVICE
        self.env_config = env_config  # Store instance
        self.dqn_config = dqn_config
        self.train_config = train_config
        self.buffer_config = buffer_config
        self.model_config = model_config
        self.reward_config = RewardConfig
        self.tb_config = TensorBoardConfig()  # Instantiate
        self.vis_config = VisConfig
        self.model_save_path = model_save_path
        self.buffer_save_path = buffer_save_path

        self.global_step = 0
        self.episode_count = 0
        self.last_image_log_step = -self.tb_config.IMAGE_LOG_FREQ

        try:
            self.current_states: List[StateType] = [env.reset() for env in self.envs]
        except Exception as e:
            print(f"FATAL ERROR during initial environment reset: {e}")
            traceback.print_exc()
            raise e

        self.current_episode_scores = np.zeros(self.num_envs, dtype=np.float32)
        self.current_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)
        self.current_episode_game_scores = np.zeros(self.num_envs, dtype=np.int32)
        self.current_episode_lines_cleared = np.zeros(self.num_envs, dtype=np.int32)

        if load_checkpoint_path:
            self._load_checkpoint(load_checkpoint_path)
        else:
            print("[Trainer] No checkpoint specified, starting agent fresh.")
            self._reset_trainer_state()

        if load_buffer_path:
            self._load_buffer_state(load_buffer_path)
        else:
            print("[Trainer] No buffer specified, starting buffer empty.")
            if not hasattr(self, "buffer") or self.buffer is None:
                self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)

        initial_beta = self._update_beta()
        initial_lr = (
            self.agent.optimizer.param_groups[0]["lr"]
            if hasattr(self.agent, "optimizer") and self.agent.optimizer.param_groups
            else 0.0
        )
        self.stats_recorder.record_step(
            {
                "buffer_size": len(self.buffer),
                "epsilon": 0.0,
                "beta": initial_beta,
                "lr": initial_lr,
                "global_step": self.global_step,
                "episode_count": self.episode_count,
            }
        )

        print(
            f"[Trainer] Init complete. Start Step={self.global_step}, Ep={self.episode_count}, Buf={len(self.buffer)}, Beta={initial_beta:.4f}, LR={initial_lr:.1e}"
        )

    # --- _load_checkpoint, _reset_trainer_state, _load_buffer_state, _save_checkpoint, _update_beta (Unchanged) ---
    def _load_checkpoint(self, path_to_load: str):
        if not os.path.isfile(path_to_load):
            print(
                f"[Trainer] LOAD WARNING: Checkpoint file not found at {path_to_load}. Starting fresh."
            )
            self._reset_trainer_state()
            return
        print(f"[Trainer] Loading agent checkpoint from: {path_to_load}")
        try:
            checkpoint = torch.load(path_to_load, map_location=self.device)
            self.agent.load_state_dict(checkpoint)
            self.global_step = checkpoint.get("global_step", 0)
            self.episode_count = checkpoint.get("episode_count", 0)
            print(
                f"[Trainer] Checkpoint loaded. Resuming from step {self.global_step}, ep {self.episode_count}"
            )
        except FileNotFoundError:
            print(
                f"[Trainer] Checkpoint file disappeared? ({path_to_load}). Starting fresh."
            )
            self._reset_trainer_state()
        except (KeyError, AttributeError, TypeError, RuntimeError) as e:
            print(
                f"[Trainer] Checkpoint load error ('{e}'). Incompatible? Starting fresh."
            )
            traceback.print_exc()
            self._reset_trainer_state()
        except Exception as e:
            print(f"[Trainer] CRITICAL ERROR loading checkpoint: {e}. Starting fresh.")
            traceback.print_exc()
            self._reset_trainer_state()

    def _reset_trainer_state(self):
        self.global_step = 0
        self.episode_count = 0

    def _load_buffer_state(self, path_to_load: str):
        if not os.path.isfile(path_to_load):
            print(
                f"[Trainer] LOAD WARNING: Buffer file not found at {path_to_load}. Starting empty."
            )
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)
            return
        print(f"[Trainer] Attempting to load buffer state from: {path_to_load}")
        try:
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)
            self.buffer.load_state(path_to_load)
            print(f"[Trainer] Buffer state loaded. Size: {len(self.buffer)}")
        except (
            FileNotFoundError,
            EOFError,
            pickle.UnpicklingError,
            ImportError,
            AttributeError,
            ValueError,
            TypeError,
        ) as e:
            print(
                f"[Trainer] ERROR loading buffer (incompatible/corrupt?): {e}. Starting empty."
            )
            traceback.print_exc()
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)
        except Exception as e:
            print(f"[Trainer] CRITICAL ERROR loading buffer: {e}. Starting empty.")
            traceback.print_exc()
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)

    def _save_checkpoint(self, is_final=False):
        prefix = "FINAL" if is_final else f"step_{self.global_step}"
        save_dir = os.path.dirname(self.model_save_path)
        os.makedirs(save_dir, exist_ok=True)
        print(
            f"[Trainer] Saving agent checkpoint ({prefix}) to: {self.model_save_path}"
        )
        try:
            agent_save_data = self.agent.get_state_dict()
            agent_save_data["global_step"] = self.global_step
            agent_save_data["episode_count"] = self.episode_count
            torch.save(agent_save_data, self.model_save_path)
            print(f"[Trainer] Agent checkpoint ({prefix}) saved.")
        except Exception as e:
            print(f"[Trainer] ERROR saving agent checkpoint ({prefix}): {e}")
            traceback.print_exc()
        print(
            f"[Trainer] Saving buffer state ({prefix}) to: {self.buffer_save_path} (Size: {len(self.buffer)})"
        )
        try:
            if hasattr(self.buffer, "flush_pending"):
                self.buffer.flush_pending()
            self.buffer.save_state(self.buffer_save_path)
            print(f"[Trainer] Buffer state ({prefix}) saved.")
        except Exception as e:
            print(f"[Trainer] ERROR saving buffer state ({prefix}): {e}")
            traceback.print_exc()

    def _update_beta(self) -> float:
        if not self.buffer_config.USE_PER:
            beta = 1.0
        else:
            start = self.buffer_config.PER_BETA_START
            end = 1.0
            frames = self.buffer_config.PER_BETA_FRAMES
            fraction = min(1.0, float(self.global_step) / max(1, frames))
            beta = start + fraction * (end - start)
            if hasattr(self.buffer, "set_beta"):
                self.buffer.set_beta(beta)
        self.stats_recorder.record_step({"beta": beta, "global_step": self.global_step})
        return beta

    # --- End Unchanged Methods ---

    def _collect_experience(self):
        """Collects one step of experience from each parallel environment."""
        actions: List[ActionType] = [-1] * self.num_envs
        step_rewards_batch = np.zeros(self.num_envs, dtype=np.float32)
        start_time = time.time()
        batch_chosen_shape_slots = np.full(self.num_envs, -1, dtype=np.int32)
        batch_shape_slot_max_qs = []

        # --- FIX: Access NUM_SHAPE_SLOTS via self.env_config ---
        num_slots = self.env_config.NUM_SHAPE_SLOTS
        # --- END FIX ---

        # 1. Select actions for all environments
        for i in range(self.num_envs):
            current_state_dict = self.current_states[i]
            valid_actions = self.envs[i].valid_actions()
            if not valid_actions:
                actions[i] = 0
                batch_chosen_shape_slots[i] = -1
                # --- FIX: Use num_slots variable ---
                batch_shape_slot_max_qs.append(
                    np.full(num_slots, -np.inf, dtype=np.float32)
                )
                # --- END FIX ---
            else:
                try:
                    actions[i] = self.agent.select_action(
                        current_state_dict, 0.0, valid_actions
                    )
                    chosen_slot, shape_qs, _ = (
                        self.agent.get_last_shape_selection_info()
                    )
                    batch_chosen_shape_slots[i] = (
                        chosen_slot if chosen_slot is not None else -1
                    )
                    # --- FIX: Use num_slots variable ---
                    qs_to_log = (
                        shape_qs if shape_qs is not None else [-np.inf] * num_slots
                    )
                    batch_shape_slot_max_qs.append(
                        np.array(qs_to_log[:num_slots], dtype=np.float32)
                    )
                    # --- END FIX ---
                except Exception as e:
                    print(f"ERROR: Agent select_action failed for env {i}: {e}")
                    traceback.print_exc()
                    actions[i] = random.choice(valid_actions)
                    batch_chosen_shape_slots[i] = -1
                    # --- FIX: Use num_slots variable ---
                    batch_shape_slot_max_qs.append(
                        np.full(num_slots, -np.inf, dtype=np.float32)
                    )
                    # --- END FIX ---

        # 2. Step all environments (Logic Unchanged)
        next_states_list: List[StateType] = [{} for _ in range(self.num_envs)]
        rewards_list = np.zeros(self.num_envs, dtype=np.float32)
        dones_list = np.zeros(self.num_envs, dtype=bool)

        for i in range(self.num_envs):
            env = self.envs[i]
            current_state_dict = self.current_states[i]
            action = actions[i]
            try:
                reward, done = env.step(action)
                next_state_dict = env.get_state()
                rewards_list[i] = reward
                dones_list[i] = done
                step_rewards_batch[i] = reward
                self.buffer.push(
                    current_state_dict, action, reward, next_state_dict, done
                )
                self.current_episode_scores[i] += reward
                self.current_episode_lengths[i] += 1
                if not done:
                    self.current_episode_game_scores[i] = env.game_score
                    self.current_episode_lines_cleared[i] = (
                        env.lines_cleared_this_episode
                    )
                if done:
                    self.episode_count += 1
                    self.stats_recorder.record_episode(
                        episode_score=self.current_episode_scores[i],
                        episode_length=self.current_episode_lengths[i],
                        episode_num=self.episode_count,
                        global_step=self.global_step + i + 1,
                        game_score=self.current_episode_game_scores[i],
                        lines_cleared=self.current_episode_lines_cleared[i],
                    )
                    next_states_list[i] = env.reset()
                    self.current_episode_scores[i] = 0.0
                    self.current_episode_lengths[i] = 0
                    self.current_episode_game_scores[i] = 0
                    self.current_episode_lines_cleared[i] = 0
                else:
                    next_states_list[i] = next_state_dict
            except Exception as e:
                print(f"ERROR: Env {i} step/reset failed (Action: {action}): {e}")
                traceback.print_exc()
                rewards_list[i] = self.reward_config.PENALTY_GAME_OVER
                dones_list[i] = True
                step_rewards_batch[i] = self.reward_config.PENALTY_GAME_OVER
                try:
                    crashed_state_reset = env.reset()
                except Exception as reset_e:
                    print(
                        f"FATAL: Env {i} failed to reset after crash: {reset_e}. Creating zero state."
                    )
                    grid_zeros = np.zeros(
                        self.env_config.GRID_STATE_SHAPE, dtype=np.float32
                    )
                    shape_zeros = np.zeros(
                        (
                            self.env_config.NUM_SHAPE_SLOTS,
                            self.env_config.SHAPE_FEATURES_PER_SHAPE,
                        ),
                        dtype=np.float32,
                    )
                    crashed_state_reset = {"grid": grid_zeros, "shapes": shape_zeros}
                self.buffer.push(
                    current_state_dict,
                    action,
                    rewards_list[i],
                    crashed_state_reset,
                    True,
                )
                self.episode_count += 1
                self.stats_recorder.record_episode(
                    episode_score=self.current_episode_scores[i] + rewards_list[i],
                    episode_length=self.current_episode_lengths[i] + 1,
                    episode_num=self.episode_count,
                    global_step=self.global_step + i + 1,
                    game_score=self.current_episode_game_scores[i],
                    lines_cleared=self.current_episode_lines_cleared[i],
                )
                next_states_list[i] = crashed_state_reset
                self.current_episode_scores[i] = 0.0
                self.current_episode_lengths[i] = 0
                self.current_episode_game_scores[i] = 0
                self.current_episode_lines_cleared[i] = 0

        self.current_states = next_states_list
        self.global_step += self.num_envs

        # 4. Record step statistics (including shape selection info)
        end_time = time.time()
        step_duration = end_time - start_time
        step_log_data = {
            "buffer_size": len(self.buffer),
            "global_step": self.global_step,
            "step_time": step_duration,
            "num_steps_processed": self.num_envs,
        }
        if self.tb_config.LOG_HISTOGRAMS:
            step_log_data["step_rewards_batch"] = step_rewards_batch
            step_log_data["action_batch"] = np.array(actions, dtype=np.int32)
            valid_chosen_slots = batch_chosen_shape_slots[
                batch_chosen_shape_slots != -1
            ]
            if len(valid_chosen_slots) > 0:
                step_log_data["chosen_shape_slot_batch"] = valid_chosen_slots
            if batch_shape_slot_max_qs:
                flat_shape_qs = np.concatenate(batch_shape_slot_max_qs)
                valid_flat_shape_qs = flat_shape_qs[np.isfinite(flat_shape_qs)]
                if len(valid_flat_shape_qs) > 0:
                    step_log_data["shape_slot_max_q_batch"] = valid_flat_shape_qs
        if self.tb_config.LOG_SHAPE_PLACEMENT_Q_VALUES:
            batch_q_vals = self.agent.get_last_batch_q_values_for_actions()
            if batch_q_vals is not None:
                step_log_data["batch_q_values_actions_taken"] = batch_q_vals

        self.stats_recorder.record_step(step_log_data)

    def _train_batch(self):
        """Samples a batch, computes loss, updates agent, and records stats."""
        if (
            len(self.buffer) < self.train_config.BATCH_SIZE
            or self.global_step < self.train_config.LEARN_START_STEP
        ):
            return

        beta = self._update_beta()
        indices, is_weights_np, batch_tuple = None, None, None

        try:
            if self.buffer_config.USE_PER:
                sample_result = self.buffer.sample(self.train_config.BATCH_SIZE)
                if sample_result:
                    batch_tuple, indices, is_weights_np = sample_result
                else:
                    print(
                        "Warning: Buffer sample returned None (PER). Skipping training step."
                    )
                    return
            else:  # Uniform sampling
                batch_tuple = self.buffer.sample(self.train_config.BATCH_SIZE)
                if batch_tuple is None:
                    print(
                        "Warning: Buffer sample returned None (Uniform). Skipping training step."
                    )
                    return
        except Exception as e:
            print(f"ERROR sampling buffer: {e}")
            traceback.print_exc()
            return

        # Compute loss and TD errors
        loss = torch.tensor(0.0)
        td_errors = None

        try:
            loss, td_errors = self.agent.compute_loss(
                batch_tuple, self.buffer_config.USE_N_STEP, is_weights_np
            )
        except Exception as e:
            print(f"ERROR computing loss: {e}")
            traceback.print_exc()
            return

        # Update agent
        grad_norm = None
        try:
            grad_norm = self.agent.update(loss)
            if grad_norm is None and self.gradient_clip_norm > 0:
                print("Warning: Agent update skipped due to likely gradient issue.")
                return
        except Exception as e:
            print(f"ERROR updating agent: {e}")
            traceback.print_exc()
            return

        # Update priorities in PER buffer
        if self.buffer_config.USE_PER and indices is not None and td_errors is not None:
            try:
                td_errors_np = ensure_numpy(td_errors)
                self.buffer.update_priorities(indices, td_errors_np)
            except Exception as e:
                print(f"ERROR updating PER priorities: {e}")
                traceback.print_exc()

        # Record training statistics
        current_lr = self.agent.optimizer.param_groups[0]["lr"]
        train_log_data = {
            "loss": loss.item(),
            "grad_norm": grad_norm if grad_norm is not None else 0.0,
            "avg_max_q": self.agent.get_last_avg_max_q(),
            "lr": current_lr,
            "global_step": self.global_step,
        }
        if self.tb_config.LOG_HISTOGRAMS and td_errors is not None:
            train_log_data["batch_td_errors"] = td_errors

        self.stats_recorder.record_step(train_log_data)

    def step(self):
        """Performs one full step: collect experience, train, update target net."""
        self._collect_experience()

        if (
            self.global_step >= self.train_config.LEARN_START_STEP
            and self.global_step % self.train_config.LEARN_FREQ == 0
        ):
            if len(self.buffer) >= self.train_config.BATCH_SIZE:
                self._train_batch()

        target_freq = self.dqn_config.TARGET_UPDATE_FREQ
        if target_freq > 0 and self.global_step > 0:
            steps_before_this_iter = self.global_step - self.num_envs
            if steps_before_this_iter // target_freq < self.global_step // target_freq:
                print(f"[Trainer] Updating target network at step {self.global_step}")
                self.agent.update_target_network()

        self.maybe_save_checkpoint()
        self._maybe_log_image()

    def _maybe_log_image(self):
        """Logs an image of a random environment state to TensorBoard periodically."""
        if not self.tb_config.LOG_IMAGES:
            return
        img_freq = self.tb_config.IMAGE_LOG_FREQ
        if img_freq <= 0:
            return
        steps_since_last = self.global_step - self.last_image_log_step
        if steps_since_last >= img_freq:
            try:
                env_idx = random.randint(0, self.num_envs - 1)
                img_array = self._get_env_image_as_numpy(env_idx)
                if img_array is not None:
                    img_tensor = torch.from_numpy(img_array).permute(
                        2, 0, 1
                    )  # HWC to CHW
                    self.stats_recorder.record_image(
                        f"Environment/Sample State Env {env_idx}",
                        img_tensor,
                        self.global_step,
                    )
                    self.last_image_log_step = self.global_step  # Update last log step
            except Exception as e:
                print(f"Error logging environment image: {e}")

    def _get_env_image_as_numpy(self, env_index: int) -> Optional[np.ndarray]:
        """Renders a single environment state to a NumPy array for logging."""
        if not (0 <= env_index < self.num_envs):
            return None
        env = self.envs[env_index]
        img_h = 300
        aspect_ratio = (self.env_config.COLS * 0.75 + 0.25) / max(
            1, self.env_config.ROWS
        )
        img_w = int(img_h * aspect_ratio)
        if img_w <= 0 or img_h <= 0:
            return None
        try:
            temp_surf = pygame.Surface((img_w, img_h))
            cell_w_px = img_w / (self.env_config.COLS * 0.75 + 0.25)
            cell_h_px = img_h / max(1, self.env_config.ROWS)
            temp_surf.fill(self.vis_config.BLACK)
            if hasattr(env, "grid") and hasattr(env.grid, "triangles"):
                for r in range(env.grid.rows):
                    for c in range(env.grid.cols):
                        if r < len(env.grid.triangles) and c < len(
                            env.grid.triangles[r]
                        ):
                            t = env.grid.triangles[r][c]
                            if t.is_death:
                                continue
                            pts = t.get_points(
                                ox=0, oy=0, cw=int(cell_w_px), ch=int(cell_h_px)
                            )
                            color = self.vis_config.GRAY
                            if t.is_occupied:
                                color = t.color if t.color else self.vis_config.RED
                            pygame.draw.polygon(temp_surf, color, pts)
            img_array = pygame.surfarray.array3d(temp_surf)
            return np.transpose(img_array, (1, 0, 2))  # W, H, C -> H, W, C
        except Exception as e:
            print(f"Error generating environment image for TB: {e}")
            traceback.print_exc()
            return None

    def maybe_save_checkpoint(self, force_save=False):
        """Saves a checkpoint periodically or if forced."""
        save_freq = self.train_config.CHECKPOINT_SAVE_FREQ
        if save_freq <= 0 and not force_save:
            return
        should_save_freq = (
            save_freq > 0
            and self.global_step > 0
            and (
                self.global_step // save_freq
                > (self.global_step - self.num_envs) // save_freq
            )
        )
        if force_save or should_save_freq:
            self._save_checkpoint(is_final=False)

    def train_loop(self):
        """Main training loop that runs until total steps are reached."""
        print("[Trainer] Starting training loop...")
        try:
            while self.global_step < TOTAL_TRAINING_STEPS:
                self.step()
        except KeyboardInterrupt:
            print("\n[Trainer] Training loop interrupted by user (Ctrl+C).")
        except Exception as e:
            print(f"\n[Trainer] CRITICAL ERROR in training loop: {e}")
            traceback.print_exc()
        finally:
            print("[Trainer] Training loop finished or terminated.")
            self.cleanup(save_final=True)

    def cleanup(self, save_final: bool = True):
        """Cleans up resources, optionally saving a final checkpoint."""
        print("[Trainer] Cleaning up resources...")
        if (
            hasattr(self, "buffer")
            and self.buffer
            and hasattr(self.buffer, "flush_pending")
        ):
            print("[Trainer] Flushing pending N-step transitions...")
            try:
                self.buffer.flush_pending()
            except Exception as flush_e:
                print(f"Error flushing buffer during cleanup: {flush_e}")
        if save_final:
            print("[Trainer] Saving final checkpoint...")
            self._save_checkpoint(is_final=True)
        else:
            print("[Trainer] Skipping final save as requested.")
        if (
            hasattr(self, "stats_recorder")
            and self.stats_recorder
            and hasattr(self.stats_recorder, "close")
        ):
            try:
                self.stats_recorder.close()
            except Exception as close_e:
                print(f"Error closing stats recorder during cleanup: {close_e}")
        print("[Trainer] Cleanup complete.")


File: utils/__init__.py


File: utils/init_checks.py
# File: utils/init_checks.py
# --- Pre-Run Sanity Checks ---
import sys
import traceback
import numpy as np
from config import EnvConfig  # Import config class
from environment.game_state import GameState


def run_pre_checks() -> bool:
    """Performs basic checks on GameState and configuration compatibility."""
    print("--- Pre-Run Checks ---")
    try:
        print("Checking GameState and Configuration Compatibility...")
        env_config_instance = EnvConfig()  # Instantiate

        gs_test = GameState()
        gs_test.reset()
        s_test_dict = gs_test.get_state()

        if not isinstance(s_test_dict, dict):
            raise TypeError(
                f"GameState.get_state() should return a dict, but got {type(s_test_dict)}"
            )
        print("GameState state type check PASSED (returned dict).")

        # --- MODIFIED: Check grid shape (2 channels) ---
        if "grid" not in s_test_dict:
            raise KeyError("State dictionary missing 'grid' key.")
        grid_state = s_test_dict["grid"]
        expected_grid_shape = (
            env_config_instance.GRID_STATE_SHAPE
        )  # Uses property (2, H, W)
        if not isinstance(grid_state, np.ndarray):
            raise TypeError(
                f"State 'grid' component should be numpy array, but got {type(grid_state)}"
            )
        if grid_state.shape != expected_grid_shape:
            raise ValueError(
                f"State 'grid' shape mismatch! GameState:{grid_state.shape}, EnvConfig:{expected_grid_shape}"
            )
        print(f"GameState 'grid' state shape check PASSED (Shape: {grid_state.shape}).")
        # --- END MODIFIED ---

        # Check 'shapes' component (unchanged)
        if "shapes" not in s_test_dict:
            raise KeyError("State dictionary missing 'shapes' key.")
        shape_state = s_test_dict["shapes"]
        expected_shape_shape = (
            env_config_instance.NUM_SHAPE_SLOTS,
            env_config_instance.SHAPE_FEATURES_PER_SHAPE,
        )
        if not isinstance(shape_state, np.ndarray):
            raise TypeError(
                f"State 'shapes' component should be numpy array, but got {type(shape_state)}"
            )
        if shape_state.shape != expected_shape_shape:
            raise ValueError(
                f"State 'shapes' shape mismatch! GameState:{shape_state.shape}, EnvConfig:{expected_shape_shape}"
            )
        print(
            f"GameState 'shapes' state shape check PASSED (Shape: {shape_state.shape})."
        )

        _ = gs_test.valid_actions()
        print("GameState valid_actions check PASSED.")
        if not hasattr(gs_test, "game_score"):
            raise AttributeError("GameState missing 'game_score' attribute!")
        print("GameState 'game_score' attribute check PASSED.")
        if not hasattr(gs_test, "lines_cleared_this_episode"):
            raise AttributeError(
                "GameState missing 'lines_cleared_this_episode' attribute!"
            )
        print("GameState 'lines_cleared_this_episode' attribute check PASSED.")

        del gs_test
        print("--- Pre-Run Checks Complete ---")
        return True
    except (NameError, ImportError) as e:
        print(f"FATAL ERROR: Import/Name error: {e}")
    except (ValueError, AttributeError, TypeError, KeyError) as e:
        print(f"FATAL ERROR during pre-run checks: {e}")
    except Exception as e:
        print(f"FATAL ERROR during GameState pre-check: {e}")
        traceback.print_exc()
    sys.exit(1)


File: utils/types.py
# File: utils/types.py
from typing import NamedTuple, Union, Tuple, List, Dict, Any, Optional
import numpy as np
import torch

# --- MODIFIED: StateType is now Dict ---
# from environment.game_state import StateType # Import from env if defined there
StateType = Dict[str, np.ndarray]  # e.g., {"grid": ndarray, "shapes": ndarray}
# --- END MODIFIED ---

# Type alias for action
ActionType = int


class Transition(NamedTuple):
    state: StateType  # State is now a Dict
    action: ActionType
    reward: float
    next_state: StateType  # Next state is now a Dict
    done: bool
    n_step_discount: Optional[float] = None


# --- Batch Types (Numpy) ---
# States and next_states are now lists of dictionaries
NumpyStateBatch = List[StateType]
NumpyActionBatch = np.ndarray
NumpyRewardBatch = np.ndarray
NumpyDoneBatch = np.ndarray
NumpyDiscountBatch = np.ndarray  # For N-step

# Standard 1-step batch
NumpyBatch = Tuple[
    NumpyStateBatch, NumpyActionBatch, NumpyRewardBatch, NumpyStateBatch, NumpyDoneBatch
]

# N-step batch
NumpyNStepBatch = Tuple[
    NumpyStateBatch,
    NumpyActionBatch,
    NumpyRewardBatch,
    NumpyStateBatch,
    NumpyDoneBatch,
    NumpyDiscountBatch,
]

# Prioritized 1-step batch
PrioritizedNumpyBatch = Tuple[NumpyBatch, np.ndarray, np.ndarray]
# ((s_dicts, a, r, ns_dicts, d), indices, weights)

# Prioritized N-step batch
PrioritizedNumpyNStepBatch = Tuple[NumpyNStepBatch, np.ndarray, np.ndarray]
# ((s_dicts, a, rn, nsn_dicts, dn, gamman), indices, weights)


# --- Batch Types (Tensor) ---
# State tensors are now tuples (grid_tensor, shape_tensor)
TensorStateBatch = Tuple[torch.Tensor, torch.Tensor]
TensorActionBatch = torch.Tensor
TensorRewardBatch = torch.Tensor
TensorDoneBatch = torch.Tensor
TensorDiscountBatch = torch.Tensor  # For N-step
TensorWeightsBatch = torch.Tensor  # For PER

# Standard 1-step batch
TensorBatch = Tuple[
    TensorStateBatch,
    TensorActionBatch,
    TensorRewardBatch,
    TensorStateBatch,
    TensorDoneBatch,
]

# N-step batch
TensorNStepBatch = Tuple[
    TensorStateBatch,
    TensorActionBatch,
    TensorRewardBatch,
    TensorStateBatch,
    TensorDoneBatch,
    TensorDiscountBatch,
]


# --- Agent State ---
AgentStateDict = Dict[str, Any]  # Remains the same


File: utils/helpers.py
# File: utils/helpers.py
import torch
import numpy as np
import random
import os
import pickle
import cloudpickle
from typing import Union, Any


def get_device() -> torch.device:
    """Gets the appropriate torch device (MPS, CUDA, or CPU)."""
    force_cpu = os.environ.get("FORCE_CPU", "false").lower() == "true"
    if force_cpu:
        print("Forcing CPU device based on environment variable.")
        return torch.device("cpu")

    if torch.backends.mps.is_available():
        device_str = "mps"
    elif torch.cuda.is_available():
        device_str = "cuda"
    else:
        device_str = "cpu"

    print(f"Using device: {device_str.upper()}")
    if device_str == "cuda":
        print(f"CUDA Device Name: {torch.cuda.get_device_name(0)}")
    elif device_str == "mps":
        print("MPS device found on MacOS.")
    return torch.device(device_str)


def set_random_seeds(seed: int = 42):
    """Sets random seeds for Python, NumPy, and PyTorch."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # Note: Setting deterministic algorithms can impact performance
        # torch.backends.cudnn.deterministic = True
        # torch.backends.cudnn.benchmark = False
    print(f"Set random seeds to {seed}")


def ensure_numpy(data: Union[np.ndarray, list, tuple, torch.Tensor]) -> np.ndarray:
    """Ensures the input data is a numpy array with float32 type."""
    try:
        if isinstance(data, np.ndarray):
            if data.dtype != np.float32:
                return data.astype(np.float32)
            return data
        elif isinstance(data, torch.Tensor):
            return data.detach().cpu().numpy().astype(np.float32)
        elif isinstance(data, (list, tuple)):
            arr = np.array(data, dtype=np.float32)
            if arr.dtype == np.object_:  # Indicates ragged array
                raise ValueError(
                    "Cannot convert ragged list/tuple to float32 numpy array."
                )
            return arr
        else:
            # Attempt conversion for single numbers or other types
            return np.array([data], dtype=np.float32)
    except (ValueError, TypeError, RuntimeError) as e:
        print(
            f"CRITICAL ERROR in ensure_numpy conversion: {e}. Input type: {type(data)}. Data (partial): {str(data)[:100]}"
        )
        raise ValueError(f"ensure_numpy failed: {e}") from e


def save_object(obj: Any, filepath: str):
    """Saves an arbitrary Python object to a file using cloudpickle."""
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, "wb") as f:
            cloudpickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)
    except Exception as e:
        print(f"Error saving object to {filepath}: {e}")
        raise e  # Re-raise after logging


def load_object(filepath: str) -> Any:
    """Loads a Python object from a file using cloudpickle."""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found for loading: {filepath}")
    try:
        with open(filepath, "rb") as f:
            obj = cloudpickle.load(f)
        return obj
    except Exception as e:
        print(f"Error loading object from {filepath}: {e}")
        raise e  # Re-raise after logging


File: agent/__init__.py


File: agent/dqn_agent.py
# File: agent/dqn_agent.py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
import random
import math
import traceback
from typing import Tuple, List, Dict, Any, Optional, Union

# --- MODIFIED: Import EnvConfig directly ---
from config import ModelConfig, EnvConfig, DQNConfig, DEVICE, TensorBoardConfig

# --- END MODIFIED ---
from agent.model_factory import create_network
from environment.game_state import StateType  # Use the Dict type from env
from utils.types import (
    ActionType,
    NumpyBatch,
    NumpyNStepBatch,
    AgentStateDict,
    TensorBatch,
    TensorNStepBatch,
)
from utils.helpers import ensure_numpy
from agent.networks.noisy_layer import NoisyLinear


class DQNAgent:
    """DQN Agent implementing two-stage action selection (shape -> placement)."""

    def __init__(
        self, config: ModelConfig, dqn_config: DQNConfig, env_config: EnvConfig
    ):
        print("[DQNAgent] Initializing...")
        self.device = DEVICE
        self.env_config = env_config  # Store instance
        self.action_dim = env_config.ACTION_DIM  # Total output dimension
        self.rows = env_config.ROWS
        self.cols = env_config.COLS
        self.num_shape_slots = env_config.NUM_SHAPE_SLOTS
        self.locations_per_shape = self.rows * self.cols

        self.gamma = dqn_config.GAMMA
        self.use_double_dqn = dqn_config.USE_DOUBLE_DQN
        self.gradient_clip_norm = dqn_config.GRADIENT_CLIP_NORM
        self.use_noisy_nets = dqn_config.USE_NOISY_NETS
        self.use_dueling = dqn_config.USE_DUELING
        self.use_distributional = dqn_config.USE_DISTRIBUTIONAL
        self.v_min = dqn_config.V_MIN
        self.v_max = dqn_config.V_MAX
        self.num_atoms = dqn_config.NUM_ATOMS
        self.dqn_config = dqn_config
        # --- NEW: Reference TensorBoard config for logging flags ---
        self.tb_config = TensorBoardConfig()
        # --- END NEW ---

        if self.use_distributional:
            if self.num_atoms <= 1:
                raise ValueError("NUM_ATOMS must be >= 2 for Distributional RL")
            self.support = torch.linspace(
                self.v_min, self.v_max, self.num_atoms, device=self.device
            )
            self.delta_z = (self.v_max - self.v_min) / max(1, self.num_atoms - 1)

        self.online_net = create_network(
            env_config=self.env_config,  # Pass instance
            action_dim=self.action_dim,  # Still use full output dim
            model_config=config,
            dqn_config=dqn_config,
        ).to(self.device)
        self.target_net = create_network(
            env_config=self.env_config,  # Pass instance
            action_dim=self.action_dim,
            model_config=config,
            dqn_config=dqn_config,
        ).to(self.device)

        print(
            f"[DQNAgent] Initial online_net device: {next(self.online_net.parameters()).device}"
        )
        print(
            f"[DQNAgent] Initial target_net device: {next(self.target_net.parameters()).device}"
        )
        self.target_net.load_state_dict(self.online_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.AdamW(
            self.online_net.parameters(),
            lr=dqn_config.LEARNING_RATE,
            eps=dqn_config.ADAM_EPS,
            weight_decay=1e-5,
        )

        self.scheduler = None
        if dqn_config.USE_LR_SCHEDULER:
            print(
                f"[DQNAgent] Using CosineAnnealingLR scheduler (T_max={dqn_config.LR_SCHEDULER_T_MAX}, eta_min={dqn_config.LR_SCHEDULER_ETA_MIN})"
            )
            self.scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=dqn_config.LR_SCHEDULER_T_MAX,
                eta_min=dqn_config.LR_SCHEDULER_ETA_MIN,
            )

        if not self.use_distributional:
            self.loss_fn = nn.SmoothL1Loss(reduction="none", beta=1.0)

        # --- Internal state for logging ---
        self._last_avg_max_q: float = 0.0
        self._last_chosen_shape_slot: Optional[int] = None
        self._last_shape_slot_max_q_values: Optional[List[float]] = None
        self._last_placement_q_values_for_chosen_shape: Optional[List[float]] = None
        # --- End Internal state ---

        self._print_init_info(dqn_config)

    def _print_init_info(self, dqn_config: DQNConfig):
        # (No changes needed here)
        print(f"[DQNAgent] Using Device: {self.device}")
        print(f"[DQNAgent] Online Network: {type(self.online_net).__name__}")
        print(f"[DQNAgent] Using Double DQN: {self.use_double_dqn}")
        print(f"[DQNAgent] Using Dueling: {self.use_dueling}")
        print(f"[DQNAgent] Using Noisy Nets: {self.use_noisy_nets}")
        print(f"[DQNAgent] Using Distributional (C51): {self.use_distributional}")
        if self.use_distributional:
            print(
                f"  - Atoms: {self.num_atoms}, Vmin: {self.v_min}, Vmax: {self.v_max}"
            )
        print(
            f"[DQNAgent] Using LR Scheduler: {self.dqn_config.USE_LR_SCHEDULER}"
            + (
                f" (T_max={self.dqn_config.LR_SCHEDULER_T_MAX}, eta_min={self.dqn_config.LR_SCHEDULER_ETA_MIN})"
                if self.dqn_config.USE_LR_SCHEDULER
                else ""
            )
        )
        print(
            f"[DQNAgent] Optimizer: AdamW (LR={dqn_config.LEARNING_RATE}, EPS={dqn_config.ADAM_EPS})"
        )
        total_params = sum(
            p.numel() for p in self.online_net.parameters() if p.requires_grad
        )
        print(f"[DQNAgent] Trainable Parameters: {total_params / 1e6:.2f} M")

    # --- MODIFIED: select_action implements two-stage logic ---
    @torch.no_grad()
    def select_action(
        self, state: StateType, epsilon: float, valid_actions_indices: List[ActionType]
    ) -> ActionType:
        """
        Selects action using a two-stage process based on Q-values:
        1. Find the best Q-value achievable for each available shape slot across its valid placements.
        2. Choose the shape slot with the highest maximum Q-value.
        3. Choose the placement corresponding to that maximum Q-value for the chosen shape.
        Applies epsilon-greedy if not using noisy nets.
        Logs intermediate values for debugging/visualization.
        """
        # Reset logging state
        self._last_chosen_shape_slot = None
        self._last_shape_slot_max_q_values = [-float("inf")] * self.num_shape_slots
        self._last_placement_q_values_for_chosen_shape = None

        # Fallback if no valid actions provided by environment
        if not valid_actions_indices:
            # print("Warning: No valid actions provided to select_action. Returning action 0.")
            self._last_avg_max_q = -float("inf")  # Indicate no valid Q found
            return 0  # Default action

        # Epsilon-greedy exploration (only if not using noisy nets)
        if not self.use_noisy_nets and random.random() < epsilon:
            self._last_avg_max_q = -float("inf")  # Exploration, no meaningful Q
            return random.choice(valid_actions_indices)

        # --- Get Q-values from network ---
        grid_np = ensure_numpy(state["grid"])
        shapes_np = ensure_numpy(state["shapes"])
        grid_t = torch.tensor(
            grid_np, dtype=torch.float32, device=self.device
        ).unsqueeze(0)
        shapes_t = torch.tensor(
            shapes_np, dtype=torch.float32, device=self.device
        ).unsqueeze(0)

        model_device = next(self.online_net.parameters()).device
        if grid_t.device != model_device:
            grid_t = grid_t.to(model_device)
        if shapes_t.device != model_device:
            shapes_t = shapes_t.to(model_device)

        self.online_net.eval()
        dist_or_q = self.online_net(
            grid_t, shapes_t
        )  # Shape [1, action_dim] or [1, action_dim, atoms]

        if self.use_distributional:
            probabilities = F.softmax(dist_or_q, dim=2)
            q_values = (probabilities * self.support).sum(
                dim=2
            )  # Shape: [1, action_dim]
        else:
            q_values = dist_or_q  # Shape: [1, action_dim]

        q_values_np = q_values.squeeze(0).cpu().numpy()  # Shape [action_dim]

        # --- Two-Stage Selection Logic ---
        best_overall_q = -float("inf")
        best_shape_slot = -1
        best_placement_idx = -1  # Placement index within 0 to ROWS*COLS-1
        placement_qs_for_best_shape: List[float] = []

        # Group valid actions by shape slot
        valid_actions_by_slot: Dict[int, List[int]] = {
            i: [] for i in range(self.num_shape_slots)
        }
        for action_idx in valid_actions_indices:
            s_idx = action_idx // self.locations_per_shape
            p_idx = action_idx % self.locations_per_shape
            if 0 <= s_idx < self.num_shape_slots:
                valid_actions_by_slot[s_idx].append(p_idx)

        # Find best action per shape slot
        for s_idx in range(self.num_shape_slots):
            valid_placements_for_slot = valid_actions_by_slot[s_idx]
            if not valid_placements_for_slot:
                # self._last_shape_slot_max_q_values remains -inf
                continue  # No valid placements for this shape

            # Get global indices for this slot's valid placements
            global_indices = [
                s_idx * self.locations_per_shape + p_idx
                for p_idx in valid_placements_for_slot
            ]
            q_values_for_valid_placements = q_values_np[global_indices]

            if (
                q_values_for_valid_placements.size == 0
            ):  # Should not happen if valid_placements_for_slot is not empty
                continue

            max_q_for_slot = np.max(q_values_for_valid_placements)
            best_placement_idx_for_slot = valid_placements_for_slot[
                np.argmax(q_values_for_valid_placements)
            ]

            self._last_shape_slot_max_q_values[s_idx] = float(max_q_for_slot)

            # Update overall best if this shape slot is better
            if max_q_for_slot > best_overall_q:
                best_overall_q = max_q_for_slot
                best_shape_slot = s_idx
                best_placement_idx = best_placement_idx_for_slot
                # Store Q values for all possible placements of this *potentially* best shape
                # (for logging) - Extract Qs for all placements (0 to R*C-1) for this slot
                start_g_idx = s_idx * self.locations_per_shape
                end_g_idx = start_g_idx + self.locations_per_shape
                placement_qs_for_best_shape = q_values_np[
                    start_g_idx:end_g_idx
                ].tolist()

        # --- Final Action Selection & Logging ---
        if best_shape_slot != -1:
            # Found a valid action
            final_action = (
                best_shape_slot * self.locations_per_shape + best_placement_idx
            )
            self._last_avg_max_q = float(best_overall_q)
            self._last_chosen_shape_slot = best_shape_slot
            # Store placement Qs only for the finally chosen shape
            self._last_placement_q_values_for_chosen_shape = placement_qs_for_best_shape

            # Sanity check: Ensure the chosen action was in the original valid list
            if final_action not in valid_actions_indices:
                print(
                    f"CRITICAL WARNING: Chosen action {final_action} (Shape:{best_shape_slot}, Place:{best_placement_idx}) not in original valid_actions list! Falling back."
                )
                # Fallback to random valid action to prevent crash
                self._last_avg_max_q = -float("inf")
                self._last_chosen_shape_slot = None
                self._last_placement_q_values_for_chosen_shape = None
                return random.choice(valid_actions_indices)

            return final_action
        else:
            # No valid action found across all shapes (should match initial check)
            # print("Warning: No valid action found during two-stage selection. Returning 0.")
            self._last_avg_max_q = -float("inf")
            return 0  # Fallback

    # --- END MODIFIED ---

    def _np_batch_to_tensor(
        self, batch: Union[NumpyBatch, NumpyNStepBatch], is_n_step: bool
    ) -> Union[TensorBatch, TensorNStepBatch]:
        """Converts numpy batch tuple (where states are dicts) to tensor tuple."""

        if is_n_step:
            states_dicts, actions, rewards, next_states_dicts, dones, discounts = batch
        else:
            states_dicts, actions, rewards, next_states_dicts, dones = batch[:5]
            discounts = None

        # Process states and next_states (lists of dicts)
        # Ensure grid features match the expected 2 channels
        grid_states = np.array([s["grid"] for s in states_dicts], dtype=np.float32)
        shape_states = np.array([s["shapes"] for s in states_dicts], dtype=np.float32)
        grid_next_states = np.array(
            [ns["grid"] for ns in next_states_dicts], dtype=np.float32
        )
        shape_next_states = np.array(
            [ns["shapes"] for ns in next_states_dicts], dtype=np.float32
        )

        # Validate grid state channel count
        expected_channels = self.env_config.GRID_FEATURES_PER_CELL
        if (
            grid_states.shape[1] != expected_channels
            or grid_next_states.shape[1] != expected_channels
        ):
            raise ValueError(
                f"Batch grid state channel mismatch! Expected {expected_channels}, got {grid_states.shape[1]} and {grid_next_states.shape[1]}. Check env config and state generation."
            )

        # Convert components to tensors
        grid_s_t = torch.tensor(grid_states, device=self.device, dtype=torch.float32)
        shape_s_t = torch.tensor(shape_states, device=self.device, dtype=torch.float32)
        a_t = torch.tensor(actions, device=self.device, dtype=torch.long).unsqueeze(1)
        r_t = torch.tensor(rewards, device=self.device, dtype=torch.float32).unsqueeze(
            1
        )
        grid_ns_t = torch.tensor(
            grid_next_states, device=self.device, dtype=torch.float32
        )
        shape_ns_t = torch.tensor(
            shape_next_states, device=self.device, dtype=torch.float32
        )
        d_t = torch.tensor(dones, device=self.device, dtype=torch.float32).unsqueeze(1)

        states_t = (grid_s_t, shape_s_t)
        next_states_t = (grid_ns_t, shape_ns_t)

        if is_n_step:
            disc_t = torch.tensor(
                discounts, device=self.device, dtype=torch.float32
            ).unsqueeze(1)
            return states_t, a_t, r_t, next_states_t, d_t, disc_t
        else:
            return states_t, a_t, r_t, next_states_t, d_t

    # --- MODIFIED: Added placeholder for action masking in target ---
    @torch.no_grad()
    def _get_target_distribution(
        self, batch: Union[TensorBatch, TensorNStepBatch], is_n_step: bool
    ) -> torch.Tensor:
        """Calculates the target distribution for C51 using Double DQN logic."""
        # --- THIS FUNCTION IS UNUSED IF USE_DISTRIBUTIONAL is False ---
        if not self.use_distributional:
            raise RuntimeError(
                "_get_target_distribution called when use_distributional is False"
            )

        if is_n_step:
            _, _, rewards, next_states_tuple, dones, discounts = batch
        else:
            _, _, rewards, next_states_tuple, dones = batch[:5]
            discounts = torch.full_like(rewards, self.gamma)

        next_grids, next_shapes = next_states_tuple
        batch_size = next_grids.size(0)

        # Double DQN: Use online net to select best action, target net to evaluate
        self.online_net.eval()
        online_next_dist_logits = self.online_net(
            next_grids, next_shapes
        )  # [B, A, N_atoms]
        online_next_probs = F.softmax(online_next_dist_logits, dim=2)
        online_expected_q = (online_next_probs * self.support).sum(dim=2)  # [B, A]

        # --- MASKING NEEDED HERE ---
        # Ideally, mask online_expected_q with valid actions for next_states
        # For now, we proceed without masking, acknowledging the limitation.
        # TODO: Implement masking if performance requires it.
        best_next_actions = online_expected_q.argmax(dim=1)  # [B]
        # --- END MASKING NOTE ---

        self.target_net.eval()
        target_next_dist_logits = self.target_net(
            next_grids, next_shapes
        )  # [B, A, N_atoms]
        target_next_probs = F.softmax(target_next_dist_logits, dim=2)  # [B, A, N_atoms]
        # Gather the distributions corresponding to the best actions selected by online net
        target_next_best_dist_probs = target_next_probs[
            torch.arange(batch_size), best_next_actions
        ]  # [B, N_atoms]

        # Project the target distribution
        Tz = rewards + discounts * self.support.unsqueeze(0) * (
            1.0 - dones
        )  # [B, N_atoms]
        Tz = Tz.clamp(self.v_min, self.v_max)

        b = (Tz - self.v_min) / self.delta_z
        lower_idx = b.floor().long()
        upper_idx = b.ceil().long()
        lower_idx[(upper_idx > 0) & (lower_idx == upper_idx)] -= 1
        upper_idx[(lower_idx < (self.num_atoms - 1)) & (lower_idx == upper_idx)] += 1
        lower_idx = lower_idx.clamp(0, self.num_atoms - 1)
        upper_idx = upper_idx.clamp(0, self.num_atoms - 1)

        weight_u = b - lower_idx.float()
        weight_l = 1.0 - weight_u

        target_dist = torch.zeros_like(target_next_best_dist_probs)  # [B, N_atoms]
        # Using index_add_ for projection (safer) - requires careful indexing
        # Flatten indices and values for batch operation
        batch_indices_l = (
            torch.arange(batch_size, device=self.device) * self.num_atoms
        ).unsqueeze(1) + lower_idx
        batch_indices_u = (
            torch.arange(batch_size, device=self.device) * self.num_atoms
        ).unsqueeze(1) + upper_idx
        values_l = (target_next_best_dist_probs * weight_l).view(-1)
        values_u = (target_next_best_dist_probs * weight_u).view(-1)

        target_dist.view(-1).index_add_(0, batch_indices_l.view(-1), values_l)
        target_dist.view(-1).index_add_(0, batch_indices_u.view(-1), values_u)

        return target_dist

    # --- END MODIFIED ---

    # --- MODIFIED: Log Q-values for chosen actions if enabled ---
    def compute_loss(
        self,
        batch: Union[NumpyBatch, NumpyNStepBatch],
        is_n_step: bool,
        is_weights: Optional[np.ndarray] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Computes the loss (SmoothL1) and TD errors."""
        if self.use_distributional:
            raise NotImplementedError(
                "Distributional loss calculation needs review with two-stage selection"
            )

        tensor_batch = self._np_batch_to_tensor(batch, is_n_step)
        states_tuple, actions, rewards, next_states_tuple, dones = tensor_batch[:5]
        grids, shapes = states_tuple
        next_grids, next_shapes = next_states_tuple
        batch_size = grids.size(0)  # Get batch size

        td_errors = None

        # --- Standard DQN (Non-distributional) ---
        discounts = (
            tensor_batch[5]  # Use pre-calculated gamma^n from NStepBatch
            if is_n_step
            else torch.full_like(rewards, self.gamma)  # Use standard gamma
        )

        # Target Q-value calculation (Double DQN)
        with torch.no_grad():
            self.online_net.eval()
            online_next_q = self.online_net(next_grids, next_shapes)  # [B, A]
            # --- MASKING NEEDED HERE ---
            # TODO: Implement masking if performance requires it.
            best_next_actions = online_next_q.argmax(dim=1, keepdim=True)  # [B, 1]
            # --- END MASKING NOTE ---

            self.target_net.eval()
            target_next_q_values = self.target_net(next_grids, next_shapes)  # [B, A]
            target_q_for_best_actions = target_next_q_values.gather(
                1, best_next_actions
            )  # [B, 1]

            # Calculate TD target
            target_q = rewards + discounts * target_q_for_best_actions * (
                1.0 - dones
            )  # [B, 1]

        # Current Q-value calculation
        self.online_net.train()
        current_q_all_actions = self.online_net(grids, shapes)  # [B, A]
        current_q = current_q_all_actions.gather(1, actions)  # [B, 1]

        # Calculate element-wise loss (Huber)
        elementwise_loss = self.loss_fn(current_q, target_q)  # [B, 1]
        td_errors = (target_q - current_q).abs().detach()  # [B, 1]

        # --- Logging Q-values (Optional) ---
        # Store batch average max Q for simple logging
        with torch.no_grad():
            self.online_net.eval()
            # TODO: Apply masking here if possible for a more accurate avg_max_q
            self._last_avg_max_q = (
                self.online_net(grids, shapes).max(dim=1)[0].mean().item()
            )

        # --- Store Q-values for actions taken in the batch (for potential histogram) ---
        if self.tb_config.LOG_SHAPE_PLACEMENT_Q_VALUES:
            # Detach and store Q-values for the specific actions taken in this batch
            self._batch_q_values_for_actions_taken = (
                current_q.detach().squeeze().cpu().numpy()
            )
        else:
            self._batch_q_values_for_actions_taken = None
        # --- End Logging Q-values ---

        # Apply PER weights and calculate final loss
        if is_weights is not None:
            is_weights_t = torch.tensor(
                is_weights, dtype=torch.float32, device=self.device
            ).unsqueeze(1)
            loss = (is_weights_t * elementwise_loss).mean()
        else:
            loss = elementwise_loss.mean()

        td_errors_for_per = td_errors.squeeze() if td_errors is not None else None

        return loss, td_errors_for_per

    # --- END MODIFIED ---

    def update(self, loss: torch.Tensor) -> Optional[float]:
        """Performs optimizer step, gradient clipping, scheduler step, and noise reset."""
        grad_norm = None
        self.optimizer.zero_grad(set_to_none=True)
        loss.backward()
        self.online_net.train()

        if self.gradient_clip_norm > 0:
            try:
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    self.online_net.parameters(),
                    max_norm=self.gradient_clip_norm,
                    error_if_nonfinite=True,
                ).item()
            except RuntimeError as clip_err:
                print(f"ERROR: Gradient clipping failed: {clip_err}")
                return None
            except Exception as clip_err:
                print(f"Warning: Error during gradient clipping: {clip_err}")
                grad_norm = None

        self.optimizer.step()

        if self.scheduler:
            self.scheduler.step()
        if self.use_noisy_nets:
            self.online_net.reset_noise()
            self.target_net.reset_noise()
        return grad_norm

    # --- NEW: Methods to retrieve logging info ---
    def get_last_avg_max_q(self) -> float:
        """Returns the average max Q value computed during the last loss calculation."""
        return self._last_avg_max_q

    def get_last_shape_selection_info(
        self,
    ) -> Tuple[Optional[int], Optional[List[float]], Optional[List[float]]]:
        """Returns info logged during the last select_action call."""
        return (
            self._last_chosen_shape_slot,
            self._last_shape_slot_max_q_values,
            self._last_placement_q_values_for_chosen_shape,
        )

    def get_last_batch_q_values_for_actions(self) -> Optional[np.ndarray]:
        """Returns the Q-values for actions taken in the last processed batch."""
        return getattr(self, "_batch_q_values_for_actions_taken", None)

    # --- END NEW ---

    def update_target_network(self):
        """Copies weights from the online network to the target network."""
        self.target_net.load_state_dict(self.online_net.state_dict())
        self.target_net.eval()

    def get_state_dict(self) -> AgentStateDict:
        """Returns the agent's state including networks, optimizer, and scheduler."""
        self.online_net.cpu()
        self.target_net.cpu()

        optim_state_cpu = {}
        if hasattr(self.optimizer, "state") and self.optimizer.state:
            for group in self.optimizer.param_groups:
                for p in group["params"]:
                    if p in self.optimizer.state:
                        state = self.optimizer.state[p]
                        for k, v in state.items():
                            if isinstance(v, torch.Tensor):
                                optim_state_cpu[id(p), k] = v.cpu()
                                state[k] = v.cpu()

        state = {
            "online_net_state_dict": self.online_net.state_dict(),
            "target_net_state_dict": self.target_net.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": (
                self.scheduler.state_dict() if self.scheduler else None
            ),
        }

        # Restore optimizer state
        if hasattr(self.optimizer, "state") and self.optimizer.state:
            for group in self.optimizer.param_groups:
                for p in group["params"]:
                    if p in self.optimizer.state:
                        state_opt = self.optimizer.state[p]
                        for k, v_cpu in state_opt.items():
                            if (
                                isinstance(v_cpu, torch.Tensor)
                                and (id(p), k) in optim_state_cpu
                            ):
                                original_tensor = optim_state_cpu[(id(p), k)].to(
                                    self.device
                                )
                                state_opt[k] = original_tensor

        self.online_net.to(self.device)
        self.target_net.to(self.device)

        return state

    def load_state_dict(self, state_dict: AgentStateDict):
        """Loads the agent's state from a state dictionary."""
        print(f"[DQNAgent] Loading state dict. Target device: {self.device}")
        try:
            self.online_net.load_state_dict(
                state_dict["online_net_state_dict"], strict=False
            )
            print("[DQNAgent] online_net state_dict loaded (strict=False).")
        except Exception as e:
            print(f"ERROR loading online_net state_dict: {e}")
            traceback.print_exc()
            raise

        if "target_net_state_dict" in state_dict:
            try:
                self.target_net.load_state_dict(
                    state_dict["target_net_state_dict"], strict=False
                )
                print("[DQNAgent] target_net state_dict loaded (strict=False).")
            except Exception as e:
                print(f"ERROR loading target_net state_dict: {e}. Copying from online.")
                self.target_net.load_state_dict(self.online_net.state_dict())
        else:
            print(
                "Warning: Target net state missing in checkpoint, copying from online."
            )
            self.target_net.load_state_dict(self.online_net.state_dict())

        self.online_net.to(self.device)
        self.target_net.to(self.device)
        print(
            f"[DQNAgent] online_net moved to device: {next(self.online_net.parameters()).device}"
        )
        print(
            f"[DQNAgent] target_net moved to device: {next(self.target_net.parameters()).device}"
        )

        if "optimizer_state_dict" in state_dict:
            try:
                self.optimizer.load_state_dict(state_dict["optimizer_state_dict"])
                for state in self.optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor) and v.device != self.device:
                            state[k] = v.to(self.device)
                print("[DQNAgent] Optimizer state loaded and moved to device.")
            except Exception as e:
                print(
                    f"Warning: Could not load optimizer state ({e}). Resetting optimizer."
                )
                self.optimizer = optim.AdamW(
                    self.online_net.parameters(),
                    lr=self.dqn_config.LEARNING_RATE,
                    eps=self.dqn_config.ADAM_EPS,
                    weight_decay=1e-5,
                )
        else:
            print(
                "Warning: Optimizer state not found in checkpoint. Resetting optimizer."
            )
            self.optimizer = optim.AdamW(
                self.online_net.parameters(),
                lr=self.dqn_config.LEARNING_RATE,
                eps=self.dqn_config.ADAM_EPS,
                weight_decay=1e-5,
            )

        if (
            self.scheduler
            and "scheduler_state_dict" in state_dict
            and state_dict["scheduler_state_dict"] is not None
        ):
            try:
                self.scheduler.load_state_dict(state_dict["scheduler_state_dict"])
                print("[DQNAgent] LR Scheduler state loaded.")
            except Exception as e:
                print(
                    f"Warning: Could not load LR scheduler state ({e}). Scheduler may reset."
                )
                self.scheduler = CosineAnnealingLR(
                    self.optimizer,
                    T_max=self.dqn_config.LR_SCHEDULER_T_MAX,
                    eta_min=self.dqn_config.LR_SCHEDULER_ETA_MIN,
                )
        elif self.scheduler:
            print(
                "Warning: LR Scheduler state not found in checkpoint. Scheduler may reset."
            )

        self.online_net.train()
        self.target_net.eval()
        print("[DQNAgent] load_state_dict complete.")


File: agent/model_factory.py
# File: agent/model_factory.py
import torch.nn as nn
from config import ModelConfig, EnvConfig, DQNConfig
from typing import Type

from agent.networks.agent_network import AgentNetwork


def create_network(
    # --- MODIFIED: Pass EnvConfig instance ---
    env_config: EnvConfig,
    # --- END MODIFIED ---
    action_dim: int,
    model_config: ModelConfig,
    dqn_config: DQNConfig,
) -> nn.Module:
    """Creates the AgentNetwork based on configuration."""

    print(
        f"[ModelFactory] Creating AgentNetwork (Dueling: {dqn_config.USE_DUELING}, NoisyNets Heads: {dqn_config.USE_NOISY_NETS})"
    )

    # Pass the specific sub-config ModelConfig.Network
    return AgentNetwork(
        # --- MODIFIED: Pass EnvConfig instance ---
        env_config=env_config,
        # --- END MODIFIED ---
        action_dim=action_dim,
        model_config=model_config.Network,  # Pass the Network sub-config
        dqn_config=dqn_config,
        dueling=dqn_config.USE_DUELING,
        use_noisy=dqn_config.USE_NOISY_NETS,
    )


File: agent/replay_buffer/uniform_buffer.py
# File: agent/replay_buffer/uniform_buffer.py
import random
import numpy as np
from collections import deque
from typing import Deque, Tuple, Optional, Any, Dict, Union, List  # Added List
from .base_buffer import ReplayBufferBase

# --- MODIFIED: Import specific StateType ---
from environment.game_state import StateType  # Use the Dict type

# --- END MODIFIED ---
from utils.types import Transition, ActionType, NumpyBatch, NumpyNStepBatch
from utils.helpers import save_object, load_object


class UniformReplayBuffer(ReplayBufferBase):
    """Standard uniform experience replay buffer."""

    def __init__(self, capacity: int):
        super().__init__(capacity)
        self.buffer: Deque[Transition] = deque(maxlen=capacity)

    def push(
        self,
        state: StateType,  # State is now a Dict
        action: ActionType,
        reward: float,
        next_state: StateType,  # Next state is now a Dict
        done: bool,
        **kwargs,
    ):
        n_step_discount = kwargs.get("n_step_discount")
        # Store the dictionary state directly in the Transition
        transition = Transition(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done,
            n_step_discount=n_step_discount,
        )
        self.buffer.append(transition)

    # --- MODIFIED: Sample unpacks dictionary states ---
    def sample(self, batch_size: int) -> Optional[Union[NumpyBatch, NumpyNStepBatch]]:
        if len(self.buffer) < batch_size:
            return None

        batch_transitions: List[Transition] = random.sample(self.buffer, batch_size)
        is_n_step = batch_transitions[0].n_step_discount is not None

        # Unpack transitions, keeping states as dicts for now
        states_dicts = [t.state for t in batch_transitions]
        actions_np = np.array([t.action for t in batch_transitions], dtype=np.int64)
        rewards_np = np.array([t.reward for t in batch_transitions], dtype=np.float32)
        next_states_dicts = [t.next_state for t in batch_transitions]
        dones_np = np.array([t.done for t in batch_transitions], dtype=np.float32)

        if is_n_step:
            discounts_np = np.array(
                [t.n_step_discount for t in batch_transitions], dtype=np.float32
            )
            # Return states as list of dicts, agent will handle conversion
            return (
                states_dicts,
                actions_np,
                rewards_np,
                next_states_dicts,
                dones_np,
                discounts_np,
            )
        else:
            # Return states as list of dicts
            return states_dicts, actions_np, rewards_np, next_states_dicts, dones_np

    # --- END MODIFIED ---

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        pass

    def set_beta(self, beta: float):
        pass

    def flush_pending(self):
        pass

    def __len__(self) -> int:
        return len(self.buffer)

    def get_state(self) -> Dict[str, Any]:
        return {"buffer": list(self.buffer)}

    def load_state_from_data(self, state: Dict[str, Any]):
        saved_buffer_list = state.get("buffer", [])
        # Ensure loaded items are Transitions (or handle potential errors)
        valid_transitions = [t for t in saved_buffer_list if isinstance(t, Transition)]
        if len(valid_transitions) != len(saved_buffer_list):
            print(
                f"Warning: Filtered out {len(saved_buffer_list) - len(valid_transitions)} invalid items during buffer load."
            )
        self.buffer = deque(valid_transitions, maxlen=self.capacity)
        print(f"[UniformReplayBuffer] Loaded {len(self.buffer)} transitions.")

    def save_state(self, filepath: str):
        state = self.get_state()
        save_object(state, filepath)

    def load_state(self, filepath: str):
        state = load_object(filepath)
        self.load_state_from_data(state)


File: agent/replay_buffer/buffer_utils.py
# File: agent/replay_buffer/buffer_utils.py
# (No structural changes, cleaner print statements)
from config import BufferConfig, DQNConfig
from .base_buffer import ReplayBufferBase
from .uniform_buffer import UniformReplayBuffer
from .prioritized_buffer import PrioritizedReplayBuffer
from .nstep_buffer import NStepBufferWrapper


def create_replay_buffer(
    config: BufferConfig, dqn_config: DQNConfig
) -> ReplayBufferBase:
    """Factory function to create the replay buffer based on configuration."""

    print("[BufferFactory] Creating replay buffer...")
    print(f"  Type: {'Prioritized' if config.USE_PER else 'Uniform'}")
    print(f"  Capacity: {config.REPLAY_BUFFER_SIZE / 1e6:.1f}M")
    if config.USE_PER:
        print(f"  PER alpha={config.PER_ALPHA}, eps={config.PER_EPSILON}")

    if config.USE_PER:
        core_buffer = PrioritizedReplayBuffer(
            capacity=config.REPLAY_BUFFER_SIZE,
            alpha=config.PER_ALPHA,
            epsilon=config.PER_EPSILON,
        )
    else:
        core_buffer = UniformReplayBuffer(capacity=config.REPLAY_BUFFER_SIZE)

    if config.USE_N_STEP and config.N_STEP > 1:
        print(
            f"  N-Step Wrapper: Enabled (N={config.N_STEP}, gamma={dqn_config.GAMMA})"
        )
        final_buffer = NStepBufferWrapper(
            wrapped_buffer=core_buffer,
            n_step=config.N_STEP,
            gamma=dqn_config.GAMMA,
        )
    else:
        print(f"  N-Step Wrapper: Disabled")
        final_buffer = core_buffer

    print(f"[BufferFactory] Final buffer type: {type(final_buffer).__name__}")
    return final_buffer


File: agent/replay_buffer/__init__.py


File: agent/replay_buffer/prioritized_buffer.py
# File: agent/replay_buffer/prioritized_buffer.py
import random
import numpy as np
from typing import Optional, Tuple, Any, Dict, Union, List
from .base_buffer import ReplayBufferBase
from .sum_tree import SumTree

# --- MODIFIED: Import specific StateType ---
from environment.game_state import StateType  # Use the Dict type

# --- END MODIFIED ---
from utils.types import (
    Transition,
    # StateType, # Removed
    ActionType,
    NumpyBatch,
    PrioritizedNumpyBatch,
    NumpyNStepBatch,
    PrioritizedNumpyNStepBatch,
)
from utils.helpers import save_object, load_object


class PrioritizedReplayBuffer(ReplayBufferBase):
    """Prioritized Experience Replay (PER) buffer using a SumTree."""

    def __init__(self, capacity: int, alpha: float, epsilon: float):
        super().__init__(capacity)
        self.tree = SumTree(capacity)
        self.alpha = alpha
        self.epsilon = epsilon
        self.beta = 0.0
        self.max_priority = 1.0

    def push(
        self,
        state: StateType,  # State is Dict
        action: ActionType,
        reward: float,
        next_state: StateType,  # Next state is Dict
        done: bool,
        **kwargs,
    ):
        """Adds new experience with maximum priority."""
        n_step_discount = kwargs.get("n_step_discount")
        transition = Transition(
            state, action, reward, next_state, done, n_step_discount
        )
        # Add with current max priority to ensure new samples get seen
        self.tree.add(self.max_priority, transition)

    # --- MODIFIED: Sample returns dict states ---
    def sample(
        self, batch_size: int
    ) -> Optional[Union[PrioritizedNumpyBatch, PrioritizedNumpyNStepBatch]]:
        """Samples batch using priorities, calculates IS weights."""
        if len(self) < batch_size:
            return None

        batch_data: List[Transition] = []
        indices = np.empty(batch_size, dtype=np.int64)
        priorities = np.empty(batch_size, dtype=np.float64)
        segment = self.tree.total() / batch_size

        for i in range(batch_size):
            s = random.uniform(segment * i, segment * (i + 1))
            s = max(1e-9, s)  # Avoid zero
            idx, p, data = self.tree.get(s)

            retries = 0
            max_retries = 5
            while not isinstance(data, Transition) and retries < max_retries:
                print(
                    f"Warning: PER sample retrieved invalid data (type: {type(data)}). Retrying..."
                )
                s = random.uniform(1e-9, self.tree.total())
                idx, p, data = self.tree.get(s)
                retries += 1

            if not isinstance(data, Transition):
                print(
                    f"ERROR: PER sample failed after {max_retries} retries. Skipping batch."
                )
                # This indicates a potential issue with the SumTree or data storage
                return None

            priorities[i] = p
            batch_data.append(data)
            indices[i] = idx

        # Calculate Importance Sampling (IS) weights
        sampling_probs = np.maximum(
            priorities / self.tree.total(), 1e-9
        )  # Avoid division by zero
        is_weights = np.power(len(self) * sampling_probs, -self.beta)
        # Normalize weights by max weight for stability
        is_weights = (is_weights / (is_weights.max() + 1e-9)).astype(np.float32)

        # Check if N-step based on first item
        is_n_step = batch_data[0].n_step_discount is not None
        batch_tuple = self._unpack_batch(batch_data, is_n_step)  # Returns dict states

        return batch_tuple, indices, is_weights

    # --- END MODIFIED ---

    # --- MODIFIED: Unpack returns dict states ---
    def _unpack_batch(
        self, batch_data: List[Transition], is_n_step: bool
    ) -> Union[NumpyBatch, NumpyNStepBatch]:
        """Unpacks list of Transitions into numpy arrays, keeping states as dicts."""
        states_dicts = [t.state for t in batch_data]
        actions_np = np.array([t.action for t in batch_data], dtype=np.int64)
        rewards_np = np.array([t.reward for t in batch_data], dtype=np.float32)
        next_states_dicts = [t.next_state for t in batch_data]
        dones_np = np.array([t.done for t in batch_data], dtype=np.float32)

        if is_n_step:
            discounts_np = np.array(
                [t.n_step_discount for t in batch_data], dtype=np.float32
            )
            return (
                states_dicts,
                actions_np,
                rewards_np,
                next_states_dicts,
                dones_np,
                discounts_np,
            )
        else:
            return states_dicts, actions_np, rewards_np, next_states_dicts, dones_np

    # --- END MODIFIED ---

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """Updates priorities of experiences at given tree indices."""
        if len(indices) != len(priorities):
            print(
                f"Error: Mismatch indices ({len(indices)}) / priorities ({len(priorities)}) in PER update"
            )
            return

        # Apply alpha transformation to TD errors
        priorities = np.power(np.abs(priorities) + self.epsilon, self.alpha)
        priorities = np.maximum(priorities, 1e-6)  # Ensure positive priorities

        for idx, priority in zip(indices, priorities):
            # Validate index corresponds to a leaf node in the tree structure
            if not (self.tree.capacity - 1 <= idx < 2 * self.tree.capacity - 1):
                # print(f"Warning: Invalid tree index {idx} provided to update_priorities. Skipping.")
                continue
            self.tree.update(idx, priority)
            self.max_priority = max(
                self.max_priority, priority
            )  # Update max priority seen

    def set_beta(self, beta: float):
        self.beta = beta

    def flush_pending(self):
        pass  # No-op for core PER buffer

    def __len__(self) -> int:
        return self.tree.n_entries

    def get_state(self) -> Dict[str, Any]:
        # Ensure data contains serializable items (should be Transitions)
        serializable_data = [
            d if isinstance(d, Transition) else None for d in self.tree.data
        ]
        return {
            "tree_nodes": self.tree.tree.copy(),
            "tree_data": serializable_data,  # Save potentially filtered list
            "tree_write_ptr": self.tree.write_ptr,
            "tree_n_entries": self.tree.n_entries,
            "max_priority": self.max_priority,
            "alpha": self.alpha,
            "epsilon": self.epsilon,
            # Beta is transient, no need to save
        }

    def load_state_from_data(self, state: Dict[str, Any]):
        if "tree_nodes" not in state or "tree_data" not in state:
            print("Error: Invalid PER state format during load. Skipping.")
            self.tree = SumTree(self.capacity)  # Reinitialize tree
            self.max_priority = 1.0
            return

        loaded_capacity = len(state["tree_data"])
        if loaded_capacity != self.capacity:
            print(
                f"Warning: Loaded PER capacity ({loaded_capacity}) != current ({self.capacity}). Recreating tree."
            )
            self.tree = SumTree(self.capacity)
            num_to_load = min(loaded_capacity, self.capacity)
            # Load only valid transitions
            valid_data = [
                d for d in state["tree_data"][:num_to_load] if isinstance(d, Transition)
            ]
            self.tree.data[: len(valid_data)] = valid_data
            self.tree.write_ptr = state.get("tree_write_ptr", 0) % self.capacity
            self.tree.n_entries = len(
                valid_data
            )  # Correct n_entries based on valid data
            # Rebuild tree priorities from loaded data (expensive but necessary if capacity changed)
            print("Rebuilding SumTree priorities from loaded data...")
            self.tree.tree.fill(0)  # Clear existing tree sums
            self.max_priority = 1.0  # Reset max priority
            for i in range(self.tree.n_entries):
                # Assign default max priority during rebuild
                self.tree.update(i + self.capacity - 1, self.max_priority)
            print(f"[PER] Rebuilt tree with {self.tree.n_entries} transitions.")

        else:  # Capacities match
            self.tree.tree = state["tree_nodes"]
            # Load data, filtering invalid entries
            valid_data = [
                d for d in state["tree_data"] if isinstance(d, Transition) or d is None
            ]  # Allow None placeholders
            if len(valid_data) != len(state["tree_data"]):
                print(
                    f"Warning: Filtered {len(state['tree_data']) - len(valid_data)} invalid items from PER data during load."
                )
            self.tree.data = np.array(valid_data, dtype=object)  # Ensure numpy array
            self.tree.write_ptr = state.get("tree_write_ptr", 0)
            self.tree.n_entries = state.get("tree_n_entries", 0)
            # Ensure n_entries is consistent with actual data
            actual_entries = sum(1 for d in self.tree.data if isinstance(d, Transition))
            if self.tree.n_entries != actual_entries:
                print(
                    f"Warning: Correcting PER n_entries from {self.tree.n_entries} to {actual_entries}"
                )
                self.tree.n_entries = actual_entries

            self.max_priority = state.get("max_priority", 1.0)
            print(f"[PER] Loaded {self.tree.n_entries} transitions.")

        self.alpha = state.get("alpha", self.alpha)
        self.epsilon = state.get("epsilon", self.epsilon)

    def save_state(self, filepath: str):
        save_object(self.get_state(), filepath)

    def load_state(self, filepath: str):
        state = load_object(filepath)
        self.load_state_from_data(state)


File: agent/replay_buffer/base_buffer.py
# File: agent/replay_buffer/base_buffer.py
# (No changes needed)
from abc import ABC, abstractmethod
from typing import Any, Optional, Tuple, Dict
import numpy as np
from utils.types import StateType, ActionType


class ReplayBufferBase(ABC):
    """Abstract base class for all replay buffers."""

    def __init__(self, capacity: int):
        self.capacity = capacity

    @abstractmethod
    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
        **kwargs  # Allow passing extra info like n_step_discount
    ):
        """Add a new experience to the buffer."""
        pass

    @abstractmethod
    def sample(
        self, batch_size: int
    ) -> Optional[Any]:  # Return type depends on PER/NStep
        """Sample a batch of experiences from the buffer."""
        pass

    @abstractmethod
    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """Update priorities for PER (no-op for uniform buffer)."""
        pass

    @abstractmethod
    def __len__(self) -> int:
        """Return the current size of the buffer."""
        pass

    @abstractmethod
    def set_beta(self, beta: float):
        """Set the beta value for PER IS weights (no-op for uniform buffer)."""
        pass

    @abstractmethod
    def flush_pending(self):
        """Process any pending transitions (e.g., for N-step)."""
        pass

    @abstractmethod
    def get_state(self) -> Dict[str, Any]:
        """Return the buffer's state as a dictionary suitable for saving."""
        pass

    @abstractmethod
    def load_state_from_data(self, state: Dict[str, Any]):
        """Load the buffer's state from a dictionary."""
        pass

    @abstractmethod
    def save_state(self, filepath: str):
        """Save the buffer's state to a file."""
        pass

    @abstractmethod
    def load_state(self, filepath: str):
        """Load the buffer's state from a file."""
        pass


File: agent/replay_buffer/nstep_buffer.py
# File: agent/replay_buffer/nstep_buffer.py
from collections import deque
import numpy as np
from typing import Deque, Tuple, Optional, Any, Dict, List
from .base_buffer import ReplayBufferBase

# --- MODIFIED: Import specific StateType ---
from environment.game_state import StateType  # Use the Dict type

# --- END MODIFIED ---
from utils.types import Transition, ActionType
from utils.helpers import save_object, load_object


class NStepBufferWrapper(ReplayBufferBase):
    """Wraps another buffer to implement N-step returns."""

    def __init__(self, wrapped_buffer: ReplayBufferBase, n_step: int, gamma: float):
        super().__init__(wrapped_buffer.capacity)
        if n_step <= 0:
            raise ValueError("N-step must be positive")
        self.wrapped_buffer = wrapped_buffer
        self.n_step = n_step
        self.gamma = gamma
        # Deque stores (state_dict, action, reward, next_state_dict, done) tuples
        self.n_step_deque: Deque[
            Tuple[StateType, ActionType, float, StateType, bool]
        ] = deque(maxlen=n_step)

    def _calculate_n_step_transition(
        self, current_deque_list: List[Tuple]
    ) -> Optional[Transition]:
        """Calculates the N-step return from a list copy of the deque."""
        if not current_deque_list:
            return None

        n_step_reward = 0.0
        discount_accum = 1.0
        effective_n = len(current_deque_list)
        state_0, action_0 = (
            current_deque_list[0][0],
            current_deque_list[0][1],
        )  # State is dict

        for i in range(effective_n):
            s, a, r, ns, d = current_deque_list[i]
            n_step_reward += discount_accum * r
            if d:  # Episode terminated within N steps
                # The next_state for the N-step transition is the state *after* the terminal action
                n_step_next_state = ns
                n_step_done = True
                n_step_discount = self.gamma ** (i + 1)
                return Transition(
                    state_0,
                    action_0,
                    n_step_reward,
                    n_step_next_state,
                    n_step_done,
                    n_step_discount,
                )
            discount_accum *= self.gamma

        # Loop completed without terminal state
        n_step_next_state = current_deque_list[-1][3]  # next_state from Nth transition
        n_step_done = current_deque_list[-1][4]  # done flag from Nth transition
        n_step_discount = self.gamma**effective_n
        return Transition(
            state_0,
            action_0,
            n_step_reward,
            n_step_next_state,
            n_step_done,
            n_step_discount,
        )

    def push(
        self,
        state: StateType,  # State is dict
        action: ActionType,
        reward: float,
        next_state: StateType,  # Next state is dict
        done: bool,
    ):
        """Adds raw transition, processes N-step if possible, pushes to wrapped buffer."""
        self.n_step_deque.append((state, action, reward, next_state, done))

        # If deque isn't full yet, only process if the *newly added* transition was terminal
        if len(self.n_step_deque) < self.n_step:
            if done:
                self._flush_on_done()  # Process partial transitions ending here
            return  # Don't process full N-step yet

        # Deque has N items, calculate N-step transition starting from the oldest
        n_step_transition = self._calculate_n_step_transition(list(self.n_step_deque))
        if n_step_transition:
            # Push the calculated N-step transition to the underlying buffer
            self.wrapped_buffer.push(
                state=n_step_transition.state,
                action=n_step_transition.action,
                reward=n_step_transition.reward,
                next_state=n_step_transition.next_state,
                done=n_step_transition.done,
                n_step_discount=n_step_transition.n_step_discount,  # Pass discount factor
            )

        # If the *newly added* transition was terminal, flush remaining partials
        if done:
            self._flush_on_done()

    def _flush_on_done(self):
        """Processes remaining partial transitions when an episode ends."""
        # The deque contains transitions leading up to and including the 'done' one.
        # We need to calculate N-step returns for sequences starting *before* the done transition.
        temp_deque = list(self.n_step_deque)
        while len(temp_deque) > 1:  # Process until only the 'done' transition remains
            # Calculate N-step starting from the current oldest
            n_step_transition = self._calculate_n_step_transition(temp_deque)
            if n_step_transition:
                # Push if valid (should always be if temp_deque not empty)
                self.wrapped_buffer.push(
                    state=n_step_transition.state,
                    action=n_step_transition.action,
                    reward=n_step_transition.reward,
                    next_state=n_step_transition.next_state,
                    done=n_step_transition.done,
                    n_step_discount=n_step_transition.n_step_discount,
                )
            temp_deque.pop(0)  # Remove the starting state we just processed
        self.n_step_deque.clear()  # Clear the deque after flushing

    def sample(self, batch_size: int) -> Any:
        # Sampling is delegated to the wrapped buffer
        return self.wrapped_buffer.sample(batch_size)

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        # Priority update is delegated
        self.wrapped_buffer.update_priorities(indices, priorities)

    def set_beta(self, beta: float):
        # Beta setting is delegated
        if hasattr(self.wrapped_buffer, "set_beta"):
            self.wrapped_buffer.set_beta(beta)

    def __len__(self) -> int:
        # Length is determined by the wrapped buffer
        return len(self.wrapped_buffer)

    def flush_pending(self):
        """Processes and pushes any remaining transitions before exit/save."""
        print(f"[NStepWrapper] Flushing {len(self.n_step_deque)} pending transitions.")
        temp_deque = list(self.n_step_deque)
        while len(temp_deque) > 0:
            n_step_transition = self._calculate_n_step_transition(temp_deque)
            if n_step_transition:
                self.wrapped_buffer.push(
                    state=n_step_transition.state,
                    action=n_step_transition.action,
                    reward=n_step_transition.reward,
                    next_state=n_step_transition.next_state,
                    done=n_step_transition.done,
                    n_step_discount=n_step_transition.n_step_discount,
                )
            temp_deque.pop(0)  # Remove the processed start state
        self.n_step_deque.clear()
        # Also flush the underlying buffer if it has its own pending mechanism
        if hasattr(self.wrapped_buffer, "flush_pending"):
            self.wrapped_buffer.flush_pending()

    def get_state(self) -> Dict[str, Any]:
        # Save pending deque and wrapped buffer state
        return {
            "n_step_deque": list(self.n_step_deque),
            "wrapped_buffer_state": self.wrapped_buffer.get_state(),
        }

    def load_state_from_data(self, state: Dict[str, Any]):
        # Load pending deque
        pending_deque_list = state.get("n_step_deque", [])
        # Ensure loaded items are valid tuples before creating deque
        valid_pending = [
            t for t in pending_deque_list if isinstance(t, tuple) and len(t) == 5
        ]
        if len(valid_pending) != len(pending_deque_list):
            print(
                f"Warning: Filtered {len(pending_deque_list) - len(valid_pending)} invalid items during NStep deque load."
            )
        self.n_step_deque = deque(valid_pending, maxlen=self.n_step)
        print(f"[NStepWrapper] Loaded {len(self.n_step_deque)} pending transitions.")

        # Load wrapped buffer state
        wrapped_state = state.get("wrapped_buffer_state")
        if wrapped_state:
            self.wrapped_buffer.load_state_from_data(wrapped_state)
        else:
            print("[NStepWrapper] Warning: No wrapped buffer state found during load.")

    def save_state(self, filepath: str):
        save_object(self.get_state(), filepath)

    def load_state(self, filepath: str):
        try:
            state_data = load_object(filepath)
            self.load_state_from_data(state_data)
        except Exception as e:
            print(f"[NStepWrapper] Load failed: {e}. Starting empty.")
            # Reset state if load fails
            self.n_step_deque.clear()
            # Optionally reset wrapped buffer too, depending on desired behavior
            # self.wrapped_buffer = type(self.wrapped_buffer)(...) # Recreate wrapped buffer


File: agent/replay_buffer/sum_tree.py
# File: agent/replay_buffer/sum_tree.py
# File: agent/replay_buffer/sum_tree.py
import numpy as np


class SumTree:
    """Simple SumTree implementation using numpy arrays for PER."""

    def __init__(self, capacity: int):
        if capacity <= 0 or not isinstance(capacity, int):
            raise ValueError("SumTree capacity must be positive integer")
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1, dtype=np.float64)  # Use float64 for sums
        self.data = np.zeros(capacity, dtype=object)  # Holds Transition objects
        self.write_ptr = 0
        self.n_entries = 0

    def _propagate(self, idx: int, change: float):
        """Propagate priority change up the tree."""
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)

    def _retrieve(self, idx: int, s: float) -> int:
        """Find leaf index for a given cumulative priority value s."""
        left = 2 * idx + 1
        right = left + 1
        if left >= len(self.tree):
            return idx  # Leaf node

        # Use tolerance for float comparison
        if s <= self.tree[left] + 1e-8:
            return self._retrieve(left, s)
        else:
            return self._retrieve(
                right, max(0.0, s - self.tree[left])
            )  # Ensure non-negative s

    def total(self) -> float:
        return self.tree[0]

    def add(self, priority: float, data: object):
        """Add new experience, overwriting oldest if full."""
        priority = max(abs(priority), 1e-6)  # Ensure positive priority
        tree_idx = self.write_ptr + self.capacity - 1
        self.data[self.write_ptr] = data
        self.update(tree_idx, priority)
        self.write_ptr = (self.write_ptr + 1) % self.capacity
        if self.n_entries < self.capacity:
            self.n_entries += 1

    def update(self, tree_idx: int, priority: float):
        """Update priority of an experience at a given tree index."""
        priority = max(abs(priority), 1e-6)
        if not (self.capacity - 1 <= tree_idx < 2 * self.capacity - 1):
            return  # Skip invalid index

        change = priority - self.tree[tree_idx]
        self.tree[tree_idx] = priority
        if abs(change) > 1e-9 and tree_idx > 0:
            self._propagate(tree_idx, change)

    def get(self, s: float) -> tuple[int, float, object]:
        """Sample an experience based on cumulative priority s. Returns: (tree_idx, priority, data)"""
        if self.total() <= 0 or self.n_entries == 0:
            return 0, 0.0, None  # Handle empty tree

        s = np.clip(s, 1e-9, self.total())  # Clip s to valid range
        idx = self._retrieve(0, s)  # Leaf node index in tree array
        data_idx = idx - self.capacity + 1  # Corresponding index in data array

        # Validate data_idx before access (important if buffer not full)
        if not (0 <= data_idx < self.n_entries):
            # Fallback: return last valid entry if index is out of bounds (rare)
            if self.n_entries > 0:
                last_valid_data_idx = (
                    self.write_ptr - 1 + self.capacity
                ) % self.capacity
                last_valid_tree_idx = last_valid_data_idx + self.capacity - 1
                priority = (
                    self.tree[last_valid_tree_idx]
                    if (
                        self.capacity - 1 <= last_valid_tree_idx < 2 * self.capacity - 1
                    )
                    else 0.0
                )
                return (last_valid_tree_idx, priority, self.data[last_valid_data_idx])
            else:
                return 0, 0.0, None  # Truly empty

        return (idx, self.tree[idx], self.data[data_idx])

    def __len__(self) -> int:
        return self.n_entries


File: agent/networks/noisy_layer.py
# File: agent/networks/noisy_layer.py
# (No changes needed, already clean)
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional


class NoisyLinear(nn.Module):
    """
    Noisy Linear Layer for Noisy Network (Factorised Gaussian Noise).
    """

    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        # Learnable weights and biases (mean parameters)
        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_mu = nn.Parameter(torch.empty(out_features))

        # Learnable noise parameters (standard deviation parameters)
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))

        # Non-learnable noise buffers
        self.register_buffer("weight_epsilon", torch.empty(out_features, in_features))
        self.register_buffer("bias_epsilon", torch.empty(out_features))

        self.reset_parameters()
        self.reset_noise()  # Initial noise generation

    def reset_parameters(self):
        """Initialize mean and std parameters."""
        mu_range = 1.0 / math.sqrt(self.in_features)
        nn.init.uniform_(self.weight_mu, -mu_range, mu_range)
        nn.init.uniform_(self.bias_mu, -mu_range, mu_range)

        # Initialize sigma parameters (std dev)
        nn.init.constant_(
            self.weight_sigma, self.std_init / math.sqrt(self.in_features)
        )
        nn.init.constant_(self.bias_sigma, self.std_init / math.sqrt(self.out_features))

    def reset_noise(self):
        """Generate new noise samples using Factorised Gaussian noise."""
        epsilon_in = self._scale_noise(self.in_features)
        epsilon_out = self._scale_noise(self.out_features)

        # Outer product for weight noise, direct sample for bias noise
        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))
        self.bias_epsilon.copy_(epsilon_out)

    def _scale_noise(self, size: int) -> torch.Tensor:
        """Generate noise tensor with sign-sqrt transformation."""
        x = torch.randn(size, device=self.weight_mu.device)  # Noise on same device
        return x.sign().mul(x.abs().sqrt())

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass with noisy parameters if training, mean parameters otherwise."""
        if self.training:
            # Sample noise is implicitly used via weight_epsilon, bias_epsilon
            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon
            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon
            # Reset noise *after* use in forward pass for next iteration?
            # Or reset in train() method? Resetting in train() is common.
        else:
            # Use mean parameters during evaluation
            weight = self.weight_mu
            bias = self.bias_mu

        return F.linear(x, weight, bias)

    def train(self, mode: bool = True):
        """Override train mode to reset noise when entering training."""
        if self.training is False and mode is True:  # If switching from eval to train
            self.reset_noise()
        super().train(mode)


File: agent/networks/__init__.py


File: agent/networks/agent_network.py
# File: agent/networks/agent_network.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

# --- MODIFIED: Import EnvConfig directly ---
from config import ModelConfig, EnvConfig, DQNConfig, DEVICE

# --- END MODIFIED ---
from typing import Tuple, List, Type, Optional

from .noisy_layer import NoisyLinear


class AgentNetwork(nn.Module):
    """
    Agent Network: CNN+MLP -> Fusion -> Dueling Heads -> Optional Distributional Output.
    Accepts separate grid and shape tensors as input.
    """

    def __init__(
        self,
        env_config: EnvConfig,  # Expecting instance
        action_dim: int,
        model_config: ModelConfig.Network,
        dqn_config: DQNConfig,
        dueling: bool,
        use_noisy: bool,
    ):
        super().__init__()
        self.dueling = dueling
        self.action_dim = action_dim
        self.env_config = env_config  # Store instance
        self.config = model_config  # Store network sub-config
        self.use_noisy = use_noisy
        self.use_distributional = dqn_config.USE_DISTRIBUTIONAL
        self.num_atoms = dqn_config.NUM_ATOMS
        self.device = DEVICE

        print(f"[AgentNetwork] Target device set to: {self.device}")
        print(
            f"[AgentNetwork] Distributional C51: {self.use_distributional} ({self.num_atoms} atoms)"
        )

        # --- MODIFIED: Calculate dims from EnvConfig instance (now 2 channels) ---
        self.grid_c, self.grid_h, self.grid_w = self.env_config.GRID_STATE_SHAPE
        # --- END MODIFIED ---
        self.shape_feat_dim = self.env_config.SHAPE_STATE_DIM
        self.num_shape_slots = self.env_config.NUM_SHAPE_SLOTS
        self.shape_feat_per_slot = self.env_config.SHAPE_FEATURES_PER_SHAPE

        print(f"[AgentNetwork] Initializing (Noisy Heads: {self.use_noisy}):")
        # --- MODIFIED: Print updated grid shape ---
        print(f"  Input Grid Shape: [B, {self.grid_c}, {self.grid_h}, {self.grid_w}]")
        # --- END MODIFIED ---
        print(
            f"  Input Shape Features Dim: {self.shape_feat_dim} ({self.num_shape_slots} slots x {self.shape_feat_per_slot} features)"
        )

        # Build network branches (CNN input channels handled automatically by self.grid_c)
        self.conv_base, conv_out_h, conv_out_w, conv_out_c = self._build_cnn_branch()
        self.conv_out_size = self._get_conv_out_size(
            (self.grid_c, self.grid_h, self.grid_w)
        )
        print(
            f"  CNN Output Dim (HxWxC): ({conv_out_h}x{conv_out_w}x{conv_out_c}) -> Flat: {self.conv_out_size}"
        )

        self.shape_mlp, self.shape_mlp_out_dim = self._build_shape_mlp_branch()
        print(f"  Shape MLP Output Dim: {self.shape_mlp_out_dim}")

        combined_features_dim = self.conv_out_size + self.shape_mlp_out_dim
        print(f"  Combined Features Dim: {combined_features_dim}")

        self.fusion_mlp, self.head_input_dim = self._build_fusion_mlp_branch(
            combined_features_dim
        )
        print(f"  Fusion MLP Output Dim (Input to Heads): {self.head_input_dim}")

        self._build_output_heads()
        head_type = NoisyLinear if self.use_noisy else nn.Linear
        output_type = "Distributional" if self.use_distributional else "Q-Value"
        print(
            f"  Using {'Dueling' if self.dueling else 'Single'} Heads ({head_type.__name__}), Output: {output_type} [{self.action_dim * (self.num_atoms if self.use_distributional else 1)} units]"
        )

        # Final check
        if hasattr(self.conv_base, "0") and hasattr(self.conv_base[0], "weight"):
            print(
                f"[AgentNetwork] Final check - conv_base device: {next(self.conv_base.parameters()).device}"
            )
        else:
            print(
                "[AgentNetwork] Final check - conv_base seems empty or has no weights."
            )

    def _build_cnn_branch(self) -> Tuple[nn.Sequential, int, int, int]:
        conv_layers: List[nn.Module] = []
        # --- Uses self.grid_c (now 2) ---
        current_channels = self.grid_c
        h, w = self.grid_h, self.grid_w
        cfg = self.config
        print(
            f"  Building CNN (Input Channels: {current_channels}) on device: {self.device}"
        )
        for i, out_channels in enumerate(cfg.CONV_CHANNELS):
            conv_layer = nn.Conv2d(
                current_channels,
                out_channels,
                kernel_size=cfg.CONV_KERNEL_SIZE,
                stride=cfg.CONV_STRIDE,
                padding=cfg.CONV_PADDING,
                bias=not cfg.USE_BATCHNORM_CONV,
            ).to(self.device)
            conv_layers.append(conv_layer)
            if cfg.USE_BATCHNORM_CONV:
                conv_layers.append(nn.BatchNorm2d(out_channels).to(self.device))
            conv_layers.append(cfg.CONV_ACTIVATION())
            current_channels = out_channels
            # --- Adjust H, W calculation (still no pooling) ---
            h = (h + 2 * cfg.CONV_PADDING - cfg.CONV_KERNEL_SIZE) // cfg.CONV_STRIDE + 1
            w = (w + 2 * cfg.CONV_PADDING - cfg.CONV_KERNEL_SIZE) // cfg.CONV_STRIDE + 1

        cnn_module = nn.Sequential(*conv_layers)
        if len(cnn_module) > 0 and hasattr(cnn_module[0], "weight"):
            print(f"    CNN Layer 0 device after build: {cnn_module[0].weight.device}")
        return cnn_module, h, w, current_channels

    def _get_conv_out_size(self, shape: Tuple[int, int, int]) -> int:
        # Ensure conv_base is on the correct device before creating dummy input
        self.conv_base.to(self.device)
        with torch.no_grad():
            # Input shape uses current self.grid_c (which is 2)
            dummy_input = torch.zeros(1, *shape, device=self.device)
            self.conv_base.eval()
            try:
                output = self.conv_base(dummy_input)
                out_size = int(np.prod(output.size()[1:]))
            except Exception as e:
                print(f"Error calculating conv output size: {e}")
                print(f"Input shape to CNN: {dummy_input.shape}")
                out_size = 1  # Placeholder
            return out_size

    def _build_shape_mlp_branch(self) -> Tuple[nn.Sequential, int]:
        shape_mlp_layers: List[nn.Module] = []
        # Input dimension is NUM_SLOTS * FEAT_PER_SHAPE
        current_dim = self.env_config.SHAPE_STATE_DIM
        cfg = self.config

        # MLP layers defined in config
        for hidden_dim in cfg.SHAPE_FEATURE_MLP_DIMS:
            lin_layer = nn.Linear(current_dim, hidden_dim).to(self.device)
            shape_mlp_layers.append(lin_layer)
            shape_mlp_layers.append(cfg.SHAPE_MLP_ACTIVATION())
            current_dim = hidden_dim

        return nn.Sequential(*shape_mlp_layers), current_dim

    def _build_fusion_mlp_branch(self, input_dim: int) -> Tuple[nn.Sequential, int]:
        fusion_layers: List[nn.Module] = []
        current_fusion_dim = input_dim
        cfg = self.config
        fusion_linear_layer_class = nn.Linear
        for i, hidden_dim in enumerate(cfg.COMBINED_FC_DIMS):
            linear_layer = fusion_linear_layer_class(
                current_fusion_dim, hidden_dim, bias=not cfg.USE_BATCHNORM_FC
            ).to(self.device)
            fusion_layers.append(linear_layer)
            if cfg.USE_BATCHNORM_FC:
                fusion_layers.append(nn.BatchNorm1d(hidden_dim).to(self.device))
            fusion_layers.append(cfg.COMBINED_ACTIVATION())
            if cfg.DROPOUT_FC > 0:
                fusion_layers.append(nn.Dropout(cfg.DROPOUT_FC).to(self.device))
            current_fusion_dim = hidden_dim
        return nn.Sequential(*fusion_layers), current_fusion_dim

    def _build_output_heads(self):
        head_linear_layer_class = NoisyLinear if self.use_noisy else nn.Linear
        output_units_per_stream = (
            self.action_dim * self.num_atoms
            if self.use_distributional
            else self.action_dim
        )
        value_units = self.num_atoms if self.use_distributional else 1

        if self.dueling:
            self.value_head = head_linear_layer_class(
                self.head_input_dim, value_units
            ).to(self.device)
            self.advantage_head = head_linear_layer_class(
                self.head_input_dim, output_units_per_stream
            ).to(self.device)
        else:
            self.output_head = head_linear_layer_class(
                self.head_input_dim, output_units_per_stream
            ).to(self.device)

    def forward(
        self, grid_tensor: torch.Tensor, shape_tensor: torch.Tensor
    ) -> torch.Tensor:
        """Forward pass, accepts separate grid and shape tensors."""
        model_device = next(self.parameters()).device
        if grid_tensor.device != model_device:
            grid_tensor = grid_tensor.to(model_device)
        if shape_tensor.device != model_device:
            shape_tensor = shape_tensor.to(model_device)

        # --- MODIFIED: Validate grid shape expects 2 channels ---
        expected_grid_shape = (self.grid_c, self.grid_h, self.grid_w)
        if grid_tensor.ndim != 4 or grid_tensor.shape[1:] != expected_grid_shape:
            raise ValueError(
                f"AgentNetwork forward: Invalid grid_tensor shape {grid_tensor.shape}. Expected [B, {self.grid_c}, {self.grid_h}, {self.grid_w}]."
            )
        # --- END MODIFIED ---

        batch_size = grid_tensor.size(0)
        expected_shape_flat_dim = self.num_shape_slots * self.shape_feat_per_slot
        if shape_tensor.ndim == 3 and shape_tensor.shape[1:] == (
            self.num_shape_slots,
            self.shape_feat_per_slot,
        ):
            shape_tensor_flat = shape_tensor.view(batch_size, -1)
        elif (
            shape_tensor.ndim == 2 and shape_tensor.shape[1] == expected_shape_flat_dim
        ):
            shape_tensor_flat = shape_tensor  # Already flattened
        else:
            raise ValueError(
                f"AgentNetwork forward: Invalid shape_tensor shape {shape_tensor.shape}. Expected [B, {self.num_shape_slots}, {self.shape_feat_per_slot}] or [B, {expected_shape_flat_dim}]."
            )

        # Process features
        conv_output = self.conv_base(grid_tensor)
        conv_output_flat = conv_output.view(batch_size, -1)

        shape_output = self.shape_mlp(shape_tensor_flat)

        combined_features = torch.cat((conv_output_flat, shape_output), dim=1)
        fused_output = self.fusion_mlp(combined_features)

        # Output Heads
        if self.dueling:
            value = self.value_head(fused_output)
            advantage = self.advantage_head(fused_output)

            if self.use_distributional:
                value = value.view(batch_size, 1, self.num_atoms)
                advantage = advantage.view(batch_size, self.action_dim, self.num_atoms)
                adv_mean = advantage.mean(dim=1, keepdim=True)
                dist_logits = value + (advantage - adv_mean)
            else:
                adv_mean = advantage.mean(dim=1, keepdim=True)
                dist_logits = value + (
                    advantage - adv_mean
                )  # Output shape [B, action_dim]

        else:  # Non-Dueling
            dist_logits = self.output_head(fused_output)
            if self.use_distributional:
                dist_logits = dist_logits.view(
                    batch_size, self.action_dim, self.num_atoms
                )
            # Else: output shape [B, action_dim]

        return dist_logits

    def reset_noise(self):
        """Resets noise in all NoisyLinear layers within the network."""
        if self.use_noisy:
            for module in self.modules():
                if isinstance(module, NoisyLinear):
                    module.reset_noise()


File: environment/game_state.py
# File: environment/game_state.py
import time
import numpy as np
from typing import List, Optional, Tuple, Dict, Union  # Added Dict, Union
from collections import deque
from typing import Deque

from .grid import Grid
from .shape import Shape
from config import EnvConfig, RewardConfig

# --- MODIFIED: Define StateType for clarity ---
StateType = Dict[str, np.ndarray]  # e.g., {"grid": ndarray, "shapes": ndarray}
# --- END MODIFIED ---


class GameState:
    def __init__(self):
        self.env_config = EnvConfig()  # Store config instance
        self.grid = Grid(self.env_config)  # Pass config to Grid
        self.shapes: List[Optional[Shape]] = [
            Shape() for _ in range(self.env_config.NUM_SHAPE_SLOTS)
        ]
        self.score = 0.0  # Cumulative RL reward
        self.game_score = 0  # Game-specific score
        self.lines_cleared_this_episode = 0
        self.blink_time = 0.0
        self.last_time = time.time()
        self.freeze_time = 0.0
        # --- NEW: Timer for line clear flash ---
        self.line_clear_flash_time = 0.0
        # --- END NEW ---
        self.game_over = False
        self._last_action_valid = True
        self.rewards = RewardConfig
        # --- NEW: Attributes for interactive play ---
        self.demo_selected_shape_idx: int = 0
        self.demo_target_row: int = self.env_config.ROWS // 2
        self.demo_target_col: int = self.env_config.COLS // 2
        # --- END NEW ---

    def reset(self) -> StateType:  # Return new StateType
        self.grid = Grid(self.env_config)
        self.shapes = [Shape() for _ in range(self.env_config.NUM_SHAPE_SLOTS)]
        self.score = 0.0
        self.game_score = 0
        self.lines_cleared_this_episode = 0
        self.blink_time = 0.0
        self.freeze_time = 0.0
        # --- NEW: Reset flash timer ---
        self.line_clear_flash_time = 0.0
        # --- END NEW ---
        self.game_over = False
        self._last_action_valid = True
        self.last_time = time.time()
        # --- NEW: Reset demo state ---
        self.demo_selected_shape_idx = 0
        self.demo_target_row = self.env_config.ROWS // 2
        self.demo_target_col = self.env_config.COLS // 2
        # --- END NEW ---
        return self.get_state()

    def valid_actions(self) -> List[int]:
        if self.game_over or self.freeze_time > 0:
            return []
        acts = []
        locations_per_shape = self.grid.rows * self.grid.cols
        for i, sh in enumerate(self.shapes):
            if not sh:
                continue
            # --- OPTIMIZATION: Check only potentially valid root cells ---
            # Instead of checking all R*C cells, we could optimize,
            # but for moderate grids, checking all is simpler and likely fast enough.
            for r in range(self.grid.rows):
                for c in range(self.grid.cols):
                    if self.grid.can_place(sh, r, c):
                        action_index = i * locations_per_shape + (
                            r * self.grid.cols + c
                        )
                        acts.append(action_index)
        return acts

    def is_over(self) -> bool:
        return self.game_over

    def is_frozen(self) -> bool:
        return self.freeze_time > 0

    # --- NEW: Method to check line clear flash status ---
    def is_line_clearing(self) -> bool:
        """Returns true if the line clear flash animation should be active."""
        return self.line_clear_flash_time > 0

    # --- END NEW ---

    def decode_act(self, a: int) -> Tuple[int, int, int]:
        locations_per_shape = self.grid.rows * self.grid.cols
        s_idx = a // locations_per_shape
        pos_idx = a % locations_per_shape
        rr = pos_idx // self.grid.cols
        cc = pos_idx % self.grid.cols
        return (s_idx, rr, cc)

    def _update_timers(self):
        now = time.time()
        dt = now - self.last_time
        self.last_time = now
        self.freeze_time = max(0, self.freeze_time - dt)
        self.blink_time = max(0, self.blink_time - dt)
        # --- NEW: Update flash timer ---
        self.line_clear_flash_time = max(0, self.line_clear_flash_time - dt)
        # --- END NEW ---

    def _handle_invalid_placement(self) -> float:
        self._last_action_valid = False
        reward = self.rewards.PENALTY_INVALID_MOVE
        # Check if game is over due to invalid move AND no valid moves left
        # Note: This check might be redundant if step() already checks valid_actions()
        # before calling this. Let's assume step() handles the game over check.
        # if not self.valid_actions():
        #     self.game_over = True
        #     self.freeze_time = 1.0
        #     reward += self.rewards.PENALTY_GAME_OVER
        return reward

    # --- MODIFIED: _handle_valid_placement updates demo_selected_shape_idx ---
    def _handle_valid_placement(
        self, shp: Shape, s_idx: int, rr: int, cc: int
    ) -> float:
        self._last_action_valid = True
        reward = 0.0  # Start with zero reward for placement

        # Place the shape
        self.grid.place(shp, rr, cc)
        self.shapes[s_idx] = None  # Remove shape from available slots
        self.game_score += len(shp.triangles)  # Update game score

        # --- Auto-select next available shape for demo mode ---
        num_slots = self.env_config.NUM_SHAPE_SLOTS
        if num_slots > 0:
            next_idx = (s_idx + 1) % num_slots  # Start checking from the next slot
            found_next = False
            for _ in range(num_slots):  # Check all slots at most once
                # Check if the slot index is valid and the shape exists
                if (
                    0 <= next_idx < len(self.shapes)
                    and self.shapes[next_idx] is not None
                ):
                    self.demo_selected_shape_idx = next_idx
                    found_next = True
                    break
                next_idx = (next_idx + 1) % num_slots  # Move to the next slot

            # If after checking all slots, none are available (should only happen briefly before refill)
            if not found_next:
                self.demo_selected_shape_idx = (
                    0  # Default to the first slot (or next available after refill)
                )
        # --- End auto-select logic ---

        # Clear lines and get rewards/score
        lines_cleared, triangles_cleared = self.grid.clear_filled_rows()
        self.lines_cleared_this_episode += lines_cleared

        # Apply line clear rewards (more significant now)
        if lines_cleared == 1:
            reward += self.rewards.REWARD_CLEAR_1
        elif lines_cleared == 2:
            reward += self.rewards.REWARD_CLEAR_2
        elif lines_cleared >= 3:
            reward += self.rewards.REWARD_CLEAR_3PLUS

        # Bonus game score for cleared triangles
        if triangles_cleared > 0:
            self.game_score += triangles_cleared * 2
            self.blink_time = 0.5
            self.freeze_time = 0.5
            # --- NEW: Trigger flash effect ---
            self.line_clear_flash_time = 0.3  # Duration of flash in seconds
            # --- END NEW ---

        # Penalty for creating holes (can be tuned)
        num_holes = self.grid.count_holes()
        reward += num_holes * self.rewards.PENALTY_HOLE_PER_HOLE

        # Refill shapes if all slots are empty
        if all(x is None for x in self.shapes):
            self.shapes = [Shape() for _ in range(self.env_config.NUM_SHAPE_SLOTS)]
            # --- If auto-select failed earlier, re-select now that shapes are refilled ---
            if not found_next:
                # Try to find the first available shape again, default to 0 if all fail somehow
                first_available = next(
                    (i for i, s in enumerate(self.shapes) if s is not None), 0
                )
                self.demo_selected_shape_idx = first_available
            # --- End re-select after refill ---

        # Check if game is over after placement (no more valid moves)
        # This check is crucial here
        if not self.valid_actions():
            self.game_over = True
            self.freeze_time = 1.0
            reward += self.rewards.PENALTY_GAME_OVER  # Apply game over penalty

        return reward

    # --- END MODIFIED ---

    def step(self, a: int) -> Tuple[float, bool]:
        """Performs one game step based on the chosen action."""
        self._update_timers()

        if self.game_over:
            return (0.0, True)
        if self.freeze_time > 0:
            return (0.0, False)

        # Check if the chosen action 'a' is actually valid *now*
        # This prevents issues if the agent selects an action that became invalid
        # between the time valid_actions() was called and step() is executed.
        # We rely on the agent masking correctly. If an invalid action is passed,
        # we penalize it.

        s_idx, rr, cc = self.decode_act(a)
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None

        # Check placement validity
        is_valid_placement = shp is not None and self.grid.can_place(shp, rr, cc)

        if is_valid_placement:
            current_rl_reward = self._handle_valid_placement(shp, s_idx, rr, cc)
        else:
            current_rl_reward = self._handle_invalid_placement()
            # If the *only* reason the game ends is because the agent chose an invalid
            # move when valid moves *did* exist, we might not set game_over here,
            # but the penalty should discourage it. If no valid moves exist at all,
            # game_over should have been set earlier or will be set in _handle_valid_placement
            # after the next valid move (if any). Let's ensure game over is checked robustly.
            if not self.valid_actions():  # Double check if NO valid actions remain
                if not self.game_over:  # Avoid applying penalty twice
                    self.game_over = True
                    self.freeze_time = 1.0
                    current_rl_reward += self.rewards.PENALTY_GAME_OVER

        # No REWARD_ALIVE_STEP anymore
        self.score += current_rl_reward
        return (current_rl_reward, self.game_over)

    # --- MODIFIED: get_state returns a dictionary ---
    def get_state(self) -> StateType:
        """
        Generates the state representation as a dictionary containing
        'grid' (numpy array [C, H, W]) and 'shapes' (numpy array [N_SLOTS, FEAT_PER_SHAPE]).
        """
        # 1. Grid Features
        grid_state = self.grid.get_feature_matrix()  # Shape: [C, H, W] C=2 now

        # 2. Shape Features
        shape_features_per = self.env_config.SHAPE_FEATURES_PER_SHAPE
        num_shapes_expected = self.env_config.NUM_SHAPE_SLOTS
        shape_features_total = num_shapes_expected * shape_features_per
        shape_rep = np.zeros(
            (num_shapes_expected, shape_features_per), dtype=np.float32
        )

        # Normalization constants (adjust if needed)
        max_tris_norm = 6.0
        max_h_norm = float(self.grid.rows)
        max_w_norm = float(self.grid.cols)

        for i in range(num_shapes_expected):
            s = self.shapes[i] if i < len(self.shapes) else None
            if s:
                tri = s.triangles
                n = len(tri)
                ups = sum(1 for (_, _, u) in tri if u)
                dns = n - ups
                mnr, mnc, mxr, mxc = s.bbox()
                height = mxr - mnr + 1
                width = mxc - mnc + 1

                # Normalize features (clip to [0, 1])
                shape_rep[i, 0] = np.clip(float(n) / max_tris_norm, 0.0, 1.0)
                shape_rep[i, 1] = np.clip(float(ups) / max_tris_norm, 0.0, 1.0)
                shape_rep[i, 2] = np.clip(float(dns) / max_tris_norm, 0.0, 1.0)
                shape_rep[i, 3] = np.clip(float(height) / max_h_norm, 0.0, 1.0)
                shape_rep[i, 4] = np.clip(float(width) / max_w_norm, 0.0, 1.0)
            # else: Features remain 0 if shape slot is empty

        # --- Return dictionary ---
        state_dict = {
            "grid": grid_state.astype(np.float32),
            "shapes": shape_rep.astype(
                np.float32
            ),  # Shape: [NUM_SLOTS, FEAT_PER_SHAPE]
        }
        return state_dict

    # --- END MODIFIED ---

    def is_blinking(self) -> bool:
        return self.blink_time > 0

    def get_shapes(self) -> List[Shape]:
        return [s for s in self.shapes if s is not None]

    # --- NEW: Methods for Interactive Control ---
    def cycle_shape(self, direction: int):
        """Cycles the selected shape index (direction +1 or -1)."""
        if self.game_over or self.freeze_time > 0:
            return
        num_slots = self.env_config.NUM_SHAPE_SLOTS
        if num_slots <= 0:
            return

        current_idx = self.demo_selected_shape_idx
        for _ in range(num_slots):  # Try all slots
            current_idx = (current_idx + direction + num_slots) % num_slots
            if (
                0 <= current_idx < len(self.shapes)
                and self.shapes[current_idx] is not None
            ):
                self.demo_selected_shape_idx = current_idx
                return  # Found a non-empty slot
        # If loop finishes, it means all slots are empty (or only one shape total)
        # Keep the current index in this case.

    def move_target(self, dr: int, dc: int):
        """Moves the target placement coordinate."""
        if self.game_over or self.freeze_time > 0:
            return
        self.demo_target_row = np.clip(self.demo_target_row + dr, 0, self.grid.rows - 1)
        self.demo_target_col = np.clip(self.demo_target_col + dc, 0, self.grid.cols - 1)

    def get_action_for_current_selection(self) -> Optional[int]:
        """Converts the current demo selection (shape, row, col) into an action index."""
        if self.game_over or self.freeze_time > 0:
            return None
        s_idx = self.demo_selected_shape_idx
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None
        if shp is None:
            return None  # No shape in selected slot

        rr, cc = self.demo_target_row, self.demo_target_col

        # Check if the placement at the target is valid *now*
        if self.grid.can_place(shp, rr, cc):
            locations_per_shape = self.grid.rows * self.grid.cols
            action_index = s_idx * locations_per_shape + (rr * self.grid.cols + cc)
            return action_index
        else:
            return None  # Invalid placement at target

    def get_current_selection_info(self) -> Tuple[Optional[Shape], int, int]:
        """Returns the currently selected shape, target row, and target col."""
        s_idx = self.demo_selected_shape_idx
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None
        return shp, self.demo_target_row, self.demo_target_col

    # --- END NEW ---


File: environment/grid.py
import numpy as np
from typing import List, Tuple, Optional
from .triangle import Triangle
from .shape import Shape
from config import EnvConfig


class Grid:
    """Represents the game board composed of Triangles."""

    def __init__(self, env_config: EnvConfig):
        self.rows = env_config.ROWS
        self.cols = env_config.COLS
        self.triangles: List[List[Triangle]] = []
        self._create()  # Call the modified create method

    # --- _create applies adjustment for extra padding ---
    def _create(self) -> None:
        """Initializes the grid with Triangle objects based on playable columns per row."""
        # Define the BASE number of desired playable columns for centering
        cols_per_row = [9, 11, 13, 15, 15, 13, 11, 9]  # For ROWS=8, COLS=15

        if len(cols_per_row) != self.rows:
            raise ValueError(
                f"Grid._create error: Length of cols_per_row ({len(cols_per_row)}) must match EnvConfig.ROWS ({self.rows})"
            )
        if max(cols_per_row) > self.cols:
            raise ValueError(
                f"Grid._create error: Max playable columns ({max(cols_per_row)}) exceeds EnvConfig.COLS ({self.cols})"
            )

        self.triangles = []
        for r in range(self.rows):
            rowt = []
            # Get the target playable width for this row before adjustment
            base_playable_cols = cols_per_row[r]

            # Calculate initial centering based on base_playable_cols
            if base_playable_cols <= 0:
                initial_death_cols_left = self.cols
            elif base_playable_cols >= self.cols:
                initial_death_cols_left = 0
            else:
                initial_death_cols_left = (self.cols - base_playable_cols) // 2
            initial_first_death_col_right = initial_death_cols_left + base_playable_cols

            # --- ADJUSTMENT: Add 1 extra dead cell padding on each side ---
            # Increase left padding by 1, decrease right boundary by 1
            # This effectively reduces the playable width by 2 compared to base_playable_cols
            adjusted_death_cols_left = initial_death_cols_left + 1
            adjusted_first_death_col_right = initial_first_death_col_right - 1
            # --- END ADJUSTMENT ---

            for c in range(self.cols):
                # Use the ADJUSTED boundaries to determine death cells
                # A cell is dead if it's left of the adjusted start or at/right of the adjusted end.
                # Ensure right boundary doesn't cross left boundary if base_playable_cols is small
                is_death_cell = (
                    (c < adjusted_death_cols_left)
                    or (
                        c >= adjusted_first_death_col_right
                        and adjusted_first_death_col_right > adjusted_death_cols_left
                    )
                    or (base_playable_cols <= 2)
                )  # Mark all dead if base playable was 2 or less after adjustment

                is_up_cell = (r + c) % 2 == 0
                tri = Triangle(r, c, is_up=is_up_cell, is_death=is_death_cell)
                rowt.append(tri)
            self.triangles.append(rowt)

    # --- END MODIFIED ---

    def valid(self, r: int, c: int) -> bool:
        """Check if (r, c) is within grid boundaries."""
        return 0 <= r < self.rows and 0 <= c < self.cols

    def can_place(self, shp: Shape, rr: int, cc: int) -> bool:
        """Checks if a shape can be placed at the target root position (rr, cc)."""
        for dr, dc, up in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if not self.valid(nr, nc):
                return False
            # Check bounds before accessing triangles list
            if not (
                0 <= nr < len(self.triangles) and 0 <= nc < len(self.triangles[nr])
            ):
                return False  # Should not happen if self.valid passed, but safety check
            tri = self.triangles[nr][nc]
            # Cannot place on death cells or occupied cells or if orientation mismatches
            if tri.is_death or tri.is_occupied or (tri.is_up != up):
                return False
        return True

    def place(self, shp: Shape, rr: int, cc: int) -> None:
        """Places a shape onto the grid at the target root position."""
        for dr, dc, _ in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if self.valid(nr, nc):
                # Check bounds before accessing triangles list
                if not (
                    0 <= nr < len(self.triangles) and 0 <= nc < len(self.triangles[nr])
                ):
                    continue  # Skip if out of bounds
                tri = self.triangles[nr][nc]
                # Double-check placement logic redundancy (can_place should prevent this)
                if not tri.is_death and not tri.is_occupied:
                    tri.is_occupied = True
                    tri.color = shp.color

    # --- MODIFIED: clear_filled_rows REMOVES gravity ---
    def clear_filled_rows(self) -> Tuple[int, int]:
        """
        Clears fully occupied rows (ignoring death cells) by marking triangles as
        unoccupied. Does NOT apply gravity.
        Returns: (number of lines cleared, number of triangles in cleared lines).
        """
        lines_cleared = 0
        triangles_cleared = 0
        rows_to_clear_indices = []

        # 1. Identify rows to clear
        for r in range(self.rows):
            # Check bounds before accessing triangles list
            if not (0 <= r < len(self.triangles)):
                continue
            rowt = self.triangles[r]
            is_row_full = True
            num_placeable_triangles_in_row = 0
            for t in rowt:
                if not t.is_death:
                    num_placeable_triangles_in_row += 1
                    if not t.is_occupied:
                        is_row_full = False
                        break  # No need to check further in this row

            # Only mark for clearing if the row has placeable triangles and all are full
            if is_row_full and num_placeable_triangles_in_row > 0:
                rows_to_clear_indices.append(r)
                lines_cleared += 1

        # 2. Clear the triangles in marked rows (set unoccupied)
        for r_idx in rows_to_clear_indices:
            # Check bounds before accessing triangles list
            if not (0 <= r_idx < len(self.triangles)):
                continue
            for t in self.triangles[r_idx]:
                if (
                    not t.is_death and t.is_occupied
                ):  # Only reset occupied, non-death cells
                    triangles_cleared += 1
                    t.is_occupied = False
                    t.color = None  # Reset color

        # --- REMOVED Gravity Logic ---
        # The grid structure remains unchanged.

        return lines_cleared, triangles_cleared

    # --- END MODIFIED ---

    def count_holes(self) -> int:
        """Counts empty, non-death cells with an occupied cell somewhere above them."""
        holes = 0
        for c in range(self.cols):
            occupied_above = False
            for r in range(self.rows):
                # Check bounds before accessing triangles list
                if not (
                    0 <= r < len(self.triangles) and 0 <= c < len(self.triangles[r])
                ):
                    continue  # Skip if out of bounds
                tri = self.triangles[r][c]
                if tri.is_death:
                    # If a death cell is encountered, reset occupied_above for that column segment
                    occupied_above = False
                    continue

                if tri.is_occupied:
                    occupied_above = True
                elif not tri.is_occupied and occupied_above:
                    holes += 1
        return holes

    def get_feature_matrix(self) -> np.ndarray:
        """
        Returns grid state as a [Channel, Height, Width] numpy array (float32).
        Channels: 0=Occupied, 1=Is_Up
        """
        grid_state = np.zeros((2, self.rows, self.cols), dtype=np.float32)
        for r in range(self.rows):
            for c in range(self.cols):
                # Check bounds before accessing triangles list
                if not (
                    0 <= r < len(self.triangles) and 0 <= c < len(self.triangles[r])
                ):
                    continue  # Skip if out of bounds
                t = self.triangles[r][c]
                if not t.is_death:  # Only consider non-death cells for features
                    grid_state[0, r, c] = 1.0 if t.is_occupied else 0.0
                    grid_state[1, r, c] = 1.0 if t.is_up else 0.0
                # else: Death cells remain 0.0 in both channels
        return grid_state


File: environment/__init__.py


File: environment/triangle.py
# File: environment/triangle.py
# (No changes needed)
from typing import Tuple, Optional, List


class Triangle:
    """Represents a single triangular cell on the grid."""

    def __init__(self, row: int, col: int, is_up: bool, is_death: bool = False):
        self.row = row
        self.col = col
        self.is_up = is_up  # True if pointing up, False if pointing down
        self.is_death = is_death  # True if part of the unplayable border
        self.is_occupied = is_death  # Occupied if it's a death cell initially
        self.color: Optional[Tuple[int, int, int]] = (
            None  # Color if occupied by a shape
        )

    def get_points(
        self, ox: int, oy: int, cw: int, ch: int
    ) -> List[Tuple[float, float]]:
        """Calculates the vertex points for drawing the triangle."""
        x = ox + self.col * (
            cw * 0.75
        )  # Horizontal position based on column and overlap
        y = oy + self.row * ch  # Vertical position based on row
        if self.is_up:
            # Points for an upward-pointing triangle
            return [(x, y + ch), (x + cw, y + ch), (x + cw / 2, y)]
        else:
            # Points for a downward-pointing triangle
            return [(x, y), (x + cw, y), (x + cw / 2, y + ch)]


File: environment/shape.py
# File: environment/shape.py
# (No changes needed)
import random
from typing import List, Tuple
from config import EnvConfig, VisConfig  # Needs VisConfig only for colors

GOOGLE_COLORS = VisConfig.GOOGLE_COLORS  # Use colors from VisConfig


class Shape:
    """Represents a polyomino-like shape made of triangles."""

    def __init__(self) -> None:
        # List of (relative_row, relative_col, is_up) tuples defining the shape
        self.triangles: List[Tuple[int, int, bool]] = []
        self.color: Tuple[int, int, int] = random.choice(GOOGLE_COLORS)
        self._generate()  # Generate the shape structure

    def _generate(self) -> None:
        """Generates a random shape by adding adjacent triangles."""
        n = random.randint(1, 5)  # Number of triangles in the shape
        first_up = random.choice([True, False])  # Orientation of the root triangle
        self.triangles.append((0, 0, first_up))  # Add the root triangle at (0,0)

        # Add remaining triangles adjacent to existing ones
        for _ in range(n - 1):
            # Find valid neighbors of the *last added* triangle
            lr, lc, lu = self.triangles[-1]
            nbrs = self._find_valid_neighbors(lr, lc, lu)
            if nbrs:
                self.triangles.append(random.choice(nbrs))
            # else: Could break early if no valid neighbors found, shape < n

    def _find_valid_neighbors(
        self, r: int, c: int, up: bool
    ) -> List[Tuple[int, int, bool]]:
        """Finds potential neighbor triangles that are not already part of the shape."""
        if up:  # Neighbors of an UP triangle are DOWN triangles
            ns = [(r, c - 1, False), (r, c + 1, False), (r + 1, c, False)]
        else:  # Neighbors of a DOWN triangle are UP triangles
            ns = [(r, c - 1, True), (r, c + 1, True), (r - 1, c, True)]
        # Return only neighbors that are not already in self.triangles
        return [n for n in ns if n not in self.triangles]

    def bbox(self) -> Tuple[int, int, int, int]:
        """Calculates the bounding box (min_r, min_c, max_r, max_c) of the shape."""
        if not self.triangles:
            return (0, 0, 0, 0)
        rr = [t[0] for t in self.triangles]
        cc = [t[1] for t in self.triangles]
        return (min(rr), min(cc), max(rr), max(cc))


File: stats/__init__.py
# File: stats/__init__.py
from .stats_recorder import StatsRecorderBase
from .simple_stats_recorder import SimpleStatsRecorder
from .tensorboard_logger import TensorBoardStatsRecorder

__all__ = ["StatsRecorderBase", "SimpleStatsRecorder", "TensorBoardStatsRecorder"]


File: stats/simple_stats_recorder.py
# File: stats/simple_stats_recorder.py
import time
from collections import deque
from typing import Deque, Dict, Any, Optional, Union, List, Tuple, Callable
import numpy as np
import torch
from .stats_recorder import StatsRecorderBase
from config import EnvConfig, StatsConfig  # Removed EnvConfig import, not needed here
import warnings


class SimpleStatsRecorder(StatsRecorderBase):
    """
    Records stats in memory using deques for rolling averages.
    Provides no-op implementations for histogram, image, hparam, graph logging.
    Primarily used internally by TensorBoardStatsRecorder for UI/console display,
    or can be used standalone for simple console logging.
    """

    def __init__(
        self,
        console_log_interval: int = 50_000,
        avg_window: int = StatsConfig.STATS_AVG_WINDOW,
    ):
        if avg_window <= 0:
            avg_window = 100
        self.console_log_interval = (
            max(1, console_log_interval) if console_log_interval > 0 else -1
        )
        self.avg_window = avg_window

        # Data Deques
        step_reward_window = max(avg_window * 10, 1000)
        self.step_rewards: Deque[float] = deque(
            maxlen=step_reward_window
        )  # Keep for potential detailed reward analysis
        self.losses: Deque[float] = deque(maxlen=avg_window)
        self.grad_norms: Deque[float] = deque(maxlen=avg_window)
        self.avg_max_qs: Deque[float] = deque(maxlen=avg_window)
        self.episode_scores: Deque[float] = deque(maxlen=avg_window)
        self.episode_lengths: Deque[int] = deque(maxlen=avg_window)
        self.game_scores: Deque[int] = deque(maxlen=avg_window)
        self.episode_lines_cleared: Deque[int] = deque(maxlen=avg_window)
        self.sps_values: Deque[float] = deque(maxlen=avg_window)
        self.buffer_sizes: Deque[int] = deque(maxlen=avg_window)
        self.beta_values: Deque[float] = deque(maxlen=avg_window)
        self.best_rl_score_history: Deque[float] = deque(maxlen=avg_window)
        self.best_game_score_history: Deque[int] = deque(maxlen=avg_window)
        self.lr_values: Deque[float] = deque(maxlen=avg_window)
        self.epsilon_values: Deque[float] = deque(
            maxlen=avg_window
        )  # Keep even if using Noisy

        # Current State / Best Values
        self.total_episodes = 0
        self.total_lines_cleared = 0
        self.current_epsilon: float = 0.0
        self.current_beta: float = 0.0
        self.current_buffer_size: int = 0
        self.current_global_step: int = 0
        self.current_sps: float = 0.0
        self.current_lr: float = 0.0

        # --- ENHANCED Best Tracking ---
        self.best_score: float = -float("inf")
        self.previous_best_score: float = -float("inf")
        self.best_score_step: int = 0

        self.best_game_score: float = -float("inf")  # Use float for consistent init
        self.previous_best_game_score: float = -float("inf")
        self.best_game_score_step: int = 0

        self.best_loss: float = float("inf")
        self.previous_best_loss: float = float("inf")
        self.best_loss_step: int = 0
        # --- END ENHANCED ---

        self.last_log_time: float = time.time()
        self.last_log_step: int = 0
        self.start_time: float = time.time()
        print(
            f"[SimpleStatsRecorder] Initialized. Avg Window: {self.avg_window}, Console Log Interval: {self.console_log_interval if self.console_log_interval > 0 else 'Disabled'}"
        )

    def record_episode(
        self,
        episode_score: float,
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,
    ):
        current_step = (
            global_step if global_step is not None else self.current_global_step
        )

        self.episode_scores.append(episode_score)
        self.episode_lengths.append(episode_length)
        if game_score is not None:
            self.game_scores.append(game_score)
        if lines_cleared is not None:
            self.episode_lines_cleared.append(lines_cleared)
            self.total_lines_cleared += lines_cleared
        self.total_episodes = episode_num
        step_info = f"at Step ~{current_step/1e6:.1f}M"

        # --- ENHANCED Best Score Tracking ---
        if episode_score > self.best_score:
            self.previous_best_score = self.best_score
            self.best_score = episode_score
            self.best_score_step = current_step
            prev_str = (
                f"{self.previous_best_score:.2f}"
                if self.previous_best_score > -float("inf")
                else "N/A"
            )
            print(
                f"\n--- 🏆 New Best RL: {self.best_score:.2f} {step_info} (Prev: {prev_str}) ---"
            )

        if game_score is not None and game_score > self.best_game_score:
            self.previous_best_game_score = self.best_game_score
            self.best_game_score = float(game_score)  # Store as float
            self.best_game_score_step = current_step
            prev_str = (
                f"{self.previous_best_game_score:.0f}"
                if self.previous_best_game_score > -float("inf")
                else "N/A"
            )
            print(
                f"--- 🎮 New Best Game: {self.best_game_score:.0f} {step_info} (Prev: {prev_str}) ---"
            )
        # --- END ENHANCED ---

        # Update history deques for plotting best scores over time
        current_best_rl = self.best_score if self.best_score > -float("inf") else 0.0
        current_best_game = (
            int(self.best_game_score) if self.best_game_score > -float("inf") else 0
        )
        self.best_rl_score_history.append(current_best_rl)
        self.best_game_score_history.append(current_best_game)

    def record_step(self, step_data: Dict[str, Any]):
        g_step = step_data.get("global_step", self.current_global_step)
        if g_step > self.current_global_step:
            self.current_global_step = g_step

        if "loss" in step_data and step_data["loss"] is not None and g_step > 0:
            current_loss = step_data["loss"]
            self.losses.append(current_loss)
            # --- ENHANCED Best Loss Tracking ---
            if current_loss < self.best_loss:
                self.previous_best_loss = self.best_loss
                self.best_loss = current_loss
                self.best_loss_step = g_step
                # Optional: Print new best loss to console
                # prev_str = f"{self.previous_best_loss:.4f}" if self.previous_best_loss < float("inf") else "N/A"
                # print(f"--- ✨ New Best Loss: {self.best_loss:.4f} at Step ~{g_step/1e6:.1f}M (Prev: {prev_str}) ---")
            # --- END ENHANCED ---

        if (
            "grad_norm" in step_data
            and step_data["grad_norm"] is not None
            and g_step > 0
        ):
            self.grad_norms.append(step_data["grad_norm"])
        # Removed step_reward tracking, focus on episode rewards
        # if "step_reward" in step_data and step_data["step_reward"] is not None:
        #     self.step_rewards.append(step_data["step_reward"])
        if (
            "avg_max_q" in step_data
            and step_data["avg_max_q"] is not None
            and g_step > 0
        ):
            self.avg_max_qs.append(step_data["avg_max_q"])
        if "beta" in step_data and step_data["beta"] is not None:
            self.current_beta = step_data["beta"]
            self.beta_values.append(self.current_beta)
        if "buffer_size" in step_data and step_data["buffer_size"] is not None:
            self.current_buffer_size = step_data["buffer_size"]
            self.buffer_sizes.append(self.current_buffer_size)
        if "lr" in step_data and step_data["lr"] is not None:
            self.current_lr = step_data["lr"]
            self.lr_values.append(self.current_lr)
        if "epsilon" in step_data and step_data["epsilon"] is not None:
            self.current_epsilon = step_data["epsilon"]
            self.epsilon_values.append(self.current_epsilon)

        if "step_time" in step_data and step_data["step_time"] > 1e-6:
            num_steps_in_call = step_data.get("num_steps_processed", 1)
            sps = num_steps_in_call / step_data["step_time"]
            self.sps_values.append(sps)
            self.current_sps = sps  # Update current SPS immediately

        self.log_summary(g_step)

    def get_summary(self, current_global_step: Optional[int] = None) -> Dict[str, Any]:
        if current_global_step is None:
            current_global_step = self.current_global_step

        # Use np.mean with checks for empty deques
        def safe_mean(q: Deque, default=0.0) -> float:
            return float(np.mean(q)) if q else default

        summary = {
            # Averages
            "avg_score_window": safe_mean(self.episode_scores),
            "avg_length_window": safe_mean(self.episode_lengths),
            "avg_loss_window": safe_mean(self.losses),
            "avg_max_q_window": safe_mean(self.avg_max_qs),
            "avg_game_score_window": safe_mean(self.game_scores),
            "avg_lines_cleared_window": safe_mean(self.episode_lines_cleared),
            "avg_sps_window": safe_mean(self.sps_values, default=self.current_sps),
            "avg_lr_window": safe_mean(self.lr_values, default=self.current_lr),
            # Current / Total
            "total_episodes": self.total_episodes,
            "beta": self.current_beta,
            "buffer_size": self.current_buffer_size,
            "steps_per_second": self.current_sps,  # Use the latest calculated SPS
            "global_step": current_global_step,
            "current_lr": self.current_lr,
            # --- ENHANCED Best Tracking ---
            "best_score": self.best_score,
            "previous_best_score": self.previous_best_score,
            "best_score_step": self.best_score_step,
            "best_game_score": self.best_game_score,
            "previous_best_game_score": self.previous_best_game_score,
            "best_game_score_step": self.best_game_score_step,
            "best_loss": self.best_loss,
            "previous_best_loss": self.previous_best_loss,
            "best_loss_step": self.best_loss_step,
            # --- END ENHANCED ---
            # Counts (for debugging/UI)
            "num_ep_scores": len(self.episode_scores),
            "num_losses": len(self.losses),
        }
        return summary

    def log_summary(self, global_step: int):
        """Logs summary statistics to the console periodically."""
        if (
            self.console_log_interval <= 0
            or global_step < self.last_log_step + self.console_log_interval
        ):
            return

        summary = self.get_summary(global_step)
        elapsed_runtime = time.time() - self.start_time
        runtime_hrs = elapsed_runtime / 3600

        best_score_val = (
            f"{summary['best_score']:.2f}"
            if summary["best_score"] > -float("inf")
            else "N/A"
        )
        best_loss_val = (
            f"{summary['best_loss']:.4f}"
            if summary["best_loss"] < float("inf")
            else "N/A"
        )

        log_str = (
            f"[{runtime_hrs:.1f}h|Stats] Step: {global_step/1e6:<6.2f}M | "
            f"Ep: {summary['total_episodes']:<7} | SPS: {summary['steps_per_second']:<5.0f} | "
            f"RLScore(Avg): {summary['avg_score_window']:<6.2f} (Best: {best_score_val}) | "
            f"Loss(Avg): {summary['avg_loss_window']:.4f} (Best: {best_loss_val}) | "
            f"LR: {summary['current_lr']:.1e} | "
            f"Buf: {summary['buffer_size']/1e6:.2f}M"
        )
        # Add PER Beta if used
        if (
            summary["beta"] > 0 and summary["beta"] < 1.0
        ):  # Crude check if PER is active
            log_str += f" | Beta: {summary['beta']:.3f}"

        print(log_str)

        self.last_log_time = time.time()
        self.last_log_step = global_step

    def get_plot_data(self) -> Dict[str, Deque]:
        """Returns copies of deques needed for UI plotting."""
        # Ensure all expected keys exist, even if empty
        return {
            "episode_scores": self.episode_scores.copy(),
            "episode_lengths": self.episode_lengths.copy(),
            "losses": self.losses.copy(),
            "avg_max_qs": self.avg_max_qs.copy(),
            "game_scores": self.game_scores.copy(),
            "episode_lines_cleared": self.episode_lines_cleared.copy(),
            "sps_values": self.sps_values.copy(),
            "buffer_sizes": self.buffer_sizes.copy(),
            "beta_values": self.beta_values.copy(),
            "best_rl_score_history": self.best_rl_score_history.copy(),
            "best_game_score_history": self.best_game_score_history.copy(),
            "lr_values": self.lr_values.copy(),
            "epsilon_values": self.epsilon_values.copy(),
        }

    # --- No-op methods ---
    def record_histogram(
        self,
        tag: str,
        values: Union[np.ndarray, torch.Tensor, List[float]],
        global_step: int,
    ):
        pass

    def record_image(
        self, tag: str, image: Union[np.ndarray, torch.Tensor], global_step: int
    ):
        pass

    def record_hparams(self, hparam_dict: Dict[str, Any], metric_dict: Dict[str, Any]):
        pass

    def record_graph(
        self, model: torch.nn.Module, input_to_model: Optional[torch.Tensor] = None
    ):
        pass

    def close(self):
        print("[SimpleStatsRecorder] Closed.")


File: stats/stats_recorder.py
# File: stats/stats_recorder.py
import time
from abc import ABC, abstractmethod
from collections import deque
from typing import Deque, List, Dict, Any, Optional, Union  # Added Union
import numpy as np
import torch  # Added torch for Tensor type hints


class StatsRecorderBase(ABC):
    """Base class for recording training statistics."""

    @abstractmethod
    def record_episode(
        self,
        episode_score: float,  # RL Score
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,
    ):
        """Record stats for a completed episode."""
        pass

    @abstractmethod
    def record_step(self, step_data: Dict[str, Any]):
        """Record stats from a training or environment step (e.g., loss, rewards)."""
        pass

    # --- New Abstract Methods ---
    @abstractmethod
    def record_histogram(
        self,
        tag: str,
        values: Union[np.ndarray, torch.Tensor, List[float]],
        global_step: int,
    ):
        """Record a histogram of values."""
        pass

    @abstractmethod
    def record_image(
        self, tag: str, image: Union[np.ndarray, torch.Tensor], global_step: int
    ):
        """Record an image."""
        pass

    @abstractmethod
    def record_hparams(self, hparam_dict: Dict[str, Any], metric_dict: Dict[str, Any]):
        """Record hyperparameters and final/key metrics."""
        pass

    @abstractmethod
    def record_graph(
        self, model: torch.nn.Module, input_to_model: Optional[torch.Tensor] = None
    ):
        """Record the model graph."""
        pass

    # --- End New ---

    @abstractmethod
    def get_summary(self, current_global_step: int) -> Dict[str, Any]:
        """Return a dictionary containing summary statistics (usually averaged)."""
        pass

    @abstractmethod
    def log_summary(self, global_step: int):
        """Trigger the logging action (e.g., print to console, write to TensorBoard)."""
        pass

    @abstractmethod
    def close(self):
        """Perform any necessary cleanup (e.g., close files/writers)."""
        pass


