File: requirements.txt
pygame>=2.1.0
numpy>=1.20.0
torch>=1.10.0
tensorboard
cloudpickle
torchvision
matplotlib

File: README.md
# TriCrack DQN - Pygame & TensorBoard RL Agent üéÆüß†üìä

This project implements a Deep Q-Network (DQN) agent trained to play "TriCrack", a custom Tetris-like game built with Pygame where players place triangular polyominoes (shapes) onto a triangular grid. The agent uses several advanced DQN techniques, including Noisy Nets for exploration, Dueling Architecture, Double DQN, Prioritized Experience Replay (PER), and N-Step Returns.

The project features a real-time Pygame visualization displaying multiple parallel game environments, performance statistics, and agent status. Comprehensive logging is integrated using TensorBoard, allowing detailed tracking of metrics, histograms of key values (Q-values, losses, rewards), environment state images, hyperparameters, and the model graph.

## Key Features ‚ú®

*   **Advanced DQN Implementation:**
    *   **Noisy Nets:** Uses noisy linear layers for efficient exploration, eliminating the need for epsilon-greedy scheduling.
    *   **Dueling DQN:** Separates value and advantage streams for better policy evaluation.
    *   **Double DQN:** Decouples action selection and evaluation to reduce Q-value overestimation.
    *   **Prioritized Experience Replay (PER):** Samples transitions based on TD-error magnitude for more efficient learning.
    *   **N-Step Returns:** Improves sample efficiency by bootstrapping over multiple steps.
*   **Custom Environment:** Includes the "TriCrack" game logic (`environment/`), featuring a unique triangular grid, shape generation, placement rules, and scoring.
*   **Pygame Visualization:** Provides an interactive UI (`ui/`) showing:
    *   Live rendering of multiple parallel environments (configurable number).
    *   Real-time statistics panel (scores, loss, SPS, buffer status, etc.).
    *   Agent status (Training, Paused, Buffering, Error).
    *   Interactive buttons (Start/Pause, Cleanup).
    *   Informative tooltips for UI elements.
*   **TensorBoard Logging:** Comprehensive logging via `stats/tensorboard_logger.py`:
    *   Scalar metrics (rewards, loss, Q-values, episode length, SPS, etc.).
    *   Histograms (Q-value distributions, TD-errors, actions, rewards per step).
    *   Image logging (sample environment states).
    *   Hyperparameter logging (saves run configuration).
    *   Model graph visualization.
*   **Configurable Network Architecture:** Uses a fusion network (`agent/networks/agent_network.py`):
    *   CNN branch processes the grid state.
    *   MLP branch processes features of available shapes.
    *   Features are fused and passed through further MLP layers before the Dueling heads.
*   **Vectorized Environments:** Runs multiple environments in parallel (`EnvConfig.NUM_ENVS`) for stable and faster training.
*   **Highly Configurable:** Centralized configuration file (`config.py`) allows easy modification of nearly all aspects: environment dimensions, reward shaping, network architecture, DQN hyperparameters, training loop settings, buffer settings, logging, and visualization.
*   **Checkpointing & Resuming:** Saves agent model state and replay buffer state periodically and allows resuming training from saved checkpoints/buffers.

## Screenshots / Demo üì∏

*(It's highly recommended to add screenshots or a GIF here!)*

*   **Placeholder:** *[Screenshot of the Pygame UI showing multiple environments, the stats panel, and buttons]*
    *   *Caption:* The main Pygame interface showing live training progress with multiple environments rendered simultaneously and key statistics displayed on the left panel.
*   **Placeholder:** *[Screenshot of the TensorBoard dashboard showing scalar plots like average score and loss]*
    *   *Caption:* TensorBoard scalar plots tracking episode rewards, loss, and other metrics over training steps.
*   **Placeholder:** *[Screenshot of the TensorBoard dashboard showing histograms like Q-value distribution]*
    *   *Caption:* TensorBoard histograms providing insights into the distribution of Q-values, TD-errors, and actions during training.
*   **Placeholder:** *[Screenshot of the TensorBoard dashboard showing logged environment images]*
    *   *Caption:* TensorBoard image tab displaying sample environment states logged periodically during training.

## Requirements üìú

*   Python (>= 3.9 recommended)
*   Pygame (>= 2.1.0)
*   NumPy (>= 1.20.0)
*   PyTorch (>= 1.10.0)
*   TensorBoard
*   Cloudpickle
*   Torchvision (potentially needed by TensorBoard for image logging, included for safety)

You can install the dependencies using the provided `requirements.txt` file.

## Setup & Installation ‚öôÔ∏è

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Configuration üõ†Ô∏è

The core configuration file is `config.py`. This file centralizes *all* hyperparameters and settings for the environment, agent, training, buffer, visualization, and logging.

Key configuration classes and parameters include:

*   **General:** `DEVICE` (auto-detects CUDA/MPS/CPU), `RANDOM_SEED`, paths.
*   **`EnvConfig`:** `NUM_ENVS`, `ROWS`, `COLS`, state/action dimensions.
*   **`RewardConfig`:** Coefficients for reward shaping (placing shapes, clearing lines, penalties).
*   **`DQNConfig`:** `GAMMA`, `TARGET_UPDATE_FREQ`, `LEARNING_RATE`, algorithm variants (`USE_DOUBLE_DQN`, `USE_DUELING`, `USE_NOISY_NETS`).
*   **`TrainConfig`:** `BATCH_SIZE`, `LEARN_START_STEP`, `TOTAL_TRAINING_STEPS`, `LEARN_FREQ`, checkpointing (`CHECKPOINT_SAVE_FREQ`, `LOAD_CHECKPOINT_PATH`).
*   **`BufferConfig`:** `REPLAY_BUFFER_SIZE`, N-Step settings (`USE_N_STEP`, `N_STEP`), PER settings (`USE_PER`, `PER_ALPHA`, `PER_BETA_START`, `PER_BETA_FRAMES`).
*   **`ModelConfig.Network`:** CNN parameters (`CONV_CHANNELS`, kernels, etc.), MLP parameters (`SHAPE_MLP_HIDDEN_DIM`, `COMBINED_FC_DIMS`), activations, batch norm, dropout.
*   **`VisConfig`:** Pygame window size, colors, number of environments to render (`NUM_ENVS_TO_RENDER`).
*   **`TensorBoardConfig`:** Logging flags (`LOG_HISTOGRAMS`, `LOG_IMAGES`), frequencies.

**Important:** Modify `config.py` directly to experiment with different settings. The project automatically creates run-specific directories for logs and checkpoints based on the `RUN_ID` generated at startup.

**Loading Checkpoints/Buffers:**
To resume training or load a pre-trained model/buffer, set the `LOAD_CHECKPOINT_PATH` and/or `LOAD_BUFFER_PATH` variables in `TrainConfig` within `config.py` to the respective file paths. Ensure the loaded state is compatible with the current configuration (especially model architecture and buffer settings like PER/N-Step).

## Running the Code ‚ñ∂Ô∏è

To start the training process with the Pygame visualization:

```bash
python main_pygame.py
```

The application will initialize the environments, agent, buffer, and logger, then open the Pygame window.

**UI Controls:**

*   **Train/Pause Button:** Click to toggle the training process.
*   **'P' Key:** Keyboard shortcut to toggle training.
*   **Cleanup This Run Button:** Click to *delete the saved agent checkpoint and buffer file specifically for the current run* and re-initialize the agent/buffer/trainer. **Use with caution!** This is useful for restarting a run from scratch without changing the `RUN_ID` or TensorBoard logs. A confirmation prompt will appear.
*   **'ESC' Key:** Cancels the cleanup confirmation prompt or exits the application if the prompt is not active.
*   **Window Resizing:** The Pygame window is resizable.

The console will print status updates, configuration details, and periodic summary statistics (if `StatsConfig.CONSOLE_LOG_FREQ` > 0).

## TensorBoard Integration üìà

All detailed logs are saved to the `logs/tensorboard/<RUN_ID>` directory. To view them:

1.  Make sure you have TensorBoard installed (`pip install tensorboard`).
2.  Run TensorBoard, pointing it to the base log directory:
    ```bash
    tensorboard --logdir logs
    ```
    *(Make sure you run this command from the project's root directory or provide the absolute path to the `logs` folder).*
3.  Open your web browser and navigate to the URL provided by TensorBoard (usually `http://localhost:6006`).

In TensorBoard, you can explore:

*   **Scalars:** Track metrics like average rewards, loss, Q-values, episode length, steps per second, PER beta, etc., over time.
*   **Histograms:** Visualize the distribution of Q-values, TD-errors, actions taken, and rewards received per step. This helps diagnose training stability and agent behavior.
*   **Images:** View sample snapshots of environment states logged periodically (if `TensorBoardConfig.LOG_IMAGES` is enabled).
*   **HParams:** See the hyperparameters used for the run and compare final metrics across different runs.
*   **Graphs:** Visualize the computational graph of the neural network model.

## Code Structure üìÅ

```
.
‚îú‚îÄ‚îÄ agent/                # DQN Agent logic
‚îÇ   ‚îú‚îÄ‚îÄ networks/         # Neural network modules (CNN+MLP fusion, Noisy Layer)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_network.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ noisy_layer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ replay_buffer/    # Replay buffer implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_buffer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ buffer_utils.py # Factory function
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nstep_buffer.py # N-Step wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prioritized_buffer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sum_tree.py     # Helper for PER
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ uniform_buffer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.py      # Main agent class
‚îÇ   ‚îú‚îÄ‚îÄ model_factory.py  # Creates the network instance
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ checkpoints/          # Directory for saving model and buffer states (run-specific subdirs)
‚îú‚îÄ‚îÄ config.py             # Central configuration file
‚îú‚îÄ‚îÄ environment/          # TriCrack game environment logic
‚îÇ   ‚îú‚îÄ‚îÄ game_state.py     # Main environment class, step logic, state representation
‚îÇ   ‚îú‚îÄ‚îÄ grid.py           # Triangular grid logic
‚îÇ   ‚îú‚îÄ‚îÄ shape.py          # Shape generation and properties
‚îÇ   ‚îú‚îÄ‚îÄ triangle.py       # Single triangle cell representation
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ logs/                 # Directory for TensorBoard logs (run-specific subdirs)
‚îú‚îÄ‚îÄ stats/                # Statistics recording and logging
‚îÇ   ‚îú‚îÄ‚îÄ simple_stats_recorder.py # Basic in-memory recorder
‚îÇ   ‚îú‚îÄ‚îÄ stats_recorder.py # Base abstract class
‚îÇ   ‚îú‚îÄ‚îÄ tensorboard_logger.py # Logs to TensorBoard
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ training/             # Training orchestration
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py        # Coordinates agent-environment interaction, learning updates
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ ui/                   # Pygame User Interface rendering
‚îÇ   ‚îú‚îÄ‚îÄ renderer.py       # Handles drawing the UI, envs, stats, tooltips
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ utils/                # Utility functions and types
‚îÇ   ‚îú‚îÄ‚îÄ helpers.py        # Device selection, seeding, saving/loading objects
‚îÇ   ‚îú‚îÄ‚îÄ types.py          # Type definitions (Transition, StateType, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ main_pygame.py        # Main application entry point, Pygame loop
‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies
‚îî‚îÄ‚îÄ README.md             # This file
```

## Core Concepts & Implementation Details üßê

*   **Environment (TriCrack):** The game involves placing randomly generated triangular shapes (1-5 triangles) onto a grid. Rows are cleared if all non-"death" cells in them are occupied. The goal is typically to maximize score (shaped reward) or lines cleared. "Death" cells form a border and cannot be used.
*   **State Representation:** The state is a flat NumPy array concatenating:
    1.  **Grid Features:** A flattened representation of the grid state `[3, H, W]`, where channels represent `Occupied`, `Is_Up`, `Is_Death`.
    2.  **Shape Features:** A flattened representation of features for each available shape slot (`EnvConfig.NUM_SHAPE_SLOTS`). Features per shape include normalized counts of triangles, up-pointing triangles, down-pointing triangles, height, and width (`EnvConfig.SHAPE_FEATURES_PER_SHAPE`).
*   **Action Space:** The action space is discrete and potentially large: `Action = Shape_Slot_Index * (Grid_Row * Grid_Col) + Grid_Position_Index`. The agent must select which available shape to place and where its root triangle should go. Invalid actions (e.g., placing outside bounds, overlapping occupied/death cells) are masked out during action selection.
*   **Reward Shaping:** The RL reward (`GameState.score`) is shaped to guide learning, defined in `RewardConfig`. It includes small rewards for placing triangles, larger rewards for clearing lines, penalties for invalid moves and creating holes, a large penalty for game over, and a small reward for surviving each step. The `GameState.game_score` tracks a separate, simpler game-native score.
*   **DQN Variants:**
    *   **Double DQN:** Uses the online network to select the best action for the next state and the target network to evaluate that action, reducing overestimation bias.
    *   **Dueling DQN:** The network head splits into a Value stream (state value V(s)) and an Advantage stream (action advantages A(s,a)). These are combined (`Q = V + (A - mean(A))`) to get final Q-values, often leading to better performance by learning state values more effectively.
    *   **Noisy Nets:** Replaces standard linear layers in the network heads with `NoisyLinear` layers. These layers add learnable parametric noise to weights and biases, inducing exploration based on the agent's uncertainty. This often performs better than traditional epsilon-greedy exploration.
    *   **PER:** Stores experiences in a SumTree based on their TD-error. Transitions with higher errors (more "surprising") are sampled more frequently. Importance sampling weights are used to correct the bias introduced by non-uniform sampling.
    *   **N-Step Returns:** Calculates returns over N steps instead of just one, allowing rewards to propagate faster and often stabilizing learning. The `NStepBufferWrapper` handles this calculation.
*   **Network Architecture:** The `AgentNetwork` uses a multi-branch approach suitable for the heterogeneous state representation:
    1.  A CNN processes the spatial grid features.
    2.  An MLP processes the shape features.
    3.  The flattened outputs are concatenated and fed into a fusion MLP.
    4.  The final layer(s) implement the (Dueling) Q-value heads, using NoisyLinear if enabled.
*   **Visualization & Logging:** `UIRenderer` manages the Pygame display. `TensorBoardStatsRecorder` leverages `SimpleStatsRecorder` for in-memory averaging (used by the UI) and writes detailed data, histograms, images, etc., to TensorBoard files using `torch.utils.tensorboard.SummaryWriter`.

## Potential Improvements / Future Work üöÄ

*   **Hyperparameter Optimization:** Systematically tune learning rates, network sizes, buffer capacity, PER/N-Step parameters, reward coefficients, etc.
*   **Architecture Exploration:** Experiment with different CNN architectures (e.g., ResNet blocks), attention mechanisms, or alternative ways to fuse grid and shape information.
*   **Advanced RL Algorithms:** Implement more modern algorithms like Rainbow DQN, PPO, SAC (requires adaptation for discrete action space), or MuZero.
*   **Game Mechanics:** Add features like gravity after line clears, different shape generation logic, or varying difficulty levels.
*   **Performance Optimization:** Profile code (e.g., environment stepping, network forward pass) and optimize bottlenecks. Investigate alternative parallelization strategies if needed.
*   **Testing Framework:** Add unit and integration tests for environment logic, agent components, and buffer operations.
*   **Curriculum Learning:** Start with simpler configurations (smaller grid, fewer shapes) and gradually increase complexity.

## License üìÑ

*(Specify your license here, e.g., MIT License)*

This project is licensed under the MIT License. See the LICENSE file for details.

---

*Happy Training!*

File: .resumed.txt
File: requirements.txt
pygame>=2.1.0
numpy>=1.20.0
torch>=1.10.0
tensorboard
cloudpickle
torchvision
matplotlib

File: README.md
# TriCrack DQN - Pygame & TensorBoard RL Agent üéÆüß†üìä

This project implements a Deep Q-Network (DQN) agent trained to play "TriCrack", a custom Tetris-like game built with Pygame where players place triangular polyominoes (shapes) onto a triangular grid. The agent uses several advanced DQN techniques, including Noisy Nets for exploration, Dueling Architecture, Double DQN, Prioritized Experience Replay (PER), and N-Step Returns.

The project features a real-time Pygame visualization displaying multiple parallel game environments, performance statistics, and agent status. Comprehensive logging is integrated using TensorBoard, allowing detailed tracking of metrics, histograms of key values (Q-values, losses, rewards), environment state images, hyperparameters, and the model graph.

## Key Features ‚ú®

*   **Advanced DQN Implementation:**
    *   **Noisy Nets:** Uses noisy linear layers for efficient exploration, eliminating the need for epsilon-greedy scheduling.
    *   **Dueling DQN:** Separates value and advantage streams for better policy evaluation.
    *   **Double DQN:** Decouples action selection and evaluation to reduce Q-value overestimation.
    *   **Prioritized Experience Replay (PER):** Samples transitions based on TD-error magnitude for more efficient learning.
    *   **N-Step Returns:** Improves sample efficiency by bootstrapping over multiple steps.
*   **Custom Environment:** Includes the "TriCrack" game logic (`environment/`), featuring a unique triangular grid, shape generation, placement rules, and scoring.
*   **Pygame Visualization:** Provides an interactive UI (`ui/`) showing:
    *   Live rendering of multiple parallel environments (configurable number).
    *   Real-time statistics panel (scores, loss, SPS, buffer status, etc.).
    *   Agent status (Training, Paused, Buffering, Error).
    *   Interactive buttons (Start/Pause, Cleanup).
    *   Informative tooltips for UI elements.
*   **TensorBoard Logging:** Comprehensive logging via `stats/tensorboard_logger.py`:
    *   Scalar metrics (rewards, loss, Q-values, episode length, SPS, etc.).
    *   Histograms (Q-value distributions, TD-errors, actions, rewards per step).
    *   Image logging (sample environment states).
    *   Hyperparameter logging (saves run configuration).
    *   Model graph visualization.
*   **Configurable Network Architecture:** Uses a fusion network (`agent/networks/agent_network.py`):
    *   CNN branch processes the grid state.
    *   MLP branch processes features of available shapes.
    *   Features are fused and passed through further MLP layers before the Dueling heads.
*   **Vectorized Environments:** Runs multiple environments in parallel (`EnvConfig.NUM_ENVS`) for stable and faster training.
*   **Highly Configurable:** Centralized configuration file (`config.py`) allows easy modification of nearly all aspects: environment dimensions, reward shaping, network architecture, DQN hyperparameters, training loop settings, buffer settings, logging, and visualization.
*   **Checkpointing & Resuming:** Saves agent model state and replay buffer state periodically and allows resuming training from saved checkpoints/buffers.

## Screenshots / Demo üì∏

*(It's highly recommended to add screenshots or a GIF here!)*

*   **Placeholder:** *[Screenshot of the Pygame UI showing multiple environments, the stats panel, and buttons]*
    *   *Caption:* The main Pygame interface showing live training progress with multiple environments rendered simultaneously and key statistics displayed on the left panel.
*   **Placeholder:** *[Screenshot of the TensorBoard dashboard showing scalar plots like average score and loss]*
    *   *Caption:* TensorBoard scalar plots tracking episode rewards, loss, and other metrics over training steps.
*   **Placeholder:** *[Screenshot of the TensorBoard dashboard showing histograms like Q-value distribution]*
    *   *Caption:* TensorBoard histograms providing insights into the distribution of Q-values, TD-errors, and actions during training.
*   **Placeholder:** *[Screenshot of the TensorBoard dashboard showing logged environment images]*
    *   *Caption:* TensorBoard image tab displaying sample environment states logged periodically during training.

## Requirements üìú

*   Python (>= 3.9 recommended)
*   Pygame (>= 2.1.0)
*   NumPy (>= 1.20.0)
*   PyTorch (>= 1.10.0)
*   TensorBoard
*   Cloudpickle
*   Torchvision (potentially needed by TensorBoard for image logging, included for safety)

You can install the dependencies using the provided `requirements.txt` file.

## Setup & Installation ‚öôÔ∏è

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Configuration üõ†Ô∏è

The core configuration file is `config.py`. This file centralizes *all* hyperparameters and settings for the environment, agent, training, buffer, visualization, and logging.

Key configuration classes and parameters include:

*   **General:** `DEVICE` (auto-detects CUDA/MPS/CPU), `RANDOM_SEED`, paths.
*   **`EnvConfig`:** `NUM_ENVS`, `ROWS`, `COLS`, state/action dimensions.
*   **`RewardConfig`:** Coefficients for reward shaping (placing shapes, clearing lines, penalties).
*   **`DQNConfig`:** `GAMMA`, `TARGET_UPDATE_FREQ`, `LEARNING_RATE`, algorithm variants (`USE_DOUBLE_DQN`, `USE_DUELING`, `USE_NOISY_NETS`).
*   **`TrainConfig`:** `BATCH_SIZE`, `LEARN_START_STEP`, `TOTAL_TRAINING_STEPS`, `LEARN_FREQ`, checkpointing (`CHECKPOINT_SAVE_FREQ`, `LOAD_CHECKPOINT_PATH`).
*   **`BufferConfig`:** `REPLAY_BUFFER_SIZE`, N-Step settings (`USE_N_STEP`, `N_STEP`), PER settings (`USE_PER`, `PER_ALPHA`, `PER_BETA_START`, `PER_BETA_FRAMES`).
*   **`ModelConfig.Network`:** CNN parameters (`CONV_CHANNELS`, kernels, etc.), MLP parameters (`SHAPE_MLP_HIDDEN_DIM`, `COMBINED_FC_DIMS`), activations, batch norm, dropout.
*   **`VisConfig`:** Pygame window size, colors, number of environments to render (`NUM_ENVS_TO_RENDER`).
*   **`TensorBoardConfig`:** Logging flags (`LOG_HISTOGRAMS`, `LOG_IMAGES`), frequencies.

**Important:** Modify `config.py` directly to experiment with different settings. The project automatically creates run-specific directories for logs and checkpoints based on the `RUN_ID` generated at startup.

**Loading Checkpoints/Buffers:**
To resume training or load a pre-trained model/buffer, set the `LOAD_CHECKPOINT_PATH` and/or `LOAD_BUFFER_PATH` variables in `TrainConfig` within `config.py` to the respective file paths. Ensure the loaded state is compatible with the current configuration (especially model architecture and buffer settings like PER/N-Step).

## Running the Code ‚ñ∂Ô∏è

To start the training process with the Pygame visualization:

```bash
python main_pygame.py
```

The application will initialize the environments, agent, buffer, and logger, then open the Pygame window.

**UI Controls:**

*   **Train/Pause Button:** Click to toggle the training process.
*   **'P' Key:** Keyboard shortcut to toggle training.
*   **Cleanup This Run Button:** Click to *delete the saved agent checkpoint and buffer file specifically for the current run* and re-initialize the agent/buffer/trainer. **Use with caution!** This is useful for restarting a run from scratch without changing the `RUN_ID` or TensorBoard logs. A confirmation prompt will appear.
*   **'ESC' Key:** Cancels the cleanup confirmation prompt or exits the application if the prompt is not active.
*   **Window Resizing:** The Pygame window is resizable.

The console will print status updates, configuration details, and periodic summary statistics (if `StatsConfig.CONSOLE_LOG_FREQ` > 0).

## TensorBoard Integration üìà

All detailed logs are saved to the `logs/tensorboard/<RUN_ID>` directory. To view them:

1.  Make sure you have TensorBoard installed (`pip install tensorboard`).
2.  Run TensorBoard, pointing it to the base log directory:
    ```bash
    tensorboard --logdir logs
    ```
    *(Make sure you run this command from the project's root directory or provide the absolute path to the `logs` folder).*
3.  Open your web browser and navigate to the URL provided by TensorBoard (usually `http://localhost:6006`).

In TensorBoard, you can explore:

*   **Scalars:** Track metrics like average rewards, loss, Q-values, episode length, steps per second, PER beta, etc., over time.
*   **Histograms:** Visualize the distribution of Q-values, TD-errors, actions taken, and rewards received per step. This helps diagnose training stability and agent behavior.
*   **Images:** View sample snapshots of environment states logged periodically (if `TensorBoardConfig.LOG_IMAGES` is enabled).
*   **HParams:** See the hyperparameters used for the run and compare final metrics across different runs.
*   **Graphs:** Visualize the computational graph of the neural network model.

## Code Structure üìÅ

```
.
‚îú‚îÄ‚îÄ agent/                # DQN Agent logic
‚îÇ   ‚îú‚îÄ‚îÄ networks/         # Neural network modules (CNN+MLP fusion, Noisy Layer)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent_network.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ noisy_layer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ replay_buffer/    # Replay buffer implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_buffer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ buffer_utils.py # Factory function
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nstep_buffer.py # N-Step wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prioritized_buffer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sum_tree.py     # Helper for PER
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ uniform_buffer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ dqn_agent.py      # Main agent class
‚îÇ   ‚îú‚îÄ‚îÄ model_factory.py  # Creates the network instance
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ checkpoints/          # Directory for saving model and buffer states (run-specific subdirs)
‚îú‚îÄ‚îÄ config.py             # Central configuration file
‚îú‚îÄ‚îÄ environment/          # TriCrack game environment logic
‚îÇ   ‚îú‚îÄ‚îÄ game_state.py     # Main environment class, step logic, state representation
‚îÇ   ‚îú‚îÄ‚îÄ grid.py           # Triangular grid logic
‚îÇ   ‚îú‚îÄ‚îÄ shape.py          # Shape generation and properties
‚îÇ   ‚îú‚îÄ‚îÄ triangle.py       # Single triangle cell representation
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ logs/                 # Directory for TensorBoard logs (run-specific subdirs)
‚îú‚îÄ‚îÄ stats/                # Statistics recording and logging
‚îÇ   ‚îú‚îÄ‚îÄ simple_stats_recorder.py # Basic in-memory recorder
‚îÇ   ‚îú‚îÄ‚îÄ stats_recorder.py # Base abstract class
‚îÇ   ‚îú‚îÄ‚îÄ tensorboard_logger.py # Logs to TensorBoard
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ training/             # Training orchestration
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py        # Coordinates agent-environment interaction, learning updates
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ ui/                   # Pygame User Interface rendering
‚îÇ   ‚îú‚îÄ‚îÄ renderer.py       # Handles drawing the UI, envs, stats, tooltips
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ utils/                # Utility functions and types
‚îÇ   ‚îú‚îÄ‚îÄ helpers.py        # Device selection, seeding, saving/loading objects
‚îÇ   ‚îú‚îÄ‚îÄ types.py          # Type definitions (Transition, StateType, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ main_pygame.py        # Main application entry point, Pygame loop
‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies
‚îî‚îÄ‚îÄ README.md             # This file
```

## Core Concepts & Implementation Details üßê

*   **Environment (TriCrack):** The game involves placing randomly generated triangular shapes (1-5 triangles) onto a grid. Rows are cleared if all non-"death" cells in them are occupied. The goal is typically to maximize score (shaped reward) or lines cleared. "Death" cells form a border and cannot be used.
*   **State Representation:** The state is a flat NumPy array concatenating:
    1.  **Grid Features:** A flattened representation of the grid state `[3, H, W]`, where channels represent `Occupied`, `Is_Up`, `Is_Death`.
    2.  **Shape Features:** A flattened representation of features for each available shape slot (`EnvConfig.NUM_SHAPE_SLOTS`). Features per shape include normalized counts of triangles, up-pointing triangles, down-pointing triangles, height, and width (`EnvConfig.SHAPE_FEATURES_PER_SHAPE`).
*   **Action Space:** The action space is discrete and potentially large: `Action = Shape_Slot_Index * (Grid_Row * Grid_Col) + Grid_Position_Index`. The agent must select which available shape to place and where its root triangle should go. Invalid actions (e.g., placing outside bounds, overlapping occupied/death cells) are masked out during action selection.
*   **Reward Shaping:** The RL reward (`GameState.score`) is shaped to guide learning, defined in `RewardConfig`. It includes small rewards for placing triangles, larger rewards for clearing lines, penalties for invalid moves and creating holes, a large penalty for game over, and a small reward for surviving each step. The `GameState.game_score` tracks a separate, simpler game-native score.
*   **DQN Variants:**
    *   **Double DQN:** Uses the online network to select the best action for the next state and the target network to evaluate that action, reducing overestimation bias.
    *   **Dueling DQN:** The network head splits into a Value stream (state value V(s)) and an Advantage stream (action advantages A(s,a)). These are combined (`Q = V + (A - mean(A))`) to get final Q-values, often leading to better performance by learning state values more effectively.
    *   **Noisy Nets:** Replaces standard linear layers in the network heads with `NoisyLinear` layers. These layers add learnable parametric noise to weights and biases, inducing exploration based on the agent's uncertainty. This often performs better than traditional epsilon-greedy exploration.
    *   **PER:** Stores experiences in a SumTree based on their TD-error. Transitions with higher errors (more "surprising") are sampled more frequently. Importance sampling weights are used to correct the bias introduced by non-uniform sampling.
    *   **N-Step Returns:** Calculates returns over N steps instead of just one, allowing rewards to propagate faster and often stabilizing learning. The `NStepBufferWrapper` handles this calculation.
*   **Network Architecture:** The `AgentNetwork` uses a multi-branch approach suitable for the heterogeneous state representation:
    1.  A CNN processes the spatial grid features.
    2.  An MLP processes the shape features.
    3.  The flattened outputs are concatenated and fed into a fusion MLP.
    4.  The final layer(s) implement the (Dueling) Q-value heads, using NoisyLinear if enabled.
*   **Visualization & Logging:** `UIRenderer` manages the Pygame display. `TensorBoardStatsRecorder` leverages `SimpleStatsRecorder` for in-memory averaging (used by the UI) and writes detailed data, histograms, images, etc., to TensorBoard files using `torch.utils.tensorboard.SummaryWriter`.

## Potential Improvements / Future Work üöÄ

*   **Hyperparameter Optimization:** Systematically tune learning rates, network sizes, buffer capacity, PER/N-Step parameters, reward coefficients, etc.
*   **Architecture Exploration:** Experiment with different CNN architectures (e.g., ResNet blocks), attention mechanisms, or alternative ways to fuse grid and shape information.
*   **Advanced RL Algorithms:** Implement more modern algorithms like Rainbow DQN, PPO, SAC (requires adaptation for discrete action space), or MuZero.
*   **Game Mechanics:** Add features like gravity after line clears, different shape generation logic, or varying difficulty levels.
*   **Performance Optimization:** Profile code (e.g., environment stepping, network forward pass) and optimize bottlenecks. Investigate alternative parallelization strategies if needed.
*   **Testing Framework:** Add unit and integration tests for environment logic, agent components, and buffer operations.
*   **Curriculum Learning:** Start with simpler configurations (smaller grid, fewer shapes) and gradually increase complexity.

## License üìÑ

*(Specify your license here, e.g., MIT License)*

This project is licensed under the MIT License. See the LICENSE file for details.

---

*Happy Training!*

File: main_pygame.py
# File: main_pygame.py
import sys
import pygame
import numpy as np
import os
import time
import traceback
import torch
from typing import List, Tuple, Optional, Dict, Any, Deque, TextIO


# --- Logger Class (Unchanged) ---
class TeeLogger:
    def __init__(self, filepath: str, original_stream: TextIO):
        self.terminal = original_stream
        try:
            self.log_file = open(filepath, "w", encoding="utf-8", buffering=1)
            print(f"[TeeLogger] Logging console output to: {filepath}")
        except Exception as e:
            self.terminal.write(
                f"FATAL ERROR: Could not open log file {filepath}: {e}\n"
            )
            self.log_file = None

    def write(self, message: str):
        self.terminal.write(message)
        if self.log_file:
            try:
                self.log_file.write(message)
            except Exception:
                pass  # Ignore errors writing to log file

    def flush(self):
        self.terminal.flush()
        if self.log_file:
            try:
                self.log_file.flush()
            except Exception:
                pass  # Ignore errors flushing log file

    def close(self):
        if self.log_file:
            try:
                self.log_file.close()
                self.log_file = None
            except Exception as e:
                self.terminal.write(f"Warning: Error closing log file: {e}\n")


# --- End Logger Class ---


# Import configurations
from config import (
    VisConfig,
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    TensorBoardConfig,
    DEVICE,
    RANDOM_SEED,
    BUFFER_SAVE_PATH,
    MODEL_SAVE_PATH,
    BASE_CHECKPOINT_DIR,
    BASE_LOG_DIR,
    RUN_LOG_DIR,
    get_config_dict,
    print_config_info_and_validate,
)

# Import core components & helpers
from environment.game_state import GameState
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from training.trainer import Trainer
from stats.stats_recorder import StatsRecorderBase
from ui.renderer import UIRenderer
from ui.input_handler import InputHandler
from utils.helpers import set_random_seeds, ensure_numpy
from utils.init_checks import run_pre_checks
from init.rl_components import (
    initialize_envs,
    initialize_agent_buffer,
    initialize_stats_recorder,
    initialize_trainer,
)


class MainApp:
    def __init__(self):
        print("Initializing Pygame Application...")
        set_random_seeds(RANDOM_SEED)
        pygame.init()
        pygame.font.init()

        # Store configs
        self.vis_config = VisConfig
        self.env_config = EnvConfig
        self.dqn_config = DQNConfig
        self.train_config = TrainConfig
        self.buffer_config = BufferConfig
        self.model_config = ModelConfig
        self.stats_config = StatsConfig
        self.tensorboard_config = TensorBoardConfig

        self.num_envs = self.env_config.NUM_ENVS
        self.config_dict = get_config_dict()

        # Ensure directories exist
        os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
        os.makedirs(os.path.dirname(BUFFER_SAVE_PATH), exist_ok=True)
        os.makedirs(self.tensorboard_config.LOG_DIR, exist_ok=True)
        print_config_info_and_validate()

        # Pygame setup
        self.screen = pygame.display.set_mode(
            (self.vis_config.SCREEN_WIDTH, self.vis_config.SCREEN_HEIGHT),
            pygame.RESIZABLE,
        )
        pygame.display.set_caption("TriCrack DQN - TensorBoard")
        self.clock = pygame.time.Clock()

        # App state
        self.is_training = False
        self.cleanup_confirmation_active = False
        self.last_cleanup_message_time = 0.0
        self.cleanup_message = ""
        self.status = "Paused"  # Initial status

        # Init Renderer FIRST
        self.renderer = UIRenderer(self.screen, self.vis_config)

        # --- MODIFIED: Remove notification_callback passing ---
        # Init RL components using helpers
        self._initialize_rl_components()
        # --- END MODIFIED ---

        # Init Input Handler (pass callbacks)
        self.input_handler = InputHandler(
            self.screen,
            self.renderer,
            self._toggle_training,
            self._request_cleanup,
            self._cancel_cleanup,
            self._confirm_cleanup,
            self._exit_app,
        )

        print("Initialization Complete. Ready to start.")
        print(f"--- tensorboard --logdir {os.path.abspath(BASE_LOG_DIR)} ---")

    # --- MODIFIED: Remove notification_callback parameter ---
    def _initialize_rl_components(self):
        """Orchestrates the initialization of RL components using helpers."""
        try:
            self.envs = initialize_envs(self.num_envs, self.env_config)
            self.agent, self.buffer = initialize_agent_buffer(
                self.model_config, self.dqn_config, self.env_config, self.buffer_config
            )
            # Stats recorder init no longer needs callback
            self.stats_recorder = initialize_stats_recorder(
                self.stats_config,
                self.tensorboard_config,
                self.config_dict,
                self.agent,
                self.env_config,
                # notification_callback=... # Removed
            )
            # Trainer init no longer needs callback
            self.trainer = initialize_trainer(
                self.envs,
                self.agent,
                self.buffer,
                self.stats_recorder,
                self.env_config,
                self.dqn_config,
                self.train_config,
                self.buffer_config,
                self.model_config,
                # notification_callback=... # Removed
            )
        except Exception as e:
            print(f"FATAL ERROR during RL component initialization: {e}")
            traceback.print_exc()
            pygame.quit()
            sys.exit(1)

    # --- END MODIFIED ---

    # --- Input Handler Callbacks (Unchanged) ---
    def _toggle_training(self):
        self.is_training = not self.is_training
        print(f"Training {'STARTED' if self.is_training else 'PAUSED'}")
        if not self.is_training:
            self._try_save_checkpoint()

    def _request_cleanup(self):
        was_training = self.is_training
        self.is_training = False
        if was_training:
            self._try_save_checkpoint()  # Save before potentially deleting
        self.cleanup_confirmation_active = True
        print("Cleanup requested. Training paused. Confirm action.")

    def _cancel_cleanup(self):
        self.cleanup_confirmation_active = False
        self.cleanup_message = "Cleanup cancelled."
        self.last_cleanup_message_time = time.time()
        print("Cleanup cancelled by user.")

    def _confirm_cleanup(self):
        self._cleanup_data()

    def _exit_app(self) -> bool:
        """Callback for input handler to signal application exit."""
        return False  # Signal to stop the main loop

    # --- Other Methods (Cleanup, Save, Update) ---
    def _cleanup_data(self):
        """Deletes current run's checkpoint/buffer and re-initializes."""
        print("\n--- CLEANUP DATA INITIATED (Current Run Only) ---")
        self.is_training = False
        self.status = "Cleaning"
        self.cleanup_confirmation_active = False
        messages = []

        # 1. Cleanup Trainer and Stats Recorder first
        if hasattr(self, "trainer") and self.trainer:
            print("Running trainer cleanup...")
            try:
                # Pass save_final=False to prevent saving during cleanup
                self.trainer.cleanup(save_final=False)
                self.trainer = None  # Release trainer object
                # Stats recorder is closed by trainer.cleanup, release ref
                if hasattr(self, "stats_recorder"):
                    self.stats_recorder = None
            except Exception as e:
                print(f"Error during trainer cleanup: {e}")
                # Attempt to close recorder manually if trainer cleanup failed
                if hasattr(self, "stats_recorder") and self.stats_recorder:
                    try:
                        self.stats_recorder.close()
                    except Exception as log_e:
                        print(
                            f"Error closing stats recorder during failed cleanup: {log_e}"
                        )
                    self.stats_recorder = None
        elif hasattr(self, "stats_recorder") and self.stats_recorder:
            # If trainer didn't exist but recorder did, close recorder
            try:
                self.stats_recorder.close()
            except Exception as log_e:
                print(f"Error closing stats recorder: {log_e}")
            self.stats_recorder = None

        # 2. Delete Checkpoint and Buffer Files
        for path, desc in [
            (MODEL_SAVE_PATH, "Agent ckpt"),
            (BUFFER_SAVE_PATH, "Buffer state"),
        ]:
            try:
                if os.path.isfile(path):
                    os.remove(path)
                    msg = f"{desc} deleted: {os.path.basename(path)}"
                    print(msg)
                    messages.append(msg)
                else:
                    msg = f"{desc} not found (current run)."
                    print(msg)
                    # messages.append(msg) # Don't show "not found" in UI message
            except OSError as e:
                msg = f"Error deleting {desc}: {e}"
                print(msg)
                messages.append(msg)

        time.sleep(0.1)  # Short pause

        # 3. Re-initialize RL components
        print("Re-initializing RL components after cleanup...")
        try:
            # --- MODIFIED: Remove notification_callback passing ---
            self._initialize_rl_components()
            # --- END MODIFIED ---
            # Re-initialize renderer to clear any old state (like toasts if they existed)
            self.renderer = UIRenderer(self.screen, self.vis_config)
            print("RL components re-initialized.")
            messages.append("RL components re-initialized.")
        except Exception as e:
            print(f"FATAL ERROR during RL component re-initialization: {e}")
            traceback.print_exc()
            self.status = "Error"
            messages.append("ERROR RE-INITIALIZING RL COMPONENTS!")

        # 4. Update UI message
        self.cleanup_message = "\n".join(messages)
        self.last_cleanup_message_time = time.time()
        if self.status != "Error":
            self.status = "Paused"  # Set status back to Paused if re-init successful
        print("--- CLEANUP DATA COMPLETE ---")

    def _try_save_checkpoint(self):
        """Saves checkpoint if not training and trainer exists."""
        if not self.is_training and hasattr(self, "trainer") and self.trainer:
            print("Saving checkpoint on pause...")
            try:
                # Force save ensures it saves even if save interval not reached
                self.trainer.maybe_save_checkpoint(force_save=True)
            except Exception as e:
                print(f"Error saving checkpoint on pause: {e}")
                traceback.print_exc()  # Show details on save error

    def _update(self):
        """Updates the application state and performs training steps."""
        # Update status string based on current state
        if self.cleanup_confirmation_active:
            self.status = "Confirm Cleanup"
        elif not self.is_training and self.status != "Error":
            self.status = "Paused"
        elif not hasattr(self, "trainer") or self.trainer is None:
            if self.status != "Error":  # Avoid overwriting existing error state
                self.status = "Error"
                print("Error: Trainer object not found during update.")
        elif self.trainer.global_step < self.train_config.LEARN_START_STEP:
            self.status = "Buffering"
        elif self.is_training:
            self.status = "Training"

        # Only perform training steps if in Training or Buffering state
        if self.status not in ["Training", "Buffering"]:
            return

        # Double-check trainer exists before stepping
        if not hasattr(self, "trainer") or self.trainer is None:
            print("Error: Trainer became unavailable during _update.")
            self.status = "Error"
            self.is_training = False
            return

        # Perform one trainer step
        try:
            step_start_time = time.time()
            self.trainer.step()
            step_duration = time.time() - step_start_time

            # Optional delay for visualization
            if self.vis_config.VISUAL_STEP_DELAY > 0:
                time.sleep(max(0, self.vis_config.VISUAL_STEP_DELAY - step_duration))

        except Exception as e:
            print(
                f"\n--- ERROR DURING TRAINING UPDATE (Step: {getattr(self.trainer, 'global_step', 'N/A')}) ---"
            )
            traceback.print_exc()
            print(f"--- Pausing training due to error. ---")
            self.is_training = False
            self.status = "Error"

    def _render(self):
        """Renders the entire UI."""
        stats_summary = {}
        plot_data: Dict[str, Deque] = {}

        # Get latest stats from the recorder
        if hasattr(self, "stats_recorder") and self.stats_recorder:
            current_step = getattr(self.trainer, "global_step", 0)
            # Use hasattr for safety, as recorder might be None during cleanup/error
            if hasattr(self.stats_recorder, "get_summary"):
                stats_summary = self.stats_recorder.get_summary(current_step)
            if hasattr(self.stats_recorder, "get_plot_data"):
                plot_data = self.stats_recorder.get_plot_data()
        elif self.status == "Error":
            # Provide minimal stats if in error state
            stats_summary = {"global_step": getattr(self.trainer, "global_step", 0)}
            plot_data = {}

        # Get buffer capacity for display
        buffer_capacity = (
            getattr(self.buffer, "capacity", 0) if hasattr(self, "buffer") else 0
        )

        # Ensure renderer exists
        if not hasattr(self, "renderer") or self.renderer is None:
            print("Error: Renderer not initialized in _render.")
            return

        # Call the main render function
        self.renderer.render_all(
            is_training=self.is_training,
            status=self.status,
            stats_summary=stats_summary,
            buffer_capacity=buffer_capacity,
            envs=(self.envs if hasattr(self, "envs") else []),
            num_envs=self.num_envs,
            env_config=self.env_config,
            cleanup_confirmation_active=self.cleanup_confirmation_active,
            cleanup_message=self.cleanup_message,
            last_cleanup_message_time=self.last_cleanup_message_time,
            tensorboard_log_dir=self.tensorboard_config.LOG_DIR,
            plot_data=plot_data,
        )

        # Clear cleanup message after a delay
        if time.time() - self.last_cleanup_message_time >= 5.0:
            self.cleanup_message = ""

    def run(self):
        """Main application loop."""
        print("Starting main application loop...")
        running = True
        try:
            while running:
                # 1. Handle User Input
                running = self.input_handler.handle_input(
                    self.cleanup_confirmation_active
                )
                if not running:
                    break  # Exit signal received

                # 2. Update State and Train
                try:
                    self._update()
                except Exception as update_err:
                    # Catch errors specifically within the update logic
                    print(f"\n--- UNHANDLED ERROR IN UPDATE LOOP ---")
                    traceback.print_exc()
                    print(f"--- Setting status to Error ---")
                    self.status = "Error"
                    self.is_training = False  # Stop training on error

                # 3. Render UI
                try:
                    self._render()
                except Exception as render_err:
                    # Catch errors specifically within the render logic
                    print(f"\n--- UNHANDLED ERROR IN RENDER LOOP ---")
                    traceback.print_exc()
                    # Don't necessarily stop training on render error, but log it
                    # self.status = "Error" # Optional: Set error status on render fail

                # 4. Control Frame Rate
                self.clock.tick(self.vis_config.FPS if self.vis_config.FPS > 0 else 0)

        except KeyboardInterrupt:
            print("\nCtrl+C detected. Exiting gracefully...")
        except Exception as e:
            # Catch any other unexpected errors in the main loop
            print("\n--- UNHANDLED EXCEPTION IN MAIN LOOP ---")
            traceback.print_exc()
            print("--- EXITING ---")
        finally:
            # --- Cleanup ---
            print("Exiting application...")
            # Ensure trainer cleanup runs to save final state if possible
            if hasattr(self, "trainer") and self.trainer:
                print("Performing final trainer cleanup...")
                try:
                    # Save final checkpoint unless cleanup already happened
                    save_on_exit = self.status != "Cleaning"
                    self.trainer.cleanup(save_final=save_on_exit)
                except Exception as final_cleanup_err:
                    print(f"Error during final trainer cleanup: {final_cleanup_err}")
            elif hasattr(self, "stats_recorder") and self.stats_recorder:
                # Close recorder if trainer cleanup didn't happen/failed
                try:
                    self.stats_recorder.close()
                except Exception as log_e:
                    print(f"Error closing stats recorder on exit: {log_e}")

            pygame.quit()
            print("Application exited.")


if __name__ == "__main__":
    # Setup Dirs
    os.makedirs(BASE_CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(BASE_LOG_DIR, exist_ok=True)
    # Ensure other necessary dirs exist (optional, depends on imports)
    # os.makedirs("ui", exist_ok=True)
    # os.makedirs("stats", exist_ok=True)

    # Setup Logging to file and console
    log_filepath = os.path.join(RUN_LOG_DIR, "console_output.log")
    os.makedirs(RUN_LOG_DIR, exist_ok=True)  # Ensure run-specific log dir exists
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    logger = TeeLogger(log_filepath, original_stdout)
    sys.stdout = logger
    sys.stderr = logger  # Redirect stderr as well

    try:
        # Perform pre-checks before starting the app
        if run_pre_checks():
            app = MainApp()
            app.run()
    except SystemExit:
        print("Exiting due to SystemExit (likely from pre-checks or init error).")
    except Exception as main_err:
        # Catch errors during App initialization or run() call
        print("\n--- UNHANDLED EXCEPTION DURING APP INITIALIZATION OR RUN ---")
        traceback.print_exc()
        print("--- EXITING DUE TO ERROR ---")
    finally:
        # Restore standard output streams and close logger
        if "logger" in locals() and logger:
            logger.close()
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        # Print final message to the actual console
        print(f"Console logging restored. Full log saved to: {log_filepath}")


File: visualization/__init__.py


File: init/rl_components.py
# File: init/rl_components.py
import sys
import traceback
import numpy as np
import torch
from typing import List, Tuple, Optional, Dict, Any, Callable

# Import configurations
from config import (
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    RewardConfig,
    TensorBoardConfig,
    DEVICE,
    BUFFER_SAVE_PATH,
    MODEL_SAVE_PATH,
    get_config_dict,
)

# Import core components
try:
    from environment.game_state import GameState
except ImportError as e:
    print(f"Error importing environment: {e}")
    sys.exit(1)
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from agent.replay_buffer.buffer_utils import create_replay_buffer
from training.trainer import Trainer
from stats.stats_recorder import StatsRecorderBase
from stats.tensorboard_logger import TensorBoardStatsRecorder
from utils.helpers import ensure_numpy


def initialize_envs(
    num_envs: int, env_config: EnvConfig
) -> List[GameState]:  # Unchanged
    print(f"Initializing {num_envs} game environments...")
    try:
        envs = [GameState() for _ in range(num_envs)]
        s_test = envs[0].reset()
        s_np = ensure_numpy(s_test)
        if s_np.shape[0] != env_config.STATE_DIM:
            raise ValueError(
                f"FATAL: State dim mismatch! Env:{s_np.shape[0]}, Cfg:{env_config.STATE_DIM}"
            )
        _ = envs[0].valid_actions()
        _, _ = envs[0].step(0)
        print(f"Successfully initialized {num_envs} environments.")
        return envs
    except Exception as e:
        print(f"FATAL ERROR during env init: {e}")
        traceback.print_exc()
        raise e


def initialize_agent_buffer(  # Unchanged
    model_config: ModelConfig,
    dqn_config: DQNConfig,
    env_config: EnvConfig,
    buffer_config: BufferConfig,
) -> Tuple[DQNAgent, ReplayBufferBase]:
    print("Initializing Agent and Buffer...")
    agent = DQNAgent(config=model_config, dqn_config=dqn_config, env_config=env_config)
    buffer = create_replay_buffer(config=buffer_config, dqn_config=dqn_config)
    print("Agent and Buffer initialized.")
    return agent, buffer


def initialize_stats_recorder(
    stats_config: StatsConfig,
    tb_config: TensorBoardConfig,
    config_dict: Dict[str, Any],
    agent: Optional[DQNAgent],
    env_config: EnvConfig,
    # --- MODIFIED: Removed notification_callback parameter ---
    # notification_callback: Optional[Callable[[str], None]] = None,
) -> StatsRecorderBase:
    """Creates the TensorBoard recorder, logs graph (on CPU) and hparams."""
    print(f"Initializing Stats Recorder (TensorBoard)...")
    avg_window = stats_config.STATS_AVG_WINDOW
    console_log_freq = stats_config.CONSOLE_LOG_FREQ

    dummy_input_cpu = None
    model_for_graph_cpu = None
    if agent and agent.online_net:
        try:
            dummy_state = np.zeros((1, env_config.STATE_DIM), dtype=np.float32)
            dummy_input_cpu = torch.tensor(dummy_state, device="cpu")

            if not hasattr(agent, "dqn_config"):
                raise AttributeError(
                    "DQNAgent instance is missing 'dqn_config' attribute needed for graph logging."
                )

            model_for_graph_cpu = type(agent.online_net)(
                state_dim=env_config.STATE_DIM,
                action_dim=env_config.ACTION_DIM,
                config=agent.online_net.config,
                env_config=agent.online_net.env_config,
                dqn_config=agent.dqn_config,
                dueling=agent.online_net.dueling,
                use_noisy=agent.online_net.use_noisy,
            ).to("cpu")

            model_for_graph_cpu.load_state_dict(agent.online_net.state_dict())
            model_for_graph_cpu.eval()
            print(
                "[Stats Init] Prepared model copy and dummy input on CPU for graph logging."
            )
        except AttributeError as ae:
            print(
                f"Warning: Attribute error preparing model/input for graph logging: {ae}. Check AgentNetwork init."
            )
            dummy_input_cpu = None
            model_for_graph_cpu = None
        except Exception as e:
            print(f"Warning: Failed to prepare model/input for graph logging: {e}")
            traceback.print_exc()
            dummy_input_cpu = None
            model_for_graph_cpu = None

    print(f"Using TensorBoard Logger (Log Dir: {tb_config.LOG_DIR})")
    try:
        # --- MODIFIED: Removed notification_callback argument from the call ---
        stats_recorder = TensorBoardStatsRecorder(
            log_dir=tb_config.LOG_DIR,
            hparam_dict=config_dict,
            model_for_graph=model_for_graph_cpu,
            dummy_input_for_graph=dummy_input_cpu,
            console_log_interval=console_log_freq,
            avg_window=avg_window,
            histogram_log_interval=tb_config.HISTOGRAM_LOG_FREQ,
            image_log_interval=tb_config.IMAGE_LOG_FREQ if tb_config.LOG_IMAGES else -1,
            # notification_callback=notification_callback, # <<< REMOVED THIS LINE
        )
        # --- END MODIFIED ---
        print("Stats Recorder initialized.")
        return stats_recorder
    except Exception as e:
        print(f"FATAL: Error initializing TensorBoardStatsRecorder: {e}. Exiting.")
        traceback.print_exc()
        raise e


def initialize_trainer(
    envs: List[GameState],
    agent: DQNAgent,
    buffer: ReplayBufferBase,
    stats_recorder: StatsRecorderBase,
    env_config: EnvConfig,
    dqn_config: DQNConfig,
    train_config: TrainConfig,
    buffer_config: BufferConfig,
    model_config: ModelConfig,
    # --- MODIFIED: Removed notification_callback parameter ---
    # notification_callback: Optional[Callable[[str], None]] = None,
) -> Trainer:
    print("Initializing Trainer...")
    trainer = Trainer(
        envs=envs,
        agent=agent,
        buffer=buffer,
        stats_recorder=stats_recorder,
        env_config=env_config,
        dqn_config=dqn_config,
        train_config=train_config,
        buffer_config=buffer_config,
        model_config=model_config,
        model_save_path=MODEL_SAVE_PATH,
        buffer_save_path=BUFFER_SAVE_PATH,
        load_checkpoint_path=train_config.LOAD_CHECKPOINT_PATH,
        load_buffer_path=train_config.LOAD_BUFFER_PATH,
        # Trainer itself doesn't directly use the callback
        # notification_callback=notification_callback # <<< REMOVED THIS LINE (already commented, but confirming)
    )
    print("Trainer initialization finished.")
    return trainer


File: init/__init__.py
# File: init/__init__.py
from .rl_components import (
    initialize_envs,
    initialize_agent_buffer,
    initialize_stats_recorder,
    initialize_trainer,
)

__all__ = [
    "initialize_envs",
    "initialize_agent_buffer",
    "initialize_stats_recorder",
    "initialize_trainer",
]


File: ui/tooltips.py
# File: ui/tooltips.py
# --- Tooltip Rendering Logic ---
import pygame
from typing import Tuple, Dict, Optional
from config import VisConfig


class TooltipRenderer:
    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.font_tooltip = self._init_font()
        self.hovered_stat_key: Optional[str] = None
        self.stat_rects: Dict[str, pygame.Rect] = {}  # Updated by UIRenderer
        self.tooltip_texts: Dict[str, str] = {}  # Updated by UIRenderer

    def _init_font(self):
        try:
            return pygame.font.SysFont(None, 18)
        except Exception as e:
            print(f"Warning: SysFont error: {e}. Using default.")
            return pygame.font.Font(None, 18)

    def check_hover(self, mouse_pos: Tuple[int, int]):
        """Checks if the mouse is hovering over any registered stat rect."""
        self.hovered_stat_key = None
        # Iterate in reverse order so tooltips for elements drawn last appear first
        for key, rect in reversed(self.stat_rects.items()):
            # Ensure rect is valid before checking collision
            if (
                rect
                and rect.width > 0
                and rect.height > 0
                and rect.collidepoint(mouse_pos)
            ):
                self.hovered_stat_key = key
                return  # Found one, stop checking

    def render_tooltip(self):
        """Renders the tooltip if a stat is being hovered over."""
        if not self.hovered_stat_key or self.hovered_stat_key not in self.tooltip_texts:
            return  # No active hover or no text for this key

        tooltip_text = self.tooltip_texts[self.hovered_stat_key]
        mouse_pos = pygame.mouse.get_pos()

        # --- Text Wrapping ---
        lines = []
        max_width = 300  # Max tooltip width
        words = tooltip_text.split(" ")
        current_line = ""
        for word in words:
            test_line = current_line + " " + word if current_line else word
            test_surf = self.font_tooltip.render(test_line, True, VisConfig.BLACK)
            if test_surf.get_width() <= max_width:
                current_line = test_line
            else:
                lines.append(current_line)
                current_line = word
        lines.append(current_line)  # Add the last line

        # --- Rendering ---
        line_surfs = [
            self.font_tooltip.render(line, True, VisConfig.BLACK) for line in lines
        ]
        if not line_surfs:
            return  # No lines to render

        total_height = sum(s.get_height() for s in line_surfs)
        max_line_width = max(s.get_width() for s in line_surfs)

        padding = 5
        tooltip_rect = pygame.Rect(
            mouse_pos[0] + 15,  # Offset from cursor
            mouse_pos[1] + 10,
            max_line_width + padding * 2,
            total_height + padding * 2,
        )

        # Ensure tooltip stays on screen
        tooltip_rect.clamp_ip(self.screen.get_rect())

        # Draw background and border
        pygame.draw.rect(self.screen, VisConfig.YELLOW, tooltip_rect, border_radius=3)
        pygame.draw.rect(self.screen, VisConfig.BLACK, tooltip_rect, 1, border_radius=3)

        # Draw text lines
        current_y = tooltip_rect.y + padding
        for surf in line_surfs:
            self.screen.blit(surf, (tooltip_rect.x + padding, current_y))
            current_y += surf.get_height()

    def update_rects_and_texts(
        self, rects: Dict[str, pygame.Rect], texts: Dict[str, str]
    ):
        """Updates the dictionaries used for hover detection and text lookup."""
        self.stat_rects = rects
        self.tooltip_texts = texts


File: ui/renderer.py
# File: ui/renderer.py
import pygame
import time
import traceback
from typing import List, Dict, Any, Optional, Tuple, Deque

from config import VisConfig, EnvConfig, TensorBoardConfig
from environment.game_state import GameState
from .panels import LeftPanelRenderer, GameAreaRenderer
from .overlays import OverlayRenderer
from .tooltips import TooltipRenderer
from .plotter import Plotter


class UIRenderer:
    """Orchestrates rendering of all UI components."""

    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config

        self.plotter = Plotter()
        self.left_panel = LeftPanelRenderer(screen, vis_config, self.plotter)
        self.game_area = GameAreaRenderer(screen, vis_config)
        self.overlays = OverlayRenderer(screen, vis_config)
        self.tooltips = TooltipRenderer(screen, vis_config)

        self.last_plot_update_time = 0
        # --- REMOVED: State for toasts ---
        # self.active_toast_messages: List[Tuple[str, float]] = []
        # self.toast_duration = 4.0
        # --- END REMOVED ---

    def check_hover(self, mouse_pos: Tuple[int, int]):
        """Passes hover check to the tooltip renderer."""
        # Update tooltip renderer with the latest clickable areas from panels
        self.tooltips.update_rects_and_texts(
            self.left_panel.get_stat_rects(), self.left_panel.get_tooltip_texts()
        )
        self.tooltips.check_hover(mouse_pos)

    def force_redraw(self):
        """Forces components like the plotter to redraw on the next frame."""
        self.plotter.last_plot_update_time = 0  # Reset plot timer to force update

    # --- REMOVED: Method to add toasts ---
    # def add_toast(self, message: str):
    #     pass # No longer used
    # --- END REMOVED ---

    # --- REMOVED: Method to clear expired toasts ---
    # def _update_toasts(self):
    #     pass # No longer used
    # --- END REMOVED ---

    def render_all(
        self,
        is_training: bool,
        status: str,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        envs: List[GameState],
        num_envs: int,
        env_config: EnvConfig,
        cleanup_confirmation_active: bool,
        cleanup_message: str,
        last_cleanup_message_time: float,
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
    ):
        """Renders all UI components in the correct order."""
        try:
            self.screen.fill(VisConfig.BLACK)  # Clear screen

            # --- REMOVED: Update active toasts ---
            # self._update_toasts()
            # --- END REMOVED ---

            # 1. Render Main Panels
            self.left_panel.render(
                is_training,
                status,
                stats_summary,
                buffer_capacity,
                tensorboard_log_dir,
                plot_data,
            )
            self.game_area.render(envs, num_envs, env_config)

            # 2. Render Overlays (if active)
            message_active = False
            # Cleanup confirmation takes priority
            if cleanup_confirmation_active:
                self.overlays.render_cleanup_confirmation()
            else:
                # Render status message (e.g., after cleanup) if cleanup confirm is NOT active
                message_active = self.overlays.render_status_message(
                    cleanup_message, last_cleanup_message_time
                )
                # --- REMOVED: Render toasts ---
                # self.overlays.render_toast_notifications(...) # Removed
                # --- END REMOVED ---

            # 3. Render Tooltip (if no blocking overlay is active)
            if not cleanup_confirmation_active and not message_active:
                # Update tooltips again *after* panels are drawn to ensure rects are current
                self.tooltips.update_rects_and_texts(
                    self.left_panel.get_stat_rects(),
                    self.left_panel.get_tooltip_texts(),
                )
                self.tooltips.render_tooltip()

            pygame.display.flip()  # Update the full display

        except pygame.error as e:
            # Handle specific Pygame errors gracefully
            if "video system not initialized" in str(e):
                print("Error: Pygame video system not initialized. Exiting render.")
                # Consider exiting or re-initializing Pygame here if appropriate
            elif "Invalid subsurface rectangle" in str(e):
                print(f"Warning: Invalid subsurface rectangle during rendering: {e}")
                # This might happen during resize or if env rendering fails
            else:
                print(f"Pygame rendering error: {e}")
                traceback.print_exc()  # Print details for other Pygame errors
        except Exception as e:
            print(f"Unexpected critical rendering error: {e}")
            traceback.print_exc()


File: ui/__init__.py
# File: ui/__init__.py
from .renderer import UIRenderer
from .input_handler import InputHandler

__all__ = ["UIRenderer", "InputHandler"]


File: ui/plotter.py
# File: ui/plotter.py
import pygame
import numpy as np
from typing import Dict, Optional, Deque, List, Union, Tuple
from collections import deque
import matplotlib
import time
import warnings

matplotlib.use("Agg")
import matplotlib.pyplot as plt
from io import BytesIO
from config import VisConfig, BufferConfig, StatsConfig, DQNConfig

# Configure Matplotlib style
plt.style.use("dark_background")
plt.rcParams.update(
    {
        "font.size": 9,
        "axes.labelsize": 9,
        "axes.titlesize": 10,
        "xtick.labelsize": 8,
        "ytick.labelsize": 8,
        "legend.fontsize": 7,
        "figure.facecolor": "#262626",
        "axes.facecolor": "#303030",
        "axes.edgecolor": "#707070",
        "axes.labelcolor": "#D0D0D0",
        "xtick.color": "#C0C0C0",
        "ytick.color": "#C0C0C0",
        "grid.color": "#505050",
        "grid.linestyle": "--",
        "grid.alpha": 0.5,
    }
)


def normalize_color_for_matplotlib(
    color_tuple_0_255: Tuple[int, int, int],
) -> Tuple[float, float, float]:
    """Converts a 0-255 RGB tuple to a 0.0-1.0 RGB tuple."""
    if isinstance(color_tuple_0_255, tuple) and len(color_tuple_0_255) == 3:
        return tuple(c / 255.0 for c in color_tuple_0_255)
    else:
        return (0.0, 0.0, 0.0)


class Plotter:
    """Handles creating Pygame surfaces from Matplotlib plots."""

    def __init__(self):
        self.plot_surface: Optional[pygame.Surface] = None
        self.last_plot_update_time: float = 0.0
        self.plot_update_interval: float = 1.0
        self.rolling_window_size = (
            StatsConfig.STATS_AVG_WINDOW
        )  # Window for the average LINE
        self.default_line_width = 1.5
        self.avg_line_width = 2.0
        self.avg_line_alpha = 0.7

    def create_plot_surface(
        self, plot_data: Dict[str, Deque], target_width: int, target_height: int
    ) -> Optional[pygame.Surface]:
        """Creates a Pygame surface containing Matplotlib plots (3x3 layout)."""
        if target_width <= 10 or target_height <= 10 or not plot_data:
            return None

        scores = list(plot_data.get("episode_scores", deque()))
        game_scores = list(plot_data.get("game_scores", deque()))
        losses = list(plot_data.get("losses", deque()))
        ep_lengths = list(plot_data.get("episode_lengths", deque()))
        sps_values = list(plot_data.get("sps_values", deque()))
        best_game_score_history = list(
            plot_data.get("best_game_score_history", deque())
        )
        lr_values = list(plot_data.get("lr_values", deque()))
        buffer_sizes = list(plot_data.get("buffer_sizes", deque()))
        beta_values = list(plot_data.get("beta_values", deque()))

        has_any_data = any(
            d
            for d in [
                scores,
                game_scores,
                losses,
                ep_lengths,
                sps_values,
                best_game_score_history,
                lr_values,
                buffer_sizes,
                beta_values,
            ]
        )
        if not has_any_data:
            return None

        fig = None
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", category=UserWarning)

                dpi = 85
                fig_width_in = target_width / dpi
                fig_height_in = target_height / dpi

                fig, axes = plt.subplots(
                    3, 3, figsize=(fig_width_in, fig_height_in), dpi=dpi, sharex=False
                )
                fig.subplots_adjust(
                    hspace=0.65,
                    wspace=0.4,
                    left=0.12,
                    right=0.97,
                    bottom=0.15,
                    top=0.92,
                )
                axes_flat = axes.flatten()

                c_rl_score = normalize_color_for_matplotlib(VisConfig.GOOGLE_COLORS[0])
                c_game_score = normalize_color_for_matplotlib(
                    VisConfig.GOOGLE_COLORS[1]
                )
                c_loss = normalize_color_for_matplotlib(VisConfig.GOOGLE_COLORS[3])
                c_len = normalize_color_for_matplotlib(VisConfig.BLUE)
                c_sps = normalize_color_for_matplotlib(VisConfig.LIGHTG)
                c_best_game = normalize_color_for_matplotlib((255, 165, 0))
                c_lr = normalize_color_for_matplotlib((255, 0, 255))
                c_buffer = normalize_color_for_matplotlib(VisConfig.RED)
                c_beta = normalize_color_for_matplotlib((100, 100, 255))
                c_avg = normalize_color_for_matplotlib(VisConfig.YELLOW)

                # --- MODIFIED: X-axis label ---
                # Label reflects the number of points *shown* on the plot,
                # which is controlled by StatsConfig.STATS_AVG_WINDOW
                plot_window_label = f"Plot Window (~{self.rolling_window_size} points)"
                # --- END MODIFIED ---

                self._plot_data_list(
                    axes_flat[0],
                    scores,
                    "RL Score",
                    c_rl_score,
                    c_avg,
                    xlabel=plot_window_label,
                )
                self._plot_data_list(
                    axes_flat[1],
                    game_scores,
                    "Game Score",
                    c_game_score,
                    c_avg,
                    xlabel=plot_window_label,
                )
                self._plot_data_list(
                    axes_flat[2],
                    losses,
                    "Loss",
                    c_loss,
                    c_avg,
                    xlabel=plot_window_label,
                    show_placeholder=True,
                    placeholder_text="Loss data after Learn Start",
                )
                self._plot_data_list(
                    axes_flat[3],
                    ep_lengths,
                    "Ep Length",
                    c_len,
                    c_avg,
                    xlabel=plot_window_label,
                )
                self._plot_data_list(
                    axes_flat[4],
                    best_game_score_history,
                    "Best Game Score",
                    c_best_game,
                    None,
                    xlabel=plot_window_label,
                )
                self._plot_data_list(
                    axes_flat[5],
                    sps_values,
                    "Steps/Sec",
                    c_sps,
                    c_avg,
                    xlabel=plot_window_label,
                )
                self._plot_data_list(
                    axes_flat[6],
                    lr_values,
                    "Learning Rate",
                    c_lr,
                    None,
                    xlabel=plot_window_label,
                    y_log_scale=True,
                )

                buffer_fill_percent = [
                    (s / max(1, BufferConfig.REPLAY_BUFFER_SIZE) * 100)
                    for s in buffer_sizes
                ]
                self._plot_data_list(
                    axes_flat[7],
                    buffer_fill_percent,
                    "Buffer Fill %",
                    c_buffer,
                    c_avg,
                    xlabel=plot_window_label,
                )

                if BufferConfig.USE_PER:
                    self._plot_data_list(
                        axes_flat[8],
                        beta_values,
                        "PER Beta",
                        c_beta,
                        c_avg,
                        xlabel=plot_window_label,
                    )
                else:
                    axes_flat[8].text(
                        0.5,
                        0.5,
                        "PER Disabled",
                        ha="center",
                        va="center",
                        transform=axes_flat[8].transAxes,
                        fontsize=8,
                        color=normalize_color_for_matplotlib(VisConfig.GRAY),
                    )
                    axes_flat[8].set_yticks([])
                    axes_flat[8].set_xticks([])

                for ax in axes_flat:
                    ax.tick_params(axis="x", rotation=0)

                buf = BytesIO()
                fig.savefig(
                    buf,
                    format="png",
                    transparent=False,
                    facecolor=plt.rcParams["figure.facecolor"],
                )
                buf.seek(0)
                plot_img_surface = pygame.image.load(buf).convert()
                buf.close()

                if (
                    plot_img_surface.get_size() != (target_width, target_height)
                    and target_width > 0
                    and target_height > 0
                ):
                    plot_img_surface = pygame.transform.smoothscale(
                        plot_img_surface, (target_width, target_height)
                    )

                return plot_img_surface

        except Exception as e:
            print(f"Error creating plot surface: {e}")
            import traceback

            traceback.print_exc()
            return None
        finally:
            if fig is not None:
                plt.close(fig)

    def _plot_data_list(
        self,
        ax,
        data: List[Union[float, int]],
        label: str,
        color,
        avg_color: Optional[Tuple[float, float, float]],
        xlabel: Optional[str] = None,  # Use the passed xlabel
        show_placeholder: bool = True,
        placeholder_text: Optional[str] = None,
        y_log_scale: bool = False,
    ):
        """Plots the list data and optionally its rolling average."""
        n_points = len(data)
        latest_val_str = ""
        window_size = self.rolling_window_size  # Use for average calculation

        # Calculate latest value/average for title
        if data:
            current_val = data[-1]
            if n_points >= window_size and avg_color is not None:
                try:
                    latest_avg = np.mean(data[-window_size:])
                    latest_val_str = f" (Now: {current_val:.3g}, Avg: {latest_avg:.3g})"
                except Exception:
                    latest_val_str = f" (Now: {current_val:.3g})"
            else:
                latest_val_str = f" (Now: {current_val:.3g})"
        ax.set_title(
            f"{label}{latest_val_str}", fontsize=plt.rcParams["axes.titlesize"]
        )

        # Handle empty data
        placeholder_text_color = normalize_color_for_matplotlib(VisConfig.GRAY)
        if n_points == 0:
            if show_placeholder:
                p_text = (
                    placeholder_text if placeholder_text else f"{label}\n(No data yet)"
                )
                ax.text(
                    0.5,
                    0.5,
                    p_text,
                    ha="center",
                    va="center",
                    transform=ax.transAxes,
                    fontsize=8,
                    color=placeholder_text_color,
                )
            ax.set_yticks([])
            ax.set_xticks([])
            return

        try:
            # Plot raw data
            x_coords = np.arange(n_points)
            ax.plot(
                x_coords,
                data,
                color=color,
                linewidth=self.default_line_width,
                label=f"{label} (Raw)",
            )

            # Plot rolling average
            if avg_color is not None and n_points >= window_size:
                weights = np.ones(window_size) / window_size
                rolling_avg = np.convolve(data, weights, mode="valid")
                avg_x_coords = np.arange(window_size - 1, n_points)
                ax.plot(
                    avg_x_coords,
                    rolling_avg,
                    color=avg_color,
                    linewidth=self.avg_line_width,
                    alpha=self.avg_line_alpha,
                    label=f"{label} (Avg {window_size})",
                )

            # Styling
            ax.tick_params(axis="both", which="major")
            if xlabel:
                ax.set_xlabel(xlabel)  # Use the generated label here
            ax.grid(
                True,
                linestyle=plt.rcParams["grid.linestyle"],
                alpha=plt.rcParams["grid.alpha"],
            )

            # Set Y limits
            min_val = np.min(data)
            max_val = np.max(data)
            padding = (max_val - min_val) * 0.1 if max_val > min_val else 1.0
            padding = max(padding, 1e-6)
            ax.set_ylim(min_val - padding, max_val + padding)

            if y_log_scale and min_val > 0:
                ax.set_yscale("log")
                ax.set_ylim(bottom=max(min_val * 0.9, 1e-9))
            else:
                ax.set_yscale("linear")

            # Set X limits
            if n_points > 1:
                ax.set_xlim(-0.02 * n_points, n_points - 1 + 0.02 * n_points)
            elif n_points == 1:
                ax.set_xlim(-0.5, 0.5)

            if n_points > 10:
                ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True, nbins=4))

        except Exception as plot_err:
            print(f"ERROR during _plot_data_list for '{label}': {plot_err}")
            error_text_color = normalize_color_for_matplotlib(VisConfig.RED)
            ax.text(
                0.5,
                0.5,
                f"Plotting Error\n({label})",
                ha="center",
                va="center",
                transform=ax.transAxes,
                fontsize=8,
                color=error_text_color,
            )
            ax.set_yticks([])
            ax.set_xticks([])

    def get_cached_or_updated_plot(
        self, plot_data: Dict[str, Deque], target_width: int, target_height: int
    ) -> Optional[pygame.Surface]:
        """Returns the cached plot surface or generates a new one if needed."""
        current_time = time.time()
        has_data = any(plot_data.get(key) for key in plot_data)

        should_update_time = (
            current_time - self.last_plot_update_time > self.plot_update_interval
        )
        size_changed = self.plot_surface and self.plot_surface.get_size() != (
            target_width,
            target_height,
        )
        first_data_received = has_data and self.plot_surface is None
        can_create_plot = target_width > 50 and target_height > 50

        if can_create_plot and (
            should_update_time or size_changed or first_data_received
        ):
            new_plot_surface = self.create_plot_surface(
                plot_data, target_width, target_height
            )
            if new_plot_surface:
                self.plot_surface = new_plot_surface
                self.last_plot_update_time = current_time

        return self.plot_surface


File: ui/overlays.py
# File: ui/overlays.py
import pygame
import time
from typing import Tuple, List
from config import VisConfig


class OverlayRenderer:
    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.fonts = self._init_fonts()

    def _init_fonts(self):
        fonts = {}
        try:
            fonts["overlay_title"] = pygame.font.SysFont(None, 36)
            fonts["overlay_text"] = pygame.font.SysFont(None, 24)
            # --- REMOVED: Toast font ---
            # fonts["toast"] = pygame.font.SysFont(None, 20)
        except Exception as e:
            print(f"Warning: SysFont error: {e}. Using default.")
            fonts["overlay_title"] = pygame.font.Font(None, 36)
            fonts["overlay_text"] = pygame.font.Font(None, 24)
            # --- REMOVED: Toast font ---
            # fonts["toast"] = pygame.font.Font(None, 20)
        return fonts

    def render_cleanup_confirmation(self):
        """Renders the confirmation dialog for cleanup."""
        current_width, current_height = self.screen.get_size()
        # Semi-transparent overlay
        overlay_surface = pygame.Surface(
            (current_width, current_height), pygame.SRCALPHA
        )
        overlay_surface.fill((0, 0, 0, 200))  # Black with alpha
        self.screen.blit(overlay_surface, (0, 0))

        center_x, center_y = current_width // 2, current_height // 2

        # Text lines
        prompt_l1 = self.fonts["overlay_title"].render(
            "DELETE CURRENT RUN DATA?", True, VisConfig.RED
        )
        prompt_l2 = self.fonts["overlay_text"].render(
            "(Agent Checkpoint & Buffer State)", True, VisConfig.WHITE
        )
        prompt_l3 = self.fonts["overlay_text"].render(
            "This action cannot be undone!", True, VisConfig.YELLOW
        )

        # Position and blit text
        self.screen.blit(
            prompt_l1, prompt_l1.get_rect(center=(center_x, center_y - 60))
        )
        self.screen.blit(
            prompt_l2, prompt_l2.get_rect(center=(center_x, center_y - 25))
        )
        self.screen.blit(prompt_l3, prompt_l3.get_rect(center=(center_x, center_y)))

        # Buttons
        confirm_yes_rect = pygame.Rect(center_x - 110, center_y + 30, 100, 40)
        confirm_no_rect = pygame.Rect(center_x + 10, center_y + 30, 100, 40)

        pygame.draw.rect(
            self.screen, (0, 150, 0), confirm_yes_rect, border_radius=5
        )  # Green YES
        pygame.draw.rect(
            self.screen, (150, 0, 0), confirm_no_rect, border_radius=5
        )  # Red NO

        yes_text = self.fonts["overlay_text"].render("YES", True, VisConfig.WHITE)
        no_text = self.fonts["overlay_text"].render("NO", True, VisConfig.WHITE)

        self.screen.blit(yes_text, yes_text.get_rect(center=confirm_yes_rect.center))
        self.screen.blit(no_text, no_text.get_rect(center=confirm_no_rect.center))

    def render_status_message(self, message: str, last_message_time: float) -> bool:
        """Renders a status message (e.g., after cleanup) temporarily."""
        # Check if message exists and hasn't expired
        if not message or (time.time() - last_message_time >= 5.0):
            return False  # Message expired or empty

        current_width, current_height = self.screen.get_size()
        lines = message.split("\n")
        max_width = 0
        msg_surfs = []

        # Render each line and find max width
        for line in lines:
            # Render with black background for better visibility
            msg_surf = self.fonts["overlay_text"].render(
                line, True, VisConfig.YELLOW, VisConfig.BLACK
            )
            msg_surfs.append(msg_surf)
            max_width = max(max_width, msg_surf.get_width())

        # Calculate background size
        total_height = (
            sum(s.get_height() for s in msg_surfs) + max(0, len(lines) - 1) * 2
        )  # Add spacing between lines
        padding = 5
        bg_rect = pygame.Rect(0, 0, max_width + padding * 2, total_height + padding * 2)

        # Position at bottom-center
        bg_rect.midbottom = (current_width // 2, current_height - 10)

        # Draw background and border
        pygame.draw.rect(self.screen, VisConfig.BLACK, bg_rect, border_radius=3)
        pygame.draw.rect(
            self.screen, VisConfig.YELLOW, bg_rect, 1, border_radius=3
        )  # Yellow border

        # Draw text lines centered within the background
        current_y = bg_rect.top + padding
        for msg_surf in msg_surfs:
            msg_rect = msg_surf.get_rect(midtop=(bg_rect.centerx, current_y))
            self.screen.blit(msg_surf, msg_rect)
            current_y += msg_surf.get_height() + 2  # Add spacing

        return True  # Indicate that a message was rendered

File: ui/input_handler.py
# File: ui/input_handler.py
# --- Pygame Input Handling Logic ---
import pygame
from typing import Tuple, Callable, Optional

# Define callback types for actions
ToggleTrainingCallback = Callable[[], None]
RequestCleanupCallback = Callable[[], None]
CancelCleanupCallback = Callable[[], None]
ConfirmCleanupCallback = Callable[[], None]
ExitAppCallback = Callable[[], bool]  # Returns False to signal exit


class InputHandler:
    def __init__(
        self,
        screen: pygame.Surface,
        renderer: "UIRenderer",  # Forward reference if needed
        toggle_training_cb: ToggleTrainingCallback,
        request_cleanup_cb: RequestCleanupCallback,
        cancel_cleanup_cb: CancelCleanupCallback,
        confirm_cleanup_cb: ConfirmCleanupCallback,
        exit_app_cb: ExitAppCallback,
    ):
        self.screen = screen
        self.renderer = renderer
        self.toggle_training_cb = toggle_training_cb
        self.request_cleanup_cb = request_cleanup_cb
        self.cancel_cleanup_cb = cancel_cleanup_cb
        self.confirm_cleanup_cb = confirm_cleanup_cb
        self.exit_app_cb = exit_app_cb

    def handle_input(self, cleanup_confirmation_active: bool) -> bool:
        """Processes Pygame events and calls appropriate callbacks. Returns False to exit."""
        mouse_pos = pygame.mouse.get_pos()
        sw, sh = self.screen.get_size()

        # Define button rects (consider moving definitions to renderer or config)
        train_btn_rect = pygame.Rect(10, 10, 100, 40)
        cleanup_btn_rect = pygame.Rect(train_btn_rect.right + 10, 10, 160, 40)
        confirm_yes_rect = pygame.Rect(sw // 2 - 110, sh // 2 + 30, 100, 40)
        confirm_no_rect = pygame.Rect(sw // 2 + 10, sh // 2 + 30, 100, 40)

        # Check for tooltip hover (delegated to renderer)
        if hasattr(self.renderer, "check_hover"):
            self.renderer.check_hover(mouse_pos)

        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return self.exit_app_cb()  # Signal exit

            if event.type == pygame.VIDEORESIZE:
                try:
                    self.screen = pygame.display.set_mode(
                        (event.w, event.h), pygame.RESIZABLE
                    )
                    if hasattr(self.renderer, "screen"):
                        self.renderer.screen = self.screen
                    if hasattr(
                        self.renderer, "force_redraw"
                    ):  # Optional: signal renderer
                        self.renderer.force_redraw()
                    print(f"Window resized: {event.w}x{event.h}")
                except pygame.error as e:
                    print(f"Error resizing window: {e}")

            if event.type == pygame.KEYDOWN:
                if event.key == pygame.K_ESCAPE:
                    if cleanup_confirmation_active:
                        self.cancel_cleanup_cb()
                    else:
                        return self.exit_app_cb()  # Signal exit
                elif event.key == pygame.K_p and not cleanup_confirmation_active:
                    self.toggle_training_cb()

            if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1:  # Left click
                if cleanup_confirmation_active:
                    if confirm_yes_rect.collidepoint(mouse_pos):
                        self.confirm_cleanup_cb()
                    elif confirm_no_rect.collidepoint(mouse_pos):
                        self.cancel_cleanup_cb()
                else:
                    if train_btn_rect.collidepoint(mouse_pos):
                        self.toggle_training_cb()
                    elif cleanup_btn_rect.collidepoint(mouse_pos):
                        self.request_cleanup_cb()
        return True  # Continue running


File: ui/panels/__init__.py
# File: ui/panels/__init__.py
from .left_panel import LeftPanelRenderer
from .game_area import GameAreaRenderer

__all__ = ["LeftPanelRenderer", "GameAreaRenderer"]


File: ui/panels/game_area.py
# File: ui/panels/game_area.py
# --- Game Area Rendering Logic ---
import pygame
import math
import traceback
from typing import List, Tuple
from config import VisConfig, EnvConfig
from environment.game_state import GameState
from environment.shape import Shape
from environment.triangle import Triangle


class GameAreaRenderer:
    def __init__(self, screen: pygame.Surface, vis_config: VisConfig):
        self.screen = screen
        self.vis_config = vis_config
        self.fonts = self._init_fonts()

    def _init_fonts(self):
        fonts = {}
        try:
            fonts["env_score"] = pygame.font.SysFont(None, 18)
            fonts["env_overlay"] = pygame.font.SysFont(None, 36)
            fonts["ui"] = pygame.font.SysFont(None, 24)  # For 'too small' message
        except Exception as e:
            print(f"Warning: SysFont error: {e}. Using default.")
            fonts["env_score"] = pygame.font.Font(None, 18)
            fonts["env_overlay"] = pygame.font.Font(None, 36)
            fonts["ui"] = pygame.font.Font(None, 24)
        return fonts

    def render(self, envs: List[GameState], num_envs: int, env_config: EnvConfig):
        """Renders the grid of game environments."""
        current_width, current_height = self.screen.get_size()
        lp_width = min(current_width, max(300, self.vis_config.LEFT_PANEL_WIDTH))
        ga_rect = pygame.Rect(lp_width, 0, current_width - lp_width, current_height)

        if num_envs <= 0 or ga_rect.width <= 0 or ga_rect.height <= 0:
            return

        render_limit = self.vis_config.NUM_ENVS_TO_RENDER
        num_to_render = num_envs if render_limit <= 0 else min(num_envs, render_limit)
        if num_to_render <= 0:
            return

        cols_env, rows_env, cell_w, cell_h = self._calculate_grid_layout(
            ga_rect, num_to_render
        )

        if cell_w > 10 and cell_h > 10:
            self._render_env_grid(
                envs,
                num_to_render,
                env_config,
                ga_rect,
                cols_env,
                rows_env,
                cell_w,
                cell_h,
            )
        else:
            self._render_too_small_message(ga_rect, cell_w, cell_h)

        if num_to_render < num_envs:
            self._render_render_limit_text(ga_rect, num_to_render, num_envs)

    def _calculate_grid_layout(
        self, ga_rect: pygame.Rect, num_to_render: int
    ) -> Tuple[int, int, int, int]:
        """Calculates the layout parameters for the environment grid."""
        aspect_ratio = ga_rect.width / max(1, ga_rect.height)
        cols_env = max(1, int(math.sqrt(num_to_render * aspect_ratio)))
        rows_env = max(1, math.ceil(num_to_render / cols_env))
        total_spacing_w = (cols_env + 1) * self.vis_config.ENV_SPACING
        total_spacing_h = (rows_env + 1) * self.vis_config.ENV_SPACING
        cell_w = max(1, (ga_rect.width - total_spacing_w) // cols_env)
        cell_h = max(1, (ga_rect.height - total_spacing_h) // rows_env)
        return cols_env, rows_env, cell_w, cell_h

    def _render_env_grid(
        self, envs, num_to_render, env_config, ga_rect, cols, rows, cell_w, cell_h
    ):
        """Iterates through the grid positions and renders each environment."""
        env_idx = 0
        for r in range(rows):
            for c in range(cols):
                if env_idx >= num_to_render:
                    break
                env_x = ga_rect.x + self.vis_config.ENV_SPACING * (c + 1) + c * cell_w
                env_y = ga_rect.y + self.vis_config.ENV_SPACING * (r + 1) + r * cell_h
                env_rect = pygame.Rect(env_x, env_y, cell_w, cell_h)
                try:
                    sub_surf = self.screen.subsurface(env_rect)
                    tri_cell_w, tri_cell_h = self._calculate_triangle_size(
                        cell_w, cell_h, env_config
                    )
                    self._render_single_env(
                        sub_surf, envs[env_idx], int(tri_cell_w), int(tri_cell_h)
                    )
                    self._render_shape_previews(sub_surf, envs[env_idx], cell_w, cell_h)
                except ValueError as subsurface_error:
                    print(
                        f"Warning: Subsurface error env {env_idx}: {subsurface_error}"
                    )
                    pygame.draw.rect(self.screen, (0, 0, 50), env_rect, 1)
                except Exception as e_render_env:
                    print(f"Error rendering env {env_idx}: {e_render_env}")
                    pygame.draw.rect(self.screen, (50, 0, 50), env_rect, 1)
                env_idx += 1

    def _calculate_triangle_size(self, cell_w, cell_h, env_config):
        """Calculates the size of individual triangles within an env cell."""
        denominator_w = env_config.COLS * 0.75 + 0.25
        denominator_h = env_config.ROWS
        tri_cell_w = cell_w / denominator_w if denominator_w > 0 else 1
        tri_cell_h = cell_h / denominator_h if denominator_h > 0 else 1
        return tri_cell_w, tri_cell_h

    def _render_single_env(
        self, surf: pygame.Surface, env: GameState, cell_w: int, cell_h: int
    ):
        """Renders the grid, scores, and overlays for a single environment."""
        try:
            bg_color = (
                VisConfig.YELLOW
                if env.is_blinking()
                else (
                    (30, 30, 100)
                    if env.is_frozen() and not env.is_over()
                    else (20, 20, 20)
                )
            )
            surf.fill(bg_color)

            # Render Grid Triangles
            if (
                hasattr(env, "grid")
                and hasattr(env.grid, "triangles")
                and cell_w > 0
                and cell_h > 0
            ):
                for r in range(env.grid.rows):
                    for c in range(env.grid.cols):
                        t = env.grid.triangles[r][c]
                        if not hasattr(t, "get_points"):
                            continue
                        try:
                            pts = t.get_points(ox=0, oy=0, cw=cell_w, ch=cell_h)
                            color = VisConfig.GRAY
                            if t.is_death:
                                color = VisConfig.BLACK
                            elif t.is_occupied:
                                color = t.color if t.color else VisConfig.RED
                            pygame.draw.polygon(surf, color, pts)
                        except Exception as e_render:
                            print(f"Error rendering tri ({r},{c}): {e_render}")
            else:
                pygame.draw.rect(surf, (255, 0, 0), surf.get_rect(), 2)
                err_txt = self.fonts["env_overlay"].render(
                    "Invalid Grid" if cell_w > 0 else "Too Small", True, VisConfig.RED
                )
                surf.blit(err_txt, err_txt.get_rect(center=surf.get_rect().center))

            # Render Scores
            score_surf = self.fonts["env_score"].render(
                f"GS: {env.game_score} | R: {env.score:.1f}",
                True,
                VisConfig.WHITE,
                (0, 0, 0, 180),
            )
            surf.blit(score_surf, (4, 4))

            # Render Overlays
            if env.is_over():
                overlay = pygame.Surface(surf.get_size(), pygame.SRCALPHA)
                overlay.fill((100, 0, 0, 180))
                surf.blit(overlay, (0, 0))
                over_text = self.fonts["env_overlay"].render(
                    "GAME OVER", True, VisConfig.WHITE
                )
                surf.blit(over_text, over_text.get_rect(center=surf.get_rect().center))
            elif env.is_frozen() and not env.is_blinking():
                freeze_text = self.fonts["env_overlay"].render(
                    "Frozen", True, VisConfig.WHITE
                )
                surf.blit(
                    freeze_text,
                    freeze_text.get_rect(
                        center=(surf.get_width() // 2, surf.get_height() - 15)
                    ),
                )

        except AttributeError as e:
            pygame.draw.rect(surf, (255, 0, 0), surf.get_rect(), 2)
            err_txt = self.fonts["env_overlay"].render(
                f"Attr Err: {e}", True, VisConfig.RED, VisConfig.BLACK
            )
            surf.blit(err_txt, err_txt.get_rect(center=surf.get_rect().center))
        except Exception as e:
            print(f"Unexpected Render Error in _render_single_env: {e}")
            traceback.print_exc()
            pygame.draw.rect(surf, (255, 0, 0), surf.get_rect(), 2)

    def _render_shape_previews(
        self, surf: pygame.Surface, env: GameState, cell_w: int, cell_h: int
    ):
        """Renders small previews of available shapes in the top-right corner."""
        available_shapes = env.get_shapes()
        if not available_shapes:
            return

        preview_dim = max(10, min(cell_w // 6, cell_h // 6, 25))
        preview_spacing = 4
        total_preview_width = (
            len(available_shapes) * preview_dim
            + max(0, len(available_shapes) - 1) * preview_spacing
        )
        start_x = surf.get_width() - total_preview_width - preview_spacing
        start_y = preview_spacing

        for i, shape in enumerate(available_shapes):
            preview_x = start_x + i * (preview_dim + preview_spacing)
            if preview_x + preview_dim <= surf.get_width():
                temp_shape_surf = pygame.Surface(
                    (preview_dim, preview_dim), pygame.SRCALPHA
                )
                temp_shape_surf.fill((0, 0, 0, 0))
                preview_cell_size = max(2, preview_dim // 5)
                self._render_single_shape(temp_shape_surf, shape, preview_cell_size)
                surf.blit(temp_shape_surf, (preview_x, start_y))

    def _render_single_shape(self, surf: pygame.Surface, shape: Shape, cell_size: int):
        """Renders a single shape onto a given surface."""
        if not shape or not shape.triangles or cell_size <= 0:
            return
        min_r, min_c, max_r, max_c = shape.bbox()
        shape_h_cells = max_r - min_r + 1
        shape_w_cells = max_c - min_c + 1
        if shape_w_cells <= 0 or shape_h_cells <= 0:
            return

        total_w_pixels = shape_w_cells * (cell_size * 0.75) + (cell_size * 0.25)
        total_h_pixels = shape_h_cells * cell_size
        if total_w_pixels <= 0 or total_h_pixels <= 0:
            return

        offset_x = (surf.get_width() - total_w_pixels) / 2 - min_c * (cell_size * 0.75)
        offset_y = (surf.get_height() - total_h_pixels) / 2 - min_r * cell_size

        for dr, dc, up in shape.triangles:
            tri = Triangle(row=dr, col=dc, is_up=up)  # Temp instance for points
            try:
                pts = tri.get_points(
                    ox=offset_x, oy=offset_y, cw=cell_size, ch=cell_size
                )
                pygame.draw.polygon(surf, shape.color, pts)
            except Exception as e:
                print(f"Warning: Error rendering shape preview tri ({dr},{dc}): {e}")

    def _render_too_small_message(self, ga_rect: pygame.Rect, cell_w: int, cell_h: int):
        """Displays a message when the environment cells are too small."""
        err_surf = self.fonts["ui"].render(
            f"Envs Too Small ({cell_w}x{cell_h})", True, VisConfig.GRAY
        )
        self.screen.blit(err_surf, err_surf.get_rect(center=ga_rect.center))

    def _render_render_limit_text(
        self, ga_rect: pygame.Rect, num_rendered: int, num_total: int
    ):
        """Displays text indicating not all environments are being rendered."""
        info_surf = self.fonts["ui"].render(
            f"Rendering {num_rendered}/{num_total} Envs",
            True,
            VisConfig.YELLOW,
            VisConfig.BLACK,
        )
        self.screen.blit(
            info_surf,
            info_surf.get_rect(bottomright=(ga_rect.right - 5, ga_rect.bottom - 5)),
        )


File: ui/panels/left_panel.py
# File: ui/panels/left_panel.py
import pygame
import os
import time
from typing import Dict, Any, Optional, Deque, Tuple

from config import (
    VisConfig,
    BufferConfig,
    StatsConfig,
    DQNConfig,
    DEVICE,
    TensorBoardConfig,
)
from config.general import TOTAL_TRAINING_STEPS
from ui.plotter import Plotter

# Tooltips specific to this panel
TOOLTIP_TEXTS = {
    "Status": "Current state: Paused, Buffering, Training, Confirm Cleanup, Cleaning, or Error.",
    "Global Steps": "Total environment steps taken / Total planned steps.",
    "Total Episodes": "Total completed episodes across all environments.",
    "Steps/Sec (Current)": f"Current avg Steps/Sec (~{StatsConfig.STATS_AVG_WINDOW} intervals). See plot for history.",
    "Buffer Fill": f"Current replay buffer fill % ({BufferConfig.REPLAY_BUFFER_SIZE / 1e6:.1f}M cap). See plot.",
    "PER Beta": f"Current PER IS exponent ({BufferConfig.PER_BETA_START:.1f}->1.0). See plot.",
    "Learning Rate": "Current learning rate. See plot for history/schedule.",
    "Train Button": "Click to Start/Pause training (or press 'P').",
    "Cleanup Button": "Click to DELETE agent ckpt & buffer for CURRENT run ONLY, then re-init.",
    "Device": f"Computation device detected ({DEVICE.type.upper()}).",
    "Network": f"CNN+MLP Fusion. Dueling={DQNConfig.USE_DUELING}, Noisy={DQNConfig.USE_NOISY_NETS}, C51={DQNConfig.USE_DISTRIBUTIONAL}",
    "TensorBoard Status": "Indicates TB logging status and log directory.",
    "Notification Area": "Displays the latest best achievements (RL Score, Game Score, Loss).",
    # --- MODIFIED Tooltips ---
    "Best RL Score Info": "Best RL Score achieved: Current Value (Previous Value) - Steps Ago",
    "Best Game Score Info": "Best Game Score achieved: Current Value (Previous Value) - Steps Ago",
    "Best Loss Info": "Best (Lowest) Loss achieved: Current Value (Previous Value) - Steps Ago",
    # --- END MODIFIED ---
}


class LeftPanelRenderer:
    def __init__(self, screen: pygame.Surface, vis_config: VisConfig, plotter: Plotter):
        self.screen = screen
        self.vis_config = vis_config
        self.plotter = plotter
        self.fonts = self._init_fonts()
        self.stat_rects: Dict[str, pygame.Rect] = {}

    def _init_fonts(self):
        """Loads necessary fonts, attempting to load DejaVuSans for notifications."""
        fonts = {}
        default_font_size = 24
        status_font_size = 28
        logdir_font_size = 16
        plot_placeholder_size = 20
        notification_size = 19
        notification_label_size = 16

        notification_font_path = os.path.join("fonts", "DejaVuSans.ttf")
        try:
            fonts["notification"] = pygame.font.Font(
                notification_font_path, notification_size
            )
            print(f"Loaded notification font: {notification_font_path}")
        except pygame.error as e:
            print(
                f"Warning: Could not load '{notification_font_path}': {e}. Falling back to SysFont."
            )
            try:
                fonts["notification"] = pygame.font.SysFont(None, notification_size)
            except Exception:
                print("Warning: SysFont fallback failed. Using default font.")
                fonts["notification"] = pygame.font.Font(None, notification_size)

        try:
            fonts["ui"] = pygame.font.SysFont(None, default_font_size)
            fonts["status"] = pygame.font.SysFont(None, status_font_size)
            fonts["logdir"] = pygame.font.SysFont(None, logdir_font_size)
            fonts["plot_placeholder"] = pygame.font.SysFont(None, plot_placeholder_size)
            fonts["notification_label"] = pygame.font.SysFont(
                None, notification_label_size
            )
        except Exception as e:
            print(f"Warning: SysFont error loading other fonts: {e}. Using default.")
            fonts["ui"] = pygame.font.Font(None, default_font_size)
            fonts["status"] = pygame.font.Font(None, status_font_size)
            fonts["logdir"] = pygame.font.Font(None, logdir_font_size)
            fonts["plot_placeholder"] = pygame.font.Font(None, plot_placeholder_size)
            fonts["notification_label"] = fonts.get(
                "notification", pygame.font.Font(None, notification_label_size)
            )

        for key, size in [
            ("ui", default_font_size),
            ("status", status_font_size),
            ("logdir", logdir_font_size),
            ("plot_placeholder", plot_placeholder_size),
            ("notification", notification_size),
            ("notification_label", notification_label_size),
        ]:
            if key not in fonts:
                print(
                    f"Warning: Font '{key}' completely failed to load. Using pygame default."
                )
                fonts[key] = pygame.font.Font(None, size)
        return fonts

    # --- MODIFIED: Clarify steps ago ---
    def _format_steps_ago(self, current_step: int, best_step: int) -> str:
        """Formats the difference in steps into a readable string (k steps, M steps)."""
        if best_step <= 0 or current_step <= best_step:
            return "Now"
        diff = current_step - best_step
        if diff < 1000:
            return f"{diff} steps ago"  # Add "steps"
        elif diff < 1_000_000:
            return f"{diff / 1000:.1f}k steps ago"  # Add "steps"
        else:
            return f"{diff / 1_000_000:.1f}M steps ago"  # Add "steps"

    # --- END MODIFIED ---

    def render(
        self,
        is_training: bool,
        status: str,
        stats_summary: Dict[str, Any],
        buffer_capacity: int,
        tensorboard_log_dir: Optional[str],
        plot_data: Dict[str, Deque],
    ):
        """Renders the entire left panel."""
        current_width, current_height = self.screen.get_size()
        lp_width = min(current_width, max(300, self.vis_config.LEFT_PANEL_WIDTH))
        lp_rect = pygame.Rect(0, 0, lp_width, current_height)

        status_color_map = {
            "Paused": (30, 30, 30),
            "Buffering": (30, 40, 30),
            "Training": (40, 30, 30),
            "Confirm Cleanup": (50, 20, 20),
            "Cleaning": (60, 30, 30),
            "Error": (60, 0, 0),
        }
        bg_color = status_color_map.get(status, (30, 30, 30))
        pygame.draw.rect(self.screen, bg_color, lp_rect)

        self.stat_rects.clear()

        # 1. Buttons
        train_btn_rect = pygame.Rect(10, 10, 100, 40)
        cleanup_btn_rect = pygame.Rect(train_btn_rect.right + 10, 10, 160, 40)
        self._draw_button(
            train_btn_rect,
            "Pause" if is_training and status == "Training" else "Train",
            (70, 70, 70),
        )
        self._draw_button(cleanup_btn_rect, "Cleanup This Run", (100, 40, 40))
        self.stat_rects["Train Button"] = train_btn_rect
        self.stat_rects["Cleanup Button"] = cleanup_btn_rect

        # 2. Status Text
        status_surf = self.fonts["status"].render(
            f"Status: {status}", True, VisConfig.YELLOW
        )
        status_rect = status_surf.get_rect(topleft=(10, train_btn_rect.bottom + 10))
        self.screen.blit(status_surf, status_rect)
        self.stat_rects["Status"] = status_rect
        current_y = status_rect.bottom + 5

        # 3. Notification Area
        notification_area_x = cleanup_btn_rect.right + 15
        notification_area_y = train_btn_rect.top
        notification_area_width = lp_width - notification_area_x - 10
        notification_line_height = self.fonts["notification"].get_linesize()
        notification_area_height = notification_line_height * 3 + 12

        if notification_area_width > 50:
            notification_area_rect = pygame.Rect(
                notification_area_x,
                notification_area_y,
                notification_area_width,
                notification_area_height,
            )
            self._render_notification_area(notification_area_rect, stats_summary)
            current_y = max(current_y, notification_area_rect.bottom + 10)
        else:
            current_y = max(current_y, train_btn_rect.bottom + 10)

        # 4. Info Text Block
        last_text_y = self._render_info_text_reduced(
            current_y, stats_summary, buffer_capacity, lp_width
        )

        # 5. TensorBoard Status
        last_text_y = self._render_tb_status(last_text_y + 10, tensorboard_log_dir)

        # 6. Plot Area
        self._render_plot_area(
            last_text_y + 15, lp_width, current_height, plot_data, status
        )

    def _draw_button(self, rect: pygame.Rect, text: str, color: Tuple[int, int, int]):
        pygame.draw.rect(self.screen, color, rect, border_radius=5)
        lbl_surf = self.fonts["ui"].render(text, True, VisConfig.WHITE)
        self.screen.blit(lbl_surf, lbl_surf.get_rect(center=rect.center))

    def _render_notification_area(self, area_rect: pygame.Rect, stats: Dict[str, Any]):
        """Renders the latest best score/loss messages with details."""
        pygame.draw.rect(self.screen, (45, 45, 45), area_rect, border_radius=3)
        pygame.draw.rect(self.screen, VisConfig.LIGHTG, area_rect, 1, border_radius=3)
        self.stat_rects["Notification Area"] = area_rect

        padding = 5
        line_height = self.fonts["notification"].get_linesize()
        label_font = self.fonts["notification_label"]
        value_font = self.fonts["notification"]
        label_color = VisConfig.LIGHTG
        value_color = VisConfig.WHITE
        prev_color = VisConfig.GRAY
        time_color = (180, 180, 100)

        current_step = stats.get("global_step", 0)

        def render_line(
            y_pos, label, current_val, prev_val, best_step, val_format, tooltip_key
        ):
            if not label_font or not value_font:
                return

            # Label
            label_surf = label_font.render(label, True, label_color)
            label_rect = label_surf.get_rect(topleft=(area_rect.left + padding, y_pos))
            self.screen.blit(label_surf, label_rect)
            current_x = label_rect.right + 4

            # Current Value
            current_val_str = (
                val_format.format(current_val)
                if current_val > -float("inf") and current_val < float("inf")
                else "N/A"
            )
            val_surf = value_font.render(current_val_str, True, value_color)
            val_rect = val_surf.get_rect(topleft=(current_x, y_pos))
            self.screen.blit(val_surf, val_rect)
            current_x = val_rect.right + 4

            # Previous Value
            prev_val_str = (
                f"({val_format.format(prev_val)})"
                if prev_val > -float("inf") and prev_val < float("inf")
                else "(N/A)"
            )
            prev_surf = label_font.render(prev_val_str, True, prev_color)
            prev_rect = prev_surf.get_rect(topleft=(current_x, y_pos + 1))
            self.screen.blit(prev_surf, prev_rect)
            current_x = prev_rect.right + 6

            # Steps Ago (using updated formatting)
            steps_ago_str = self._format_steps_ago(current_step, best_step)
            time_surf = label_font.render(steps_ago_str, True, time_color)
            time_rect = time_surf.get_rect(topleft=(current_x, y_pos + 1))
            # Clip rendering to the notification area width
            available_width = area_rect.right - time_rect.left - padding
            if time_rect.width > available_width:
                time_rect.width = available_width  # Truncate if too long
                # Optionally add "..." if truncated? For now, just clip.
            self.screen.blit(time_surf, time_rect, area=time_rect)  # Use area clipping

            combined_line_rect = (
                label_rect.union(val_rect).union(prev_rect).union(time_rect)
            )
            self.stat_rects[tooltip_key] = combined_line_rect.clip(area_rect)

        y = area_rect.top + padding
        render_line(
            y,
            "RL Score:",
            stats.get("best_score", -float("inf")),
            stats.get("previous_best_score", -float("inf")),
            stats.get("best_score_step", 0),
            "{:.2f}",
            "Best RL Score Info",
        )
        y += line_height
        render_line(
            y,
            "Game Score:",
            stats.get("best_game_score", -float("inf")),
            stats.get("previous_best_game_score", -float("inf")),
            stats.get("best_game_score_step", 0),
            "{:.0f}",
            "Best Game Score Info",
        )
        y += line_height
        render_line(
            y,
            "Loss:",
            stats.get("best_loss", float("inf")),
            stats.get("previous_best_loss", float("inf")),
            stats.get("best_loss_step", 0),
            "{:.4f}",
            "Best Loss Info",
        )

    def _render_info_text_reduced(
        self, y_start: int, stats: Dict[str, Any], capacity: int, panel_width: int
    ) -> int:
        """Renders the main block of statistics text."""
        line_height = self.fonts["ui"].get_linesize()
        buffer_size = stats.get("buffer_size", 0)
        buffer_perc = (buffer_size / max(1, capacity) * 100) if capacity > 0 else 0.0
        global_step = stats.get("global_step", 0)

        info_lines = [
            (
                "Global Steps",
                f"{global_step/1e6:.2f}M / {TOTAL_TRAINING_STEPS/1e6:.1f}M",
            ),
            ("Total Episodes", f"{stats.get('total_episodes', 0)}"),
            ("Steps/Sec (Current)", f"{stats.get('steps_per_second', 0.0):.1f}"),
            ("Buffer Fill", f"{buffer_perc:.1f}% ({buffer_size/1e6:.2f}M)"),
            (
                "PER Beta",
                f"{stats.get('beta', 0.0):.3f}" if BufferConfig.USE_PER else "N/A",
            ),
            ("Learning Rate", f"{stats.get('current_lr', 0.0):.1e}"),
            ("Device", f"{DEVICE.type.upper()}"),
            (
                "Network",
                f"Duel={DQNConfig.USE_DUELING}, Noisy={DQNConfig.USE_NOISY_NETS}, C51={DQNConfig.USE_DISTRIBUTIONAL}",
            ),
        ]

        last_y = y_start
        current_stat_rects = {}
        for idx, (key, value_str) in enumerate(info_lines):
            try:
                key_surf = self.fonts["ui"].render(f"{key}:", True, VisConfig.LIGHTG)
                key_rect = key_surf.get_rect(topleft=(10, y_start + idx * line_height))
                value_surf = self.fonts["ui"].render(
                    f"{value_str}", True, VisConfig.WHITE
                )
                value_rect = value_surf.get_rect(
                    topleft=(key_rect.right + 5, key_rect.top)
                )
                combined_rect = key_rect.union(value_rect)
                combined_rect.width = min(combined_rect.width, panel_width - 15)
                self.screen.blit(key_surf, key_rect)
                self.screen.blit(value_surf, value_rect)
                current_stat_rects[key] = combined_rect
                last_y = combined_rect.bottom
            except Exception as e:
                print(f"Error rendering stat line '{key}': {e}")
                last_y += line_height

        self.stat_rects.update(current_stat_rects)
        return last_y

    def _render_tb_status(self, y_start: int, log_dir: Optional[str]) -> int:
        """Renders the TensorBoard status line and log directory."""
        tb_active = TensorBoardConfig.LOG_HISTOGRAMS or TensorBoardConfig.LOG_IMAGES
        tb_color = VisConfig.GOOGLE_COLORS[0] if tb_active else VisConfig.GRAY
        tb_text = f"TensorBoard: {'Logging Active' if tb_active else 'Logging Minimal'}"

        tb_surf = self.fonts["ui"].render(tb_text, True, tb_color)
        tb_rect = tb_surf.get_rect(topleft=(10, y_start))
        self.screen.blit(tb_surf, tb_rect)

        last_y = tb_rect.bottom
        combined_tb_rect = tb_rect

        if log_dir:
            try:
                rel_log_dir = os.path.relpath(log_dir)
                panel_char_width = self.vis_config.LEFT_PANEL_WIDTH // 8
                if len(rel_log_dir) > panel_char_width:
                    parts = log_dir.replace("\\", "/").split("/")
                    if len(parts) >= 2:
                        rel_log_dir = os.path.join("...", *parts[-2:])
                    else:
                        rel_log_dir = os.path.basename(log_dir)
            except ValueError:
                rel_log_dir = os.path.basename(log_dir)

            dir_surf = self.fonts["logdir"].render(
                f"Log Dir: {rel_log_dir}", True, VisConfig.LIGHTG
            )
            dir_rect = dir_surf.get_rect(topleft=(10, tb_rect.bottom + 2))
            self.screen.blit(dir_surf, dir_rect)
            combined_tb_rect = tb_rect.union(dir_rect)
            last_y = combined_tb_rect.bottom

        self.stat_rects["TensorBoard Status"] = combined_tb_rect
        return last_y

    def _render_plot_area(
        self,
        y_start: int,
        panel_width: int,
        screen_height: int,
        plot_data: Dict[str, Deque],
        status: str,
    ):
        """Renders the Matplotlib plot area."""
        plot_area_height = screen_height - y_start - 10
        plot_area_width = panel_width - 20

        if plot_area_width <= 50 or plot_area_height <= 50:
            return

        plot_surface = self.plotter.get_cached_or_updated_plot(
            plot_data, plot_area_width, plot_area_height
        )
        plot_area_rect = pygame.Rect(10, y_start, plot_area_width, plot_area_height)

        if plot_surface:
            self.screen.blit(plot_surface, plot_area_rect.topleft)
        else:
            pygame.draw.rect(self.screen, (40, 40, 40), plot_area_rect, 1)
            placeholder_text = "Waiting for data..."
            if status == "Buffering":
                placeholder_text = "Buffering... Waiting for data..."
            elif status == "Error":
                placeholder_text = "Plotting disabled due to error."

            placeholder_surf = self.fonts["plot_placeholder"].render(
                placeholder_text, True, VisConfig.GRAY
            )
            placeholder_rect = placeholder_surf.get_rect(center=plot_area_rect.center)
            clipped_placeholder_rect = placeholder_rect.clip(plot_area_rect)
            blit_pos = (clipped_placeholder_rect.left, clipped_placeholder_rect.top)
            blit_area = clipped_placeholder_rect.move(
                -placeholder_rect.left, -placeholder_rect.top
            )
            self.screen.blit(placeholder_surf, blit_pos, area=blit_area)

    def get_stat_rects(self) -> Dict[str, pygame.Rect]:
        """Returns the dictionary of rects for tooltip handling."""
        return self.stat_rects.copy()

    def get_tooltip_texts(self) -> Dict[str, str]:
        """Returns the dictionary of tooltip texts."""
        return TOOLTIP_TEXTS


File: config/__init__.py
# File: config/__init__.py
# Expose core config classes and general constants/paths
from .core import (
    VisConfig,
    EnvConfig,
    RewardConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
)
from .general import (
    DEVICE,
    RANDOM_SEED,
    RUN_ID,
    BASE_CHECKPOINT_DIR,
    BASE_LOG_DIR,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,  # Need this path constant
    BUFFER_SAVE_PATH,
    MODEL_SAVE_PATH,
    TOTAL_TRAINING_STEPS,  # Need this constant
)
from .utils import get_config_dict
from .validation import print_config_info_and_validate

# --- MODIFIED: Assign LOG_DIR after imports ---
# This ensures the correct run-specific directory is set in the config class
# before it's potentially used elsewhere (like in the stats recorder init).
TensorBoardConfig.LOG_DIR = RUN_LOG_DIR
# --- END MODIFIED ---

# You can choose to expose everything or just specific items
__all__ = [
    # Core Classes
    "VisConfig",
    "EnvConfig",
    "RewardConfig",
    "DQNConfig",
    "TrainConfig",
    "BufferConfig",
    "ModelConfig",
    "StatsConfig",
    "TensorBoardConfig",
    # General Constants/Paths
    "DEVICE",
    "RANDOM_SEED",
    "RUN_ID",
    "BASE_CHECKPOINT_DIR",
    "BASE_LOG_DIR",
    "RUN_CHECKPOINT_DIR",
    "RUN_LOG_DIR",
    "BUFFER_SAVE_PATH",
    "MODEL_SAVE_PATH",
    "TOTAL_TRAINING_STEPS",
    # Utils/Validation
    "get_config_dict",
    "print_config_info_and_validate",
]


File: config/core.py
# File: config/core.py
# --- Core Configuration Classes ---
import torch
from typing import Deque, Dict, Any, List, Type

from .general import TOTAL_TRAINING_STEPS


# --- Visualization (Pygame) ---
class VisConfig:
    SCREEN_WIDTH = 1600
    SCREEN_HEIGHT = 900
    VISUAL_STEP_DELAY = 0  
    LEFT_PANEL_WIDTH = SCREEN_WIDTH // 2
    ENV_SPACING = 6
    FPS = 0  # Set to 0 for uncapped FPS (max speed)
    WHITE = (255, 255, 255)
    BLACK = (0, 0, 0)
    LIGHTG = (140, 140, 140)
    GRAY = (50, 50, 50)
    RED = (255, 50, 50)
    BLUE = (50, 50, 255)
    YELLOW = (255, 255, 100)
    GOOGLE_COLORS = [(15, 157, 88), (244, 180, 0), (66, 133, 244), (219, 68, 55)]
    # --- FASTER CONFIG ---
    NUM_ENVS_TO_RENDER = 48  


# --- Environment ---
class EnvConfig:
    # --- FASTER CONFIG ---
    NUM_ENVS = 256  # Reduce parallel envs significantly
    ROWS = 6  # Smaller grid
    COLS = 10  # Smaller grid
    # --- END FASTER CONFIG ---
    GRID_FEATURES_PER_CELL = 3
    SHAPE_FEATURES_PER_SHAPE = 5
    NUM_SHAPE_SLOTS = 3
    # STATE_DIM and ACTION_DIM will be recalculated based on ROWS/COLS
    STATE_DIM = (ROWS * COLS * GRID_FEATURES_PER_CELL) + (
        NUM_SHAPE_SLOTS * SHAPE_FEATURES_PER_SHAPE
    )
    ACTION_DIM = NUM_SHAPE_SLOTS * (ROWS * COLS)


# --- Reward Shaping (RL Reward) ---
class RewardConfig:  # Keep rewards the same unless debugging reward issues
    REWARD_PLACE_PER_TRI = 0.005
    REWARD_CLEAR_1 = 1.0
    REWARD_CLEAR_2 = 3.0
    REWARD_CLEAR_3PLUS = 6.0
    PENALTY_INVALID_MOVE = -0.5
    PENALTY_HOLE_PER_HOLE = -0.1
    PENALTY_GAME_OVER = -10.0
    REWARD_ALIVE_STEP = 0.001


# --- DQN Algorithm ---
class DQNConfig:
    GAMMA = 0.99
    # --- FASTER CONFIG ---
    TARGET_UPDATE_FREQ = 5_000  # Update target net much more often
    LEARNING_RATE = 5e-5  # Slightly higher LR might speed up initial learning
    # --- END FASTER CONFIG ---
    ADAM_EPS = 1e-4
    GRADIENT_CLIP_NORM = 10.0
    USE_DOUBLE_DQN = True
    # --- FASTER CONFIG ---
    USE_DUELING = True  # Disable Dueling (simpler head)
    USE_NOISY_NETS = True  # Keep Noisy for exploration unless debugging exploration
    USE_DISTRIBUTIONAL = True  # Disable C51 (significant computation reduction)
    # --- END FASTER CONFIG ---
    V_MIN = -15.0  # Ignored if USE_DISTRIBUTIONAL is False
    V_MAX = 15.0  # Ignored if USE_DISTRIBUTIONAL is False
    NUM_ATOMS = 51  # Ignored if USE_DISTRIBUTIONAL is False
    USE_LR_SCHEDULER = True  # Keep scheduler unless debugging LR issues
    LR_SCHEDULER_T_MAX: int = TOTAL_TRAINING_STEPS  # Keep T_max related to total steps
    LR_SCHEDULER_ETA_MIN: float = 1e-7


# --- Training Loop ---
class TrainConfig:
    # --- FASTER CONFIG ---
    BATCH_SIZE = 32  # Smaller batch size for more frequent updates
    LEARN_START_STEP = 1_000  # Start learning much earlier
    LEARN_FREQ = 4  # Learn more frequently relative to env steps
    CHECKPOINT_SAVE_FREQ = 20_000  # Save less often
    # --- END FASTER CONFIG ---
    LOAD_CHECKPOINT_PATH: str | None = None
    LOAD_BUFFER_PATH: str | None = None


# --- Replay Buffer ---
class BufferConfig:
    # --- FASTER CONFIG ---
    REPLAY_BUFFER_SIZE = 100_000  # Smaller buffer uses less RAM, faster init/save/load
    USE_N_STEP = True
    N_STEP = 5  # Smaller N-step lookahead
    USE_PER = False  # Disable PER (faster sampling)
    # --- END FASTER CONFIG ---
    PER_ALPHA = 0.6  # Ignored if USE_PER is False
    PER_BETA_START = 0.4  # Ignored if USE_PER is False
    PER_BETA_FRAMES = 25_000_000  # Ignored if USE_PER is False
    PER_EPSILON = 1e-6  # Ignored if USE_PER is False


# --- Model Architecture ---
class ModelConfig:
    class Network:
        # --- FASTER CONFIG ---
        # NOTE: These dimensions depend on the EnvConfig ROWS/COLS above
        # You might need to adjust pooling/strides if the grid gets too small
        HEIGHT = EnvConfig.ROWS  # Use updated EnvConfig
        WIDTH = EnvConfig.COLS  # Use updated EnvConfig
        CONV_CHANNELS = [32, 64]  # Fewer/smaller CNN layers
        CONV_KERNEL_SIZE = 3
        CONV_STRIDE = 1
        CONV_PADDING = 1
        POOL_KERNEL_SIZE = 2
        POOL_STRIDE = 2  # Ensure pooling doesn't reduce dimensions below 1x1
        CONV_ACTIVATION = torch.nn.ReLU
        USE_BATCHNORM_CONV = True  # Disable BatchNorm
        SHAPE_MLP_HIDDEN_DIM = 64  # Smaller shape MLP
        SHAPE_MLP_ACTIVATION = torch.nn.ReLU
        COMBINED_FC_DIMS = [256]  # Smaller/fewer fusion layers
        COMBINED_ACTIVATION = torch.nn.ReLU
        USE_BATCHNORM_FC = True  # Disable BatchNorm
        DROPOUT_FC = 0.0  # Disable Dropout
        # --- END FASTER CONFIG ---


# --- Statistics and Logging ---
class StatsConfig:
    STATS_AVG_WINDOW = 100  # Smaller window for faster reflection of changes
    # --- FASTER CONFIG ---
    CONSOLE_LOG_FREQ = 5_000  # Log to console more often to see progress
    # --- END FASTER CONFIG ---


# --- TensorBoard Logging ---
class TensorBoardConfig:
    # --- FASTER CONFIG ---
    LOG_HISTOGRAMS = False  # Disable histograms (major speedup)
    HISTOGRAM_LOG_FREQ = 100_000  # Ignored if LOG_HISTOGRAMS is False
    LOG_IMAGES = False  # Disable image logging
    IMAGE_LOG_FREQ = 500_000  # Ignored if LOG_IMAGES is False
    # --- END FASTER CONFIG ---


File: config/utils.py
# File: config/utils.py
# --- Configuration Utilities ---
import torch
from typing import Dict, Any
from .core import (
    VisConfig,
    EnvConfig,
    RewardConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
)
from .general import DEVICE, RANDOM_SEED, RUN_ID


def get_config_dict() -> Dict[str, Any]:
    """Returns a flat dictionary of all relevant config values for logging."""
    all_configs = {}

    # Helper to flatten classes (excluding methods, dunders, ClassVars, nested Classes)
    def flatten_class(cls, prefix=""):
        d = {}
        for k, v in cls.__dict__.items():
            # Basic check: not dunder, not callable, not a type (nested class)
            if not k.startswith("__") and not callable(v) and not isinstance(v, type):
                # More robust checking might be needed for complex configs
                d[f"{prefix}{k}"] = v
        return d

    all_configs.update(flatten_class(VisConfig, "Vis."))
    all_configs.update(flatten_class(EnvConfig, "Env."))
    all_configs.update(flatten_class(RewardConfig, "Reward."))
    all_configs.update(flatten_class(DQNConfig, "DQN."))
    all_configs.update(flatten_class(TrainConfig, "Train."))
    all_configs.update(flatten_class(BufferConfig, "Buffer."))
    all_configs.update(
        flatten_class(ModelConfig.Network, "Model.Net.")
    )  # Flatten sub-class explicitly
    all_configs.update(flatten_class(StatsConfig, "Stats."))
    all_configs.update(flatten_class(TensorBoardConfig, "TB."))

    # Add general constants
    all_configs["General.DEVICE"] = str(DEVICE)
    all_configs["General.RANDOM_SEED"] = RANDOM_SEED
    all_configs["General.RUN_ID"] = RUN_ID

    # Filter out None values specifically for paths that might not be set
    # This prevents logging None for load paths if they aren't specified
    all_configs = {
        k: v for k, v in all_configs.items() if not (k.endswith("_PATH") and v is None)
    }

    # Convert torch.nn activation functions to string representation for logging
    for key, value in all_configs.items():
        if isinstance(value, type) and issubclass(value, torch.nn.Module):
            all_configs[key] = value.__name__
        # Handle potential list values (e.g., CONV_CHANNELS) - convert to string?
        # Tensorboard hparams prefers simple types (int, float, bool, str)
        if isinstance(value, list):
            all_configs[key] = str(value)

    return all_configs


File: config/general.py
# File: config/general.py
# --- General Constants and Paths ---
import torch
import os
import time
from utils.helpers import get_device

# --- General ---
DEVICE = get_device()
RANDOM_SEED = 42
RUN_ID = f"run_{time.strftime('%Y%m%d_%H%M%S')}"
BASE_CHECKPOINT_DIR = "checkpoints"
BASE_LOG_DIR = "logs"

# --- Define Total Steps Here ---
# This makes it available for default LR scheduler T_max
TOTAL_TRAINING_STEPS = 10_000_000

# --- Derived Paths (using RUN_ID) ---
RUN_CHECKPOINT_DIR = os.path.join(BASE_CHECKPOINT_DIR, RUN_ID)
RUN_LOG_DIR = os.path.join(BASE_LOG_DIR, "tensorboard", RUN_ID)

BUFFER_SAVE_PATH = os.path.join(RUN_CHECKPOINT_DIR, "replay_buffer_state.pkl")
MODEL_SAVE_PATH = os.path.join(RUN_CHECKPOINT_DIR, "dqn_agent_state.pth")

# --- REMOVED Assign derived paths to Config classes ---
# from .core import TensorBoardConfig # <<< REMOVE THIS IMPORT
# TensorBoardConfig.LOG_DIR = RUN_LOG_DIR # <<< REMOVE THIS ASSIGNMENT
# --- END REMOVED ---


File: config/validation.py
# File: config/validation.py
import os, torch
from .core import (
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    StatsConfig,
    TensorBoardConfig,
    VisConfig,
)
from .general import (
    RUN_ID,
    DEVICE,
    MODEL_SAVE_PATH,
    BUFFER_SAVE_PATH,
    RUN_CHECKPOINT_DIR,
    RUN_LOG_DIR,
    TOTAL_TRAINING_STEPS,
)


def print_config_info_and_validate():
    print("-" * 70)
    print(f"RUN ID: {RUN_ID}")
    print(f"Log Directory: {RUN_LOG_DIR}")
    print(f"Checkpoint Directory: {RUN_CHECKPOINT_DIR}")
    print(f"Device: {DEVICE}")
    print(
        f"TB Logging: Histograms={'ON' if TensorBoardConfig.LOG_HISTOGRAMS else 'OFF'}, Images={'ON' if TensorBoardConfig.LOG_IMAGES else 'OFF'}"
    )
    if EnvConfig.GRID_FEATURES_PER_CELL != 3:
        print(
            "Warning: Network assumes 3 features per cell (Occupied, Is_Up, Is_Death)."
        )
    if TrainConfig.LOAD_CHECKPOINT_PATH:
        print(
            "*" * 70
            + f"\n*** Warning: LOAD CHECKPOINT from: {TrainConfig.LOAD_CHECKPOINT_PATH} ***\n*** Ensure ckpt matches current Model/DQN Config (distributional, scheduler). ***\n"
            + "*" * 70
        )
    else:
        print("--- Starting training from scratch (no checkpoint specified). ---")
    if TrainConfig.LOAD_BUFFER_PATH:
        print(
            "*" * 70
            + f"\n*** Warning: LOAD BUFFER from: {TrainConfig.LOAD_BUFFER_PATH} ***\n*** Ensure buffer matches current Buffer Config (PER, N-Step). ***\n"
            + "*" * 70
        )
    else:
        print("--- Starting with an empty replay buffer (no buffer specified). ---")
    print(f"--- Using Noisy Nets: {DQNConfig.USE_NOISY_NETS} ---")
    print(
        f"--- Using Distributional (C51): {DQNConfig.USE_DISTRIBUTIONAL} (Atoms: {DQNConfig.NUM_ATOMS}, Vmin: {DQNConfig.V_MIN}, Vmax: {DQNConfig.V_MAX}) ---"
    )
    print(
        f"--- Using LR Scheduler: {DQNConfig.USE_LR_SCHEDULER}"
        + (
            f" (CosineAnnealingLR, T_max={DQNConfig.LR_SCHEDULER_T_MAX}, eta_min={DQNConfig.LR_SCHEDULER_ETA_MIN})"
            if DQNConfig.USE_LR_SCHEDULER
            else ""
        )
        + " ---"
    )
    print(
        f"Config: Env=(R={EnvConfig.ROWS}, C={EnvConfig.COLS}), StateDim={EnvConfig.STATE_DIM}, ActionDim={EnvConfig.ACTION_DIM}"
    )
    cnn_str = str(ModelConfig.Network.CONV_CHANNELS).replace(" ", "")
    mlp_str = str(ModelConfig.Network.COMBINED_FC_DIMS).replace(" ", "")
    print(
        f"Network: CNN={cnn_str}, ShapeMLP={ModelConfig.Network.SHAPE_MLP_HIDDEN_DIM}, Fusion={mlp_str}, Dueling={DQNConfig.USE_DUELING}"
    )
    print(
        f"Training: NUM_ENVS={EnvConfig.NUM_ENVS}, TOTAL_STEPS={TOTAL_TRAINING_STEPS/1e6:.1f}M, BUFFER={BufferConfig.REPLAY_BUFFER_SIZE/1e6:.1f}M, BATCH={TrainConfig.BATCH_SIZE}"
    )
    print(
        f"Buffer: PER={BufferConfig.USE_PER}, N-Step={BufferConfig.N_STEP if BufferConfig.USE_N_STEP else 'N/A'}"
    )
    print(
        f"Stats: AVG_WINDOW={StatsConfig.STATS_AVG_WINDOW}, Console Log Freq={StatsConfig.CONSOLE_LOG_FREQ}"
    )
    if EnvConfig.NUM_ENVS >= 1024:
        print(
            "*" * 70
            + f"\n*** Warning: NUM_ENVS={EnvConfig.NUM_ENVS}. Monitor system resources. ***"
            + (
                "\n*** Using MPS device. Performance varies. Force CPU via env var if needed. ***"
                if DEVICE.type == "mps"
                else ""
            )
            + "\n"
            + "*" * 70
        )
    print(
        f"--- Rendering {VisConfig.NUM_ENVS_TO_RENDER if VisConfig.NUM_ENVS_TO_RENDER > 0 else 'ALL'} of {EnvConfig.NUM_ENVS} environments ---"
    )
    print("-" * 70)


File: training/__init__.py


File: training/trainer.py
# File: training/trainer.py
import time
import torch
import torch.nn.functional as F  # Import F for potential use in logging Q-values
import numpy as np
import os
import pickle
import random
import traceback
import pygame
from typing import List, Optional, Union, Tuple, Callable
from collections import deque

from config import (
    EnvConfig,
    DQNConfig,
    TrainConfig,
    BufferConfig,
    ModelConfig,
    DEVICE,
    TensorBoardConfig,
    VisConfig,
    RewardConfig,
    TOTAL_TRAINING_STEPS,
)
from environment.game_state import GameState
from agent.dqn_agent import DQNAgent
from agent.replay_buffer.base_buffer import ReplayBufferBase
from agent.replay_buffer.buffer_utils import create_replay_buffer
from stats.stats_recorder import StatsRecorderBase
from utils.helpers import ensure_numpy, load_object, save_object
from utils.types import ActionType, StateType


class Trainer:
    """Orchestrates the DQN training process."""

    def __init__(
        self,
        envs: List[GameState],
        agent: DQNAgent,
        buffer: ReplayBufferBase,
        stats_recorder: StatsRecorderBase,
        env_config: EnvConfig,
        dqn_config: DQNConfig,
        train_config: TrainConfig,
        buffer_config: BufferConfig,
        model_config: ModelConfig,
        model_save_path: str,
        buffer_save_path: str,
        load_checkpoint_path: Optional[str] = None,
        load_buffer_path: Optional[str] = None,
    ):

        print("[Trainer] Initializing...")
        self.envs = envs
        self.agent = agent
        self.buffer = buffer
        self.stats_recorder = stats_recorder
        self.num_envs = env_config.NUM_ENVS
        self.device = DEVICE
        self.env_config = env_config
        self.dqn_config = dqn_config
        self.train_config = train_config
        self.buffer_config = buffer_config
        self.model_config = model_config
        self.reward_config = RewardConfig
        self.tb_config = TensorBoardConfig
        self.vis_config = VisConfig
        self.model_save_path = model_save_path
        self.buffer_save_path = buffer_save_path

        # Initialize state tracking
        self.global_step = 0
        self.episode_count = 0
        self.last_image_log_step = (
            -self.tb_config.IMAGE_LOG_FREQ
        )  # Ensure first log happens

        # Initialize environment states and episode tracking arrays
        try:
            self.current_states: List[StateType] = [
                ensure_numpy(env.reset()) for env in self.envs
            ]
        except Exception as e:
            print(f"FATAL ERROR during initial environment reset: {e}")
            traceback.print_exc()
            raise e
        self.current_episode_scores = np.zeros(self.num_envs, dtype=np.float32)
        self.current_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)
        self.current_episode_game_scores = np.zeros(self.num_envs, dtype=np.int32)
        self.current_episode_lines_cleared = np.zeros(self.num_envs, dtype=np.int32)

        # Load checkpoint and buffer if paths are provided
        if load_checkpoint_path:
            self._load_checkpoint(load_checkpoint_path)
        else:
            print("[Trainer] No checkpoint specified, starting agent fresh.")
            self._reset_trainer_state()  # Ensure state is reset if no checkpoint

        if load_buffer_path:
            self._load_buffer_state(load_buffer_path)
        else:
            print("[Trainer] No buffer specified, starting buffer empty.")
            # Ensure buffer is created even if not loading
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)

        # Initialize PER beta and record initial stats
        initial_beta = self._update_beta()
        initial_lr = (
            self.agent.optimizer.param_groups[0]["lr"]
            if hasattr(self.agent, "optimizer")
            else 0.0
        )
        self.stats_recorder.record_step(
            {
                "buffer_size": len(self.buffer),
                "epsilon": 0.0,  # Epsilon is effectively 0 with Noisy Nets
                "beta": initial_beta,
                "lr": initial_lr,
                "global_step": self.global_step,
                "episode_count": self.episode_count,
            }
        )

        print(
            f"[Trainer] Init complete. Start Step={self.global_step}, Ep={self.episode_count}, Buf={len(self.buffer)}, Beta={initial_beta:.4f}, LR={initial_lr:.1e}"
        )

    def _load_checkpoint(self, path_to_load: str):
        """Loads agent and trainer state from a checkpoint file."""
        if not os.path.isfile(path_to_load):
            print(
                f"[Trainer] LOAD WARNING: Checkpoint file not found at {path_to_load}. Starting fresh."
            )
            self._reset_trainer_state()
            return

        print(f"[Trainer] Loading agent checkpoint from: {path_to_load}")
        try:
            # Load the entire state dict first
            checkpoint = torch.load(path_to_load, map_location=self.device)

            # Load into agent (handles net, optim, scheduler)
            self.agent.load_state_dict(checkpoint)

            # Load trainer state (global step, episode count)
            self.global_step = checkpoint.get("global_step", 0)
            self.episode_count = checkpoint.get("episode_count", 0)
            print(
                f"[Trainer] Checkpoint loaded. Resuming from step {self.global_step}, ep {self.episode_count}"
            )

        except FileNotFoundError:
            print(
                f"[Trainer] Checkpoint file disappeared? ({path_to_load}). Starting fresh."
            )
            self._reset_trainer_state()
        except KeyError as e:
            print(
                f"[Trainer] Checkpoint missing key '{e}'. Incompatible? Starting fresh."
            )
            self._reset_trainer_state()
        except Exception as e:
            print(f"[Trainer] CRITICAL ERROR loading checkpoint: {e}. Starting fresh.")
            traceback.print_exc()
            self._reset_trainer_state()

    def _reset_trainer_state(self):
        """Resets trainer-specific state variables."""
        self.global_step = 0
        self.episode_count = 0

    def _load_buffer_state(self, path_to_load: str):
        """Loads replay buffer state from a file."""
        if not os.path.isfile(path_to_load):
            print(
                f"[Trainer] LOAD WARNING: Buffer file not found at {path_to_load}. Starting empty."
            )
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)
            return

        print(f"[Trainer] Attempting to load buffer state from: {path_to_load}")
        try:
            # Recreate buffer first to ensure correct type and config
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)
            self.buffer.load_state(
                path_to_load
            )  # Load state into the new buffer instance
            print(f"[Trainer] Buffer state loaded. Size: {len(self.buffer)}")
        except (
            FileNotFoundError,
            EOFError,
            pickle.UnpicklingError,
            ImportError,
            AttributeError,
            ValueError,
        ) as e:
            print(
                f"[Trainer] ERROR loading buffer (incompatible/corrupt?): {e}. Starting empty."
            )
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)
        except Exception as e:
            print(f"[Trainer] CRITICAL ERROR loading buffer: {e}. Starting empty.")
            traceback.print_exc()
            self.buffer = create_replay_buffer(self.buffer_config, self.dqn_config)

    def _save_checkpoint(self, is_final=False):
        """Saves agent and buffer state."""
        prefix = "FINAL" if is_final else f"step_{self.global_step}"
        save_dir = os.path.dirname(self.model_save_path)
        os.makedirs(save_dir, exist_ok=True)

        # Save Agent State
        print(
            f"[Trainer] Saving agent checkpoint ({prefix}) to: {self.model_save_path}"
        )
        try:
            # Get agent state (includes scheduler now)
            agent_save_data = self.agent.get_state_dict()
            # Add trainer state to the same dictionary
            agent_save_data["global_step"] = self.global_step
            agent_save_data["episode_count"] = self.episode_count

            torch.save(agent_save_data, self.model_save_path)
            print(f"[Trainer] Agent checkpoint ({prefix}) saved.")
        except Exception as e:
            print(f"[Trainer] ERROR saving agent checkpoint ({prefix}): {e}")
            traceback.print_exc()

        # Save Buffer State
        print(
            f"[Trainer] Saving buffer state ({prefix}) to: {self.buffer_save_path} (Size: {len(self.buffer)})"
        )
        try:
            # Flush any pending N-step transitions before saving
            if hasattr(self.buffer, "flush_pending"):
                self.buffer.flush_pending()
            self.buffer.save_state(self.buffer_save_path)
            print(f"[Trainer] Buffer state ({prefix}) saved.")
        except Exception as e:
            print(f"[Trainer] ERROR saving buffer state ({prefix}): {e}")
            traceback.print_exc()

    def _update_beta(self) -> float:
        """Updates PER beta based on global step and records it."""
        if not self.buffer_config.USE_PER:
            beta = 1.0  # Beta is irrelevant if PER is not used
        else:
            start = self.buffer_config.PER_BETA_START
            end = 1.0
            frames = self.buffer_config.PER_BETA_FRAMES
            fraction = min(
                1.0, float(self.global_step) / max(1, frames)
            )  # Avoid division by zero
            beta = start + fraction * (end - start)
            if hasattr(self.buffer, "set_beta"):
                self.buffer.set_beta(beta)

        # Record beta regardless of PER usage (will be 1.0 if not used)
        self.stats_recorder.record_step({"beta": beta, "global_step": self.global_step})
        return beta

    def _collect_experience(self):
        """Collects one step of experience from each parallel environment."""
        actions: List[ActionType] = [
            -1
        ] * self.num_envs  # Initialize with invalid action
        step_rewards_batch = np.zeros(self.num_envs, dtype=np.float32)
        start_time = time.time()

        # 1. Select actions for all environments
        for i in range(self.num_envs):
            # If env was done, it should have been reset in the previous iteration's step handling
            # We use self.current_states[i] which holds the state after potential reset
            valid_actions = self.envs[i].valid_actions()
            if not valid_actions:
                # This happens if the game ends immediately upon reset or enters a state with no moves
                # print(f"Warning: Env {i} has no valid actions at step start. Using action 0.")
                actions[i] = (
                    0  # Choose a default action (e.g., 0) or handle appropriately
                )
            else:
                try:
                    # Epsilon is ignored by agent if using Noisy Nets
                    actions[i] = self.agent.select_action(
                        self.current_states[i], 0.0, valid_actions
                    )
                except Exception as e:
                    print(f"ERROR: Agent select_action failed for env {i}: {e}")
                    traceback.print_exc()
                    actions[i] = random.choice(
                        valid_actions
                    )  # Fallback to random valid action

        # 2. Step all environments with selected actions
        next_states_list: List[StateType] = [
            np.zeros_like(self.current_states[0]) for _ in range(self.num_envs)
        ]
        rewards_list = np.zeros(self.num_envs, dtype=np.float32)
        dones_list = np.zeros(self.num_envs, dtype=bool)

        for i in range(self.num_envs):
            env = self.envs[i]
            current_state = self.current_states[i]  # State used for action selection
            action = actions[i]

            try:
                # --- FIX: Correct Experience Collection ---
                # Step the environment
                reward, done = env.step(action)

                # Get the state resulting from the action *before* potential reset
                next_state_raw = env.get_state()

                # Store results for this step
                rewards_list[i] = reward
                dones_list[i] = done
                step_rewards_batch[i] = reward  # For logging batch rewards

                # Push the transition (S, A, R, S') to the buffer
                # S' is the state *before* reset if done is True
                self.buffer.push(
                    current_state, action, reward, ensure_numpy(next_state_raw), done
                )

                # Update episode trackers
                self.current_episode_scores[i] += reward
                self.current_episode_lengths[i] += 1
                if not done:
                    # Update game score/lines only if not done (use final values otherwise)
                    self.current_episode_game_scores[i] = env.game_score
                    self.current_episode_lines_cleared[i] = (
                        env.lines_cleared_this_episode
                    )

                # Handle environment termination
                if done:
                    self.episode_count += 1
                    # Record completed episode stats
                    self.stats_recorder.record_episode(
                        episode_score=self.current_episode_scores[i],
                        episode_length=self.current_episode_lengths[i],
                        episode_num=self.episode_count,
                        global_step=self.global_step + i + 1,  # Approximate step count
                        game_score=self.current_episode_game_scores[
                            i
                        ],  # Use final game score
                        lines_cleared=self.current_episode_lines_cleared[
                            i
                        ],  # Use final lines cleared
                    )
                    # Reset environment and store the *new* starting state for the *next* iteration
                    next_states_list[i] = ensure_numpy(env.reset())
                    # Reset episode trackers for this environment
                    self.current_episode_scores[i] = 0.0
                    self.current_episode_lengths[i] = 0
                    self.current_episode_game_scores[i] = 0
                    self.current_episode_lines_cleared[i] = 0
                else:
                    # If not done, the next state for the next iteration is the one we observed
                    next_states_list[i] = ensure_numpy(next_state_raw)
                # --- END FIX ---

            except Exception as e:
                print(f"ERROR: Env {i} step/reset failed (Action: {action}): {e}")
                traceback.print_exc()
                # Handle environment crash: record minimal reward, mark as done, reset
                rewards_list[i] = self.reward_config.PENALTY_GAME_OVER
                dones_list[i] = True
                step_rewards_batch[i] = self.reward_config.PENALTY_GAME_OVER
                # Attempt to reset the crashed environment
                try:
                    crashed_state_reset = ensure_numpy(env.reset())
                except Exception as reset_e:
                    print(
                        f"FATAL: Env {i} failed to reset after crash: {reset_e}. Replacing state with zeros."
                    )
                    crashed_state_reset = np.zeros_like(self.current_states[i])

                # Push a terminal transition for the failed step
                self.buffer.push(
                    current_state, action, rewards_list[i], crashed_state_reset, True
                )

                # Record episode as ended due to error
                self.episode_count += 1
                self.stats_recorder.record_episode(
                    episode_score=self.current_episode_scores[i]
                    + rewards_list[i],  # Include penalty
                    episode_length=self.current_episode_lengths[i] + 1,
                    episode_num=self.episode_count,
                    global_step=self.global_step + i + 1,
                    game_score=self.current_episode_game_scores[
                        i
                    ],  # Last known game score
                    lines_cleared=self.current_episode_lines_cleared[
                        i
                    ],  # Last known lines
                )

                # Store the reset state for the next iteration
                next_states_list[i] = crashed_state_reset
                # Reset episode trackers
                self.current_episode_scores[i] = 0.0
                self.current_episode_lengths[i] = 0
                self.current_episode_game_scores[i] = 0
                self.current_episode_lines_cleared[i] = 0

        # 3. Update current states for the next iteration
        self.current_states = next_states_list
        self.global_step += self.num_envs  # Increment global step count

        # 4. Record step statistics
        end_time = time.time()
        step_duration = end_time - start_time
        step_log_data = {
            "buffer_size": len(self.buffer),
            "global_step": self.global_step,
            "step_time": step_duration,
            "num_steps_processed": self.num_envs,
        }
        # Optionally log batch rewards/actions for histograms
        if self.tb_config.LOG_HISTOGRAMS:
            step_log_data["step_rewards_batch"] = step_rewards_batch
            step_log_data["action_batch"] = np.array(actions, dtype=np.int32)
        self.stats_recorder.record_step(step_log_data)

    def _train_batch(self):
        """Samples a batch, computes loss, updates agent, and records stats."""
        # Check if learning should start
        if (
            len(self.buffer) < self.train_config.BATCH_SIZE
            or self.global_step < self.train_config.LEARN_START_STEP
        ):
            return  # Not enough samples or too early to learn

        # Update PER beta
        beta = self._update_beta()

        # Sample from buffer
        is_n_step = self.buffer_config.USE_N_STEP and self.buffer_config.N_STEP > 1
        indices, is_weights_np, batch_np_tuple = None, None, None
        try:
            if self.buffer_config.USE_PER:
                sample_result = self.buffer.sample(self.train_config.BATCH_SIZE)
                if sample_result:
                    batch_np_tuple, indices, is_weights_np = sample_result
                else:
                    # Handle case where buffer sample returns None (e.g., PER error)
                    print(
                        "Warning: Buffer sample returned None. Skipping training step."
                    )
                    return
            else:  # Uniform sampling
                batch_np_tuple = self.buffer.sample(self.train_config.BATCH_SIZE)

            if batch_np_tuple is None:  # Should be caught above, but double-check
                print(
                    "Warning: Buffer sample returned None tuple. Skipping training step."
                )
                return

        except Exception as e:
            print(f"ERROR sampling buffer: {e}")
            traceback.print_exc()
            return

        # Compute loss and TD errors
        raw_q_values = None  # For potential histogram logging
        loss = torch.tensor(0.0)  # Default loss
        td_errors = None
        try:
            # --- Optional: Log raw Q-values before loss computation ---
            # This requires converting batch to tensor first
            if self.tb_config.LOG_HISTOGRAMS:
                with torch.no_grad():
                    tensor_batch_log = self.agent._np_batch_to_tensor(
                        batch_np_tuple, is_n_step
                    )
                    self.agent.online_net.eval()  # Use eval mode for consistent Q-values
                    if self.agent.use_distributional:
                        dist_logits = self.agent.online_net(tensor_batch_log[0])
                        raw_q_values = (
                            F.softmax(dist_logits, dim=2) * self.agent.support
                        ).sum(dim=2)
                    else:
                        raw_q_values = self.agent.online_net(tensor_batch_log[0])
                    self.agent.online_net.train()  # Switch back to train mode
            # --- End Optional Logging ---

            # Compute loss (handles distributional/standard, PER weights)
            loss, td_errors = self.agent.compute_loss(
                batch_np_tuple, is_n_step, is_weights_np
            )

        except Exception as e:
            print(f"ERROR computing loss: {e}")
            traceback.print_exc()
            # Potentially skip update if loss computation fails critically
            return

        # Update agent (optimizer step, scheduler step, noise reset)
        grad_norm = None
        try:
            grad_norm = self.agent.update(loss)  # Update includes scheduler step
        except Exception as e:
            print(f"ERROR updating agent: {e}")
            traceback.print_exc()
            # Potentially skip priority update if agent update fails
            return

        # Update priorities in PER buffer
        if self.buffer_config.USE_PER and indices is not None and td_errors is not None:
            try:
                # Ensure td_errors are on CPU and numpy for SumTree
                td_errors_np = td_errors.squeeze().cpu().numpy()
                self.buffer.update_priorities(indices, td_errors_np)
            except Exception as e:
                print(f"ERROR updating PER priorities: {e}")
                traceback.print_exc()  # Log error but continue

        # Record training statistics
        current_lr = self.agent.optimizer.param_groups[0]["lr"]
        train_log_data = {
            "loss": loss.item(),
            "grad_norm": grad_norm if grad_norm is not None else 0.0,
            "avg_max_q": self.agent.get_last_avg_max_q(),  # Get Q value from agent
            "lr": current_lr,
            "global_step": self.global_step,
        }
        # Add optional histograms
        if self.tb_config.LOG_HISTOGRAMS:
            if raw_q_values is not None:
                train_log_data["batch_q_values"] = (
                    raw_q_values  # Log Q-values if calculated
                )
            if td_errors is not None:
                train_log_data["batch_td_errors"] = td_errors  # Log TD errors
        self.stats_recorder.record_step(train_log_data)

    def step(self):
        """Performs one full step: collect experience, train, update target net."""
        # 1. Collect experience from environments
        self._collect_experience()

        # 2. Perform training step(s) if conditions met
        if (
            self.global_step >= self.train_config.LEARN_START_STEP
            and self.global_step % self.train_config.LEARN_FREQ == 0
        ):
            # Check buffer size again just before training
            if len(self.buffer) >= self.train_config.BATCH_SIZE:
                self._train_batch()
            # else: print(f"Skipping train step {self.global_step}: Buffer size {len(self.buffer)} < Batch size {self.train_config.BATCH_SIZE}")

        # 3. Update target network periodically
        target_freq = self.dqn_config.TARGET_UPDATE_FREQ
        if target_freq > 0 and self.global_step > 0:
            # Check if the update threshold was crossed *during the last num_envs steps*
            steps_before_this_iter = self.global_step - self.num_envs
            if steps_before_this_iter // target_freq < self.global_step // target_freq:
                print(f"[Trainer] Updating target network at step {self.global_step}")
                self.agent.update_target_network()

        # 4. Maybe save checkpoint and log images
        self.maybe_save_checkpoint()
        self._maybe_log_image()

    def _maybe_log_image(self):
        """Logs an image of a random environment state to TensorBoard periodically."""
        if not self.tb_config.LOG_IMAGES:
            return

        img_freq = self.tb_config.IMAGE_LOG_FREQ
        # Check if the logging threshold was crossed during the last num_envs steps
        steps_before_this_iter = self.global_step - self.num_envs
        if (
            img_freq > 0
            and steps_before_this_iter // img_freq < self.global_step // img_freq
        ):
            try:
                env_idx = random.randint(0, self.num_envs - 1)
                img_array = self._get_env_image_as_numpy(env_idx)
                if img_array is not None:
                    # Convert HWC (Pygame) to CHW (TensorBoard)
                    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)
                    self.stats_recorder.record_image(
                        f"Environment/Sample State Env {env_idx}",
                        img_tensor,
                        self.global_step,
                    )
            except Exception as e:
                print(f"Error logging environment image: {e}")

    def _get_env_image_as_numpy(self, env_index: int) -> Optional[np.ndarray]:
        """Renders a single environment state to a NumPy array for logging."""
        if not (0 <= env_index < self.num_envs):
            return None

        env = self.envs[env_index]
        # Define desired image dimensions (can be adjusted)
        img_h = 300
        # Calculate width based on aspect ratio of the triangular grid
        aspect_ratio = (self.env_config.COLS * 0.75 + 0.25) / max(
            1, self.env_config.ROWS
        )
        img_w = int(img_h * aspect_ratio)

        if img_w <= 0 or img_h <= 0:
            return None  # Invalid dimensions

        try:
            # Create a temporary Pygame surface
            temp_surf = pygame.Surface((img_w, img_h))
            # Calculate cell dimensions for rendering on this surface
            cell_w_px = img_w / (self.env_config.COLS * 0.75 + 0.25)
            cell_h_px = img_h / max(1, self.env_config.ROWS)

            temp_surf.fill(self.vis_config.BLACK)  # Background

            # Render grid triangles
            if hasattr(env, "grid") and hasattr(env.grid, "triangles"):
                for r in range(env.grid.rows):
                    for c in range(env.grid.cols):
                        # Basic check for grid structure validity
                        if r < len(env.grid.triangles) and c < len(
                            env.grid.triangles[r]
                        ):
                            t = env.grid.triangles[r][c]
                            pts = t.get_points(
                                ox=0, oy=0, cw=int(cell_w_px), ch=int(cell_h_px)
                            )
                            color = self.vis_config.GRAY  # Default for empty
                            if t.is_death:
                                color = (10, 10, 10)  # Darker gray for death cells
                            elif t.is_occupied:
                                color = (
                                    t.color if t.color else self.vis_config.RED
                                )  # Use shape color or red fallback
                            pygame.draw.polygon(temp_surf, color, pts)
                        # else: print(f"Warning: Grid index out of bounds ({r},{c}) during image render")

            # Convert Pygame surface to NumPy array (HWC format)
            img_array = pygame.surfarray.array3d(temp_surf)
            # Pygame gives W, H, C -> Transpose to H, W, C
            return np.transpose(img_array, (1, 0, 2))

        except Exception as e:
            print(f"Error generating environment image for TB: {e}")
            traceback.print_exc()
            return None

    def maybe_save_checkpoint(self, force_save=False):
        """Saves a checkpoint periodically or if forced."""
        save_freq = self.train_config.CHECKPOINT_SAVE_FREQ
        if save_freq <= 0 and not force_save:
            return  # Saving disabled unless forced

        # Check if the save threshold was crossed during the last num_envs steps
        steps_before_this_iter = self.global_step - self.num_envs
        should_save_freq = (
            save_freq > 0
            and self.global_step > 0
            and steps_before_this_iter // save_freq < self.global_step // save_freq
        )

        if force_save or should_save_freq:
            self._save_checkpoint(is_final=False)

    def train_loop(self):
        """Main training loop that runs until total steps are reached."""
        print("[Trainer] Starting training loop...")
        try:
            while self.global_step < TOTAL_TRAINING_STEPS:
                self.step()
        except KeyboardInterrupt:
            print("\n[Trainer] Training loop interrupted by user (Ctrl+C).")
        except Exception as e:
            print(f"\n[Trainer] CRITICAL ERROR in training loop: {e}")
            traceback.print_exc()
        finally:
            print("[Trainer] Training loop finished or terminated.")
            self.cleanup(save_final=True)  # Ensure cleanup and final save

    def cleanup(self, save_final: bool = True):
        """Cleans up resources, optionally saving a final checkpoint."""
        print("[Trainer] Cleaning up resources...")

        # Flush N-step buffer if applicable
        if (
            hasattr(self, "buffer")
            and self.buffer
            and hasattr(self.buffer, "flush_pending")
        ):
            print("[Trainer] Flushing pending N-step transitions...")
            try:
                self.buffer.flush_pending()
            except Exception as flush_e:
                print(f"Error flushing buffer during cleanup: {flush_e}")

        # Save final checkpoint
        if save_final:
            print("[Trainer] Saving final checkpoint...")
            self._save_checkpoint(is_final=True)
        else:
            print("[Trainer] Skipping final save as requested.")

        # Close stats recorder (e.g., TensorBoard writer)
        if (
            hasattr(self, "stats_recorder")
            and self.stats_recorder
            and hasattr(self.stats_recorder, "close")
        ):
            try:
                self.stats_recorder.close()
            except Exception as close_e:
                print(f"Error closing stats recorder during cleanup: {close_e}")

        print("[Trainer] Cleanup complete.")


File: utils/__init__.py


File: utils/init_checks.py
# File: utils/init_checks.py
# --- Pre-Run Sanity Checks ---
import sys
import traceback
from config import EnvConfig

# Import core components
try:
    from environment.game_state import GameState
except ImportError as e:
    print(f"Error importing environment: {e}")
    sys.exit(1)


def run_pre_checks() -> bool:
    """Performs basic checks on GameState and configuration compatibility."""
    print("--- Pre-Run Checks ---")
    try:
        print("Checking GameState and Configuration Compatibility...")
        gs_test = GameState()
        gs_test.reset()
        s_test = gs_test.get_state()
        if len(s_test) != EnvConfig.STATE_DIM:
            raise ValueError(
                f"State Dim Mismatch! GameState:{len(s_test)}, EnvConfig:{EnvConfig.STATE_DIM}"
            )
        print(f"GameState state dimension check PASSED (Length: {len(s_test)}).")

        _ = gs_test.valid_actions()
        print("GameState valid_actions check PASSED.")

        if not hasattr(gs_test, "game_score"):
            raise AttributeError("GameState missing 'game_score' attribute!")
        print("GameState 'game_score' attribute check PASSED.")

        if not hasattr(gs_test, "lines_cleared_this_episode"):
            raise AttributeError(
                "GameState missing 'lines_cleared_this_episode' attribute!"
            )
        print("GameState 'lines_cleared_this_episode' attribute check PASSED.")

        del gs_test
        print("--- Pre-Run Checks Complete ---")
        return True
    except (NameError, ImportError) as e:
        print(f"FATAL ERROR: Import/Name error: {e}")
    except (ValueError, AttributeError) as e:
        print(f"FATAL ERROR during pre-run checks: {e}")
    except Exception as e:
        print(f"FATAL ERROR during GameState pre-check: {e}")
        traceback.print_exc()

    # If any check fails and raises an exception that's caught here, exit.
    sys.exit(1)  # Exit if checks fail


File: utils/types.py
# File: utils/types.py
from typing import NamedTuple, Union, Tuple, List, Dict, Any, Optional
import numpy as np
import torch


class Transition(NamedTuple):
    state: np.ndarray
    action: int
    reward: float  # For N-step buffer, this holds the N-step RL reward
    next_state: np.ndarray  # For N-step buffer, this holds the N-step next state
    done: bool  # For N-step buffer, this holds the N-step done flag
    n_step_discount: Optional[float] = None  # gamma^k for N-step


# Type alias for state
StateType = np.ndarray
# Type alias for action
ActionType = int

# --- Batch Types (Numpy) ---
# Standard 1-step batch
NumpyBatch = Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]
# (states, actions, rewards, next_states, dones)

# N-step batch (includes discount factor gamma^k)
NumpyNStepBatch = Tuple[
    np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray
]
# (states, actions, n_step_rewards, n_step_next_states, n_step_dones, n_step_discounts)

# Prioritized 1-step batch
PrioritizedNumpyBatch = Tuple[NumpyBatch, np.ndarray, np.ndarray]
# ((s,a,r,ns,d), indices, weights)

# Prioritized N-step batch
PrioritizedNumpyNStepBatch = Tuple[NumpyNStepBatch, np.ndarray, np.ndarray]
# ((s,a,rn,nsn,dn,gamman), indices, weights)


# --- Batch Types (Tensor) ---
# Standard 1-step batch
TensorBatch = Tuple[
    torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor
]
# (states, actions, rewards, next_states, dones)

# N-step batch (includes discount factor gamma^k)
TensorNStepBatch = Tuple[
    torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor
]
# (states, actions, n_step_rewards, n_step_next_states, n_step_dones, n_step_discounts)


# --- Agent State ---
AgentStateDict = Dict[str, Any]


File: utils/helpers.py
# File: utils/helpers.py
import torch
import numpy as np
import random
import os
import pickle
import cloudpickle
from typing import Union, Any


def get_device() -> torch.device:
    """Gets the appropriate torch device (MPS, CUDA, or CPU)."""
    force_cpu = os.environ.get("FORCE_CPU", "false").lower() == "true"
    if force_cpu:
        print("Forcing CPU device based on environment variable.")
        return torch.device("cpu")

    if torch.backends.mps.is_available():
        device_str = "mps"
    elif torch.cuda.is_available():
        device_str = "cuda"
    else:
        device_str = "cpu"

    print(f"Using device: {device_str.upper()}")
    if device_str == "cuda":
        print(f"CUDA Device Name: {torch.cuda.get_device_name(0)}")
    elif device_str == "mps":
        print("MPS device found on MacOS.")
    return torch.device(device_str)


def set_random_seeds(seed: int = 42):
    """Sets random seeds for Python, NumPy, and PyTorch."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # Note: Setting deterministic algorithms can impact performance
        # torch.backends.cudnn.deterministic = True
        # torch.backends.cudnn.benchmark = False
    print(f"Set random seeds to {seed}")


def ensure_numpy(data: Union[np.ndarray, list, tuple, torch.Tensor]) -> np.ndarray:
    """Ensures the input data is a numpy array with float32 type."""
    try:
        if isinstance(data, np.ndarray):
            if data.dtype != np.float32:
                return data.astype(np.float32)
            return data
        elif isinstance(data, torch.Tensor):
            return data.detach().cpu().numpy().astype(np.float32)
        elif isinstance(data, (list, tuple)):
            arr = np.array(data, dtype=np.float32)
            if arr.dtype == np.object_:  # Indicates ragged array
                raise ValueError(
                    "Cannot convert ragged list/tuple to float32 numpy array."
                )
            return arr
        else:
            # Attempt conversion for single numbers or other types
            return np.array([data], dtype=np.float32)
    except (ValueError, TypeError, RuntimeError) as e:
        print(
            f"CRITICAL ERROR in ensure_numpy conversion: {e}. Input type: {type(data)}. Data (partial): {str(data)[:100]}"
        )
        raise ValueError(f"ensure_numpy failed: {e}") from e


def save_object(obj: Any, filepath: str):
    """Saves an arbitrary Python object to a file using cloudpickle."""
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, "wb") as f:
            cloudpickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)
    except Exception as e:
        print(f"Error saving object to {filepath}: {e}")
        raise e  # Re-raise after logging


def load_object(filepath: str) -> Any:
    """Loads a Python object from a file using cloudpickle."""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found for loading: {filepath}")
    try:
        with open(filepath, "rb") as f:
            obj = cloudpickle.load(f)
        return obj
    except Exception as e:
        print(f"Error loading object from {filepath}: {e}")
        raise e  # Re-raise after logging


File: agent/__init__.py


File: agent/dqn_agent.py
# File: agent/dqn_agent.py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
import random
import math
import traceback
from typing import Tuple, List, Dict, Any, Optional, Union

from config import EnvConfig, ModelConfig, DQNConfig, DEVICE
from agent.model_factory import create_network
from utils.types import (
    StateType,
    ActionType,
    NumpyBatch,
    NumpyNStepBatch,
    AgentStateDict,
    TensorBatch,
    TensorNStepBatch,
)
from utils.helpers import ensure_numpy
from agent.networks.noisy_layer import NoisyLinear


class DQNAgent:
    """DQN Agent using Noisy Nets, Dueling, Double DQN, C51, and LR Scheduling."""

    def __init__(
        self, config: ModelConfig, dqn_config: DQNConfig, env_config: EnvConfig
    ):
        print("[DQNAgent] Initializing...")
        self.device = DEVICE
        self.action_dim = env_config.ACTION_DIM
        self.gamma = dqn_config.GAMMA
        self.use_double_dqn = dqn_config.USE_DOUBLE_DQN
        self.gradient_clip_norm = dqn_config.GRADIENT_CLIP_NORM
        self.use_noisy_nets = dqn_config.USE_NOISY_NETS
        self.use_dueling = dqn_config.USE_DUELING
        self.use_distributional = dqn_config.USE_DISTRIBUTIONAL
        self.v_min = dqn_config.V_MIN
        self.v_max = dqn_config.V_MAX
        self.num_atoms = dqn_config.NUM_ATOMS
        self.dqn_config = dqn_config  # Store config

        if self.use_distributional:
            if self.num_atoms <= 1:
                raise ValueError("NUM_ATOMS must be >= 2 for Distributional RL")
            self.support = torch.linspace(
                self.v_min, self.v_max, self.num_atoms, device=self.device
            )
            # Ensure delta_z is not zero if num_atoms is 1 (although prevented above)
            self.delta_z = (self.v_max - self.v_min) / max(1, self.num_atoms - 1)

        self.online_net = create_network(
            env_config.STATE_DIM, self.action_dim, config, dqn_config
        ).to(self.device)
        self.target_net = create_network(
            env_config.STATE_DIM, self.action_dim, config, dqn_config
        ).to(self.device)
        print(
            f"[DQNAgent] Initial online_net device: {next(self.online_net.parameters()).device}"
        )
        print(
            f"[DQNAgent] Initial target_net device: {next(self.target_net.parameters()).device}"
        )
        self.target_net.load_state_dict(self.online_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.AdamW(
            self.online_net.parameters(),
            lr=dqn_config.LEARNING_RATE,
            eps=dqn_config.ADAM_EPS,
            weight_decay=1e-5,
        )

        self.scheduler = None
        if dqn_config.USE_LR_SCHEDULER:
            print(
                f"[DQNAgent] Using CosineAnnealingLR scheduler (T_max={dqn_config.LR_SCHEDULER_T_MAX}, eta_min={dqn_config.LR_SCHEDULER_ETA_MIN})"
            )
            self.scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=dqn_config.LR_SCHEDULER_T_MAX,
                eta_min=dqn_config.LR_SCHEDULER_ETA_MIN,
            )

        if not self.use_distributional:
            self.loss_fn = nn.SmoothL1Loss(reduction="none", beta=1.0)
        self._last_avg_max_q: float = 0.0
        self._print_init_info(dqn_config)

    def _print_init_info(self, dqn_config: DQNConfig):
        print(f"[DQNAgent] Using Device: {self.device}")
        print(f"[DQNAgent] Online Network: {type(self.online_net).__name__}")
        print(f"[DQNAgent] Using Double DQN: {self.use_double_dqn}")
        print(f"[DQNAgent] Using Dueling: {self.use_dueling}")
        print(f"[DQNAgent] Using Noisy Nets: {self.use_noisy_nets}")
        print(f"[DQNAgent] Using Distributional (C51): {self.use_distributional}")
        if self.use_distributional:
            print(
                f"  - Atoms: {self.num_atoms}, Vmin: {self.v_min}, Vmax: {self.v_max}"
            )
        print(
            f"[DQNAgent] Using LR Scheduler: {self.dqn_config.USE_LR_SCHEDULER}"
            + (
                f" (T_max={self.dqn_config.LR_SCHEDULER_T_MAX}, eta_min={self.dqn_config.LR_SCHEDULER_ETA_MIN})"
                if self.dqn_config.USE_LR_SCHEDULER
                else ""
            )
        )
        print(
            f"[DQNAgent] Optimizer: AdamW (LR={dqn_config.LEARNING_RATE}, EPS={dqn_config.ADAM_EPS})"
        )
        total_params = sum(
            p.numel() for p in self.online_net.parameters() if p.requires_grad
        )
        print(f"[DQNAgent] Trainable Parameters: {total_params / 1e6:.2f} M")

    @torch.no_grad()
    def select_action(
        self, state: StateType, epsilon: float, valid_actions: List[ActionType]
    ) -> ActionType:
        if not valid_actions:
            return 0

        state_t = torch.tensor(
            ensure_numpy(state), dtype=torch.float32, device=self.device
        ).unsqueeze(0)
        model_device = next(self.online_net.parameters()).device
        if state_t.device != model_device:
            state_t = state_t.to(model_device)

        self.online_net.eval()
        dist_or_q = self.online_net(state_t)

        if self.use_distributional:
            probabilities = F.softmax(dist_or_q, dim=2)
            q_values = (probabilities * self.support).sum(dim=2)
        else:
            q_values = dist_or_q

        q_values_masked = torch.full_like(q_values[0], -float("inf"))
        valid_action_indices = torch.tensor(
            valid_actions, dtype=torch.long, device=q_values.device
        )

        if valid_action_indices.numel() > 0:
            max_valid_idx = torch.max(valid_action_indices)
            if max_valid_idx < q_values.shape[1]:
                q_values_masked[valid_action_indices] = q_values[
                    0, valid_action_indices
                ]
            else:
                print(
                    f"Warning: Max valid action index ({max_valid_idx}) >= action_dim ({q_values.shape[1]}). Choosing random valid action."
                )
                return random.choice(valid_actions)
        else:
            return 0

        best_action = torch.argmax(q_values_masked).item()
        q_val_of_best_action = q_values_masked[best_action].item()
        self._last_avg_max_q = (
            q_val_of_best_action
            if q_val_of_best_action > -float("inf")
            else -float("inf")
        )

        return best_action

    def _np_batch_to_tensor(
        self, batch: Union[NumpyBatch, NumpyNStepBatch], is_n_step: bool
    ) -> Union[TensorBatch, TensorNStepBatch]:
        """Converts numpy batch tuple to tensor tuple on the correct device."""
        if is_n_step:
            s, a, rn, nsn, dn, gamma_n = batch
            tensors = [
                torch.tensor(arr, device=self.device)
                for arr in [s, a, rn, nsn, dn, gamma_n]
            ]
            tensors[0] = tensors[0].float()
            tensors[1] = tensors[1].long().unsqueeze(1)
            tensors[2] = tensors[2].float().unsqueeze(1)
            tensors[3] = tensors[3].float()
            tensors[4] = tensors[4].float().unsqueeze(1)
            tensors[5] = tensors[5].float().unsqueeze(1)
            return tuple(tensors)
        else:
            s, a, r, ns, d = batch
            tensors = [
                torch.tensor(arr, device=self.device) for arr in [s, a, r, ns, d]
            ]
            tensors[0] = tensors[0].float()
            tensors[1] = tensors[1].long().unsqueeze(1)
            tensors[2] = tensors[2].float().unsqueeze(1)
            tensors[3] = tensors[3].float()
            tensors[4] = tensors[4].float().unsqueeze(1)
            return tuple(tensors)

    @torch.no_grad()
    def _get_target_distribution(
        self, batch: Union[TensorBatch, TensorNStepBatch], is_n_step: bool
    ) -> torch.Tensor:
        """Calculates the target distribution for C51 using Double DQN logic."""
        if is_n_step:
            _, _, rewards, next_states, dones, discounts = batch
        else:
            _, _, rewards, next_states, dones = batch[:5]
            discounts = torch.full_like(rewards, self.gamma)

        batch_size = next_states.size(0)

        self.online_net.eval()
        online_next_dist_logits = self.online_net(next_states)
        online_next_probs = F.softmax(online_next_dist_logits, dim=2)
        online_expected_q = (online_next_probs * self.support).sum(dim=2)
        best_next_actions = online_expected_q.argmax(dim=1)

        self.target_net.eval()
        target_next_dist_logits = self.target_net(next_states)
        target_next_probs = F.softmax(target_next_dist_logits, dim=2)
        target_next_best_dist_probs = target_next_probs[
            torch.arange(batch_size), best_next_actions
        ]

        Tz = rewards + discounts * self.support.unsqueeze(0) * (1.0 - dones)
        Tz = Tz.clamp(self.v_min, self.v_max)

        # --- MODIFIED: Fix index_add_ logic ---
        # Calculate indices and weights
        b = (Tz - self.v_min) / self.delta_z
        lower_idx = b.floor().long()
        upper_idx = b.ceil().long()
        lower_idx[(upper_idx > 0) & (lower_idx == upper_idx)] -= 1
        upper_idx[(lower_idx < (self.num_atoms - 1)) & (lower_idx == upper_idx)] += 1
        lower_idx = lower_idx.clamp(0, self.num_atoms - 1)
        upper_idx = upper_idx.clamp(0, self.num_atoms - 1)
        weight_u = b - lower_idx.float()
        weight_l = 1.0 - weight_u

        # Project onto target distribution tensor using a loop (safer than complex indexing)
        target_dist = torch.zeros(batch_size, self.num_atoms, device=self.device)
        for i in range(batch_size):
            # Use index_add_ *per batch item* to avoid race conditions if indices repeat within an item
            target_dist[i].index_add_(
                0, lower_idx[i], target_next_best_dist_probs[i] * weight_l[i]
            )
            target_dist[i].index_add_(
                0, upper_idx[i], target_next_best_dist_probs[i] * weight_u[i]
            )
        # --- END MODIFIED ---

        return target_dist

    def compute_loss(
        self,
        batch: Union[NumpyBatch, NumpyNStepBatch],
        is_n_step: bool,
        is_weights: Optional[np.ndarray] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Computes the loss (Cross-Entropy for C51, SmoothL1 otherwise) and TD errors."""
        tensor_batch = self._np_batch_to_tensor(batch, is_n_step)

        if self.use_distributional:
            states, actions, _, _, _, _ = tensor_batch
            batch_size = states.size(0)
            target_distribution = self._get_target_distribution(tensor_batch, is_n_step)
            self.online_net.train()
            current_dist_logits = self.online_net(states)
            action_idx = actions.view(batch_size, 1, 1).expand(-1, -1, self.num_atoms)
            current_dist_logits_for_actions = current_dist_logits.gather(
                1, action_idx
            ).squeeze(1)
            current_log_probs = F.log_softmax(current_dist_logits_for_actions, dim=1)
            elementwise_loss = -(target_distribution * current_log_probs).sum(dim=1)
            td_errors = elementwise_loss.detach()
            with torch.no_grad():
                self.online_net.eval()
                q_vals = (F.softmax(self.online_net(states), dim=2) * self.support).sum(
                    dim=2
                )
                self._last_avg_max_q = q_vals.max(dim=1)[0].mean().item()
        else:
            states, actions, rewards, next_states, dones = tensor_batch[:5]
            discounts = (
                tensor_batch[5]  # Use pre-calculated gamma^n from NStepBatch
                if is_n_step
                else torch.full_like(rewards, self.gamma)  # Use standard gamma
            )
            with torch.no_grad():
                self.online_net.eval()
                best_next_actions = self.online_net(next_states).argmax(
                    dim=1, keepdim=True
                )
                self.target_net.eval()
                target_next_q = self.target_net(next_states).gather(
                    1, best_next_actions
                )
                target_q = rewards + discounts * target_next_q * (1.0 - dones)
            self.online_net.train()
            current_q = self.online_net(states).gather(1, actions)
            elementwise_loss = self.loss_fn(current_q, target_q)
            td_errors = (target_q - current_q).abs().detach()
            with torch.no_grad():
                self.online_net.eval()
                self._last_avg_max_q = (
                    self.online_net(states).max(dim=1)[0].mean().item()
                )

        is_weights_t = (
            torch.tensor(is_weights, dtype=torch.float32, device=self.device)
            if is_weights is not None
            else None
        )
        loss = (
            (is_weights_t * elementwise_loss.squeeze()).mean()
            if is_weights_t is not None
            else elementwise_loss.mean()
        )
        return loss, td_errors.squeeze()

    def update(self, loss: torch.Tensor) -> Optional[float]:
        """Performs optimizer step, gradient clipping, scheduler step, and noise reset."""
        grad_norm = None
        self.optimizer.zero_grad(set_to_none=True)
        loss.backward()
        self.online_net.train()
        if self.gradient_clip_norm > 0:
            try:
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    self.online_net.parameters(), max_norm=self.gradient_clip_norm
                ).item()
            except Exception as clip_err:
                print(f"Warning: Error during gradient clipping: {clip_err}")
                grad_norm = None
        self.optimizer.step()
        if self.scheduler:
            self.scheduler.step()
        if self.use_noisy_nets:
            self.online_net.reset_noise()
            self.target_net.reset_noise()
        return grad_norm

    def get_last_avg_max_q(self) -> float:
        """Returns the average max Q value computed during the last loss calculation."""
        return self._last_avg_max_q

    def update_target_network(self):
        """Copies weights from the online network to the target network."""
        self.target_net.load_state_dict(self.online_net.state_dict())
        self.target_net.eval()

    def get_state_dict(self) -> AgentStateDict:
        """Returns the agent's state including networks, optimizer, and scheduler."""
        self.online_net.cpu()
        self.target_net.cpu()
        optim_state_cpu = {}
        for group in self.optimizer.param_groups:
            for p in group["params"]:
                if p in self.optimizer.state:
                    state = self.optimizer.state[p]
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.cpu()
        state = {
            "online_net_state_dict": self.online_net.state_dict(),
            "target_net_state_dict": self.target_net.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": (
                self.scheduler.state_dict() if self.scheduler else None
            ),
        }
        self.online_net.to(self.device)
        self.target_net.to(self.device)
        for group in self.optimizer.param_groups:
            for p in group["params"]:
                if p in self.optimizer.state:
                    state = self.optimizer.state[p]
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(self.device)
        return state

    def load_state_dict(self, state_dict: AgentStateDict):
        """Loads the agent's state from a state dictionary."""
        print(f"[DQNAgent] Loading state dict. Target device: {self.device}")
        try:
            self.online_net.load_state_dict(
                state_dict["online_net_state_dict"], strict=False
            )
            print("[DQNAgent] online_net state_dict loaded (strict=False).")
        except Exception as e:
            print(f"ERROR loading online_net state_dict: {e}")
            traceback.print_exc()
            raise
        if "target_net_state_dict" in state_dict:
            try:
                self.target_net.load_state_dict(
                    state_dict["target_net_state_dict"], strict=False
                )
                print("[DQNAgent] target_net state_dict loaded (strict=False).")
            except Exception as e:
                print(f"ERROR loading target_net state_dict: {e}. Copying from online.")
                self.target_net.load_state_dict(self.online_net.state_dict())
        else:
            print(
                "Warning: Target net state missing in checkpoint, copying from online."
            )
            self.target_net.load_state_dict(self.online_net.state_dict())
        self.online_net.to(self.device)
        self.target_net.to(self.device)
        print(
            f"[DQNAgent] online_net moved to device: {next(self.online_net.parameters()).device}"
        )
        print(
            f"[DQNAgent] target_net moved to device: {next(self.target_net.parameters()).device}"
        )
        if "optimizer_state_dict" in state_dict:
            try:
                self.optimizer.load_state_dict(state_dict["optimizer_state_dict"])
                for state in self.optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor) and v.device != self.device:
                            state[k] = v.to(self.device)
                print("[DQNAgent] Optimizer state loaded and moved to device.")
            except Exception as e:
                print(
                    f"Warning: Could not load optimizer state ({e}). Resetting optimizer."
                )
                self.optimizer = optim.AdamW(
                    self.online_net.parameters(),
                    lr=self.dqn_config.LEARNING_RATE,
                    eps=self.dqn_config.ADAM_EPS,
                    weight_decay=1e-5,
                )
        else:
            print(
                "Warning: Optimizer state not found in checkpoint. Resetting optimizer."
            )
            self.optimizer = optim.AdamW(
                self.online_net.parameters(),
                lr=self.dqn_config.LEARNING_RATE,
                eps=self.dqn_config.ADAM_EPS,
                weight_decay=1e-5,
            )
        if (
            self.scheduler
            and "scheduler_state_dict" in state_dict
            and state_dict["scheduler_state_dict"] is not None
        ):
            try:
                self.scheduler.load_state_dict(state_dict["scheduler_state_dict"])
                print("[DQNAgent] LR Scheduler state loaded.")
            except Exception as e:
                print(
                    f"Warning: Could not load LR scheduler state ({e}). Scheduler may reset."
                )
                self.scheduler = CosineAnnealingLR(
                    self.optimizer,
                    T_max=self.dqn_config.LR_SCHEDULER_T_MAX,
                    eta_min=self.dqn_config.LR_SCHEDULER_ETA_MIN,
                )
        elif self.scheduler:
            print(
                "Warning: LR Scheduler state not found in checkpoint. Scheduler may reset."
            )
        self.online_net.train()
        self.target_net.eval()
        print("[DQNAgent] load_state_dict complete.")


File: agent/model_factory.py
# File: agent/model_factory.py
# (Largely unchanged, just cleaner print)
import torch.nn as nn
from config import ModelConfig, EnvConfig, DQNConfig
from typing import Type

from agent.networks.agent_network import AgentNetwork


def create_network(
    state_dim: int,
    action_dim: int,
    model_config: ModelConfig,
    dqn_config: DQNConfig,
) -> nn.Module:
    """Creates the AgentNetwork based on configuration."""

    print(
        f"[ModelFactory] Creating AgentNetwork (Dueling: {dqn_config.USE_DUELING}, NoisyNets Heads: {dqn_config.USE_NOISY_NETS})"
    )

    # Pass the specific sub-config ModelConfig.Network
    return AgentNetwork(
        state_dim=state_dim,
        action_dim=action_dim,
        config=model_config.Network,  # Pass the Network sub-config
        env_config=EnvConfig,  # AgentNetwork needs EnvConfig
        dueling=dqn_config.USE_DUELING,
        use_noisy=dqn_config.USE_NOISY_NETS,
        dqn_config=dqn_config,
    )


File: agent/replay_buffer/uniform_buffer.py
# File: agent/replay_buffer/uniform_buffer.py
# (No structural changes, cleanup comments)
import random
import numpy as np
from collections import deque
from typing import Deque, Tuple, Optional, Any, Dict, Union
from .base_buffer import ReplayBufferBase
from utils.types import Transition, StateType, ActionType, NumpyBatch, NumpyNStepBatch
from utils.helpers import save_object, load_object


class UniformReplayBuffer(ReplayBufferBase):
    """Standard uniform experience replay buffer."""

    def __init__(self, capacity: int):
        super().__init__(capacity)
        self.buffer: Deque[Transition] = deque(maxlen=capacity)

    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
        **kwargs,  # Accept potential n_step_discount from NStepWrapper
    ):
        n_step_discount = kwargs.get("n_step_discount")
        transition = Transition(
            state=state,
            action=action,
            reward=reward,
            next_state=next_state,
            done=done,
            n_step_discount=n_step_discount,
        )
        self.buffer.append(transition)

    def sample(self, batch_size: int) -> Optional[Union[NumpyBatch, NumpyNStepBatch]]:
        if len(self.buffer) < batch_size:
            return None

        batch_transitions = random.sample(self.buffer, batch_size)
        is_n_step = batch_transitions[0].n_step_discount is not None

        if is_n_step:
            s, a, rn, nsn, dn, gamma_n = zip(
                *[
                    (
                        t.state,
                        t.action,
                        t.reward,
                        t.next_state,
                        t.done,
                        t.n_step_discount,
                    )
                    for t in batch_transitions
                ]
            )
            states_np = np.array(s, dtype=np.float32)
            actions_np = np.array(a, dtype=np.int64)
            rewards_np = np.array(rn, dtype=np.float32)
            next_states_np = np.array(nsn, dtype=np.float32)
            dones_np = np.array(dn, dtype=np.float32)
            discounts_np = np.array(gamma_n, dtype=np.float32)
            return (
                states_np,
                actions_np,
                rewards_np,
                next_states_np,
                dones_np,
                discounts_np,
            )
        else:
            s, a, r, ns, d = zip(
                *[
                    (t.state, t.action, t.reward, t.next_state, t.done)
                    for t in batch_transitions
                ]
            )
            states_np = np.array(s, dtype=np.float32)
            actions_np = np.array(a, dtype=np.int64)
            rewards_np = np.array(r, dtype=np.float32)
            next_states_np = np.array(ns, dtype=np.float32)
            dones_np = np.array(d, dtype=np.float32)
            return states_np, actions_np, rewards_np, next_states_np, dones_np

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        pass  # No-op for uniform buffer

    def set_beta(self, beta: float):
        pass  # No-op for uniform buffer

    def flush_pending(self):
        pass  # No-op for uniform buffer

    def __len__(self) -> int:
        return len(self.buffer)

    def get_state(self) -> Dict[str, Any]:
        # Convert deque to list for robust serialization
        return {"buffer": list(self.buffer)}

    def load_state_from_data(self, state: Dict[str, Any]):
        saved_buffer_list = state.get("buffer", [])
        self.buffer = deque(saved_buffer_list, maxlen=self.capacity)
        print(f"[UniformReplayBuffer] Loaded {len(self.buffer)} transitions.")

    def save_state(self, filepath: str):
        state = self.get_state()
        save_object(state, filepath)

    def load_state(self, filepath: str):
        state = load_object(filepath)
        self.load_state_from_data(state)


File: agent/replay_buffer/buffer_utils.py
# File: agent/replay_buffer/buffer_utils.py
# (No structural changes, cleaner print statements)
from config import BufferConfig, DQNConfig
from .base_buffer import ReplayBufferBase
from .uniform_buffer import UniformReplayBuffer
from .prioritized_buffer import PrioritizedReplayBuffer
from .nstep_buffer import NStepBufferWrapper


def create_replay_buffer(
    config: BufferConfig, dqn_config: DQNConfig
) -> ReplayBufferBase:
    """Factory function to create the replay buffer based on configuration."""

    print("[BufferFactory] Creating replay buffer...")
    print(f"  Type: {'Prioritized' if config.USE_PER else 'Uniform'}")
    print(f"  Capacity: {config.REPLAY_BUFFER_SIZE / 1e6:.1f}M")
    if config.USE_PER:
        print(f"  PER alpha={config.PER_ALPHA}, eps={config.PER_EPSILON}")

    if config.USE_PER:
        core_buffer = PrioritizedReplayBuffer(
            capacity=config.REPLAY_BUFFER_SIZE,
            alpha=config.PER_ALPHA,
            epsilon=config.PER_EPSILON,
        )
    else:
        core_buffer = UniformReplayBuffer(capacity=config.REPLAY_BUFFER_SIZE)

    if config.USE_N_STEP and config.N_STEP > 1:
        print(
            f"  N-Step Wrapper: Enabled (N={config.N_STEP}, gamma={dqn_config.GAMMA})"
        )
        final_buffer = NStepBufferWrapper(
            wrapped_buffer=core_buffer,
            n_step=config.N_STEP,
            gamma=dqn_config.GAMMA,
        )
    else:
        print(f"  N-Step Wrapper: Disabled")
        final_buffer = core_buffer

    print(f"[BufferFactory] Final buffer type: {type(final_buffer).__name__}")
    return final_buffer


File: agent/replay_buffer/__init__.py


File: agent/replay_buffer/prioritized_buffer.py
# File: agent/replay_buffer/prioritized_buffer.py
# File: agent/replay_buffer/prioritized_buffer.py
import random
import numpy as np
from typing import Optional, Tuple, Any, Dict, Union, List
from .base_buffer import ReplayBufferBase
from .sum_tree import SumTree
from utils.types import (
    Transition,
    StateType,
    ActionType,
    NumpyBatch,
    PrioritizedNumpyBatch,
    NumpyNStepBatch,
    PrioritizedNumpyNStepBatch,
)
from utils.helpers import save_object, load_object


class PrioritizedReplayBuffer(ReplayBufferBase):
    """Prioritized Experience Replay (PER) buffer using a SumTree."""

    def __init__(self, capacity: int, alpha: float, epsilon: float):
        super().__init__(capacity)
        self.tree = SumTree(capacity)
        self.alpha = alpha
        self.epsilon = epsilon
        self.beta = 0.0  # Annealed externally
        self.max_priority = 1.0

    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
        **kwargs,
    ):
        """Adds new experience with maximum priority."""
        n_step_discount = kwargs.get("n_step_discount")
        transition = Transition(
            state, action, reward, next_state, done, n_step_discount
        )
        self.tree.add(self.max_priority, transition)

    def sample(
        self, batch_size: int
    ) -> Optional[Union[PrioritizedNumpyBatch, PrioritizedNumpyNStepBatch]]:
        """Samples batch using priorities, calculates IS weights."""
        if len(self) < batch_size:
            return None

        batch_data: List[Transition] = []
        indices = np.empty(batch_size, dtype=np.int64)
        priorities = np.empty(batch_size, dtype=np.float64)
        segment = self.tree.total() / batch_size

        for i in range(batch_size):
            s = random.uniform(segment * i, segment * (i + 1))
            s = max(1e-9, s)
            idx, p, data = self.tree.get(s)

            retries = 0
            max_retries = 5
            while (
                not isinstance(data, Transition) and retries < max_retries
            ):  # Retry if invalid data
                s = random.uniform(1e-9, self.tree.total())
                idx, p, data = self.tree.get(s)
                retries += 1
            if not isinstance(data, Transition):
                print(
                    f"ERROR: PER sample failed after {max_retries} retries. Skipping batch."
                )
                return None

            priorities[i] = p
            batch_data.append(data)
            indices[i] = idx

        sampling_probs = np.maximum(priorities / self.tree.total(), 1e-9)
        is_weights = np.power(len(self) * sampling_probs, -self.beta)
        is_weights = (is_weights / (is_weights.max() + 1e-9)).astype(np.float32)

        # Check if N-step based on first item
        is_n_step = batch_data[0].n_step_discount is not None
        batch_tuple = self._unpack_batch(batch_data, is_n_step)
        return batch_tuple, indices, is_weights

    def _unpack_batch(
        self, batch_data: List[Transition], is_n_step: bool
    ) -> Union[NumpyBatch, NumpyNStepBatch]:
        """Unpacks list of Transitions into numpy arrays."""
        if is_n_step:
            s, a, rn, nsn, dn, gamma_n = zip(
                *[
                    (
                        t.state,
                        t.action,
                        t.reward,
                        t.next_state,
                        t.done,
                        t.n_step_discount,
                    )
                    for t in batch_data
                ]
            )
            return (
                np.array(s, dtype=np.float32),
                np.array(a, dtype=np.int64),
                np.array(rn, dtype=np.float32),
                np.array(nsn, dtype=np.float32),
                np.array(dn, dtype=np.float32),
                np.array(gamma_n, dtype=np.float32),
            )
        else:
            s, a, r, ns, d = zip(
                *[
                    (t.state, t.action, t.reward, t.next_state, t.done)
                    for t in batch_data
                ]
            )
            return (
                np.array(s, dtype=np.float32),
                np.array(a, dtype=np.int64),
                np.array(r, dtype=np.float32),
                np.array(ns, dtype=np.float32),
                np.array(d, dtype=np.float32),
            )

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """Updates priorities of experiences at given tree indices."""
        if len(indices) != len(priorities):
            print(f"Error: Mismatch indices/priorities in PER update")
            return

        priorities = np.power(np.abs(priorities) + self.epsilon, self.alpha)
        for idx, priority in zip(indices, priorities):
            if not (self.tree.capacity - 1 <= idx < 2 * self.tree.capacity - 1):
                continue  # Skip invalid index
            self.tree.update(idx, priority)
            self.max_priority = max(self.max_priority, priority)

    def set_beta(self, beta: float):
        self.beta = beta

    def flush_pending(self):
        pass  # No-op

    def __len__(self) -> int:
        return self.tree.n_entries

    def get_state(self) -> Dict[str, Any]:
        return {
            "tree_nodes": self.tree.tree.copy(),
            "tree_data": self.tree.data.copy(),
            "tree_write_ptr": self.tree.write_ptr,
            "tree_n_entries": self.tree.n_entries,
            "max_priority": self.max_priority,
            "alpha": self.alpha,
            "epsilon": self.epsilon,
        }

    def load_state_from_data(self, state: Dict[str, Any]):
        if "tree_nodes" not in state or "tree_data" not in state:
            print("Error: Invalid PER state format. Skipping.")
            return

        loaded_capacity = len(state["tree_data"])
        if loaded_capacity != self.capacity:
            print(
                f"Warning: Loaded PER capacity ({loaded_capacity}) != current ({self.capacity}). Recreating tree."
            )
            self.tree = SumTree(self.capacity)
            num_to_load = min(loaded_capacity, self.capacity)
            self.tree.data[:num_to_load] = state["tree_data"][:num_to_load]
            self.tree.write_ptr = state.get("tree_write_ptr", 0) % self.capacity
            self.tree.n_entries = min(state.get("tree_n_entries", 0), self.capacity)
            self.tree.tree.fill(0)
            self.max_priority = 1.0  # Reset priorities
            print(f"[PER] Loaded {self.tree.n_entries} transitions (priorities reset).")
        else:
            self.tree.tree = state["tree_nodes"]
            self.tree.data = state["tree_data"]
            self.tree.write_ptr = state.get("tree_write_ptr", 0)
            self.tree.n_entries = state.get("tree_n_entries", 0)
            self.max_priority = state.get("max_priority", 1.0)
            print(f"[PER] Loaded {self.tree.n_entries} transitions.")
        self.alpha = state.get("alpha", self.alpha)
        self.epsilon = state.get("epsilon", self.epsilon)

    def save_state(self, filepath: str):
        save_object(self.get_state(), filepath)

    def load_state(self, filepath: str):
        self.load_state_from_data(load_object(filepath))


File: agent/replay_buffer/base_buffer.py
# File: agent/replay_buffer/base_buffer.py
# (No changes needed)
from abc import ABC, abstractmethod
from typing import Any, Optional, Tuple, Dict
import numpy as np
from utils.types import StateType, ActionType


class ReplayBufferBase(ABC):
    """Abstract base class for all replay buffers."""

    def __init__(self, capacity: int):
        self.capacity = capacity

    @abstractmethod
    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
        **kwargs  # Allow passing extra info like n_step_discount
    ):
        """Add a new experience to the buffer."""
        pass

    @abstractmethod
    def sample(
        self, batch_size: int
    ) -> Optional[Any]:  # Return type depends on PER/NStep
        """Sample a batch of experiences from the buffer."""
        pass

    @abstractmethod
    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """Update priorities for PER (no-op for uniform buffer)."""
        pass

    @abstractmethod
    def __len__(self) -> int:
        """Return the current size of the buffer."""
        pass

    @abstractmethod
    def set_beta(self, beta: float):
        """Set the beta value for PER IS weights (no-op for uniform buffer)."""
        pass

    @abstractmethod
    def flush_pending(self):
        """Process any pending transitions (e.g., for N-step)."""
        pass

    @abstractmethod
    def get_state(self) -> Dict[str, Any]:
        """Return the buffer's state as a dictionary suitable for saving."""
        pass

    @abstractmethod
    def load_state_from_data(self, state: Dict[str, Any]):
        """Load the buffer's state from a dictionary."""
        pass

    @abstractmethod
    def save_state(self, filepath: str):
        """Save the buffer's state to a file."""
        pass

    @abstractmethod
    def load_state(self, filepath: str):
        """Load the buffer's state from a file."""
        pass


File: agent/replay_buffer/nstep_buffer.py
# File: agent/replay_buffer/nstep_buffer.py
# File: agent/replay_buffer/nstep_buffer.py
from collections import deque
import numpy as np
from typing import Deque, Tuple, Optional, Any, Dict, List
from .base_buffer import ReplayBufferBase
from utils.types import Transition, StateType, ActionType
from utils.helpers import save_object, load_object


class NStepBufferWrapper(ReplayBufferBase):
    """Wraps another buffer to implement N-step returns."""

    def __init__(self, wrapped_buffer: ReplayBufferBase, n_step: int, gamma: float):
        super().__init__(wrapped_buffer.capacity)
        if n_step <= 0:
            raise ValueError("N-step must be positive")
        self.wrapped_buffer = wrapped_buffer
        self.n_step = n_step
        self.gamma = gamma
        # Deque stores (s, a, r, ns, d) tuples
        self.n_step_deque: Deque[
            Tuple[StateType, ActionType, float, StateType, bool]
        ] = deque(maxlen=n_step)

    def _calculate_n_step_transition(
        self, current_deque_list: List[Tuple]
    ) -> Optional[Transition]:
        """Calculates the N-step return from a list copy of the deque."""
        if not current_deque_list:
            return None

        n_step_reward = 0.0
        discount_accum = 1.0
        effective_n = len(current_deque_list)
        state_0, action_0 = current_deque_list[0][0], current_deque_list[0][1]

        for i in range(effective_n):
            s, a, r, ns, d = current_deque_list[i]
            n_step_reward += discount_accum * r
            if d:  # Episode terminated within N steps
                return Transition(
                    state_0, action_0, n_step_reward, ns, True, self.gamma ** (i + 1)
                )
            discount_accum *= self.gamma

        # Loop completed without terminal state
        n_step_next_state = current_deque_list[-1][3]  # next_state from Nth transition
        n_step_done = current_deque_list[-1][4]  # done flag from Nth transition
        return Transition(
            state_0,
            action_0,
            n_step_reward,
            n_step_next_state,
            n_step_done,
            self.gamma**effective_n,
        )

    def push(
        self,
        state: StateType,
        action: ActionType,
        reward: float,
        next_state: StateType,
        done: bool,
    ):
        """Adds raw transition, processes N-step if possible, pushes to wrapped buffer."""
        self.n_step_deque.append((state, action, reward, next_state, done))

        if len(self.n_step_deque) < self.n_step:
            if done:
                self._flush_on_done()  # Process partial if episode ends early
            return

        # Deque has N items, calculate N-step transition from the oldest start
        n_step_transition = self._calculate_n_step_transition(list(self.n_step_deque))
        if n_step_transition:
            self.wrapped_buffer.push(
                **n_step_transition._asdict()
            )  # Push unpacked dict

        if done:
            self._flush_on_done()  # Flush remaining starts if new transition was terminal

    def _flush_on_done(self):
        """Processes remaining partial transitions when an episode ends."""
        temp_deque = list(self.n_step_deque)
        while len(temp_deque) > 1:  # Stop when only the 'done' transition remains
            temp_deque.pop(0)  # Remove the already processed starting state
            n_step_transition = self._calculate_n_step_transition(temp_deque)
            if n_step_transition:
                self.wrapped_buffer.push(**n_step_transition._asdict())
        self.n_step_deque.clear()

    def sample(self, batch_size: int) -> Any:
        return self.wrapped_buffer.sample(batch_size)

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        self.wrapped_buffer.update_priorities(indices, priorities)

    def set_beta(self, beta: float):
        if hasattr(self.wrapped_buffer, "set_beta"):
            self.wrapped_buffer.set_beta(beta)

    def __len__(self) -> int:
        return len(self.wrapped_buffer)

    def flush_pending(self):
        """Processes and pushes any remaining transitions before exit/save."""
        print(f"[NStepWrapper] Flushing {len(self.n_step_deque)} pending transitions.")
        temp_deque = list(self.n_step_deque)
        while len(temp_deque) > 0:
            n_step_transition = self._calculate_n_step_transition(temp_deque)
            if n_step_transition:
                self.wrapped_buffer.push(**n_step_transition._asdict())
            temp_deque.pop(0)
        self.n_step_deque.clear()
        if hasattr(self.wrapped_buffer, "flush_pending"):
            self.wrapped_buffer.flush_pending()

    def get_state(self) -> Dict[str, Any]:
        return {
            "n_step_deque": list(self.n_step_deque),
            "wrapped_buffer_state": self.wrapped_buffer.get_state(),
        }

    def load_state_from_data(self, state: Dict[str, Any]):
        self.n_step_deque = deque(state.get("n_step_deque", []), maxlen=self.n_step)
        print(f"[NStepWrapper] Loaded {len(self.n_step_deque)} pending transitions.")
        wrapped_state = state.get("wrapped_buffer_state")
        if wrapped_state:
            self.wrapped_buffer.load_state_from_data(wrapped_state)
        else:
            print("[NStepWrapper] Warning: No wrapped buffer state found.")

    def save_state(self, filepath: str):
        save_object(self.get_state(), filepath)

    def load_state(self, filepath: str):
        try:
            self.load_state_from_data(load_object(filepath))
        except Exception as e:
            print(f"[NStepWrapper] Load failed: {e}. Starting empty.")


File: agent/replay_buffer/sum_tree.py
# File: agent/replay_buffer/sum_tree.py
# File: agent/replay_buffer/sum_tree.py
import numpy as np


class SumTree:
    """Simple SumTree implementation using numpy arrays for PER."""

    def __init__(self, capacity: int):
        if capacity <= 0 or not isinstance(capacity, int):
            raise ValueError("SumTree capacity must be positive integer")
        self.capacity = capacity
        self.tree = np.zeros(2 * capacity - 1, dtype=np.float64)  # Use float64 for sums
        self.data = np.zeros(capacity, dtype=object)  # Holds Transition objects
        self.write_ptr = 0
        self.n_entries = 0

    def _propagate(self, idx: int, change: float):
        """Propagate priority change up the tree."""
        parent = (idx - 1) // 2
        self.tree[parent] += change
        if parent != 0:
            self._propagate(parent, change)

    def _retrieve(self, idx: int, s: float) -> int:
        """Find leaf index for a given cumulative priority value s."""
        left = 2 * idx + 1
        right = left + 1
        if left >= len(self.tree):
            return idx  # Leaf node

        # Use tolerance for float comparison
        if s <= self.tree[left] + 1e-8:
            return self._retrieve(left, s)
        else:
            return self._retrieve(
                right, max(0.0, s - self.tree[left])
            )  # Ensure non-negative s

    def total(self) -> float:
        return self.tree[0]

    def add(self, priority: float, data: object):
        """Add new experience, overwriting oldest if full."""
        priority = max(abs(priority), 1e-6)  # Ensure positive priority
        tree_idx = self.write_ptr + self.capacity - 1
        self.data[self.write_ptr] = data
        self.update(tree_idx, priority)
        self.write_ptr = (self.write_ptr + 1) % self.capacity
        if self.n_entries < self.capacity:
            self.n_entries += 1

    def update(self, tree_idx: int, priority: float):
        """Update priority of an experience at a given tree index."""
        priority = max(abs(priority), 1e-6)
        if not (self.capacity - 1 <= tree_idx < 2 * self.capacity - 1):
            return  # Skip invalid index

        change = priority - self.tree[tree_idx]
        self.tree[tree_idx] = priority
        if abs(change) > 1e-9 and tree_idx > 0:
            self._propagate(tree_idx, change)

    def get(self, s: float) -> tuple[int, float, object]:
        """Sample an experience based on cumulative priority s. Returns: (tree_idx, priority, data)"""
        if self.total() <= 0 or self.n_entries == 0:
            return 0, 0.0, None  # Handle empty tree

        s = np.clip(s, 1e-9, self.total())  # Clip s to valid range
        idx = self._retrieve(0, s)  # Leaf node index in tree array
        data_idx = idx - self.capacity + 1  # Corresponding index in data array

        # Validate data_idx before access (important if buffer not full)
        if not (0 <= data_idx < self.n_entries):
            # Fallback: return last valid entry if index is out of bounds (rare)
            if self.n_entries > 0:
                last_valid_data_idx = (
                    self.write_ptr - 1 + self.capacity
                ) % self.capacity
                last_valid_tree_idx = last_valid_data_idx + self.capacity - 1
                priority = (
                    self.tree[last_valid_tree_idx]
                    if (
                        self.capacity - 1 <= last_valid_tree_idx < 2 * self.capacity - 1
                    )
                    else 0.0
                )
                return (last_valid_tree_idx, priority, self.data[last_valid_data_idx])
            else:
                return 0, 0.0, None  # Truly empty

        return (idx, self.tree[idx], self.data[data_idx])

    def __len__(self) -> int:
        return self.n_entries


File: agent/networks/noisy_layer.py
# File: agent/networks/noisy_layer.py
# (No changes needed, already clean)
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional


class NoisyLinear(nn.Module):
    """
    Noisy Linear Layer for Noisy Network (Factorised Gaussian Noise).
    """

    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        # Learnable weights and biases (mean parameters)
        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_mu = nn.Parameter(torch.empty(out_features))

        # Learnable noise parameters (standard deviation parameters)
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))

        # Non-learnable noise buffers
        self.register_buffer("weight_epsilon", torch.empty(out_features, in_features))
        self.register_buffer("bias_epsilon", torch.empty(out_features))

        self.reset_parameters()
        self.reset_noise()  # Initial noise generation

    def reset_parameters(self):
        """Initialize mean and std parameters."""
        mu_range = 1.0 / math.sqrt(self.in_features)
        nn.init.uniform_(self.weight_mu, -mu_range, mu_range)
        nn.init.uniform_(self.bias_mu, -mu_range, mu_range)

        # Initialize sigma parameters (std dev)
        nn.init.constant_(
            self.weight_sigma, self.std_init / math.sqrt(self.in_features)
        )
        nn.init.constant_(self.bias_sigma, self.std_init / math.sqrt(self.out_features))

    def reset_noise(self):
        """Generate new noise samples using Factorised Gaussian noise."""
        epsilon_in = self._scale_noise(self.in_features)
        epsilon_out = self._scale_noise(self.out_features)

        # Outer product for weight noise, direct sample for bias noise
        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))
        self.bias_epsilon.copy_(epsilon_out)

    def _scale_noise(self, size: int) -> torch.Tensor:
        """Generate noise tensor with sign-sqrt transformation."""
        x = torch.randn(size, device=self.weight_mu.device)  # Noise on same device
        return x.sign().mul(x.abs().sqrt())

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass with noisy parameters if training, mean parameters otherwise."""
        if self.training:
            # Sample noise is implicitly used via weight_epsilon, bias_epsilon
            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon
            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon
            # Reset noise *after* use in forward pass for next iteration?
            # Or reset in train() method? Resetting in train() is common.
        else:
            # Use mean parameters during evaluation
            weight = self.weight_mu
            bias = self.bias_mu

        return F.linear(x, weight, bias)

    def train(self, mode: bool = True):
        """Override train mode to reset noise when entering training."""
        if self.training is False and mode is True:  # If switching from eval to train
            self.reset_noise()
        super().train(mode)


File: agent/networks/__init__.py


File: agent/networks/agent_network.py
# File: agent/networks/agent_network.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math

# --- Import specific configs needed ---
from config import ModelConfig, EnvConfig, DQNConfig, DEVICE

# ---
from typing import Tuple, List, Type

from .noisy_layer import NoisyLinear


class AgentNetwork(nn.Module):
    """
    Agent Network: CNN+MLP -> Fusion -> Dueling Heads -> Optional Distributional Output.
    """

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        config: ModelConfig.Network,
        env_config: EnvConfig,
        dqn_config: DQNConfig,  # Pass full DQNConfig
        dueling: bool,
        use_noisy: bool,
    ):
        super().__init__()
        self.dueling = dueling
        self.action_dim = action_dim
        self.env_config = env_config
        self.config = config
        self.use_noisy = use_noisy
        # --- Store Distributional Config ---
        self.use_distributional = dqn_config.USE_DISTRIBUTIONAL
        self.num_atoms = dqn_config.NUM_ATOMS
        # ---
        self.device = DEVICE
        print(f"[AgentNetwork] Target device set to: {self.device}")
        print(
            f"[AgentNetwork] Distributional C51: {self.use_distributional} ({self.num_atoms} atoms)"
        )

        self._calculate_dims()
        if state_dim != self.expected_total_dim:
            raise ValueError(
                f"AgentNetwork init: State dim mismatch! Input:{state_dim} != Expected:{self.expected_total_dim}"
            )

        print(f"[AgentNetwork] Initializing (Noisy Heads: {self.use_noisy}):")
        print(
            f"  Input Dim: {state_dim} (Grid: {self.expected_grid_flat_dim}, Shape: {self.expected_shape_flat_dim})"
        )

        # Build network branches
        self.conv_base, conv_out_h, conv_out_w, conv_out_c = self._build_cnn_branch()
        self.conv_out_size = self._get_conv_out_size(
            (self.grid_feat_per_cell, self.grid_h, self.grid_w)
        )
        print(
            f"  CNN Output Dim (HxWxC): ({conv_out_h}x{conv_out_w}x{conv_out_c}) -> Flat: {self.conv_out_size}"
        )

        self.shape_mlp, self.shape_mlp_out_dim = self._build_shape_mlp_branch()
        print(f"  Shape MLP Output Dim: {self.shape_mlp_out_dim}")

        combined_features_dim = self.conv_out_size + self.shape_mlp_out_dim
        print(f"  Combined Features Dim: {combined_features_dim}")

        self.fusion_mlp, self.head_input_dim = self._build_fusion_mlp_branch(
            combined_features_dim
        )
        print(f"  Fusion MLP Output Dim (Input to Heads): {self.head_input_dim}")

        # --- Modified Head Building ---
        self._build_output_heads()
        head_type = NoisyLinear if self.use_noisy else nn.Linear
        output_type = "Distributional" if self.use_distributional else "Q-Value"
        print(
            f"  Using {'Dueling' if self.dueling else 'Single'} Heads ({head_type.__name__}), Output: {output_type}"
        )
        # ---
        print(
            f"[AgentNetwork] Final check - conv_base device: {next(self.conv_base.parameters()).device}"
        )

    def _calculate_dims(self):
        self.grid_h = self.env_config.ROWS
        self.grid_w = self.env_config.COLS
        self.grid_feat_per_cell = self.env_config.GRID_FEATURES_PER_CELL
        self.expected_grid_flat_dim = (
            self.grid_h * self.grid_w * self.grid_feat_per_cell
        )
        self.num_shape_slots = self.env_config.NUM_SHAPE_SLOTS
        self.shape_feat_per_shape = self.env_config.SHAPE_FEATURES_PER_SHAPE
        self.expected_shape_flat_dim = self.num_shape_slots * self.shape_feat_per_shape
        self.expected_total_dim = (
            self.expected_grid_flat_dim + self.expected_shape_flat_dim
        )

    def _build_cnn_branch(self) -> Tuple[nn.Sequential, int, int, int]:
        conv_layers: List[nn.Module] = []
        current_channels = self.grid_feat_per_cell
        h, w = self.grid_h, self.grid_w
        cfg = self.config
        print(f"  Building CNN on device: {self.device}")
        for i, out_channels in enumerate(cfg.CONV_CHANNELS):
            conv_layer = nn.Conv2d(
                current_channels,
                out_channels,
                kernel_size=cfg.CONV_KERNEL_SIZE,
                stride=cfg.CONV_STRIDE,
                padding=cfg.CONV_PADDING,
                bias=not cfg.USE_BATCHNORM_CONV,
            ).to(self.device)
            conv_layers.append(conv_layer)
            if cfg.USE_BATCHNORM_CONV:
                conv_layers.append(nn.BatchNorm2d(out_channels).to(self.device))
            conv_layers.append(cfg.CONV_ACTIVATION())
            pool_layer = nn.MaxPool2d(
                kernel_size=cfg.POOL_KERNEL_SIZE, stride=cfg.POOL_STRIDE
            ).to(self.device)
            conv_layers.append(pool_layer)
            current_channels = out_channels
            h = (h + 2 * cfg.CONV_PADDING - cfg.CONV_KERNEL_SIZE) // cfg.CONV_STRIDE + 1
            w = (w + 2 * cfg.CONV_PADDING - cfg.CONV_KERNEL_SIZE) // cfg.CONV_STRIDE + 1
            h = (h - cfg.POOL_KERNEL_SIZE) // cfg.POOL_STRIDE + 1
            w = (w - cfg.POOL_KERNEL_SIZE) // cfg.POOL_STRIDE + 1
        cnn_module = nn.Sequential(*conv_layers)
        if len(cnn_module) > 0 and hasattr(cnn_module[0], "weight"):
            print(f"    CNN Layer 0 device after build: {cnn_module[0].weight.device}")
        return cnn_module, h, w, current_channels

    def _get_conv_out_size(self, shape: Tuple[int, int, int]) -> int:
        with torch.no_grad():
            dummy_input = torch.zeros(1, *shape, device=self.device)
            self.conv_base.eval()
            output = self.conv_base(dummy_input)
            # self.conv_base.train() # Not needed, forward handles mode
            return int(np.prod(output.size()[1:]))

    def _build_shape_mlp_branch(self) -> Tuple[nn.Sequential, int]:
        shape_mlp_layers: List[nn.Module] = []
        lin1 = nn.Linear(
            self.expected_shape_flat_dim, self.config.SHAPE_MLP_HIDDEN_DIM
        ).to(self.device)
        shape_mlp_layers.append(lin1)
        shape_mlp_layers.append(self.config.SHAPE_MLP_ACTIVATION())
        return nn.Sequential(*shape_mlp_layers), self.config.SHAPE_MLP_HIDDEN_DIM

    def _build_fusion_mlp_branch(self, input_dim: int) -> Tuple[nn.Sequential, int]:
        fusion_layers: List[nn.Module] = []
        current_fusion_dim = input_dim
        cfg = self.config
        fusion_linear_layer_class = nn.Linear
        for i, hidden_dim in enumerate(cfg.COMBINED_FC_DIMS):
            linear_layer = fusion_linear_layer_class(
                current_fusion_dim, hidden_dim, bias=not cfg.USE_BATCHNORM_FC
            ).to(self.device)
            fusion_layers.append(linear_layer)
            if cfg.USE_BATCHNORM_FC:
                fusion_layers.append(nn.BatchNorm1d(hidden_dim).to(self.device))
            fusion_layers.append(cfg.COMBINED_ACTIVATION())
            if cfg.DROPOUT_FC > 0:
                fusion_layers.append(nn.Dropout(cfg.DROPOUT_FC).to(self.device))
            current_fusion_dim = hidden_dim
        return nn.Sequential(*fusion_layers), current_fusion_dim

    def _build_output_heads(self):
        """Builds the final heads, adjusting output size for distributional RL."""
        head_linear_layer_class = NoisyLinear if self.use_noisy else nn.Linear
        # --- Determine output units based on distributional flag ---
        output_units_per_stream = (
            self.action_dim * self.num_atoms
            if self.use_distributional
            else self.action_dim
        )
        value_units = self.num_atoms if self.use_distributional else 1
        # ---

        if self.dueling:
            self.value_head = head_linear_layer_class(
                self.head_input_dim, value_units
            ).to(self.device)
            self.advantage_head = head_linear_layer_class(
                self.head_input_dim, output_units_per_stream
            ).to(self.device)
        else:
            self.output_head = head_linear_layer_class(
                self.head_input_dim, output_units_per_stream
            ).to(self.device)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass, returns Q-values or distributions (logits)."""
        model_device = next(self.parameters()).device
        if x.device != model_device:
            x = x.to(model_device)
        if x.dim() != 2 or x.size(1) != self.expected_total_dim:
            raise ValueError(
                f"AgentNetwork forward: Invalid input shape {x.shape}. Expected [B, {self.expected_total_dim}]."
            )
        batch_size = x.size(0)

        # Split and process features
        grid_features_flat = x[:, : self.expected_grid_flat_dim]
        shape_features_flat = x[:, self.expected_grid_flat_dim :]
        grid_features_reshaped = grid_features_flat.view(
            batch_size, self.grid_feat_per_cell, self.grid_h, self.grid_w
        )
        conv_output = self.conv_base(grid_features_reshaped)
        conv_output_flat = conv_output.view(batch_size, -1)
        shape_output = self.shape_mlp(shape_features_flat)
        combined_features = torch.cat((conv_output_flat, shape_output), dim=1)
        fused_output = self.fusion_mlp(combined_features)

        # Output Heads
        if self.dueling:
            value = self.value_head(fused_output)  # Shape: [B, 1] or [B, N_ATOMS]
            advantage = self.advantage_head(
                fused_output
            )  # Shape: [B, A] or [B, A*N_ATOMS]

            if self.use_distributional:
                # Reshape for distributional: value[B, N], advantage[B, A, N]
                value = value.view(batch_size, 1, self.num_atoms)  # Add action dim
                advantage = advantage.view(batch_size, self.action_dim, self.num_atoms)
                # Combine using mean subtraction only on advantage stream
                adv_mean = advantage.mean(
                    dim=1, keepdim=True
                )  # Mean over actions [B, 1, N]
                dist_logits = value + (advantage - adv_mean)  # Shape: [B, A, N]
                # Apply Softmax over atoms dimension later (in agent or loss)
            else:
                # Standard Dueling Q-value combination
                adv_mean = advantage.mean(
                    dim=1, keepdim=True
                )  # Mean over actions [B, 1]
                dist_logits = value + (advantage - adv_mean)  # Shape: [B, A] (Q-values)

        else:  # Non-Dueling
            dist_logits = self.output_head(
                fused_output
            )  # Shape: [B, A] or [B, A*N_ATOMS]
            if self.use_distributional:
                # Reshape for distributional: [B, A, N]
                dist_logits = dist_logits.view(
                    batch_size, self.action_dim, self.num_atoms
                )

        # --- Return logits; softmax is applied in agent/loss function ---
        return dist_logits

    def reset_noise(self):
        """Resets noise in all NoisyLinear layers within the network."""
        if self.use_noisy:
            for module in self.modules():
                if isinstance(module, NoisyLinear):
                    module.reset_noise()


File: environment/game_state.py
# File: environment/game_state.py
import time
import numpy as np
from typing import List, Optional, Tuple
from collections import deque  # Import deque
from typing import Deque  # Import Deque

from .grid import Grid
from .shape import Shape
from config import EnvConfig, RewardConfig


class GameState:
    def __init__(self):
        self.grid = Grid()
        self.shapes: List[Optional[Shape]] = [
            Shape() for _ in range(EnvConfig.NUM_SHAPE_SLOTS)
        ]
        self.score = 0.0  # Cumulative RL reward
        self.game_score = 0  # Game-specific score

        # --- MODIFIED: Initialize lines cleared tracker ---
        self.lines_cleared_this_episode = 0
        # --- END MODIFIED ---

        self.blink_time = 0.0
        self.last_time = time.time()
        self.freeze_time = 0.0
        self.game_over = False
        self._last_action_valid = True
        self.rewards = RewardConfig

    def reset(self) -> np.ndarray:
        self.grid = Grid()
        self.shapes = [Shape() for _ in range(EnvConfig.NUM_SHAPE_SLOTS)]
        self.score = 0.0
        self.game_score = 0

        # --- MODIFIED: Reset lines cleared tracker ---
        self.lines_cleared_this_episode = 0
        # --- END MODIFIED ---

        self.blink_time = 0.0
        self.freeze_time = 0.0
        self.game_over = False
        self._last_action_valid = True
        self.last_time = time.time()
        return self.get_state()

    # valid_actions, is_over, is_frozen, decode_act remain the same
    def valid_actions(self) -> List[int]:
        if self.game_over or self.freeze_time > 0:
            return []
        acts = []
        locations_per_shape = self.grid.rows * self.grid.cols
        for i, sh in enumerate(self.shapes):
            if not sh:
                continue
            for r in range(self.grid.rows):
                for c in range(self.grid.cols):
                    if self.grid.can_place(sh, r, c):
                        action_index = i * locations_per_shape + (
                            r * self.grid.cols + c
                        )
                        acts.append(action_index)
        return acts

    def is_over(self) -> bool:
        return self.game_over

    def is_frozen(self) -> bool:
        return self.freeze_time > 0

    def decode_act(self, a: int) -> Tuple[int, int, int]:
        locations_per_shape = self.grid.rows * self.grid.cols
        s_idx = a // locations_per_shape
        pos_idx = a % locations_per_shape
        rr = pos_idx // self.grid.cols
        cc = pos_idx % self.grid.cols
        return (s_idx, rr, cc)

    def _update_timers(self):
        now = time.time()
        dt = now - self.last_time
        self.last_time = now
        self.freeze_time = max(0, self.freeze_time - dt)
        self.blink_time = max(0, self.blink_time - dt)

    def _handle_invalid_placement(self) -> float:
        self._last_action_valid = False
        reward = self.rewards.PENALTY_INVALID_MOVE
        # Check if game is over due to invalid move AND no valid moves left
        if not self.valid_actions():
            self.game_over = True
            self.freeze_time = 1.0  # Freeze on game over
            reward += self.rewards.PENALTY_GAME_OVER
        return reward

    def _handle_valid_placement(
        self, shp: Shape, s_idx: int, rr: int, cc: int
    ) -> float:
        self._last_action_valid = True
        reward = 0.0

        # Reward for placing shape
        reward += self.rewards.REWARD_PLACE_PER_TRI * len(shp.triangles)
        self.game_score += len(shp.triangles)  # Simple game score increment

        # Place the shape
        self.grid.place(shp, rr, cc)
        self.shapes[s_idx] = None  # Remove shape from available slots

        # Clear lines and get rewards/score
        lines_cleared, triangles_cleared = self.grid.clear_filled_rows()

        # --- MODIFIED: Accumulate lines cleared ---
        self.lines_cleared_this_episode += lines_cleared
        # --- END MODIFIED ---

        # Apply line clear rewards
        if lines_cleared == 1:
            reward += self.rewards.REWARD_CLEAR_1
        elif lines_cleared == 2:
            reward += self.rewards.REWARD_CLEAR_2
        elif lines_cleared >= 3:
            reward += self.rewards.REWARD_CLEAR_3PLUS

        # Bonus game score for cleared triangles
        if triangles_cleared > 0:
            self.game_score += triangles_cleared * 2  # Example bonus score
            self.blink_time = 0.5  # Visual feedback
            self.freeze_time = 0.5  # Short pause after clearing

        # Penalty for creating holes
        num_holes = self.grid.count_holes()
        reward += num_holes * self.rewards.PENALTY_HOLE_PER_HOLE

        # Refill shapes if all slots are empty
        if all(x is None for x in self.shapes):
            self.shapes = [Shape() for _ in range(EnvConfig.NUM_SHAPE_SLOTS)]

        # Check if game is over after placement (no more valid moves)
        if not self.valid_actions():
            self.game_over = True
            self.freeze_time = 1.0  # Freeze on game over
            reward += self.rewards.PENALTY_GAME_OVER

        return reward

    def step(self, a: int) -> Tuple[float, bool]:
        """Performs one game step based on the chosen action."""
        self._update_timers()

        # If already game over or frozen, return immediately
        if self.game_over:
            return (0.0, True)
        if self.freeze_time > 0:
            return (0.0, False)  # Return 0 reward, not done

        # Decode action
        s_idx, rr, cc = self.decode_act(a)

        # Check if the chosen shape exists and placement is valid
        shp = self.shapes[s_idx] if 0 <= s_idx < len(self.shapes) else None
        is_valid_placement = shp is not None and self.grid.can_place(shp, rr, cc)

        # Calculate reward based on placement validity
        if is_valid_placement:
            current_rl_reward = self._handle_valid_placement(shp, s_idx, rr, cc)
        else:
            current_rl_reward = self._handle_invalid_placement()

        # Add small reward for surviving the step if not game over
        if not self.game_over:
            current_rl_reward += self.rewards.REWARD_ALIVE_STEP

        # Update total RL score
        self.score += current_rl_reward

        # Return the RL reward for this step and the done flag
        return (current_rl_reward, self.game_over)

    # get_state remains the same
    def get_state(self) -> np.ndarray:
        """Generates the state representation as a flat numpy array."""
        # 1. Grid Features
        # [Channel, Height, Width] -> Flattened
        board_state = self.grid.get_feature_matrix().flatten()

        # 2. Shape Features
        shape_features_per = EnvConfig.SHAPE_FEATURES_PER_SHAPE
        num_shapes_expected = EnvConfig.NUM_SHAPE_SLOTS
        shape_features_total = num_shapes_expected * shape_features_per
        shape_rep = np.zeros(shape_features_total, dtype=np.float32)

        idx = 0
        # Normalization constants (adjust if needed)
        max_tris_norm = 6.0  # Max expected triangles in a shape + 1
        max_h_norm = float(self.grid.rows)
        max_w_norm = float(self.grid.cols)

        for i in range(num_shapes_expected):
            s = self.shapes[i] if i < len(self.shapes) else None
            if s:
                tri = s.triangles
                n = len(tri)
                ups = sum(1 for (_, _, u) in tri if u)
                dns = n - ups
                mnr, mnc, mxr, mxc = s.bbox()
                height = mxr - mnr + 1
                width = mxc - mnc + 1

                # Normalize features (clip to [0, 1])
                shape_rep[idx] = np.clip(float(n) / max_tris_norm, 0.0, 1.0)
                shape_rep[idx + 1] = np.clip(float(ups) / max_tris_norm, 0.0, 1.0)
                shape_rep[idx + 2] = np.clip(float(dns) / max_tris_norm, 0.0, 1.0)
                shape_rep[idx + 3] = np.clip(float(height) / max_h_norm, 0.0, 1.0)
                shape_rep[idx + 4] = np.clip(float(width) / max_w_norm, 0.0, 1.0)
            # else: Features remain 0 if shape slot is empty

            idx += shape_features_per

        # 3. Concatenate grid and shape features
        state_array = np.concatenate((board_state, shape_rep))

        # Validation check
        if len(state_array) != EnvConfig.STATE_DIM:
            raise ValueError(
                f"State length mismatch in get_state: Got {len(state_array)}, Expected {EnvConfig.STATE_DIM}"
            )
        return state_array.astype(np.float32)  # Ensure float32

    def is_blinking(self) -> bool:
        """Returns True if the game should be blinking (e.g., after line clear)."""
        return self.blink_time > 0

    def get_shapes(self) -> List[Shape]:
        """Returns a list of the currently available shapes (excluding None)."""
        return [s for s in self.shapes if s is not None]


File: environment/grid.py
# File: environment/grid.py
# (No structural changes, cleanup comments)
import numpy as np
from typing import List, Tuple
from .triangle import Triangle
from .shape import Shape
from config import EnvConfig


class Grid:
    """Represents the game board composed of Triangles."""

    def __init__(self):
        self.rows = EnvConfig.ROWS
        self.cols = EnvConfig.COLS
        # Padding defines 'death' cells on the sides
        # Extend padding with 0 if grid is taller than 8 rows
        self.pad = [4, 3, 2, 1, 1, 2, 3, 4] + [0] * max(0, self.rows - 8)
        self.triangles: List[List[Triangle]] = []
        self._create()

    def _create(self) -> None:
        """Initializes the grid with Triangle objects."""
        self.triangles = []
        for r in range(self.rows):
            rowt = []
            p = self.pad[
                r % len(self.pad)
            ]  # Use modulo for robustness if rows > len(pad)
            for c in range(self.cols):
                # Determine if cell is a 'death' cell based on padding
                is_death_cell = c < p or c >= self.cols - p
                # Determine orientation (up/down) based on position
                is_up_cell = (r + c) % 2 == 0
                tri = Triangle(r, c, is_up=is_up_cell, is_death=is_death_cell)
                rowt.append(tri)
            self.triangles.append(rowt)

    def valid(self, r: int, c: int) -> bool:
        """Check if (r, c) is within grid boundaries."""
        return 0 <= r < self.rows and 0 <= c < self.cols

    def can_place(self, shp: Shape, rr: int, cc: int) -> bool:
        """Checks if a shape can be placed at the target root position (rr, cc)."""
        for dr, dc, up in shp.triangles:
            nr, nc = rr + dr, cc + dc  # Absolute grid coordinates
            if not self.valid(nr, nc):
                return False  # Out of bounds
            tri = self.triangles[nr][nc]
            # Cannot place if cell is death, already occupied, or wrong orientation
            if tri.is_death or tri.is_occupied or (tri.is_up != up):
                return False
        return True

    def place(self, shp: Shape, rr: int, cc: int) -> None:
        """Places a shape onto the grid at the target root position."""
        for dr, dc, _ in shp.triangles:
            nr, nc = rr + dr, cc + dc
            if self.valid(nr, nc):  # Safety check
                tri = self.triangles[nr][nc]
                # Place only if valid (not death, not occupied)
                if not tri.is_death and not tri.is_occupied:
                    tri.is_occupied = True
                    tri.color = shp.color
                # else: Log warning if trying to place on invalid cell? Usually caught by can_place.
            # else: Log warning if trying to place out of bounds? Should be caught by can_place.

    def clear_filled_rows(self) -> Tuple[int, int]:
        """
        Clears fully occupied rows (ignoring death cells).
        Returns: (number of lines cleared, number of triangles in cleared lines).
        """
        lines_cleared = 0
        triangles_cleared = 0
        rows_to_clear_indices = []

        for r in range(self.rows):
            rowt = self.triangles[r]
            is_row_full = True
            num_placeable_triangles_in_row = 0
            for t in rowt:
                if not t.is_death:
                    num_placeable_triangles_in_row += 1
                    if not t.is_occupied:
                        is_row_full = False
                        # break # Optimization: stop checking row if one empty cell found

            # Only clear if the row has placeable triangles and all are occupied
            if is_row_full and num_placeable_triangles_in_row > 0:
                rows_to_clear_indices.append(r)
                lines_cleared += 1

        # Clear the identified rows and count cleared triangles
        for r_idx in rows_to_clear_indices:
            for t in self.triangles[r_idx]:
                if not t.is_death and t.is_occupied:
                    triangles_cleared += 1
                    t.is_occupied = False  # Reset state
                    t.color = None

        # Note: Gravity is not implemented here (complex for this grid)

        return lines_cleared, triangles_cleared

    def count_holes(self) -> int:
        """Counts empty, non-death cells with an occupied cell somewhere above them."""
        holes = 0
        for c in range(self.cols):  # Iterate column by column
            occupied_above = False
            for r in range(self.rows):  # Iterate row by row within column
                tri = self.triangles[r][c]
                if tri.is_death:
                    continue  # Ignore death cells

                if tri.is_occupied:
                    occupied_above = True  # Mark that we've seen an occupied cell above
                elif not tri.is_occupied and occupied_above:
                    # This is an empty cell below an occupied one in the same column
                    holes += 1
        return holes

    def get_feature_matrix(self) -> np.ndarray:
        """
        Returns grid state as a [Channel, Height, Width] numpy array (float32).
        Channels: 0=Occupied, 1=Is_Up, 2=Is_Death
        """
        grid_state = np.zeros((3, self.rows, self.cols), dtype=np.float32)
        for r in range(self.rows):
            for c in range(self.cols):
                t = self.triangles[r][c]
                grid_state[0, r, c] = 1.0 if t.is_occupied else 0.0
                grid_state[1, r, c] = 1.0 if t.is_up else 0.0
                grid_state[2, r, c] = 1.0 if t.is_death else 0.0
        return grid_state


File: environment/__init__.py


File: environment/triangle.py
# File: environment/triangle.py
# (No changes needed)
from typing import Tuple, Optional, List


class Triangle:
    """Represents a single triangular cell on the grid."""

    def __init__(self, row: int, col: int, is_up: bool, is_death: bool = False):
        self.row = row
        self.col = col
        self.is_up = is_up  # True if pointing up, False if pointing down
        self.is_death = is_death  # True if part of the unplayable border
        self.is_occupied = is_death  # Occupied if it's a death cell initially
        self.color: Optional[Tuple[int, int, int]] = (
            None  # Color if occupied by a shape
        )

    def get_points(
        self, ox: int, oy: int, cw: int, ch: int
    ) -> List[Tuple[float, float]]:
        """Calculates the vertex points for drawing the triangle."""
        x = ox + self.col * (
            cw * 0.75
        )  # Horizontal position based on column and overlap
        y = oy + self.row * ch  # Vertical position based on row
        if self.is_up:
            # Points for an upward-pointing triangle
            return [(x, y + ch), (x + cw, y + ch), (x + cw / 2, y)]
        else:
            # Points for a downward-pointing triangle
            return [(x, y), (x + cw, y), (x + cw / 2, y + ch)]


File: environment/shape.py
# File: environment/shape.py
# (No changes needed)
import random
from typing import List, Tuple
from config import EnvConfig, VisConfig  # Needs VisConfig only for colors

GOOGLE_COLORS = VisConfig.GOOGLE_COLORS  # Use colors from VisConfig


class Shape:
    """Represents a polyomino-like shape made of triangles."""

    def __init__(self) -> None:
        # List of (relative_row, relative_col, is_up) tuples defining the shape
        self.triangles: List[Tuple[int, int, bool]] = []
        self.color: Tuple[int, int, int] = random.choice(GOOGLE_COLORS)
        self._generate()  # Generate the shape structure

    def _generate(self) -> None:
        """Generates a random shape by adding adjacent triangles."""
        n = random.randint(1, 5)  # Number of triangles in the shape
        first_up = random.choice([True, False])  # Orientation of the root triangle
        self.triangles.append((0, 0, first_up))  # Add the root triangle at (0,0)

        # Add remaining triangles adjacent to existing ones
        for _ in range(n - 1):
            # Find valid neighbors of the *last added* triangle
            lr, lc, lu = self.triangles[-1]
            nbrs = self._find_valid_neighbors(lr, lc, lu)
            if nbrs:
                self.triangles.append(random.choice(nbrs))
            # else: Could break early if no valid neighbors found, shape < n

    def _find_valid_neighbors(
        self, r: int, c: int, up: bool
    ) -> List[Tuple[int, int, bool]]:
        """Finds potential neighbor triangles that are not already part of the shape."""
        if up:  # Neighbors of an UP triangle are DOWN triangles
            ns = [(r, c - 1, False), (r, c + 1, False), (r + 1, c, False)]
        else:  # Neighbors of a DOWN triangle are UP triangles
            ns = [(r, c - 1, True), (r, c + 1, True), (r - 1, c, True)]
        # Return only neighbors that are not already in self.triangles
        return [n for n in ns if n not in self.triangles]

    def bbox(self) -> Tuple[int, int, int, int]:
        """Calculates the bounding box (min_r, min_c, max_r, max_c) of the shape."""
        if not self.triangles:
            return (0, 0, 0, 0)
        rr = [t[0] for t in self.triangles]
        cc = [t[1] for t in self.triangles]
        return (min(rr), min(cc), max(rr), max(cc))


File: stats/__init__.py
# File: stats/__init__.py
from .stats_recorder import StatsRecorderBase
from .simple_stats_recorder import SimpleStatsRecorder
from .tensorboard_logger import TensorBoardStatsRecorder

__all__ = ["StatsRecorderBase", "SimpleStatsRecorder", "TensorBoardStatsRecorder"]


File: stats/simple_stats_recorder.py
# File: stats/simple_stats_recorder.py
import time
from collections import deque
from typing import Deque, Dict, Any, Optional, Union, List, Tuple, Callable
import numpy as np
import torch
from .stats_recorder import StatsRecorderBase
from config import EnvConfig, StatsConfig
import warnings


class SimpleStatsRecorder(StatsRecorderBase):
    """
    Records stats in memory using deques for rolling averages.
    Provides no-op implementations for histogram, image, hparam, graph logging.
    Primarily used internally by TensorBoardStatsRecorder for UI/console display,
    or can be used standalone for simple console logging.
    """

    def __init__(
        self,
        console_log_interval: int = 50_000,
        avg_window: int = StatsConfig.STATS_AVG_WINDOW,
    ):
        if avg_window <= 0:
            avg_window = 100
        self.console_log_interval = (
            max(1, console_log_interval) if console_log_interval > 0 else -1
        )
        self.avg_window = avg_window

        # Data Deques
        step_reward_window = max(avg_window * 10, 1000)
        self.step_rewards: Deque[float] = deque(maxlen=step_reward_window)
        self.losses: Deque[float] = deque(maxlen=avg_window)
        self.grad_norms: Deque[float] = deque(maxlen=avg_window)
        self.avg_max_qs: Deque[float] = deque(maxlen=avg_window)
        self.episode_scores: Deque[float] = deque(maxlen=avg_window)
        self.episode_lengths: Deque[int] = deque(maxlen=avg_window)
        self.game_scores: Deque[int] = deque(maxlen=avg_window)
        self.episode_lines_cleared: Deque[int] = deque(maxlen=avg_window)
        self.sps_values: Deque[float] = deque(maxlen=avg_window)
        self.buffer_sizes: Deque[int] = deque(maxlen=avg_window)
        self.beta_values: Deque[float] = deque(maxlen=avg_window)
        self.best_rl_score_history: Deque[float] = deque(maxlen=avg_window)
        self.best_game_score_history: Deque[int] = deque(maxlen=avg_window)
        self.lr_values: Deque[float] = deque(maxlen=avg_window)
        self.epsilon_values: Deque[float] = deque(maxlen=avg_window)

        # Current State / Best Values
        self.total_episodes = 0
        self.total_lines_cleared = 0
        self.current_epsilon: float = 0.0
        self.current_beta: float = 0.0
        self.current_buffer_size: int = 0
        self.current_global_step: int = 0
        self.current_sps: float = 0.0
        self.current_lr: float = 0.0

        # --- MODIFIED: Detailed Best Tracking ---
        self.best_score: float = -float("inf")
        self.previous_best_score: float = -float("inf")
        self.best_score_step: int = 0

        self.best_game_score: int = -float("inf")  # Use float for consistent init
        self.previous_best_game_score: int = -float("inf")
        self.best_game_score_step: int = 0

        self.best_loss: float = float("inf")  # Lower is better for loss
        self.previous_best_loss: float = float("inf")
        self.best_loss_step: int = 0
        # --- END MODIFIED ---

        # Timing / Logging Control
        self.last_log_time: float = time.time()
        self.last_log_step: int = 0
        self.start_time: float = time.time()
        print(
            f"[SimpleStatsRecorder] Initialized. Avg Window: {self.avg_window}, Console Log Interval: {self.console_log_interval if self.console_log_interval > 0 else 'Disabled'}"
        )

    def record_episode(
        self,
        episode_score: float,
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,
    ):
        # Use provided global_step or the internally tracked one
        current_step = (
            global_step if global_step is not None else self.current_global_step
        )

        self.episode_scores.append(episode_score)
        self.episode_lengths.append(episode_length)
        if game_score is not None:
            self.game_scores.append(game_score)
        if lines_cleared is not None:
            self.episode_lines_cleared.append(lines_cleared)
            self.total_lines_cleared += lines_cleared
        self.total_episodes = episode_num
        step_info = f"at Step ~{current_step/1e6:.1f}M"

        # --- MODIFIED: Update detailed best score tracking ---
        if episode_score > self.best_score:
            self.previous_best_score = self.best_score  # Store old best
            self.best_score = episode_score
            self.best_score_step = current_step  # Record step
            prev_str = (
                f"{self.previous_best_score:.2f}"
                if self.previous_best_score > -float("inf")
                else "N/A"
            )
            print(
                f"\n--- üèÜ New Best RL: {self.best_score:.2f} {step_info} (Prev: {prev_str}) ---"
            )

        if game_score is not None and game_score > self.best_game_score:
            self.previous_best_game_score = self.best_game_score  # Store old best
            self.best_game_score = game_score
            self.best_game_score_step = current_step  # Record step
            prev_str = (
                f"{self.previous_best_game_score:.0f}"
                if self.previous_best_game_score > -float("inf")
                else "N/A"
            )
            print(
                f"--- üéÆ New Best Game: {self.best_game_score} {step_info} (Prev: {prev_str}) ---"
            )
        # --- END MODIFIED ---

        # Update history deques for plotting best scores over time
        current_best_rl = self.best_score if self.best_score > -float("inf") else 0.0
        current_best_game = (
            self.best_game_score if self.best_game_score > -float("inf") else 0
        )
        self.best_rl_score_history.append(current_best_rl)
        self.best_game_score_history.append(current_best_game)

    def record_step(self, step_data: Dict[str, Any]):
        g_step = step_data.get("global_step", self.current_global_step)
        if g_step > self.current_global_step:
            self.current_global_step = g_step

        # Append data to deques if present in step_data
        if "loss" in step_data and step_data["loss"] is not None and g_step > 0:
            current_loss = step_data["loss"]
            self.losses.append(current_loss)
            # --- MODIFIED: Track best loss ---
            if current_loss < self.best_loss:
                self.previous_best_loss = self.best_loss
                self.best_loss = current_loss
                self.best_loss_step = g_step
                prev_str = (
                    f"{self.previous_best_loss:.4f}"
                    if self.previous_best_loss < float("inf")
                    else "N/A"
                )
                # Optional: Print new best loss to console
                # print(f"--- ‚ú® New Best Loss: {self.best_loss:.4f} at Step ~{g_step/1e6:.1f}M (Prev: {prev_str}) ---")
            # --- END MODIFIED ---

        if (
            "grad_norm" in step_data
            and step_data["grad_norm"] is not None
            and g_step > 0
        ):
            self.grad_norms.append(step_data["grad_norm"])
        if "step_reward" in step_data and step_data["step_reward"] is not None:
            self.step_rewards.append(step_data["step_reward"])
        if (
            "avg_max_q" in step_data
            and step_data["avg_max_q"] is not None
            and g_step > 0
        ):
            self.avg_max_qs.append(step_data["avg_max_q"])
        if "beta" in step_data and step_data["beta"] is not None:
            self.current_beta = step_data["beta"]
            self.beta_values.append(self.current_beta)
        if "buffer_size" in step_data and step_data["buffer_size"] is not None:
            self.current_buffer_size = step_data["buffer_size"]
            self.buffer_sizes.append(self.current_buffer_size)
        if "lr" in step_data and step_data["lr"] is not None:
            self.current_lr = step_data["lr"]
            self.lr_values.append(self.current_lr)
        if "epsilon" in step_data and step_data["epsilon"] is not None:
            self.current_epsilon = step_data["epsilon"]
            self.epsilon_values.append(self.current_epsilon)

        # Calculate SPS
        if "step_time" in step_data and step_data["step_time"] > 1e-6:
            num_steps_in_call = step_data.get("num_steps_processed", 1)
            sps = num_steps_in_call / step_data["step_time"]
            self.sps_values.append(sps)
            self.current_sps = sps

        # Trigger console log periodically
        self.log_summary(g_step)

    def get_summary(self, current_global_step: Optional[int] = None) -> Dict[str, Any]:
        if current_global_step is None:
            current_global_step = self.current_global_step

        # Calculate averages safely
        avg_sps = np.mean(self.sps_values) if self.sps_values else self.current_sps
        avg_score = np.mean(self.episode_scores) if self.episode_scores else 0.0
        avg_length = np.mean(self.episode_lengths) if self.episode_lengths else 0.0
        avg_loss = np.mean(self.losses) if self.losses else 0.0
        avg_max_q = np.mean(self.avg_max_qs) if self.avg_max_qs else 0.0
        avg_game_score = np.mean(self.game_scores) if self.game_scores else 0.0
        avg_lines_cleared = (
            np.mean(self.episode_lines_cleared) if self.episode_lines_cleared else 0.0
        )
        avg_lr = np.mean(self.lr_values) if self.lr_values else self.current_lr

        summary = {
            # Averages
            "avg_score_window": avg_score,
            "avg_length_window": avg_length,
            "avg_loss_window": avg_loss,
            "avg_max_q_window": avg_max_q,
            "avg_game_score_window": avg_game_score,
            "avg_lines_cleared_window": avg_lines_cleared,
            "avg_sps_window": avg_sps,
            "avg_lr_window": avg_lr,
            # Current / Total
            "total_episodes": self.total_episodes,
            "beta": self.current_beta,
            "buffer_size": self.current_buffer_size,
            "steps_per_second": self.current_sps,
            "global_step": current_global_step,
            "current_lr": self.current_lr,
            # --- MODIFIED: Add detailed best tracking ---
            "best_score": self.best_score,
            "previous_best_score": self.previous_best_score,
            "best_score_step": self.best_score_step,
            "best_game_score": self.best_game_score,
            "previous_best_game_score": self.previous_best_game_score,
            "best_game_score_step": self.best_game_score_step,
            "best_loss": self.best_loss,
            "previous_best_loss": self.previous_best_loss,
            "best_loss_step": self.best_loss_step,
            # --- END MODIFIED ---
            # Counts (for debugging/UI)
            "num_ep_scores": len(self.episode_scores),
            "num_losses": len(self.losses),
        }
        return summary

    def log_summary(self, global_step: int):
        """Logs summary statistics to the console periodically."""
        if (
            self.console_log_interval <= 0
            or global_step < self.last_log_step + self.console_log_interval
        ):
            return

        summary = self.get_summary(global_step)
        elapsed_runtime = time.time() - self.start_time
        runtime_hrs = elapsed_runtime / 3600

        best_score_val = (
            summary["best_score"] if summary["best_score"] > -float("inf") else "N/A"
        )
        if isinstance(best_score_val, float):
            best_score_val = f"{best_score_val:.2f}"

        best_loss_val = (
            summary["best_loss"] if summary["best_loss"] < float("inf") else "N/A"
        )
        if isinstance(best_loss_val, float):
            best_loss_val = f"{best_loss_val:.4f}"

        log_str = (
            f"[{runtime_hrs:.1f}h|Stats] Step: {global_step/1e6:<6.2f}M | "
            f"Ep: {summary['total_episodes']:<7} | SPS: {summary['steps_per_second']:<5.0f} | "
            f"RLScore(Avg): {summary['avg_score_window']:<6.2f} (Best: {best_score_val}) | "
            f"Loss(Avg): {summary['avg_loss_window']:.4f} (Best: {best_loss_val}) | "
            f"LR: {summary['current_lr']:.1e} | "
            f"Buf: {summary['buffer_size']/1e6:.2f}M"
        )
        print(log_str)

        self.last_log_time = time.time()
        self.last_log_step = global_step

    def get_plot_data(self) -> Dict[str, Deque]:
        """Returns copies of deques needed for UI plotting."""
        return {
            "episode_scores": self.episode_scores.copy(),
            "episode_lengths": self.episode_lengths.copy(),
            "losses": self.losses.copy(),
            "avg_max_qs": self.avg_max_qs.copy(),
            "game_scores": self.game_scores.copy(),
            "episode_lines_cleared": self.episode_lines_cleared.copy(),
            "sps_values": self.sps_values.copy(),
            "buffer_sizes": self.buffer_sizes.copy(),
            "beta_values": self.beta_values.copy(),
            "best_rl_score_history": self.best_rl_score_history.copy(),
            "best_game_score_history": self.best_game_score_history.copy(),
            "lr_values": self.lr_values.copy(),
            "epsilon_values": self.epsilon_values.copy(),
        }

    # --- No-op methods for compatibility with base class ---
    def record_histogram(
        self,
        tag: str,
        values: Union[np.ndarray, torch.Tensor, List[float]],
        global_step: int,
    ):
        pass

    def record_image(
        self, tag: str, image: Union[np.ndarray, torch.Tensor], global_step: int
    ):
        pass

    def record_hparams(self, hparam_dict: Dict[str, Any], metric_dict: Dict[str, Any]):
        pass

    def record_graph(
        self, model: torch.nn.Module, input_to_model: Optional[torch.Tensor] = None
    ):
        pass

    def close(self):
        """Closes the recorder (no-op for simple recorder)."""
        print("[SimpleStatsRecorder] Closed.")


File: stats/stats_recorder.py
# File: stats/stats_recorder.py
import time
from abc import ABC, abstractmethod
from collections import deque
from typing import Deque, List, Dict, Any, Optional, Union  # Added Union
import numpy as np
import torch  # Added torch for Tensor type hints


class StatsRecorderBase(ABC):
    """Base class for recording training statistics."""

    @abstractmethod
    def record_episode(
        self,
        episode_score: float,  # RL Score
        episode_length: int,
        episode_num: int,
        global_step: Optional[int] = None,
        game_score: Optional[int] = None,
        lines_cleared: Optional[int] = None,
    ):
        """Record stats for a completed episode."""
        pass

    @abstractmethod
    def record_step(self, step_data: Dict[str, Any]):
        """Record stats from a training or environment step (e.g., loss, rewards)."""
        pass

    # --- New Abstract Methods ---
    @abstractmethod
    def record_histogram(
        self,
        tag: str,
        values: Union[np.ndarray, torch.Tensor, List[float]],
        global_step: int,
    ):
        """Record a histogram of values."""
        pass

    @abstractmethod
    def record_image(
        self, tag: str, image: Union[np.ndarray, torch.Tensor], global_step: int
    ):
        """Record an image."""
        pass

    @abstractmethod
    def record_hparams(self, hparam_dict: Dict[str, Any], metric_dict: Dict[str, Any]):
        """Record hyperparameters and final/key metrics."""
        pass

    @abstractmethod
    def record_graph(
        self, model: torch.nn.Module, input_to_model: Optional[torch.Tensor] = None
    ):
        """Record the model graph."""
        pass

    # --- End New ---

    @abstractmethod
    def get_summary(self, current_global_step: int) -> Dict[str, Any]:
        """Return a dictionary containing summary statistics (usually averaged)."""
        pass

    @abstractmethod
    def log_summary(self, global_step: int):
        """Trigger the logging action (e.g., print to console, write to TensorBoard)."""
        pass

    @abstractmethod
    def close(self):
        """Perform any necessary cleanup (e.g., close files/writers)."""
        pass


